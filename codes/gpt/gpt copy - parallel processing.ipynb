{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import copy\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import fitz\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_PATH = \"../../files/data-science_book/data-science.pdf\"\n",
    "TEX_PATH = \"../../files/data-science_book/outputs/data-science_pg_sep_bib.tex\"\n",
    "# IMG_DIR = \"/content/drive/My Drive/pdf2latex/new_approach_test/images\"  # found images are stored in this subfolder\n",
    "OUTPUT_TEX_FILE = \"../../files/data-science_book/outputs/data-science_cleaned_2p__toc.tex\"\n",
    "\n",
    "# TOC_PATH  = \"../../files/data-science_book/data-science_toc.pdf\"\n",
    "# toc = pymupdf.open(TOC_PATH)\n",
    "# all_toc_text = \"\"\n",
    "# for page_num in range(len(toc)):\n",
    "#     page = toc[page_num]\n",
    "#     text = page.get_text()\n",
    "#     all_toc_text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "#     all_toc_text += text.strip() + \"\\n\"\n",
    "\n",
    "\n",
    "toc = pymupdf.open(BOOK_PATH)\n",
    "table_of_contents  = toc.get_toc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Preface', 6],\n",
       " [1, 'Contents', 12],\n",
       " [1, '1\\rWhat is Data Science?', 19],\n",
       " [2, '1.1 Computer Science, Data Science, and Real', 20],\n",
       " [2, 'Science', 20],\n",
       " [2, '1.2 Asking Interesting Questions from Data', 22],\n",
       " [3, '1.2.1 The Baseball Encyclopedia', 23],\n",
       " [3, '1.2.2 The Internet Movie Database (IMDb)', 25],\n",
       " [3, '1.2.3 Google Ngrams', 28],\n",
       " [3, '1.2.4 New York Taxi Records', 29],\n",
       " [2, '1.3 Properties of Data', 32],\n",
       " [3, '1.3.1 Structured vs. Unstructured Data', 32],\n",
       " [3, '1.3.2 Quantitative vs. Categorical Data', 33],\n",
       " [3, '1.3.3 Big Data vs. Little Data', 33],\n",
       " [2, '1.4 Classi\\x0ccation and Regression', 34],\n",
       " [2, '1.5 Data Science Television: The Quant Shop', 35],\n",
       " [3, '1.5.1 Kaggle Challenges', 37],\n",
       " [2, '1.6 About the War Stories', 37],\n",
       " [2, '1.7 War Story: Answering the Right Question', 39],\n",
       " [2, '1.8 Chapter Notes', 40],\n",
       " [2, '1.9 Exercises', 41],\n",
       " [1, '2\\rMathematical Preliminaries', 44],\n",
       " [2, '2.1 Probability', 44],\n",
       " [3, '2.1.1 Probability vs. Statistics', 46],\n",
       " [3, '2.1.2 Compound Events and Independence', 47],\n",
       " [3, '2.1.3 Conditional Probability', 48],\n",
       " [3, '2.1.4 Probability Distributions', 49],\n",
       " [2, '2.2 Descriptive Statistics', 51],\n",
       " [3, '2.2.1 Centrality Measures', 51],\n",
       " [3, '2.2.2 Variability Measures', 53],\n",
       " [3, '2.2.3 Interpreting Variance', 54],\n",
       " [3, '2.2.4 Characterizing Distributions', 56],\n",
       " [2, '2.3 Correlation Analysis', 57],\n",
       " [3, '2.3.1 Correlation Coe\\x0ecients: Pearson and Spearman Rank', 58],\n",
       " [3, '2.3.2 The Power and Signi\\x0ccance of Correlation', 60],\n",
       " [3, '2.3.3 Correlation Does Not Imply Causation!', 62],\n",
       " [3, '2.3.4 Detecting Periodicities by Autocorrelation', 63],\n",
       " [2, '2.4 Logarithms', 64],\n",
       " [3, '2.4.1 Logarithms and Multiplying Probabilities', 65],\n",
       " [3, '2.4.2 Logarithms and Ratios', 65],\n",
       " [3, '2.4.3 Logarithms and Normalizing Skewed Distributions', 66],\n",
       " [2, '2.5 War Story: Fitting Designer Genes', 67],\n",
       " [2, '2.6 Chapter Notes', 69],\n",
       " [2, '2.7 Exercises', 70],\n",
       " [1, '3\\rData Munging', 74],\n",
       " [2, '3.1 Languages for Data Science', 74],\n",
       " [3, '3.1.1 The Importance of Notebook Environments', 76],\n",
       " [3, '3.1.2 Standard Data Formats', 78],\n",
       " [2, '3.2 Collecting Data', 81],\n",
       " [3, '3.2.1 Hunting', 81],\n",
       " [3, '3.2.2 Scraping', 84],\n",
       " [3, '3.2.3 Logging', 85],\n",
       " [2, '3.3 Cleaning Data', 86],\n",
       " [3, '3.3.1 Errors vs. Artifacts', 86],\n",
       " [3, '3.3.2 Data Compatibility', 89],\n",
       " [3, '3.3.3 Dealing with Missing Values', 93],\n",
       " [3, '3.3.4 Outlier Detection', 95],\n",
       " [2, '3.4 War Story: Beating the Market', 96],\n",
       " [2, '3.5 Crowdsourcing', 97],\n",
       " [3, '3.5.1 The Penny Demo', 98],\n",
       " [3, '3.5.2 When is the Crowd Wise?', 99],\n",
       " [3, '3.5.3 Mechanisms for Aggregation', 100],\n",
       " [3, '3.5.4 Crowdsourcing Services', 101],\n",
       " [3, '3.5.5 Gami\\x0ccation', 105],\n",
       " [2, '3.6 Chapter Notes', 107],\n",
       " [2, '3.7 Exercises', 107],\n",
       " [1, '4\\rScores and Rankings', 111],\n",
       " [2, '4.1 The Body Mass Index (BMI)', 112],\n",
       " [2, '4.2 Developing Scoring Systems', 115],\n",
       " [3, '4.2.1 Gold Standards and Proxies', 115],\n",
       " [3, '4.2.2 Scores vs. Rankings', 116],\n",
       " [3, '4.2.3 Recognizing Good Scoring Functions', 117],\n",
       " [2, '4.3 Z-scores and Normalization', 119],\n",
       " [2, '4.4 Advanced Ranking Techniques', 120],\n",
       " [3, '4.4.1 Elo Rankings', 120],\n",
       " [3, '4.4.2 Merging Rankings', 124],\n",
       " [3, '4.4.3 Digraph-based Rankings', 125],\n",
       " [3, '4.4.4 PageRank', 127],\n",
       " [2, \"4.5 War Story: Clyde's Revenge\", 127],\n",
       " [2, \"4.6 Arrow's Impossibility Theorem\", 130],\n",
       " [2, \"4.7 War Story: Who's Bigger?\", 131],\n",
       " [2, '4.8 Chapter Notes', 134],\n",
       " [2, '4.9 Exercises', 135],\n",
       " [1, '5\\rStatistical Analysis', 137],\n",
       " [2, '5.1 Statistical Distributions', 138],\n",
       " [3, '5.1.1 The Binomial Distribution', 139],\n",
       " [3, '5.1.2 The Normal Distribution', 140],\n",
       " [3, '5.1.3 Implications of the Normal Distribution', 142],\n",
       " [3, '5.1.4 Poisson Distribution', 143],\n",
       " [3, '5.1.5 Power Law Distributions', 145],\n",
       " [2, '5.2 Sampling from Distributions', 148],\n",
       " [3, '5.2.1 Random Sampling beyond One Dimension', 149],\n",
       " [2, '5.3 Statistical Signi\\x0ccance', 151],\n",
       " [3, '5.3.1 The Signi\\x0ccance of Signi\\x0ccance', 151],\n",
       " [3, '5.3.2 The T-test: Comparing Population Means', 153],\n",
       " [3, '5.3.3 The Kolmogorov-Smirnov Test', 155],\n",
       " [3, '5.3.4 The Bonferroni Correction', 157],\n",
       " [3, '5.3.5 False Discovery Rate', 158],\n",
       " [2, '5.4 War Story: Discovering the Fountain of Youth?', 159],\n",
       " [2, '5.5 Permutation Tests and P-values', 161],\n",
       " [3, '5.5.1 Generating Random Permutations', 163],\n",
       " [3, \"5.5.2 DiMaggio's Hitting Streak\", 164],\n",
       " [2, '5.6 Bayesian Reasoning', 166],\n",
       " [2, '5.7 Chapter Notes', 167],\n",
       " [2, '5.8 Exercises', 167],\n",
       " [1, '6\\rVisualizing Data', 171],\n",
       " [2, '6.1 Exploratory Data Analysis', 172],\n",
       " [3, '6.1.1 Confronting a New Data Set', 172],\n",
       " [3, \"6.1.2 Summary Statistics and Anscombe's Quartet\", 175],\n",
       " [3, '6.1.3 Visualization Tools', 176],\n",
       " [2, '6.2 Developing a Visualization Aesthetic', 178],\n",
       " [3, '6.2.1 Maximizing Data-Ink Ratio', 179],\n",
       " [3, '6.2.2 Minimizing the Lie Factor', 180],\n",
       " [3, '6.2.3 Minimizing Chartjunk', 181],\n",
       " [3, '6.2.4 Proper Scaling and Labeling', 183],\n",
       " [3, '6.2.5 E\\x0bective Use of Color and Shading', 184],\n",
       " [3, '6.2.6 The Power of Repetition', 185],\n",
       " [2, '6.3 Chart Types', 186],\n",
       " [3, '6.3.1 Tabular Data', 186],\n",
       " [3, '6.3.2 Dot and Line Plots', 190],\n",
       " [3, '6.3.3 Scatter Plots', 193],\n",
       " [3, '6.3.4 Bar Plots and Pie Charts', 195],\n",
       " [3, '6.3.5 Histograms', 199],\n",
       " [3, '6.3.6 Data Maps', 203],\n",
       " [2, '6.4 Great Visualizations', 205],\n",
       " [3, \"6.4.1 Marey's Train Schedule\", 205],\n",
       " [3, \"6.4.2 Snow's Cholera Map\", 207],\n",
       " [3, \"6.4.3 New York's Weather Year\", 208],\n",
       " [2, '6.5 Reading Graphs', 208],\n",
       " [3, '6.5.1 The Obscured Distribution', 209],\n",
       " [3, '6.5.2 Overinterpreting Variance', 209],\n",
       " [2, '6.6 Interactive Visualization', 211],\n",
       " [2, '6.7 War Story: TextMapping the World', 212],\n",
       " [2, '6.8 Chapter Notes', 214],\n",
       " [2, '6.9 Exercises', 215],\n",
       " [1, '7\\rMathematical Models', 217],\n",
       " [2, '7.1 Philosophies of Modeling', 217],\n",
       " [3, \"7.1.1 Occam's Razor\", 217],\n",
       " [3, '7.1.2 Bias{Variance Trade-O\\x0bs', 218],\n",
       " [3, '7.1.3 What Would Nate Silver Do?', 219],\n",
       " [2, '7.2 A Taxonomy of Models', 221],\n",
       " [3, '7.2.1 Linear vs. Non-Linear Models', 222],\n",
       " [3, '7.2.2 Blackbox vs. Descriptive Models', 222],\n",
       " [3, '7.2.3 First-Principle vs. Data-Driven Models', 223],\n",
       " [3, '7.2.4 Stochastic vs. Deterministic Models', 224],\n",
       " [3, '7.2.5 Flat vs. Hierarchical Models', 225],\n",
       " [2, '7.3 Baseline Models', 226],\n",
       " [3, '7.3.1 Baseline Models for Classi\\x0ccation', 226],\n",
       " [3, '7.3.2 Baseline Models for Value Prediction', 228],\n",
       " [2, '7.4 Evaluating Models', 228],\n",
       " [3, '7.4.1 Evaluating Classi\\x0cers', 229],\n",
       " [3, '7.4.2 Receiver-Operator Characteristic (ROC) Curves', 234],\n",
       " [3, '7.4.3 Evaluating Multiclass Systems', 235],\n",
       " [3, '7.4.4 Evaluating Value Prediction Models', 237],\n",
       " [2, '7.5 Evaluation Environments', 240],\n",
       " [3, '7.5.1 Data Hygiene for Evaluation', 241],\n",
       " [3, '7.5.2 Amplifying Small Evaluation Sets', 242],\n",
       " [2, '7.6 War Story: 100% Accuracy', 244],\n",
       " [2, '7.7 Simulation Models', 245],\n",
       " [2, '7.8 War Story: Calculated Bets', 246],\n",
       " [2, '7.9 Chapter Notes', 249],\n",
       " [2, '7.10 Exercises', 250],\n",
       " [1, '8\\rLinear Algebra', 253],\n",
       " [2, '8.1 The Power of Linear Algebra', 253],\n",
       " [3, '8.1.1 Interpreting Linear Algebraic Formulae', 254],\n",
       " [3, '8.1.2 Geometry and Vectors', 256],\n",
       " [2, '8.2 Visualizing Matrix Operations', 257],\n",
       " [3, '8.2.1 Matrix Addition', 258],\n",
       " [3, '8.2.2 Matrix Multiplication', 259],\n",
       " [3, '8.2.3 Applications of Matrix Multiplication', 260],\n",
       " [3, '8.2.4 Identity Matrices and Inversion', 264],\n",
       " [3, '8.2.5 Matrix Inversion and Linear Systems', 266],\n",
       " [3, '8.2.6 Matrix Rank', 267],\n",
       " [2, '8.3 Factoring Matrices', 268],\n",
       " [3, '8.3.1 Why Factor Feature Matrices?', 268],\n",
       " [3, '8.3.2 LU Decomposition and Determinants', 270],\n",
       " [2, '8.4 Eigenvalues and Eigenvectors', 271],\n",
       " [3, '8.4.1 Properties of Eigenvalues', 271],\n",
       " [3, '8.4.2 Computing Eigenvalues', 272],\n",
       " [2, '8.5 Eigenvalue Decomposition', 273],\n",
       " [3, '8.5.1 Singular Value Decomposition', 274],\n",
       " [3, '8.5.2 Principal Components Analysis', 276],\n",
       " [2, '8.6 War Story: The Human Factors', 278],\n",
       " [2, '8.7 Chapter Notes', 279],\n",
       " [2, '8.8 Exercises', 279],\n",
       " [1, '9 Linear and Logistic Regression', 282],\n",
       " [2, '9.1 Linear Regression', 283],\n",
       " [3, '9.1.1 Linear Regression and Duality', 283],\n",
       " [3, '9.1.2 Error in Linear Regression', 284],\n",
       " [3, '9.1.3 Finding the Optimal Fit', 285],\n",
       " [2, '9.2 Better Regression Models', 287],\n",
       " [3, '9.2.1 Removing Outliers', 287],\n",
       " [3, '9.2.2 Fitting Non-Linear Functions', 288],\n",
       " [3, '9.2.3 Feature and Target Scaling', 289],\n",
       " [3, '9.2.4 Dealing with Highly-Correlated Features', 292],\n",
       " [2, '9.3 War Story: Taxi Deriver', 292],\n",
       " [2, '9.4 Regression as Parameter Fitting', 294],\n",
       " [3, '9.4.1 Convex Parameter Spaces', 295],\n",
       " [3, '9.4.2 Gradient Descent Search', 296],\n",
       " [3, '9.4.3 What is the Right Learning Rate?', 298],\n",
       " [3, '9.4.4 Stochastic Gradient Descent', 300],\n",
       " [2, '9.5 Simplifying Models through Regularization', 301],\n",
       " [3, '9.5.1 Ridge Regression', 301],\n",
       " [3, '9.5.2 LASSO Regression', 302],\n",
       " [3, '9.5.3 Trade-O\\x0bs between Fit and Complexity', 303],\n",
       " [2, '9.6 Classi\\x0ccation and Logistic Regression', 304],\n",
       " [3, '9.6.1 Regression for Classi\\x0ccation', 305],\n",
       " [3, '9.6.2 Decision Boundaries', 306],\n",
       " [3, '9.6.3 Logistic Regression', 307],\n",
       " [2, '9.7 Issues in Logistic Classi\\x0ccation', 310],\n",
       " [3, '9.7.1 Balanced Training Classes', 310],\n",
       " [3, '9.7.2 Multi-Class Classi\\x0ccation', 312],\n",
       " [3, '9.7.3 Hierarchical Classi\\x0ccation', 313],\n",
       " [3, '9.7.4 Partition Functions and Multinomial Regression', 314],\n",
       " [2, '9.8 Chapter Notes', 315],\n",
       " [2, '9.9 Exercises', 316],\n",
       " [1, '10\\rDistance and Network Methods', 318],\n",
       " [2, '10.1 Measuring Distances', 318],\n",
       " [3, '10.1.1 Distance Metrics', 319],\n",
       " [3, '10.1.2 The', 320],\n",
       " [3, 'Distance Metric', 320],\n",
       " [3, '10.1.3 Working in Higher Dimensions', 322],\n",
       " [3, '10.1.4 Dimensional Egalitarianism', 323],\n",
       " [3, '10.1.5 Points vs. Vectors', 324],\n",
       " [3, '10.1.6 Distances between Probability Distributions', 325],\n",
       " [2, '10.2 Nearest Neighbor Classi\\x0ccation', 326],\n",
       " [3, '10.2.1 Seeking Good Analogies', 327],\n",
       " [3, '10.2.2', 328],\n",
       " [3, '-Nearest Neighbors', 328],\n",
       " [3, '10.2.3 Finding Nearest Neighbors', 330],\n",
       " [3, '10.2.4 Locality Sensitive Hashing', 332],\n",
       " [2, '10.3 Graphs, Networks, and Distances', 334],\n",
       " [3, '10.3.1 Weighted Graphs and Induced Networks', 335],\n",
       " [3, '10.3.2 Talking About Graphs', 336],\n",
       " [3, '10.3.3 Graph Theory', 338],\n",
       " [2, '10.4 PageRank', 340],\n",
       " [2, '10.5 Clustering', 342],\n",
       " [3, '10.5.1', 345],\n",
       " [3, '-means Clustering', 345],\n",
       " [3, '10.5.2 Agglomerative Clustering', 351],\n",
       " [3, '10.5.3 Comparing Clusterings', 356],\n",
       " [3, '10.5.4 Similarity Graphs and Cut-Based Clustering', 356],\n",
       " [2, '10.6 War Story: Cluster Bombing', 359],\n",
       " [2, '10.7 Chapter Notes', 360],\n",
       " [2, '10.8 Exercises', 361],\n",
       " [1, '11\\rMachine Learning', 365],\n",
       " [2, '11.1 Naive Bayes', 368],\n",
       " [3, '11.1.1 Formulation', 368],\n",
       " [3, '11.1.2 Dealing with Zero Counts (Discounting)', 370],\n",
       " [2, '11.2 Decision Tree Classi\\x0cers', 371],\n",
       " [3, '11.2.1 Constructing Decision Trees', 373],\n",
       " [3, '11.2.2 Realizing Exclusive Or', 375],\n",
       " [3, '11.2.3 Ensembles of Decision Trees', 376],\n",
       " [2, '11.3 Boosting and Ensemble Learning', 377],\n",
       " [3, '11.3.1 Voting with Classi\\x0cers', 377],\n",
       " [3, '11.3.2 Boosting Algorithms', 378],\n",
       " [2, '11.4 Support Vector Machines', 380],\n",
       " [3, '11.4.1 Linear SVMs', 383],\n",
       " [3, '11.4.2 Non-linear SVMs', 383],\n",
       " [3, '11.4.3 Kernels', 385],\n",
       " [2, '11.5 Degrees of Supervision', 386],\n",
       " [3, '11.5.1 Supervised Learning', 386],\n",
       " [3, '11.5.2 Unsupervised Learning', 386],\n",
       " [3, '11.5.3 Semi-supervised Learning', 388],\n",
       " [3, '11.5.4 Feature Engineering', 389],\n",
       " [2, '11.6 Deep Learning', 391],\n",
       " [3, '11.6.1 Networks and Depth', 392],\n",
       " [3, '11.6.2 Backpropagation', 396],\n",
       " [3, '11.6.3 Word and Graph Embeddings', 397],\n",
       " [2, '11.7 War Story: The Name Game', 399],\n",
       " [2, '11.8 Chapter Notes', 401],\n",
       " [2, '11.9 Exercises', 402],\n",
       " [1, '12\\rBig Data: Achieving Scale', 405],\n",
       " [2, '12.1 What is Big Data?', 406],\n",
       " [3, '12.1.1 Big Data as Bad Data', 406],\n",
       " [3, '12.1.2 The Three Vs', 408],\n",
       " [2, '12.2 War Story: Infrastructure Matters', 409],\n",
       " [2, '12.3 Algorithmics for Big Data', 411],\n",
       " [3, '12.3.1 Big Oh Analysis', 411],\n",
       " [3, '12.3.2 Hashing', 413],\n",
       " [3, '12.3.3 Exploiting the Storage Hierarchy', 415],\n",
       " [3, '12.3.4 Streaming and Single-Pass Algorithms', 416],\n",
       " [2, '12.4 Filtering and Sampling', 417],\n",
       " [3, '12.4.1 Deterministic Sampling Algorithms', 418],\n",
       " [3, '12.4.2 Randomized and Stream Sampling', 420],\n",
       " [2, '12.5 Parallelism', 420],\n",
       " [3, '12.5.1 One, Two, Many', 421],\n",
       " [3, '12.5.2 Data Parallelism', 423],\n",
       " [3, '12.5.3 Grid Search', 423],\n",
       " [3, '12.5.4 Cloud Computing Services', 424],\n",
       " [2, '12.6 MapReduce', 424],\n",
       " [3, '12.6.1 Map-Reduce Programming', 426],\n",
       " [3, '12.6.2 MapReduce under the Hood', 428],\n",
       " [2, '12.7 Societal and Ethical Implications', 430],\n",
       " [2, '12.8 Chapter Notes', 433],\n",
       " [2, '12.9 Exercises', 433],\n",
       " [1, '13\\rCoda', 436],\n",
       " [2, '13.1 Get a Job!', 436],\n",
       " [2, '13.2 Go to Graduate School!', 437],\n",
       " [2, '13.3 Professional Consulting Services', 438],\n",
       " [1, 'Bibliography', 439],\n",
       " [1, 'Index', 446]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = api_key\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pymupdf.open(BOOK_PATH)\n",
    "doc.page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEX_PATH, 'r') as file:\n",
    "    tex_file_contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_text_data(page_number, span_counter, text_data, doc):\n",
    "    page = doc[page_number]\n",
    "    # print(page)\n",
    "\n",
    "    # Read page text as a dictionary, suppressing extra spaces in CJK fonts\n",
    "    blocks = page.get_text(\"dict\", flags=0)[\"blocks\"]\n",
    "    # print(blocks)\n",
    "    line_number_in_page = 0\n",
    "    span_number_in_page = 0\n",
    "    # print(\"--Old--\")\n",
    "    for block_number, b in enumerate(blocks):  # Iterate through the text blocks\n",
    "        span_number_in_block = 0  # Initialize span counter for the block\n",
    "\n",
    "        # print(b[\"lines\"])\n",
    "\n",
    "        for l in b[\"lines\"]:  # Iterate through the text lines\n",
    "            # print(l)\n",
    "            line_number_in_page += 1\n",
    "            span_number_in_line = 0  # Initialize span counter for the line\n",
    "            # print(\"Spans : \"+ str(len(l[\"spans\"])))\n",
    "            for s in l[\"spans\"]:  # Iterate through the text spans\n",
    "                 # Create a deep copy of the original span dictionary to preserve all its properties\n",
    "                span_data = copy.deepcopy(s)\n",
    "\n",
    "                # Temporary removal to check hwo it works\n",
    "                del span_data[\"size\"]\n",
    "                # del span_data[\"flags\"]\n",
    "                del span_data[\"bidi\"]\n",
    "                del span_data[\"char_flags\"]\n",
    "                del span_data[\"ascender\"]\n",
    "                del span_data[\"descender\"]\n",
    "                del span_data['origin']\n",
    "                del span_data['bbox']\n",
    "                del span_data['color']\n",
    "                del span_data['font']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Add additional properties if needed\n",
    "                # span_data[\"page_number\"] = page_number\n",
    "                # span_data[\"span_number_overall\"] = span_counter\n",
    "                # span_data[\"span_number_in_line\"] = span_number_in_line\n",
    "                # span_data[\"span_number_in_block\"] = span_number_in_block\n",
    "                # span_data[\"span_number_in_page\"] = span_number_in_page\n",
    "                # span_data[\"block_number\"] = block_number\n",
    "\n",
    "                # Extract and store bounding box information\n",
    "                # x0, y0, x1, y1 = span_data[\"bbox\"]\n",
    "                # span_data[\"indent_left\"] = x0\n",
    "                # span_data[\"indent_top\"] = y0\n",
    "                # span_data[\"x1\"] = x1\n",
    "                # span_data[\"y1\"] = y1\n",
    "\n",
    "                # Decompose flags to determine font styles\n",
    "                decomposed_flags = flags_decomposer(span_data[\"flags\"])\n",
    "                span_data[\"is_italic\"] = \"italic\" in decomposed_flags\n",
    "                span_data[\"is_bold\"] = \"bold\" in decomposed_flags\n",
    "                span_data[\"is_superscript\"] = \"superscript\" in decomposed_flags\n",
    "\n",
    "                del span_data[\"flags\"]\n",
    "\n",
    "                # Append the dictionary to the text_data list\n",
    "                text_data.append(span_data)\n",
    "                # Increase the overall counters\n",
    "                span_counter += 1\n",
    "                span_number_in_line += 1  # Increase the span counter within the line\n",
    "                span_number_in_block += 1  # Increase the span counter within the block\n",
    "                span_number_in_page += 1\n",
    "    # print(\"---Old End---\")\n",
    "    return text_data, span_counter\n",
    "\n",
    "def flags_decomposer(flags):\n",
    "    \"\"\"Make font flags human readable.\"\"\"\n",
    "    l = []\n",
    "    if flags & 2 ** 0:\n",
    "        l.append(\"superscript\")\n",
    "    if flags & 2 ** 1:\n",
    "        l.append(\"italic\")\n",
    "    if flags & 2 ** 2:\n",
    "        l.append(\"serifed\")\n",
    "    else:\n",
    "        l.append(\"sans\")\n",
    "    if flags & 2 ** 3:\n",
    "        l.append(\"monospaced\")\n",
    "    else:\n",
    "        l.append(\"proportional\")\n",
    "    if flags & 2 ** 4:\n",
    "        l.append(\"bold\")\n",
    "    return \", \".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length : 772\n"
     ]
    }
   ],
   "source": [
    "text_data = []\n",
    "span_counter = 0\n",
    "for i in range(16, 16+16):\n",
    "  text_data, span_counter = get_page_text_data(i, span_counter, text_data, doc)\n",
    "  # text_data, span_counter = get_page_text_data(i, span_counter, doc)\n",
    "\n",
    "print(\"Length : \" + str(len(text_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', tex_file_contents)\n",
    "page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', tex_file_contents))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n     ```\\n     \\x08egin{figure}[h]\\n         \\\\centering\\n         \\\\includegraphics{filename}\\n         \\\\caption{Caption text}\\n         \\\\label{fig:label}\\n     \\\\end{figure}\\n     ```  \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_page_command = \"\"\"\n",
    "You will receive an unformatted LaTeX (.tex) file part of a book along with a separate JSON file containing formatting instructions.  \n",
    "Your task is to format the LaTeX file part according to the JSON data while ensuring proper structure and presentation for a book.  \n",
    "\n",
    "### **Formatting Guidelines:**  \n",
    "\n",
    "**1. Apply JSON Formatting Instructions:**  \n",
    "   - Modify only the necessary parts based on JSON data.  \n",
    "   - Do **not** make arbitrary changes—only apply specified formatting corrections.  \n",
    "\n",
    "**2. Book Structure:**  \n",
    "   - The given tex file considers the document as an article, but you need to treat it as a book in the imports section as well as the actual data.\n",
    "   - Add a usepackage command for indexing, makeidx and \\makeindex in the import section\n",
    "   - Generate table of contents dynamically.\n",
    "   - Organize content into proper **chapters, sections, and subsections** \n",
    "   - **Do not assume chapter starts based on recurring text** (e.g., headers repeated on every page).  \n",
    "   - If chapter names and numbers appear on every page in the JSON, **ignore them** when determining chapter breaks.  \n",
    "   - **Remove hardcoded numbering** for chapters and sections, allowing LaTeX to handle it automatically.  \n",
    "   - Make the Contents Page dynamically if contents is present in the .tex file part. Do not hardcode the table of contents.\n",
    "\n",
    "**3. Image Handling:**  \n",
    "   - Convert all instances of `\\includegraphics{}` into a proper `figure` environment:  \n",
    "\n",
    "**4. Table Formatting:**  \n",
    "   - Ensure tables are properly structured with appropriate spacing, alignment, and captions for readability.  \n",
    "\n",
    "**5. Italics Handling:**  \n",
    "   - Apply italics **only** to content explicitly marked as italicized in the JSON data.  \n",
    "\n",
    "**6. Document Setup:**  \n",
    "   - This is the **first part of the book**, so include **all necessary LaTeX imports and the document class**.  \n",
    "   - **Do not modify LaTeX package imports unless explicitly required in the JSON file.** \n",
    "   - Do **not** manually start or end the document unless such commands are explicitly present.  \n",
    "\n",
    "**7. Strict Output Requirements:**  \n",
    "   - The output **must be pure LaTeX code**—**no explanations, comments, or markdown syntax.**  \n",
    "   - The formatted output will be **directly appended** to the `.tex` file, so it must be immediately compilable.  \n",
    "\n",
    "**8. Accuracy and Consistency:**  \n",
    "   - Since the book is processed in parts, formatting should be **consistent across all sections**.  \n",
    "   - **Do not introduce new formatting styles** that conflict with previous or upcoming sections.  \n",
    "   - Ensure that all content is preserved and formatted correctly—no missing text, no misinterpretations.  \n",
    "\n",
    "**Final Note:**  \n",
    "Errors in formatting can **significantly affect the compiled document.** Ensure precise execution of all instructions while preserving the document's original meaning and intent.  \n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "next_pages_prompt = \"\"\"\n",
    "You will receive a portion of a LaTeX (.tex) file part of a book along with a separate JSON file containing formatting instructions.  \n",
    "Your task is to format this LaTeX file part according to the provided JSON data while maintaining consistency with previous sections.  \n",
    "\n",
    "### **Formatting Guidelines:**  \n",
    "\n",
    "**1. Apply JSON Formatting Instructions:**  \n",
    "   - Modify only the necessary parts as specified in the JSON data.  \n",
    "   - Do **not** assume formatting—only apply explicit corrections.  \n",
    "\n",
    "**2. Maintain Book Structure:**\n",
    "   - The given tex file considers the document as an article, but you need to treat it as a book in the actual data.\n",
    "   - Organize content into proper **chapters, sections, and subsections** only if explicitly marked in the `.tex` file.  \n",
    "   - **Do not assume chapter starts based on recurring text** (e.g., headers repeated on every page).  \n",
    "   - If chapter names and numbers appear on every page in the JSON, **ignore them** when determining chapter breaks.  \n",
    "   - **Remove hardcoded numbering** on chapters, sections and subsections and rely on LaTeX’s automatic numbering system strictly.\n",
    "   - Dont use * tags like \\section*{section name} as they remove the latex numbering system.  \n",
    "   - Make the Contents Page dynamically if contents is present in the .tex file part. Do not hardcode the table of contents.\n",
    "**3. Image Handling:**  \n",
    "   - Convert `\\includegraphics{}` into a properly formatted `figure` environment:  \n",
    "\n",
    "\n",
    "**4. Table Formatting:**  \n",
    "   - Ensure tables are properly structured, aligned, and formatted for readability.  \n",
    "\n",
    "**5. Italics Handling:**  \n",
    "   - Apply italics **only** to content explicitly marked as italicized in the JSON data.  \n",
    "\n",
    "**6. Document Integrity:**  \n",
    "   - **Do not add any LaTeX preamble, document class, or import statements.**  \n",
    "   - **Do not modify LaTeX package imports unless explicitly required in the JSON file.** \n",
    "   - **Do not include `\\begin{document}` or `\\end{document}`** unless explicitly present in the provided `.tex` file.  \n",
    "\n",
    "**7. Strict Output Requirements:**  \n",
    "   - The output **must be pure LaTeX code**—no explanations, comments, or markdown syntax.  \n",
    "   - The formatted output will be **directly appended** to an existing `.tex` file, so it must be immediately compilable.  \n",
    "\n",
    "**8. Accuracy and Consistency:**  \n",
    "   - Ensure formatting is **consistent with previous sections** of the book.  \n",
    "   - **Do not introduce new formatting styles** that conflict with earlier parts.  \n",
    "   - Ensure **all content is retained**, formatted correctly, and adheres to the document’s original intent.  \n",
    "\n",
    "**Final Note:**  \n",
    "Errors in formatting can **significantly impact the final compiled document.** Follow the instructions precisely to maintain a high-quality, structured LaTeX book.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "     ```\n",
    "     \\begin{figure}[h]\n",
    "         \\centering\n",
    "         \\includegraphics{filename}\n",
    "         \\caption{Caption text}\n",
    "         \\label{fig:label}\n",
    "     \\end{figure}\n",
    "     ```  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(command, data, prev_response, temperature=1):\n",
    "  first_page_prompt = f\"{command} \\n {data}\"\n",
    "  default_page_prompt = f\"\"\"{command} \\n{data}\"\"\"\n",
    "  prompt_content = first_page_prompt if prev_response == \"\" else default_page_prompt\n",
    "  response =  client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You convert PDF documents to LaTeX.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt_content}\"}\n",
    "    ],\n",
    "  temperature=temperature\n",
    "  )\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_page_data = {}\n",
    "page_numbers = []\n",
    "for i in range(len(doc)):\n",
    "    page = doc[i]\n",
    "    page_numbers.append(page.get_label())\n",
    "    book_page_data[i] = page.get_text(\"text\").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_data(start_indx, end_indx, doc):\n",
    "    text_data = []\n",
    "    span_counter = 0\n",
    "    for i in range(start_indx, end_indx+1):\n",
    "        text_data, span_counter = get_page_text_data(i, span_counter, text_data, doc)\n",
    "    # text_data, span_counter = get_page_text_data(i, span_counter, doc)\n",
    "\n",
    "    # print(\"Length : \" + str(len(text_data)))\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 323/323 [53:02<00:00,  9.85s/it]  \n"
     ]
    }
   ],
   "source": [
    "start_indx = 0\n",
    "\n",
    "tex_start_pos = 0\n",
    "tex_end_pos = 0\n",
    "\n",
    "first_part = 1\n",
    "\n",
    "parts = len(page_breaks)\n",
    "counter = 1\n",
    "for page in tqdm(page_breaks[:parts]):\n",
    "    \n",
    "    end_indx = page_numbers.index(page)\n",
    "    text_data = get_pages_data(start_indx, end_indx, doc)\n",
    "\n",
    "    tex_end_pos = page_positions[int(page)]\n",
    "    tex_contents = tex_file_contents[tex_start_pos:tex_end_pos]\n",
    "\n",
    "    # gpt api call\n",
    "    combined_data = (\n",
    "    \"Below is pre-generated TeX code without proper formatting.\\n\\n\"\n",
    "    f\"{tex_contents}\\n\\n\"\n",
    "    \"Below is the JSON  data which contains formatting :\\n\\n\"\n",
    "    f\"{text_data}\"\n",
    "    )\n",
    "    if counter == parts:\n",
    "      combined_data += \"\\n\\n\"\n",
    "      combined_data += \"This was the last part, close the latex document with end document. Before that, make an index using \\makeindex command and similarly make a bibliography.\"\n",
    "    else:\n",
    "       combined_data += \"\\n\\n\"\n",
    "       combined_data += f\"This is the {counter} part of the book, do not close the latex document with end document.\"\n",
    "\n",
    "    command = first_page_command if first_part==1 else next_pages_prompt\n",
    "    response = generate_response(combined_data, command, \"\") # reversed the combined_data and command\n",
    "        \n",
    "    first_part = 0\n",
    "    counter+=1\n",
    "    with open(OUTPUT_TEX_FILE, 'a') as f:\n",
    "      f.write(response + \"\\n\")\n",
    "      f.write(f\"%---- Page End Break Here ---- Page : {page}\\n\")\n",
    "\n",
    "\n",
    "    # update positions\n",
    "    tex_start_pos = tex_end_pos+1\n",
    "    start_indx = end_indx+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 70/327 [08:55<25:54,  6.05s/it] "
     ]
    }
   ],
   "source": [
    "# final till the end check\n",
    "\n",
    "def remove_latex_and_ticks(text):\n",
    "    return re.sub(r'```latex|```', '', text)\n",
    "\n",
    "start_indx = 0\n",
    "\n",
    "tex_start_pos = 0\n",
    "tex_end_pos = 0\n",
    "\n",
    "first_part = 1\n",
    "\n",
    "parts = len(page_breaks)\n",
    "counter = 1\n",
    "\n",
    "skipper = 1\n",
    "for page in tqdm(page_breaks[:parts]):\n",
    "    \n",
    "    if skipper != 2:\n",
    "       skipper += 1\n",
    "       continue\n",
    "\n",
    "    skipper = 1\n",
    "    \n",
    "    end_indx = page_numbers.index(page)\n",
    "    text_data = get_pages_data(start_indx, end_indx, doc)\n",
    "\n",
    "    tex_end_pos = page_positions[int(page)]\n",
    "    tex_contents = tex_file_contents[tex_start_pos:tex_end_pos]\n",
    "\n",
    "    # gpt api call\n",
    "    combined_data = (\n",
    "    \"Below is pre-generated TeX code without proper formatting.\\n\\n\"\n",
    "    f\"{tex_contents}\\n\\n\"\n",
    "    \"Below is the JSON  data which contains formatting :\\n\\n\"\n",
    "    f\"{text_data}\"\n",
    "    )\n",
    "    if counter == parts:\n",
    "      combined_data += \"\\n\\n\"\n",
    "      combined_data += \"This was the last part, close the latex document with end document. Before that, make an index using \\makeindex command and similarly make a bibliography.\"\n",
    "    else:\n",
    "       combined_data += \"\\n\\n\"\n",
    "       combined_data += f\"This is the {counter} part of the book, do not close the latex document with end document.\"\n",
    "\n",
    "    command = first_page_command if first_part==1 else next_pages_prompt\n",
    "    response = generate_response(combined_data, command, \"\") # reversed the combined_data and command\n",
    "    response = remove_latex_and_ticks(response)\n",
    "    first_part = 0\n",
    "    counter+=1\n",
    "\n",
    "    with open(OUTPUT_TEX_FILE, 'a') as f:\n",
    "      f.write(response + \"\\n\")\n",
    "      f.write(f\"%---- Page End Break Here ---- Page : {page}\\n\")\n",
    "\n",
    "\n",
    "    # update positions\n",
    "    tex_start_pos = tex_end_pos+1\n",
    "    start_indx = end_indx+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:02<00:00, 130.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Function to remove unwanted LaTeX ticks\n",
    "def remove_latex_and_ticks(text):\n",
    "    return re.sub(r'```latex|```', '', text)\n",
    "\n",
    "# Function to handle API call\n",
    "def process_part(index, start_indx, end_indx, tex_start_pos, tex_end_pos, counter, first_part, parts, page, doc):\n",
    "    text_data = get_pages_data(start_indx, end_indx, doc)\n",
    "    tex_contents = tex_file_contents[tex_start_pos:tex_end_pos]\n",
    "\n",
    "    # Construct the combined data for API call\n",
    "    combined_data = (\n",
    "        \"Below is pre-generated TeX code without proper formatting.\\n\\n\"\n",
    "        f\"{tex_contents}\\n\\n\"\n",
    "        \"Below is the JSON data which contains formatting:\\n\\n\"\n",
    "        f\"{text_data}\"\n",
    "    )\n",
    "    \n",
    "    if counter == parts-1:\n",
    "        combined_data += \"\\n\\nThis was the last part, close the LaTeX document with end document. Before that, make an index using \\\\makeindex command and similarly make a bibliography.\"\n",
    "    else:\n",
    "        combined_data += f\"\\n\\nThis is the {counter} part of the book, do not close the LaTeX document with end document\"\n",
    "\n",
    "    command = first_page_command if first_part == 1 else next_pages_prompt\n",
    "    response = generate_response(combined_data, command, \"\")  # API call\n",
    "    response = remove_latex_and_ticks(response)\n",
    "\n",
    "    return index, response, page  # Return index for ordering\n",
    "\n",
    "# Parallel processing with batching\n",
    "start_indx = 0\n",
    "tex_start_pos = 0\n",
    "first_part = 1\n",
    "parts = len(page_breaks)\n",
    "counter = 1\n",
    "skipper = 1\n",
    "\n",
    "responses_dict = {}  # Store responses by index\n",
    "batch_size = 200  # Number of parallel requests at a time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=batch_size) as executor:  \n",
    "    futures_list = []  # Store pending API calls\n",
    "\n",
    "    for idx, page in enumerate(tqdm(page_breaks[:parts])):\n",
    "        if skipper != 2:\n",
    "            skipper += 1\n",
    "            continue\n",
    "        skipper = 1\n",
    "\n",
    "        end_indx = page_numbers.index(page)\n",
    "        tex_end_pos = page_positions[int(page)]\n",
    "\n",
    "        # Submit task and add to batch\n",
    "        future = executor.submit(process_part, idx, start_indx, end_indx, tex_start_pos, tex_end_pos, counter, first_part, parts, page, doc)\n",
    "        futures_list.append(future)\n",
    "\n",
    "        # Update positions\n",
    "        tex_start_pos = tex_end_pos + 1\n",
    "        start_indx = end_indx + 1\n",
    "        first_part = 0\n",
    "        counter += 1\n",
    "\n",
    "        # Process in batches of 5\n",
    "        if len(futures_list) >= batch_size:\n",
    "            for future in tqdm(as_completed(futures_list)):  # Wait for all in batch\n",
    "                index, response, page = future.result()\n",
    "                responses_dict[index] = (response, page)\n",
    "            futures_list = []  # Clear batch before next set\n",
    "\n",
    "            # Optional: Small delay to avoid rate limits (modify as per API rules)\n",
    "            time.sleep(5)\n",
    "\n",
    "    # Process remaining pending requests\n",
    "    for future in as_completed(futures_list):\n",
    "        index, response, page = future.result()\n",
    "        responses_dict[index] = (response, page)\n",
    "\n",
    "# Write responses in order\n",
    "with open(OUTPUT_TEX_FILE, 'a') as f:\n",
    "    for index in sorted(responses_dict.keys()):  # Ensure ordered writing\n",
    "        response, page = responses_dict[index]\n",
    "        f.write(response + \"\\n\")\n",
    "        f.write(f\"%---- Page End Break Here ---- Page : {page}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
