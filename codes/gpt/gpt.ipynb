{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import copy\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import fitz\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_PATH = \"../../files/data-science_book/data-science.pdf\"\n",
    "TEX_PATH = \"../../files/data-science_book/outputs/data-science_indexed.tex\"\n",
    "# IMG_DIR = \"/content/drive/My Drive/pdf2latex/new_approach_test/images\"  # found images are stored in this subfolder\n",
    "OUTPUT_TEX_FILE = \"../../files/data-science_book/outputs/data-science_cleaned.tex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = api_key\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pymupdf.open(BOOK_PATH)\n",
    "doc.page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEX_PATH, 'r') as file:\n",
    "    tex_file_contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'% This LaTeX document needs to be compiled with XeLaTeX.\\n\\\\documentclass[10pt]{article}\\n\\\\usepackage[utf8]{inputenc}\\n\\\\usepackage{ucharclasses}\\n\\\\usepackage{graphicx}\\n\\\\usepackage[export]{adjustbox}\\n\\\\graphicspath{ {./images/} }\\n\\\\usepackage{hyperref}\\n\\\\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}\\n\\\\urlstyle{same}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage[version=4]{mhchem}\\n\\\\usepackage{stmaryrd}\\n\\\\usepackage{underscore}\\n\\\\usepackage{fvextra, csquotes}\\n\\\\usepackage{multirow}\\n\\\\usepackage{polyglossia}\\n\\\\usepackage{fontspec}\\n\\\\setmainlanguage{english}\\n\\\\setotherlanguages{bengali}\\n\\\\IfFontExistsTF{Noto Serif Bengali}\\n{\\\\newfontfamily\\\\bengalifont{Noto Serif Bengali}}\\n{\\\\IfFontExistsTF{Kohinoor Bangla}\\n  {\\\\newfontfamily\\\\bengalifont{Kohinoor Bangla}}\\n  {\\\\IfFontExistsTF{Bangla MN}\\n    {\\\\newfontfamily\\\\bengalifont{Bangla MN}}\\n    {\\\\IfFontExistsTF{Lohit Bengali}\\n      {\\\\newfontfamily\\\\bengalifont{Lohit Bengali}}\\n      {\\\\IfFontExistsTF{FreeSerif}\\n        {\\\\newfontfamily\\\\bengalifont{FreeSerif}}\\n        {\\\\newfontfamily\\\\bengalifont{Arial Unicode MS}}\\n}}}}\\n\\\\IfFontExistsTF{CMU Serif}\\n{\\\\newfontfamily\\\\lgcfont{CMU Serif}}\\n{\\\\IfFontExistsTF{DejaVu Sans}\\n  {\\\\newfontfamily\\\\lgcfont{DejaVu Sans}}\\n  {\\\\newfontfamily\\\\lgcfont{Georgia}}\\n}\\n\\\\setDefaultTransitions{\\\\lgcfont}{}\\n\\\\setTransitionsFor{Bengali}{\\\\bengalifont}{\\\\lgcfont}\\n\\n\\\\title{Texts in Computer Science }\\n\\n\\\\author{Statistical Distributions}\\n\\\\date{}\\n\\n\\n%New command to display footnote whose markers will always be hidden\\n\\\\let\\\\svthefootnote\\\\thefootnote\\n\\\\newcommand\\\\blfootnotetext[1]{%\\n  \\\\let\\\\thefootnote\\\\relax\\\\footnote{#1}%\\n  \\\\addtocounter{footnote}{-1}%\\n  \\\\let\\\\thefootnote\\\\svthefootnote%\\n}\\n\\n%Overriding the \\\\footnotetext command to hide the marker if its value is `0`\\n\\\\let\\\\svfootnotetext\\\\footnotetext\\n\\\\renewcommand\\\\footnotetext[2][?]{%\\n  \\\\if\\\\relax#1\\\\relax%\\n    \\\\ifnum\\\\value{footnote}=0\\\\blfootnotetext{#2}\\\\else\\\\svfootnotetext{#2}\\\\fi%\\n  \\\\else%\\n    \\\\if?#1\\\\ifnum\\\\value{footnote}=0\\\\blfootnotetext{#2}\\\\else\\\\svfootnotetext{#2}\\\\fi%\\n    \\\\else\\\\svfootnotetext[#1]{#2}\\\\fi%\\n  \\\\fi\\n}\\n\\n\\\\begin{document}\\n\\\\maketitle\\n\\\\section*{TEXTS IN COMPUTER SCIENCE}\\n\\\\section*{THE}\\n\\\\section*{Data Science Design}\\n MANUAL\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-001}\\n\\\\end{center}\\n\\n\\\\section*{Steven S. Slaiena}\\nA) Springer\\n\\n\\n\\n\\\\section*{Steven S. Skiena}\\n\\\\section*{The Data Science Design Manual}\\nSpringer\\n\\nISSN 1868-0941\\\\\\\\\\nISSN 1868-095X (electronic)\\\\\\\\\\nTexts in Computer Science\\\\\\\\\\nISBN 978-3-319-55443-3\\\\\\\\\\nISBN 978-3-319-55444-0 (eBook)\\\\\\\\\\n\\\\href{https://doi.org/10.1007/978-3-319-55444-0}{https://doi.org/10.1007/978-3-319-55444-0}\\\\\\\\\\nLibrary of Congress Control Number: 2017943201\\\\\\\\\\nThis book was advertised with a copyright holder in the name of the publisher in error, whereas the author(s) holds the copyright.\\\\\\\\\\nÂ© The Author(s) 2017\\\\\\\\\\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\\\\\\\\\\nThe use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.\\\\\\\\\\nThe publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n\\nPrinted on acid-free paper\\\\\\\\\\nThis Springer imprint is published by Springer Nature\\\\\\\\\\nThe registered company is Springer International Publishing AG\\\\\\\\\\nThe registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland\\n\\n\\\\section*{Preface}\\nMaking sense of the world around us requires obtaining and analyzing data from our environment. Several technology trends have recently collided, providing new opportunities to apply our data analysis savvy to greater challenges than ever before.\\n\\nComputer storage capacity has increased exponentially; indeed remembering has become so cheap that it is almost impossible to get computer systems to forget. Sensing devices increasingly monitor everything that can be observed: video streams, social media interactions, and the position of anything that moves. Cloud computing enables us to harness the power of massive numbers of machines to manipulate this data. Indeed, hundreds of computers are summoned each time you do a Google search, scrutinizing all of your previous activity just to decide which is the best ad to show you next.\\n\\nThe result of all this has been the birth of data science, a new field devoted to maximizing value from vast collections of information. As a discipline, data science sits somewhere at the intersection of statistics, computer science, and machine learning, but it is building a distinct heft and character of its own. This book serves as an introduction to data science, focusing on the skills and principles needed to build systems for collecting, analyzing, and interpreting data.\\n\\nMy professional experience as a researcher and instructor convinces me that one major challenge of data science is that it is considerably more subtle than it looks. Any student who has ever computed their grade point average (GPA) can be said to have done rudimentary statistics, just as drawing a simple scatter plot lets you add experience in data visualization to your resume. But meaningfully analyzing and interpreting data requires both technical expertise and wisdom. That so many people do these basics so badly provides my inspiration for writing this book.\\n\\n\\\\section*{To the Reader}\\nI have been gratified by the warm reception that my book The Algorithm Design Manual [Ski08] has received since its initial publication in 1997. It has been recognized as a unique guide to using algorithmic techniques to solve problems that often arise in practice. The book you are holding covers very different material, but with the same motivation.\\n\\nI\\n%---- Page End Break Here ---- Page : v\\nn particular, here I stress the following basic principles as fundamental to becoming a good data scientist:\\n\\n\\\\begin{itemize}\\n  \\\\item Valuing doing the simple things right: Data science isn\\'t rocket science. Students and practitioners often get lost in technological space, pursuing the most advanced machine learning methods, the newest open source software libraries, or the glitziest visualization techniques. However, the heart of data science lies in doing the simple things right: understanding the application domain, cleaning and integrating relevant data sources, and presenting your results clearly to others.\\\\\\\\\\nSimple doesn\\'t mean easy, however. Indeed it takes considerable insight and experience to ask the right questions, and sense whether you are moving toward correct answers and actionable insights. I resist the temptation to drill deeply into clean, technical material here just because it is teachable. There are plenty of other books which will cover the intricacies of machine learning algorithms or statistical hypothesis testing. My mission here is to lay the groundwork of what really matters in analyzing data.\\n  \\\\item Developing mathematical intuition: Data science rests on a foundation of mathematics, particularly statistics and linear algebra. It is important to understand this material on an intuitive level: why these concepts were developed, how they are useful, and when they work best. I illustrate operations in linear algebra by presenting pictures of what happens to matrices when you manipulate them, and statistical concepts by examples and reducto ad absurdum arguments. My goal here is transplanting intuition into the reader.\\\\\\\\\\nBut I strive to minimize the amount of formal mathematics used in presenting this material. Indeed, I will present exactly one formal proof in this book, an incorrect proof where the associated theorem is obviously false. The moral here is not that mathematical rigor doesn\\'t matter, because of course it does, but that genuine rigor is impossible until after there is comprehension.\\n  \\\\item Think like a computer scientist, but act like a statistician: Data science provides an umbrella linking computer scientists, statisticians, and domain specialists. But each community has its own distinct styles of thinking and action, which gets stamped into the souls of its members.\\\\\\\\\\nIn this book, I emphasize approaches which come most naturally to computer scientists, particularly the algorithmic manipulation of data, the use of machine learning, and the mastery of scale. But I also seek to transmit the core values of statistical reasoning: the need to understand the application domain, proper appreciation of the small, the quest for significance, and a hunger for exploration.\\\\\\\\\\nNo discipline has a monopoly on the truth. The best data scientists incorporate tools from multiple areas, and this book strives to be a relatively neutral ground where rival philosophies can come to reason together.\\n\\n%---- Page End Break Here ---- Page : vi\\n\\\\end{itemize}\\n\\nEqually important is what you will not find in this book. I do not emphasize any particular language or suite of data analysis tools. Instead, this book provides a high-level discussion of important design principles. I seek to operate at a conceptual level more than a technical one. The goal of this manual is to get you going in the right direction as quickly as possible, with whatever software tools you find most accessible.\\n\\n\\\\section*{To the Instructor}\\nThis book covers enough material for an \"Introduction to Data Science\" course at the undergraduate or early graduate student levels. I hope that the reader has completed the equivalent of at least one programming course and has a bit of prior exposure to probability and statistics, but more is always better than less.\\n\\nI have made a full set of lecture slides for teaching this course available online at \\\\href{http://www.data-manual.com}{http://www.data-manual.com}. Data resources for projects and assignments are also available there to aid the instructor. Further, I make available online video lectures using these slides to teach a full-semester data science course. Let me help teach your class, through the magic of the web!\\n\\nPedagogical features of this book include:\\n\\n\\\\begin{itemize}\\n  \\\\item War Stories: To provide a better perspective on how data science techniques apply to the real world, I include a collection of \"war stories,\" or tales from our experience with real problems. The moral of these stories is that these methods are not just theory, but important tools to be pulled out and used as needed.\\n  \\\\item False Starts: Most textbooks present methods as a fait accompli, obscuring the ideas involved in designing them, and the subtle reasons why other approaches fail. The war stories illustrate my reasoning process on certain applied problems, but I weave such coverage into the core material as well.\\n  \\\\item Take-Home Lessons: Highlighted \"take-home\" lesson boxes scattered through each chapter emphasize the big-picture concepts to learn from each chapter.\\n  \\\\item Homework Problems: I provide a wide range of exercises for homework and self-study. Many are traditional exam-style problems, but there are also larger-scale implementation challenges and smaller-scale interview questions, reflecting the questions students might encounter when searching for a job. Degree of difficulty ratings have been assigned to all problems.\\\\\\\\\\nIn lieu of an answer key, a Solution Wiki has been set up, where solutions to all even numbered problems will be solicited by crowdsourcing. A similar system with my Algorithm Design Manual produced coherent solutions,\\\\\\n%---- Page End Break Here ---- Page : vii\\n\\\\\\nor so I am told. As a matter of principle I refuse to look at them, so let the buyer beware.\\n  \\\\item Kaggle Challenges: Kaggle (\\\\href{http://www.kaggle.com}{www.kaggle.com}) provides a forum for data scientists to compete in, featuring challenging real-world problems on fascinating data sets, and scoring to test how good your model is relative to other submissions. The exercises for each chapter include three relevant Kaggle challenges, to serve as a source of inspiration, self-study, and data for other projects and investigations.\\n  \\\\item Data Science Television: Data science remains mysterious and even threatening to the broader public. The Quant Shop is an amateur take on what a data science reality show should be like. Student teams tackle a diverse array of real-world prediction problems, and try to forecast the outcome of future events. Check it out at \\\\href{http://www.quant-shop.com}{http://www.quant-shop.com}.\\\\\\\\\\nA series of eight 30-minute episodes has been prepared, each built around a particular real-world prediction problem. Challenges include pricing art at an auction, picking the winner of the Miss Universe competition, and forecasting when celebrities are destined to die. For each, we observe as a student team comes to grips with the problem, and learn along with them as they build a forecasting model. They make their predictions, and we watch along with them to see if they are right or wrong.\\\\\\\\\\nIn this book, The Quant Shop is used to provide concrete examples of prediction challenges, to frame discussions of the data science modeling pipeline from data acquisition to evaluation. I hope you find them fun, and that they will encourage you to conceive and take on your own modeling challenges.\\n  \\\\item Chapter Notes: Finally, each tutorial chapter concludes with a brief notes section, pointing readers to primary sources and additional references.\\n\\\\end{itemize}\\n\\n\\\\section*{Dedication}\\nMy bright and loving daughters Bonnie and Abby are now full-blown teenagers, meaning that they don\\'t always process statistical evidence with as much alacrity as I would I desire. I dedicate this book to them, in the hope that their analysis skills improve to the point that they always just agree with me.\\n\\nAnd I dedicate this book to my beautiful wife Renee, who agrees with me even when she doesn\\'t agree with me, and loves me beyond the support of all creditable evidence.\\n\\n\\\\section*{Acknowledgments}\\nMy list of people to thank is large enough that I have probably missed some. I will try to do enumerate them systematically to minimize omissions, but ask those I\\'ve unfairly neglected for absolution.\\n\\n%---- Page End Break Here ---- Page : viii\\n\\nFirst, I thank those who made concrete contributions to help me put this book together. Yeseul Lee served as an apprentice on this project, helping with figures, exercises, and more during summer 2016 and beyond. You will see evidence of her handiwork on almost every page, and I greatly appreciate her help and dedication. Aakriti Mittal and Jack Zheng also contributed to a few of the figures.\\n\\nStudents in my Fall 2016 Introduction to Data Science course (CSE 519) helped to debug the manuscript, and they found plenty of things to debug. I particularly thank Rebecca Siford, who proposed over one hundred corrections on her own. Several data science friends/sages reviewed specific chapters for me, and I thank Anshul Gandhi, Yifan Hu, Klaus Mueller, Francesco Orabona, Andy Schwartz, and Charles Ward for their efforts here.\\n\\nI thank all the Quant Shop students from Fall 2015 whose video and modeling efforts are so visibly on display. I particularly thank Jan (Dini) DiskinZimmerman, whose editing efforts went so far beyond the call of duty I felt like a felon for letting her do it.\\n\\nMy editors at Springer, Wayne Wheeler and Simon Rees, were a pleasure to work with as usual. I also thank all the production and marketing people who helped get this book to you, including Adrian Pieron and Annette Anlauf.\\n\\nSeveral exercises were originated by colleagues or inspired by other sources. Reconstructing the original sources years later can be challenging, but credits for each problem (to the best of my recollection) appear on the website.\\n\\nMuch of what I know about data science has been learned through working with other people. These include my Ph.D. students, particularly Rami al-Rfou, Mikhail Bautin, Haochen Chen, Yanqing Chen, Vivek Kulkarni, Levon Lloyd, Andrew Mehler, Bryan Perozzi, Yingtao Tian, Junting Ye, Wenbin Zhang, and postdoc Charles Ward. I fondly remember all of my Lydia project masters students over the years, and remind you that my prize offer to the first one who names their daughter Lydia remains unclaimed. I thank my other collaborators with stories to tell, including Bruce Futcher, Justin Gardin, Arnout van de Rijt, and Oleksii Starov.\\n\\nI remember all members of the General Sentiment/Canrock universe, particularly Mark Fasciano, with whom I shared the start-up dream and experienced what happens when data hits the real world. I thank my colleagues at Yahoo Labs/Research during my 2015-2016 sabbatical year, when much of this book was conceived. I single out \\\\index{Hamming\\\\index{hedge fund}, Richard W.}Amanda Stent, who enabled me to be at Yahoo during that particularly difficult year in the company\\'s history. I learned valuable things from other people who have taught related data science courses, including Andrew Ng and Hans-Peter Pfister, and thank them all for their help.\\n\\nIf you have a procedure with ten parameters, you probably missed some.\\n\\n\\\\begin{itemize}\\n  \\\\item Alan Perlis\\n\\\\end{itemize}\\n\\n\\\\section*{Caveat}\\nIt is traditional for the author to magnanimously accept the blame for whatever deficiencies remain. I don\\'t. Any errors, deficiencies, or problems in this book are somebody else\\'s fault, but I would appreciate knowing about them so as to determine who is to blame.\\n\\n\\n\\n\\\\section*{Contents}\\n1 What is Data Science? ..... 1\\\\\\\\\\n1.1 Computer Science, Data Science, and Real Science ..... 2\\\\\\\\\\n1.2 Asking Interesting Questions from Data ..... 4\\\\\\\\\\n1.2.1 The Baseball Encyclopedia ..... 5\\\\\\\\\\n1.2.2 The Internet Movie Database (IMDb) ..... 7\\\\\\\\\\n1.2.3 Google Ngrams ..... 10\\\\\\\\\\n1.2.4 New York Taxi Records ..... 11\\\\\\\\\\n1.3 Properties of Data ..... 14\\\\\\\\\\n1.3.1 Structured vs. Unstructured Data ..... 14\\\\\\\\\\n1.3.2 Quantitative vs. Categorical Data ..... 15\\\\\\\\\\n1.3.3 Big Data vs. Little Data ..... 15\\\\\\\\\\n1.4 Classification and Regression ..... 16\\\\\\\\\\n1.5 Data Science Television: The Quant Shop ..... 17\\\\\\\\\\n1.5.1 Kaggle Challenges ..... 19\\\\\\\\\\n1.6 About the W\\\\index{comput\\\\index{data centrism}\\\\index{method centrism}e\\\\inde\\\\index{scientist}x{data scientist}r scientist}ar Stories ..... 19\\\\\\\\\\n1.7 War Story: Answering the Right Question ..... 21\\\\\\\\\\n1.8 Chapter Notes ..... 22\\\\\\\\\\n1.9 Exercises ..... 23\\\\\\\\\\n2 Mathematical Preliminaries ..... 27\\\\\\\\\\n2.1 Probability ..... 27\\\\\\\\\\n2.1.1 Probability vs. Statistics ..... 29\\\\\\\\\\n2.1.2 Compound Events and Independence ..... 30\\\\\\\\\\n2.1.3 Conditional Probability ..... 31\\\\\\\\\\n2.1.4 Probability Distributions ..... 32\\\\\\\\\\n2.2 Descriptive Statistics ..... 34\\\\\\\\\\n2.2.1 Centrality Measures ..... 34\\\\\\\\\\n2.2.2 Variability Measures ..... 36\\\\\\\\\\n2.2.3 Interpreting Variance ..... 37\\\\\\\\\\n2.2.4 Characterizing Distributions ..... 39\\\\\\\\\\n2.3 Correlation Analysis ..... 40\\\\\\\\\\n2.3.1 Correlation Coefficients: Pearson and Spearman Rank ..... 41\\\\\\\\\\n2.3.2 The Power and Significance of Correlation ..... 43\\\\\\\\\\n2.3.3 Correlation Does Not Imply Causation! ..... 45\\\\\\\\\\n2.3.4 Detecting Periodicities by Autocorrelation ..... 46\\\\\\\\\\n2.4 Logarithms ..... 47\\\\\\\\\\n2.4.1 Logarithms and Multiplying Probabilities ..... 48\\\\\\\\\\n2.4.2 Logarithms and Ratios ..... 48\\\\\\\\\\n2.4.3 Logarithms and Normalizing Skewed Distributions ..... 49\\\\\\\\\\n2.5 War Story: Fitting Designer Genes ..... 50\\\\\\\\\\n2.6 Chapter Notes ..... 52\\\\\\\\\\n2.7 Exercises ..... 53\\\\\\\\\\n3 Data Munging ..... 57\\\\\\\\\\n3.1 Languages for Data Science ..... 57\\\\\\\\\\n3.1.1 The Importance of Notebook Environments ..... 59\\\\\\\\\\n3.1.2 Standard Data Formats ..... 61\\\\\\\\\\n3.2 Collecting Data ..... 64\\\\\\\\\\n3.2.1 Hunting ..... 64\\\\\\\\\\n3.2.2 Scraping ..... 67\\\\\\\\\\\\index{p\\\\index{robustness}recision}\\n3.2.3 Logging ..... 68\\\\\\\\\\n3.3 Cleaning Data ..... 69\\\\\\\\\\n3.3.1 Errors vs. Artifacts ..... 69\\\\\\\\\\n3.3.2 Data Compatibility ..... 72\\\\\\\\\\n3.3.3 Dealing with Missing Values ..... 76\\\\\\\\\\n3.3.4 Outlier Detection ..... 78\\\\\\\\\\n3.4 War Story: Beating the Market ..... 79\\\\\\\\\\n3.5 Crowdsourcing ..... 80\\\\\\\\\\n3.5.1 The Penny Demo ..... 81\\\\\\\\\\n3.5.2 When is the Crowd Wise? ..... 82\\\\\\\\\\n3.5.3 Mechanisms for Aggregation ..... 83\\\\\\\\\\n3.5.4 Crowdsourcing Services ..... 84\\\\\\\\\\n3.5.5 Gamification ..... 88\\\\\\\\\\n3.6 Chapter Notes ..... 90\\\\\\\\\\n3.7 Exercises ..... 90\\\\\\\\\\n4 Scores and Rankings ..... 95\\\\\\\\\\n4.1 The Body Mass Index (BMI) ..... 96\\\\\\\\\\n4.2 Developing Scoring Systems ..... 99\\\\\\\\\\n4.2.1 Gold Standards and Proxies ..... 99\\\\\\\\\\n4.2.2 Scores vs. Rankings ..... 100\\\\\\\\\\n4.2.3 Recognizing Good Scoring Functions ..... 101\\\\\\\\\\n4.3 Z-scores and Normalization ..... 103\\\\\\\\\\n4.4 Advanced Ranking Techniques ..... 104\\\\\\\\\\n4.4.1 Elo Rankings ..... 104\\\\\\\\\\n4.4.2 Merging Rankings ..... 108\\\\\\\\\\n4.4.3 Digraph-based Rankings ..... 109\\\\\\\\\\n4.4.4 PageRank ..... 111\\\\\\\\\\n4.5 War Story: Clyde\\'s Revenge ..... 111\\\\\\\\\\n4.6 Arrow\\'s Impossibility Theorem ..... 114\\\\\\\\\\n4.7 War Story: Who\\'s Bigger? ..... 115\\\\\\\\\\n4.8 Chapter Notes ..... 118\\\\\\\\\\n4.9 Exercises ..... 119\\\\\\\\\\n5 Statistical Analysis ..... 121\\\\\\\\\\n5.1 Statistical Distributions ..... 122\\\\\\\\\\n5.1.1 The Binomial Distribution ..... 123\\\\\\\\\\n5.1.2 The Normal Distribution ..... 124\\\\\\\\\\n5.1.3 Implications of the Normal Distribution ..... 126\\\\\\\\\\n5.1.4 Poisson Distribution ..... 127\\\\\\\\\\n5.1.5 Power Law Distributions ..... 129\\\\\\\\\\n5.2 Sampling from Distributions ..... 132\\\\\\\\\\n5.2.1 Random Sampling beyond One Dimension ..... 133\\\\\\\\\\n5.3 Statistical Significance ..... 135\\\\\\\\\\n5.3.1 The Significance of Significance ..... 135\\\\\\\\\\n5.3.2 The T-test: Comparing Population Means ..... 137\\\\\\\\\\n5.3.3 The Kolmogorov-Smirnov Test ..... 139\\\\\\\\\\n5.3.4 The Bonferroni Correction ..... 141\\\\\\\\\\n5.3.5 False Discovery Rate ..... 142\\\\\\\\\\n5.4 War Story: Discovering the Fountain of Youth? ..... 143\\\\\\\\\\n5.5 Permutation Tests and P-values ..... 145\\\\\\\\\\n5.5.1 Generating Random Permutations ..... 147\\\\\\\\\\n5.5.2 DiMaggio\\'s Hitting Streak ..... 148\\\\\\\\\\n5.6 Bayesian Reasoning ..... 150\\\\\\\\\\n5.7 Chapter Notes ..... 151\\\\\\\\\\n5.8 Exercises ..... 151\\\\\\\\\\n6 Visualizing Data ..... 155\\\\\\\\\\n6.1 Exploratory Data Analysis ..... 156\\\\\\\\\\n6.1.1 Confronting a New Data Set ..... 156\\\\\\\\\\n6.1.2 Summary Statistics and Anscombe\\'s Quartet ..... 159\\\\\\\\\\n6.1.3 Visualization Tools ..... 160\\\\\\\\\\n6.2 Developing a Visualization Aesthetic ..... 162\\\\\\\\\\n6.2.1 Maximizing Data-Ink Ratio ..... 163\\\\\\\\\\n6.2.2 Minimizing the Lie Factor ..... 164\\\\\\\\\\n6.2.3 Minimizing Chartjunk ..... 165\\\\\\\\\\n6.2.4 Proper Scaling and Labeling ..... 167\\\\\\\\\\n6.2.5 Effective Use of Color and Shading ..... 168\\\\\\\\\\n6.2.6 The Power of Repetition ..... 169\\\\\\\\\\n6.3 Chart Types ..... 170\\\\\\\\\\n6.3.1 Tabular Data ..... 170\\\\\\\\\\n6.3.2 Dot and Line Plots ..... 174\\\\\\\\\\n6.3.3 Scatter Plots ..... 177\\\\\\\\\\n6.3.4 Bar Plots and Pie Charts ..... 179\\\\\\\\\\n6.3.5 Histograms ..... 183\\\\\\\\\\n6.3.6 Data Maps ..... 187\\\\\\\\\\n6.4 Great Visualizations ..... 189\\\\\\\\\\n6.4.1 Marey\\'s Train Schedule ..... 189\\\\\\\\\\n6.4.2 Snow\\'s Cholera Map ..... 191\\\\\\\\\\n6.4.3 New York\\'s Weather Year ..... 192\\\\\\\\\\n6.5 Reading Graphs ..... 192\\\\\\\\\\n6.5.1 The Obscured Distribution ..... 193\\\\\\\\\\n6.5.2 Overinterpreting Variance ..... 193\\\\\\\\\\n6.6 Interactive Visualization ..... 195\\\\\\\\\\n6.7 War Story: TextMapping the World ..... 196\\\\\\\\\\n6.8 Chapter Notes ..... 198\\\\\\\\\\n6.9 Exercises ..... 199\\\\\\\\\\n7 Mathematical Models ..... 201\\\\\\\\\\n7.1 Philosophies of Modeling ..... 201\\\\\\\\\\n7.1.1 Occam\\'s Razor ..... 201\\\\\\\\\\n7.1.2 Bias-Variance Trade-Offs ..... 202\\\\\\\\\\n7.1.3 What Would Nate Silver Do? ..... 203\\\\\\\\\\n7.2 A Taxonomy of Models ..... 205\\\\\\\\\\n7.2.1 Linear vs. Non-Linear Models ..... 206\\\\\\\\\\n7.2.2 Blackbox vs. Descriptive Models ..... 206\\\\\\\\\\n7.2.3 First-Principle vs. Data-Driven Models ..... 207\\\\\\\\\\n7.2.4 Stochastic vs. Deterministic Models ..... 208\\\\\\\\\\n7.2.5 Flat vs. Hierarchical Models ..... 209\\\\\\\\\\n7.3 Baseline Models ..... 210\\\\\\\\\\n7.3.1 Baseline Models for Classification ..... 210\\\\\\\\\\n7.3.2 Baseline Models for Value Prediction ..... 212\\\\\\\\\\n7.4 Evaluating Models ..... 212\\\\\\\\\\n7.4.1 Evaluating Classifiers ..... 213\\\\\\\\\\n7.4.2 Receiver-Operator Characteristic (ROC) Curves ..... 218\\\\\\\\\\n7.4.3 Evaluating Multiclass Systems ..... 219\\\\\\\\\\n7.4.4 Evaluating Value Prediction Models ..... 221\\\\\\\\\\n7.5 Evaluation Environments ..... 224\\\\\\\\\\n7.5.1 Data Hygiene for Evaluation ..... 225\\\\\\\\\\n7.5.2 Amplifying Small Evaluation Sets ..... 226\\\\\\\\\\n7.6 War Story: 100\\\\% Accuracy ..... 228\\\\\\\\\\n7.7 Simulation Models ..... 229\\\\\\\\\\n7.8 War Story: Calculated Bets ..... 230\\\\\\\\\\n7.9 Chapter Notes ..... 233\\\\\\\\\\n7.10 Exercises ..... 234\\\\\\\\\\n8 Linear Algebra ..... 237\\\\\\\\\\n8.1 The Power of Linear Algebra ..... 237\\\\\\\\\\n8.1.1 Interpreting Linear Algebraic Formulae ..... 238\\\\\\\\\\n8.1.2 Geometry and Vectors ..... 240\\\\\\\\\\n8.2 Visualizing Matrix Operations ..... 241\\\\\\\\\\n8.2.1 Matrix Addition ..... 242\\\\\\\\\\n8.2.2 Matrix Multiplication ..... 243\\\\\\\\\\n8.2.3 Applications of Matrix Multiplication ..... 244\\\\\\\\\\n8.2.4 Identity Matrices and Inversion ..... 248\\\\\\\\\\n8.2.5 Matrix Inversion and Linear Systems ..... 250\\\\\\\\\\n8.2.6 Matrix Rank ..... 251\\\\\\\\\\n8.3 Factoring Matrices ..... 252\\\\\\\\\\n8.3.1 Why Factor Feature Matrices? ..... 252\\\\\\\\\\n8.3.2 LU Decomposition and Determinants ..... 254\\\\\\\\\\n8.4 Eigenvalues and Eigenvectors ..... 255\\\\\\\\\\n8.4.1 Properties of Eigenvalues ..... 255\\\\\\\\\\n8.4.2 Computing Eigenvalues ..... 256\\\\\\\\\\n8.5 Eigenvalue Decomposition ..... 257\\\\\\\\\\n8.5.1 Singular Value Decomposition ..... 258\\\\\\\\\\n8.5.2 Principal Components Analysis ..... 260\\\\\\\\\\n8.6 War Story: The Human Factors ..... 262\\\\\\\\\\n8.7 Chapter Notes ..... 263\\\\\\\\\\n8.8 Exercises ..... 263\\\\\\\\\\n9 Linear and Logistic Regression ..... 267\\\\\\\\\\n9.1 Linear Regression ..... 268\\\\\\\\\\n9.1.1 Linear Regression and Duality ..... 268\\\\\\\\\\n9.1.2 Error in Linear Regression ..... 269\\\\\\\\\\n9.1.3 Finding the Optimal Fit ..... 270\\\\\\\\\\n9.2 Better Regression Models ..... 272\\\\\\\\\\n9.2.1 Removing Outliers ..... 272\\\\\\\\\\n9.2.2 Fitting Non-Linear Functions ..... 273\\\\\\\\\\n9.2.3 Feature and Target Scaling ..... 274\\\\\\\\\\n9.2.4 Dealing with Highly-Correlated Features ..... 277\\\\\\\\\\n9.3 War Story: Taxi Deriver ..... 277\\\\\\\\\\n9.4 Regression as Parameter Fitting ..... 279\\\\\\\\\\n9.4.1 Convex Parameter Spaces ..... 280\\\\\\\\\\n9.4.2 Gradient Descent Search ..... 281\\\\\\\\\\n9.4.3 What is the Right Learning Rate? ..... 283\\\\\\\\\\n9.4.4 Stochastic Gradient Descent ..... 285\\\\\\\\\\n9.5 Simplifying Models through Regularization ..... 286\\\\\\\\\\n9.5.1 Ridge Regression ..... 286\\\\\\\\\\n9.5.2 LASSO Regression ..... 287\\\\\\\\\\n9.5.3 Trade-Offs between Fit and Complexity ..... 288\\\\\\\\\\n9.6 Classification and Logistic Regression ..... 289\\\\\\\\\\n9.6.1 Regression for Classification ..... 290\\\\\\\\\\n9.6.2 Decision Boundaries ..... 291\\\\\\\\\\n9.6.3 Logistic Regression ..... 292\\\\\\\\\\n9.7 Issues in Logistic Classification ..... 295\\\\\\\\\\n9.7.1 Balanced Training Classes ..... 295\\\\\\\\\\n9.7.2 Multi-Class Classification ..... 297\\\\\\\\\\n9.7.3 Hierarchical Classification ..... 298\\\\\\\\\\n9.7.4 Partition Functions and Multinomial Regression ..... 299\\\\\\\\\\n9.8 Chapter Notes ..... 300\\\\\\\\\\n9.9 Exercises ..... 301\\\\\\\\\\n10 Distance and Network Methods ..... 303\\\\\\\\\\n10.1 Measuring Distances ..... 303\\\\\\\\\\n10.1.1 Distance Metrics ..... 304\\\\\\\\\\n10.1.2 The $L_{k}$ Distance Metric ..... 305\\\\\\\\\\n10.1.3 Working in Higher Dimensions ..... 307\\\\\\\\\\n10.1.4 Dimensional Egalitarianism ..... 308\\\\\\\\\\n10.1.5 Points vs. Vectors ..... 309\\\\\\\\\\n10.1.6 Distances between Probability Distributions ..... 310\\\\\\\\\\n10.2 Nearest Neighbor Classification ..... 311\\\\\\\\\\n10.2.1 Seeking Good Analogies ..... 312\\\\\\\\\\n10.2.2 $k$-Nearest Neighbors ..... 313\\\\\\\\\\n10.2.3 Finding Nearest Neighbors ..... 315\\\\\\\\\\n10.2.4 Locality Sensitive Hashing ..... 317\\\\\\\\\\n10.3 Graphs, Networks, and Distances ..... 319\\\\\\\\\\n10.3.1 Weig\\\\index{data processing}hted\\\\index{Google Ngrams} Graphs and Induced Networks ..... 320\\\\\\\\\\n10.3.2 Talking About Graphs ..... 321\\\\\\\\\\n10.3.3 Graph Theory ..... 323\\\\\\\\\\n10.4 PageRank ..... 325\\\\\\\\\\n10.5 Clustering ..... 327\\\\\\\\\\n10.5.1 $k$-means Clustering ..... 330\\\\\\\\\\n10.5.2 Agglomerative Clustering ..... 336\\\\\\\\\\n10.5.3 Comparing Clusterings ..... 341\\\\\\\\\\n10.5.4 Similarity Graphs and Cut-Based Clustering ..... 341\\\\\\\\\\n10.6 War Story: Cluster Bombing ..... 344\\\\\\\\\\n10.7 Chapter Notes ..... 345\\\\\\\\\\n10.8 Exercises ..... 346\\\\\\\\\\n11 Machine Learning ..... 351\\\\\\\\\\n11.1 Naive Bayes ..... 354\\\\\\\\\\n11.1.1 Formulation ..... 354\\\\\\\\\\n11.1.2 Dealing with Zero Counts (Discounting) ..... 356\\\\\\\\\\n11.2 Decision Tree Classifiers ..... 357\\\\\\\\\\n11.2.1 Constructing Decision Trees ..... 359\\\\\\\\\\n11.2.2 Realizing Exclusive Or ..... 361\\\\\\\\\\n11.2.3 Ensembles of Decision Trees ..... 362\\\\\\\\\\n11.3 Boosting and Ensemble Learning ..... 363\\\\\\\\\\n11.3.1 Voting with Classifiers ..... 363\\\\\\\\\\n11.3.2 Boosting Algorithms ..... 364\\\\\\\\\\n11.4 Support Vector Machines ..... 366\\\\\\\\\\n11.4.1 Linear SVMs ..... 369\\\\\\\\\\n11.4.2 Non-linear SVMs ..... 369\\\\\\\\\\n11.4.3 Kernels ..... 371\\\\\\\\\\n11.5 Degrees of Supervision ..... 372\\\\\\\\\\n11.5.1 Supervised Learning ..... 372\\\\\\\\\\n11.5.2 Unsupervised Learning ..... 372\\\\\\\\\\n11.5.3 Semi-supervised Learning ..... 374\\\\\\\\\\n11.5.4 Feature Engineering ..... 375\\\\\\\\\\n11.6 Deep Learning ..... 377\\\\\\\\\\n11.6.1 Networks and Depth ..... 378\\\\\\\\\\n11.6.2 Backpropagation ..... 382\\\\\\\\\\n11.6.3 Word and Graph Embeddings ..... 383\\\\\\\\\\n11.7 War Story: The Name Game ..... 385\\\\\\\\\\n11.8 Chapter Notes ..... 387\\\\\\\\\\n11.9 Exercises ..... 388\\\\\\\\\\n12 Big Data: Achieving Scale ..... 391\\\\\\\\\\n12.1 What is Big Data? ..... 392\\\\\\\\\\n12.1.1 Big Data as Bad Data ..... 392\\\\\\\\\\n12.1.2 The Three Vs ..... 394\\\\\\\\\\n12.2 War Story: Infrastructure Matters ..... 395\\\\\\\\\\n12.3 Algorithmics for Big Data ..... 397\\\\\\\\\\n12.3.1 Big Oh Analysis ..... 397\\\\\\\\\\n12.3.2 Hashing ..... 399\\\\\\\\\\n12.3.3 Exploiting the Storage Hierarchy ..... 401\\\\\\\\\\n12.3.4 Streaming and Single-Pass Algorithms ..... 402\\\\\\\\\\n12.4 Filtering and Sampling ..... 403\\\\\\\\\\n12.4.1 Deterministic Sampling Algorithms ..... 404\\\\\\\\\\n12.4.2 Randomized and Stream Sampling ..... 406\\\\\\\\\\n12.5 Parallelism ..... 406\\\\\\\\\\n12.5.1 One, Two, Many ..... 407\\\\\\\\\\n12.5.2 Data Parallelism ..... 409\\\\\\\\\\n12.5.3 Grid Search ..... 409\\\\\\\\\\n12.5.4 Cloud Computing Services ..... 410\\\\\\\\\\n12.6 MapReduce ..... 410\\\\\\\\\\n12.6.1 Map-Reduce Programming ..... 412\\\\\\\\\\n12.6.2 MapReduce under the Hood ..... 414\\\\\\\\\\n12.7 Societal and Ethical Implications ..... 416\\\\\\\\\\n12.8 Chapter Notes ..... 419\\\\\\\\\\n12.9 Exercises ..... 419\\\\\\\\\\n13 Coda ..... 423\\\\\\\\\\n13.1 Get a Job! ..... 423\\\\\\\\\\n13.2 Go to Graduate School! ..... 424\\\\\\\\\\n13.3 Professional Consulting Services ..... 425\\\\\\\\\\n14 Bibliography ..... 427\\n\\n\\\\section*{Chapter 1}\\n\\\\section*{What is Data Science?}\\nWhat is data science? Like any emerging field, it hasn\\'t been completely defined yet, but you know enough about it to be interested or else you wouldn\\'t be reading this book.\\n\\nI think of data science as lying at the intersection of computer science, statistics, and substantive application domains. From computer science comes machine learning and high-performance computing technologies for dealing with scale. From statistics comes a long tradition of exploratory data analysis, significance testing, and visualization. From application domains in business and the sciences comes challenges worthy of battle, and evaluation standards to assess when they have been adequately conquered.\\n\\nBut these are all well-established fields. Why data science, and why now? I see three reasons for this sudden burst of activity:\\n\\n\\\\begin{itemize}\\n  \\\\item New technology makes it possible to capture, annotate, and store vast amounts of social media, logging, and sensor data. After you have amassed all this data, you begin to wonder what you can do with it.\\n  \\\\item Computing advances make it possible to analyze data in novel ways and at ever increasing scales. Cloud computing architectures give even the little guy access to vast power when they need it. New approaches to machine learning have lead to amazing advances in longstanding problems, like computer vision and natural language processing.\\n  \\\\item Prominent technology companies (like Google and Facebook) and quantitative hedge funds (like Renaissance Technologies and TwoSigma) have proven the power of modern data analytics. Success stories applying data to such diverse areas as sports management (Moneyball [Lew04) and election forecasting (Nate Silver [Sil12]) have served as role models to bring data science to a large popular audience.\\n\\\\end{itemize}\\n\\nThis introductory chapter has three missions. First, I will try to explain how good data scientists think, and how this differs from the mindset of traditional programmers and software developers. Second, we will look at data sets in terms of the potential for what they can be used for, and learn to ask the broader questions they are capable of answering. Finally, I introduce a collection of data analysis challenges that will be used throughout this book as motivating examples.\\n\\n\\\\subsection*{1.1 Computer Science, Data Science, and Real Science}\\nComputer scientists, by nature, don\\'t respect data. They have traditionally been taught that the algorithm was the thing, and that data was just meat to be passed through a sausage grinder.\\n\\nSo to qualify as an effective data scientist, you must first learn to think like a real scientist. Real scientists strive to understand the natural world, which is a complicated and messy place. By contrast, computer scientists tend to build their own clean and organized virtual worlds and live comfortably within them. Scientists obsess about discovering things, while computer scientists invent rather than discover.\\n\\nPeople\\'s mindsets strongly color how they think and act, causing misunderstandings when we try to communicate outside our tribes. So fundamental are these biases that we are often unaware we have them. Examples of the cultural differences between computer science and real science include:\\n\\n\\\\begin{itemize}\\n  \\\\item Data vs. method centrism: Scientists are data driven, while computer scientists are algorithm driven. Real scientists spend enormous amounts of effort collecting data to answer their question of interest. They invent fancy measuring devices, stay up all night tending to experiments, and devote most of their thinking to how to get the data they need.\\\\\\\\\\nBy contrast, computer scientists obsess about methods: which algorithm is better than which other algorithm, which programming language is best for a job, which program is better than which other program. The details of the data set they are working on seem comparably unexciting.\\n  \\\\item Concern about results: Real scientists care about answers. They analyze data to discover something about how the world works. Good scientists care about whether the results make sense, because they care about what the answers mean.\\\\\\\\\\nBy contrast, bad computer scientists worry about producing plausiblelooking numbers. As soon as the numbers stop looking grossly wrong, they are presumed to be right. This is because they are personally less invested in what can be learned from a computation, as opposed to getting it done quickly and efficiently.\\n \\n%---- Page End Break Here ---- Page : 2\\n \\\\item Robustness: Real scientists are comfortable with the idea that data has errors. In general, computer scientists are not. Scientists think a lot about possible sources of bias or error in their data, and how these possible problems can effect the conclusions derived from them. Good programmers use strong data-typing and parsing methodologies to guard against formatting errors, but the concerns here are different.\\\\\\\\\\nBecoming aware that data can have errors is empowering. Computer scientists chant \"garbage in, garbage out\" as a defensive mantra to ward off criticism, a way to say that\\'s not my job. Real scientists get close enough to their data to smell it, giving it the sniff test to decide whether it is likely to be garbage.\\n  \\\\item Precision: Nothing is ever completely true or false in science, while everything is either true or false in computer science or mathematics.\\\\\\\\\\nGenerally speaking, computer scientists are happy printing floating point numbers to as many digits as possible: $8 / 13=0.61538461538$. Real scientists will use only two significant digits: $8 / 13 \\\\approx 0.62$. Computer scientists care what a number is, while real scientists care what it means.\\n\\\\end{itemize}\\n\\nAspiring data scientists must learn to think like real scientists. Your job is going to be to turn numbers into insight. It is important to understand the why as much as the how.\\n\\nTo be fair, it benefits real scientists to think like data scientists as well. New experimental technologies enable measuring systems on vastly greater scale than ever possible before, through technologies like full-genome sequencing in biology and full-sky telescope surveys in astronomy. With new breadth of view comes new levels of vision.\\n\\nTraditional hypothesis-driven science was based on asking specific questions of the world and then generating the specific data needed to confirm or deny it. This is now augmented by data-driven science, which instead focuses on generating data on a previously unheard of scale or resolution, in the belief that new discoveries will come as soon as one is able to look at it. Both ways of thinking will be important to us:\\n\\n\\\\begin{itemize}\\n  \\\\item Given a problem, what available data will help us answer it?\\n  \\\\item Given a data set, what interesting problems can we apply it to?\\n\\\\end{itemize}\\n\\nThere is another way to capture this basic distinction between software engineering and data science. It is that software developers are hired to build systems, while data scientists are hired to produce insights.\\n\\nThis may be a point of contention for some developers. There exist an important class of engineers who wrangle the massive distributed infrastructures necessary to store and analyze, say, financial transaction or social media data\\\\\\n%---- Page End Break Here ---- Page : 3\\n\\\\\\non a full Facebook or Twitter-level of scale. Indeed, I will devote Chapter 12 to the distinctive challenges of big data infrastructures. These engineers are building tools and systems to support data science, even though they may not personally mine the data they wrangle. Do they qualify as data scientists?\\n\\nThis is a fair question, one I will finesse a bit so as to maximize the potential readership of this book. But I do believe that the better such engineers understand the full data analysis pipeline, the more likely they will be able to build powerful tools capable of providing important insights. A major goal of this book is providing big data engineer\\\\index{big data engineer}s with the intellectual tools to think like big data scientists.\\n\\n\\\\subsection*{1.2 asking interesting questions\\\\index{asking interesting questions} from Data}\\nGood data scientists develop an inherent curiosity about the world around them, particularly in the associated domains and applications they are working on. They enjoy talking shop with the people whose data they work with. They ask them questions: What is the coolest thing you have learned about this field? Why did you get interested in it? What do you hope to learn by analyzing your data set? Data scientists always ask questions.\\n\\nGood data scientists have wide-ranging interests. They read the newspaper every day to get a broader perspective on what is exciting. They understand that the world is an interesting place. Knowing a little something about everything equips them to play in other people\\'s backyards. They are brave enough to get out of their comfort zones a bit, and driven to learn more once they get there.\\n\\nSoftware developers are not really encouraged to ask questions, but data scientists are. We ask questions like:\\n\\n\\\\begin{itemize}\\n  \\\\item What things might you be able to learn from a given data set?\\n  \\\\item What do you/your people really want to know about the world?\\n  \\\\item What will it mean to you once you find out?\\n\\\\end{itemize}\\n\\nComputer scientists traditionally do not really appreciate data. Think about the way algorithm performance is experimentally measured. Usually the program is run on \"random data\" to see how long it takes. They rarely even look at the results of the computation, except to verify that it is correct and efficient. Since the \"data\" is meaningless, the results cannot be important. In contrast, real data sets are a scarce resource, which required hard work and imagination to obtain.\\n\\nBecoming a data scientist requires learning to ask questions about data, so let\\'s practice. Each of the subsections below will introduce an interesting data set. After you understand what kind of information is available, try to come up with, say, five interesting questions you might explore/answer with access to this data set.\\\\\\n%---- Page End Break Here ---- Page : 4\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-023}\\n\\nFigure 1.1: Statistical information on the performance of Babe Ruth can be found at \\\\href{http://www.baseball-reference.com}{http://www.baseball-reference.com}.\\n\\nThe key is thinking broadly: the answers to big, general questions often lie buried in highly-specific data sets, which were by no means designed to contain them.\\n\\n\\\\subsection*{1.2.1 The Basebal\\\\index{Barzu\\\\index{baseball encyclopedia}n, Jacques}l Encyclopedia}\\nBaseball has long had an outsized importance in the world of data science. This sport has been called the national pastime of the United States; indeed, French historian Jacques Barzun observed that \"Whoever wants to \\\\index{General Sentiment}know the heart and mind of America had better \\\\index{natural language processing}learn baseball.\" I realize that many readers are not Ame\\\\index{Stony Brook University}rican, and even those that a\\\\index{Wikipedia}re might be completely disinterested in sports. But stick with me for a while.\\n\\nWhat makes baseball important to data science is its extensive statistical record of play, dating back for well over a hundred years. Baseball is a sport of discrete events: pitchers throw balls and batters try to hit them - that naturally lends itself to informative statistics. Fans get immersed in these statistics as children, building their intuition about the strengths and limitations of quantitative analysis. Some of these children grow up to become data scientists. Indeed, the success of Brad Pitt\\'s statistically-minded baseball team in the movie Moneyball\\\\index{Moneyball} remains the American public\\'s most vivid contact with data science.\\n\\nThis historical baseball record is available at \\\\href{http://www}{http://www}. baseball-reference . com. There you will find complete statistical data on the performance of every player who even stepped on the field. This includes summary statistics of each season\\'s batting, pitching, and fielding record, plus information about teams\\\\\\n%---- Page End Break Here ---- Page : 5\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-024}\\n\\nFigure 1.2: Personal information on every major league baseball\\\\index{major league baseball} player is available at \\\\href{http://www.baseball-reference.com}{http://www.baseball-reference.com}.\\\\\\\\\\nand awards as shown in Figure 1.1\\\\\\\\\\nBut more than just statistics, there is metadata\\\\index{metadata} on the life and careers of all the people who have ever played major league baseball, as shown in Figure 1.2 We get the vital statistics of each player (height, weight, handedness) and their lifespan (when/where they were born and died). We also get salary information (how much each player got paid every season) and transaction data (how did they get to be the property of each team they played for).\\n\\nNow, I realize that many of you do not have the slightest knowledge of or interest in baseball. This sport is somewhat reminiscent of cricket, if that helps. But remember that as a data scientist, it is your job to be interested in the world around you. Think of this as chance to learn something.\\n\\nSo what interesting questions can you answer with this baseball data set? Try to write down five questions before moving on. Don\\'t worry, I will wait here for you to finish.\\n\\nThe most obvious types of questions to answer with this data are directly related to baseball:\\n\\n\\\\begin{itemize}\\n  \\\\item How can we best measure an individual player\\'s skill or value?\\n  \\\\item How fairly do trades between teams generally work out?\\n  \\\\item What is the general trajectory of player\\'s performance level as they mature and age?\\n  \\\\item To what extent does batting performance correlate with position played? For example, are outfielders really better hitters than infielders?\\n\\\\end{itemize}\\n\\nThese are interesting questions. But even more interesting are questions about demographic and social issues. Almost 20,000 major league baseball play-\\\\\\n%---- Page End Break Here ---- Page : 6\\n\\\\\\ners have taken the field over the past 150 years, providing a large, extensivelydocumented cohort of men who can serve as a proxy for even larger, less welldocumented populations. Indeed, we can use this baseball player data to answer questions like:\\n\\n\\\\begin{itemize}\\n  \\\\item Do left-handed people have shorter lifespans than right-handers? Handedness is not captured in most demographic data sets, but has been diligently assembled here. Indeed, analysis of this data set has been used to show that right-handed people live longer than lefties [HC88]\\n  \\\\item How often do people return to live in the same place where they were born? Locations of birth and death have been extensively recorded in this data set. Further, almost all of these people played at least part of their career far from home, thus exposing them to the wider world at a critical time in their youth.\\n  \\\\item Do player salaries generally reflect past, present, or future performance?\\n  \\\\item To what extent have heights and weights been increasing in the population at large?\\n\\\\end{itemize}\\n\\nThere are two particular themes to be aware of here. First, the identifiers and reference tags (i.e. the metadata) often prove more interesting in a data set than the stuff we are supposed to care about, here the statistical record of play.\\n\\nSecond is the idea of a statistical proxy\\\\index{statistical proxy}, where you use the data set you have to substitute for the one you really want. The data set of your dreams likely does not exist, or may be locked away behind a corporate wall even if it does. A good data scientist is a pragmatist, seeing what they can do with what they have instead of bemoaning what they cannot get their hands on.\\n\\n\\\\subsection*{1.2.2 The Internet Movie Database\\\\index{Internet Movie Database} (IMDb\\\\index{IMDb})}\\nEverybody loves the movies. The Internet Movie Database (IMDb) provides crowdsourced and curated data about all aspects of the motion picture industry, at \\\\href{http://www.imdb.com}{www.imdb.com}. IMDb currently contains data on over 3.3 million movies and TV programs. For each film, IMDb includes its title, running time, genres, date of release, and a full list of cast and crew. There is financial data about each production, including the budget for making the film and how well it did at the box office.\\n\\nFinally, there are extensive ratings for each film from viewers and critics. This rating data consists of scores on a zero to ten stars scale, cross-tabulated into averages by age and gender. Written reviews are often included, explaining why a particular critic awarded a given number of stars. There are also links between films: for example, identifying which other films have been watched most often by viewers of It\\'s a Wonderful Life.\\n\\nEvery actor, director, producer, and crew member associated with a film merits an entry in IMDb, which now contains records on 6.5 million people.\\\\\\n%---- Page End Break Here ---- Page : 7\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-026}\\n\\nFigure 1.3: Representative film data from the Internet Movie Database.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-026(1)}\\n\\nWon 1 Oscar. Another 25 wins \\\\& 19 nominations. See more awards *\\n\\nFigure 1.4: Representative actor data from the Internet Movie Database.\\n\\nThese happen to include my brother, cousin, and sister-in-law. Each actor is linked to every film they appeared in, with a description of their role and their ordering in the credits. Available data about each personality includes birth/death dates, height, awards, and family relations.\\n\\nSo what kind of questions can you answer with this movie data?\\n\\nPerhaps the most natural questions to ask IMDb involve identifying the extremes of movies and actors:\\n\\n\\\\begin{itemize}\\n  \\\\item Which actors appeared in the most films? Earned the most money? Appeared in the lowest rated films? Had the longest career or the shortest lifespan?\\n  \\\\item What was the highest rated film each year, or the best in each genre? Which movies lost the most money, had the highest-powered casts, or got the least favorable reviews.\\n\\\\end{itemize}\\n\\nThen there are larger-scale questions one can ask about the nature of the motion picture business itself:\\n\\n\\\\begin{itemize}\\n  \\\\item How well does movie gross correlate with viewer ratings or awards? Do customers instinctively flock to trash, or is virtue on the part of the creative team properly rewarded?\\n  \\\\item How do Hollywood movies compare to Bollywood movies, in terms of ratings, budget, and gross? Are American movies better received than foreign films, and how does this differ between U.S. and non-U.S. reviewers?\\n  \\\\item What is the age distribution of actors and actresses in films? How much younger is the actress playing the wife, on average, than the actor playing the husband? Has this disparity been increasing or decreasing with time?\\n  \\\\item Live fast, die young, and leave a good-looking corpse? Do movie stars live longer or shorter lives than bit players, or compared to the general public?\\n\\\\end{itemize}\\n\\nAssuming that people working together on a film get to know each other, the cast and crew data can be used to build a social network of the movie business. What does the social network of actors look like? The Oracle of Bacon\\\\index{Oracle of Bacon} (\\\\href{https://oracleofbacon.org/}{https://oracleofbacon.org/}) posits Kevin Bacon as the center of the Hollywood universe and \\\\index{statistics}generates the shortest path to Bacon from any other actor. Other actors, like Samuel L. Jackson, prove even more central.\\n\\nMore critically, \\\\index{de MÂ´erÂ´e, Chevalier}can we analyze this data to determine the p\\\\index{pro\\\\index{probability vs. statistics}bability}robability that someone will like a given movie?\\\\\\\\\\\\index{large-s\\\\index{Netï¬ix prize}cale question}index{collaborative ï¬ltering}index{Bacon, Kevin} The technique of collaborative filtering finds people who liked films that I also liked, and recommends other films that they liked as good candidates for me. The 2007 Netflix Prize was a $\\\\$ 1,000,000$ competition to produce a ratings engine $10 \\\\%$ better than the proprietary Netflix system. The ultimate winner of this prize (BellKor) used a variety of data sources and techniques, including the analysis of links [BK07.\\\\\\n%---- Page End Break Here ---- Page : 9\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-028}\\n\\nFigure 1.5: The rise and fall of data processing, as witnessed by Google Ngrams.\\n\\n\\\\subsection*{1.2.3 Google Ngrams}\\nPrinted books have been the primary repository of human knowledge since Gutenberg\\'s invention of movable type in 1439. Physical objects live somewhat uneasily in today\\'s digital world, but technology has a way of reducing everything to data. As part of its mission to organize the world\\'s information, Google undertook an effort to scan all of the world\\'s published books. They haven\\'t quite gotten there yet, but the 30 million books thus far digitized represent over $20 \\\\%$ of all books ever published.\\n\\nGoogle uses this data to improve search results, and provide fresh access to out-of-print books. But perhaps the coolest product is Google Ngrams, an amazing resource for monitoring changes in the cultural zeitgeist. It provides the frequency with which short phrases occur in books published each year. Each phrase must occur at least forty times in their scanned book corpus. This eliminates obscure words and phrases, but leaves over two billion time series available for analysis.\\n\\nThis rich data set shows how language use has changed over the past 200 years, and has been widely applied to cultural trend analysis [MAV ${ }^{+} 11$. Figure 1.5 uses this data to show how the word data fell out of favor when thinking about computing. Data processing was the popular term associated with the computing field during the punched card and spinning magnetic tape era of the 1950s. The Ngrams data shows that the rapid rise of Computer Science did not eclipse Data Processing until 1980. Even today, Data Science remains almost invisible on this scale.\\n\\nCheck out Google Ngrams at \\\\href{http://books.google.com/ngrams}{http://books.google.com/ngrams}. I promise you will enjoy playing with it. Compare hot dog to tofu, science against religion, freedom to justice, and sex vs. marriage, to better understand this fantastic telescope for looking into the past.\\n\\nBut once you are done playing, think of bigger things you could do if you got your hands on this data. Assume you have access to the annual number of references for all words/phrases published in books over the past 200 years.\\n\\n%---- Page End Break Here ---- Page : 10\\n\\nGoogle makes this data freely available. So what are you going to do with it?\\n\\nObserving the time series associated with particular words using the Ngrams Viewer is fun. But more sophisticated historical trends can be captured by aggregating multiple time series together. The following types of questions seem particularly interesting to me:\\n\\n\\\\begin{itemize}\\n  \\\\item How has the amount of cursing changed over time? Use of the fourletter words I am most familiar with seem to have exploded since 1960, although it is perhaps less clear whether this reflects increased cussing or lower publication standards.\\n  \\\\item How often do new words emerge and get popular? Do these words tend to stay in common usage, or rapidly fade away? Can we detect when words change meaning over time, like the transition of gay from happy to homosexual?\\n  \\\\item Have standards of spelling been improving or deteriorating with time, especially now that we have entered the era of automated spell checking? Rarely-occurring words that are only one character removed from a commonly-used word are likely candidates to be spelling errors (e.g. algorithm vs. algorthm). Aggregated over many different misspellings, are such errors increasing or decreasing?\\n\\\\end{itemize}\\n\\nYou can also use this Ngrams corpus to build a language model that captures the meaning and usage of the words in a given language\\\\index{cumulative density function}. We will discuss word embeddings in Section 11.6 .3 which are powerful tools for building language models. Frequency counts reveal which words are most popular. The frequency of word pairs appearing next to each other can be used to improve speech recognition systems, helping to distinguish whether the speaker said that\\'s too bad or that\\'s\\\\index{records from New York} to bad. These millions of books provide an ample data set to build representative models from.\\n\\n\\\\subsection*{1.2.4 New York Taxi Records}\\nEvery financial transaction today leaves a data trail behind it. Following these paths can lead to interesting insights.\\n\\nTaxi cabs form an important part of the urban transportation network\\\\index{urban transportation network}. They roam the streets of the city looking for customers, and then drive them to their destination for a fare proportional to the length of the trip. Each cab contains a metering device to calculate the cost of the trip as a function of time. This meter serves as a record keeping device, and a mechanism to ensure that the driver charges the proper amount for each trip.\\n\\nThe taxi meters currently employed in New York cabs can do many things beyond calculating fares. They act as credit card terminals, providing a way\\n\\n%---- Page End Break Here ---- Page : 11\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\\n\\\\hline\\nVendor ID & passen\\\\index{centrality measures}ger count & trip dis\\\\index{Apple iPhone sales}tance & pick\\\\index{descriptive statistics}up lon\\\\index{mean}gitude & \\\\begin{tabular}{l}\\npickup\\\\_ \\\\\\\\\\nlatitude \\\\\\\\\\n\\\\end{tabular} & dropoff longitude & dropoff\\\\_ & payment type & \\\\( \\\\begin{aligned} & \\\\text { tip_- } \\\\\\\\ & \\\\text { amount } \\\\end{aligned} \\\\) & total amount \\\\\\\\\\n\\\\hline\\n2 & 1 & 7.22 & -73.9998 & 40.74334 & -73.9428 & 40.80662 & 2 & 0 & 30.8 \\\\\\\\\\n\\\\hline\\n1 & 1 & 2.3 & -73.977 & 40.7749 & -73.9783 & 40.74986 & 1 & 2.93 & 16.23 \\\\\\\\\\n\\\\hline\\n1 & 1 & 1.5 & -73.9591 & 40.77513 & -73.9804 & 40.78231 & 1 & 1.65 & 9.95 \\\\\\\\\\n\\\\hline\\n1 & 1 & 0.9 & -73.9766 & 40.78075 & -73.9706 & 40.78885 & 1 & 1.45 & 8.75 \\\\\\\\\\n\\\\hline\\n2 & 1 & 2.44 & -73.9786 & 40.78592 & -73.9974 & 40.7563 & 1 & 2 & 16.3 \\\\\\\\\\n\\\\hline\\n2 & 1 & 3.36 & -73.9764 & 40.78589 & -73.9424 & 40.82209 & 1 & 3.58 & 17.88 \\\\\\\\\\n\\\\hline\\n2 & 2 & 2.34 & -73.9862 & 40.76087 & -73.9569 & 40.77156 & 1 & 1 & 13.8 \\\\\\\\\\n\\\\hline\\n2 & 1 & 10.19 & -73.79 & 40.64406 & -73.9312 & 40.67588 & 2 & 0 & 32.8 \\\\\\\\\\n\\\\hline\\n1 & 2 & 3.3 & -73.9937 & 40.72738 & -73.9982 & 40.7641 & 1 & 2 & 21.3 \\\\\\\\\\n\\\\hline\\n1 & 1 & 1.8 & -73.9949 & 40.74006 & -73.9767 & 40.74934 & 1 & 1.85 & 11.15 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 1.6: Representative fields from the New York city taxi cab data: pick up and dropoff points, distances, and fares.\\\\\\\\\\nfor customers to pay for rides without cash. They are integrated with Global Positioning System\\\\index{Global Positioning System}s (GPS), recording the exact location of every pickup and drop off. And finally, since they are on a wireless network, these boxes can communicate all of this data back to a central server.\\n\\nThe result is a database documenting every single trip by all taxi cabs in one of the world\\'s greatest cities, a small portion of which is shown in Figure 1.6. Because the New York Taxi and Limousine Commission is a public agency, its non-confidential data is available to all under the Freedom of Information Act\\\\index{Freedom of Information Act} (FOA).\\n\\nEvery ride generates two records: one with data on the trip, the other with details of the fare. Each trip is keyed to the medallion (license) of each car coupled with the identifier of each driver. For each trip, we get the time/date of pickup and drop-off, as well as the GPS coordinates (longitude and latitude) of the starting location and destination. We do not get GPS data of the route they traveled between these points, but to some extent that can be inferred by the shortest path between them.\\n\\nAs for fare data, we get the metered cost of each trip, including tax, surcharge and tolls. It is traditional to pay the driver a tip for service, the amount of which is also recorded in the data.\\n\\nSo I\\'m talking to you. This taxi data is readily available, with records of over 80 million trips over the past several years. What are you going to do with it?\\n\\nAny interesting data set can be used to answer questions on many different scales. This taxi fare data can help us better understand the transportation industry, but also how the city works and how we could make it work even better. Natural questions with respect to the taxi industry include:\\\\\\n%---- Page End Break Here ---- Page : 12\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-031}\\n\\nFigure 1.7: Which neighborhoods in New York city tip most generously? The relatively remote outer boroughs of Brooklyn and Queens, where trips are longest and supply is relatively scarce.\\n\\n\\\\begin{itemize}\\n  \\\\item How much money do drivers make each night, on average? What is the distribution? Do drivers make more on sunny days or rainy days?\\n  \\\\item Where are the best spots in the city for drivers to cruise, in order to pick up profitable fares? How does this vary at different times of the day?\\n  \\\\item How far do drivers travel over the course of a night\\'s work? We can\\'t answer this exactly using this data set, because it does not provide GPS data of the route traveled between fares. But we do know the last place of drop off, the next place of pickup, and how long it took to get between them. Together, this should provide enough information to make a sound estimate.\\n  \\\\item Which drivers take their unsuspecting out-of-town passengers for a \"ride,\" running up the meter on what should be a much shorter, cheaper trip?\\n  \\\\item How much are drivers tipped, and why? Do faster drivers get tipped better? How do tipping rate\\\\index{tipping rate}s vary by neighborhood, and is it the rich neighborhoods or poor neighborhoods which prove more generous?\\\\\\\\\\nI will confess we did an analysis of this, which I will further describe in the war story of Section 9.3 . We found a variety of interesting patterns SS15. Figure 1.7 shows that Manhattanites are generally cheapskates relative to large swaths of Brooklyn, Queens, and Staten Island, where trips are longer and street cabs a rare but welcome sight.\\n\\n%---- Page End Break Here ---- Page : 13\\n\\\\end{itemize}\\n\\nBut the bigger questions have to do with understanding transportation in the city. We can use the taxi travel times as a sensor to measure the level of traffic in the city at a fine level. How much slower is traffic during rush hour than other times, and where are delays the worst? Identifying problem areas is the first step to proposing solutions, by changing the timing patterns of traffic lights, running more buses, or creating high-occupancy only lanes.\\n\\nSimilarly we can use the taxi data to measure transportation flows across the city. Where are people traveling to, at different tim\\\\index{Pearson correlation coeï¬cient}es of the day? This tells us much more than just congestion. By looking at the taxi data, we should be able to see tourists going from hotels to attractions, executives from fancy neighborhoods to Wall Street, and drunks returning home from nightclubs after a bender.\\n\\nData like this is essential to designing better transportation systems. It is wasteful for a single rider to travel from point $a$ to point $b$ when there is another rider at point $a+\\\\epsilon$ who also wants to get there. Analysis of the taxi data enables accurate simulation of a ride sharing system, so we can accurately evaluate the demands and cost reductions of such a service.\\n\\n\\\\subsection*{1.3 properties\\\\index{properties} of Data}\\nThis book is about techniques for analyzing data. But what is the underlying stuff that we will be studying? This section provides a brief taxonomy of the properties of data, so we can better appreciate and understand what we will be working on.\\n\\n\\\\subsection*{1.3.1 structured\\\\index{structured} vs. unstructured\\\\index{unstructured} Data}\\nCertain data sets are nicely structured, like the tables in a database or spreadsheet program. Others record information about the state of the world, but in a more heterogeneous way. Perhaps it is a large text corpus with images and links like Wikipedia, or the complicated mix of notes and test results appearing in personal medical records.\\n\\nGenerally speaking, this book will focus on dealing with structured data. Data is often represented by a matrix\\\\index{matrix}, where the r\\\\index{types}ows of the matrix represent distinct items or records, and the columns represent distinct properties of these items. For example, a data set about U.S. cities might contain one row for each city, with columns representing features like state, population, and area.\\n\\nWhen confronted with an unstructured data source, such as a collection of tweets from Twitter, our first step is generally to build a matrix to structure it. A bag of words\\\\index{bag of words} model will construct a matrix with a row for each tweet, and a column for each frequently used vocabulary word. Matrix entry $M[i, j]$ then denotes the number of times tweet $i$ contains word $j$. Such matrix formulations will motivate our discussion of linear algebra, in Chapter 8\\n\\n\\n%---- Page End Break Here ---- Page : 14\\n\\\\subsection*{1.3.2 quantitative vs. categorical\\\\index{quantitative vs. categorical} Data}\\nQuantitative data consists of numerical values, like height and weight. Such data can be incorporated directly into algebraic formulas and mathematical models, or displayed in conventional graphs and charts.\\n\\nBy contrast, categorical data consists of labels describing the properties of the objects under investigation, like gender, hair color, and occupation. This descriptive information can be every bit as precise and meaningful as numerical data, but it cannot be worked with using the same techniques.\\n\\nCategorical data can usually be coded numerically. For example, gender might be represented as male $=0$ or female $=1$. But things get more complicated when there are more than two characters per feature, especially when there is not an implicit order between them. We may be able to encode hair colors as numbers by assigning each shade a distinct value like gray hair $=0$, red hair $=1$, and blond hair $=2$. However, we cannot really treat these values as numbers, for anything other than simple identity testing. Does it make any sense to talk about the maximum or minimum hair color? What is the interpretation of my hair color minus your hair color?\\n\\nMost of what we do in this book will revolve around numerical data. But keep an eye out for categorical features, and methods that work for them. Classification and clustering methods can be thought of as generating categorical labels from numerical data,\\\\index{big vs. little} and will be a primary focus in this book.\\n\\n\\\\subsection*{1.3.3 Big Data vs. Little Data}\\nData science has become conflated in the public eye with big data, the analysis of massive data sets resulting from computer logs and sensor devices. In principle, having more data is always better than having less, because you can always throw some of it away by sampling to get a smaller set if necessary.\\n\\nBig data is an exciting phenomenon, and we will discuss it in Chapter 12 But in practice, there are difficulties in working with large data sets. Throughout this book we will look at algorithms and best practices for analyzing data. In general, things get harder once the volume gets too large. The challenges of big data include:\\n\\n\\\\begin{itemize}\\n  \\\\item The analysis cycle time slows as data size grows: Computational operations on data sets take longer as their volume increases. Small spreadsheets provide instantaneous response, allowing you to experiment and play what $i f$ ? But large spreadsheets can be slow and clumsy to work with, and massive-enough data sets might take hours or days to get answers from.\\n\\\\end{itemize}\\n\\nClever algorithms can permit amazing things to be done with big data, but staying small generally leads to faster analysis and exploration.\\n\\n\\\\begin{itemize}\\n  \\\\item Large data sets are complex to visualize: Plots with millions of points on them are impossible to display on computer screens or printed images, let alone conceptually understand. How can we ever hope to really understand something we cannot see?\\n \\n%---- Page End Break Here ---- Page : 15\\n \\\\item Simple models do not require massive data to fit or evaluate: A typical data science task might be to make a decision (say, whether I should offer this fellow life insurance?) on the basis of a small number of variables: say age, gender, height, weight, and the presence or absence of existing medical conditions.\\n\\\\end{itemize}\\n\\nIf I have this data on 1 million people with their associated life outcomes, I should be able to build a good general model of coverage risk. It probably wouldn\\'t help me build a substantially better model if I had this data on hundreds of millions of people. The decision criteria on only a few variables (like age and martial status) cannot be too complex, and should be robust over a large number of applicants. Any observation that is so subtle it requires massive data to tease out will prove irrelevant to a large business which is based on volume.\\n\\nBig data is sometimes called bad data. It is often gathered as the by-product of a given system or procedure, instead of being purposefully collected to answer your question at hand. The result is that we might have to go to heroic efforts to make sense o\\\\index{classiï¬cation}f something just because we have it.\\n\\nConsider the problem of getting a pulse on voter preferences among presidential candidates. The big data approach might analyze massive Twitter or Facebook feeds, interpreting clues to their opinions in the text. The small data approach might be to conduct a poll, asking a few hundred people this specific question and tabulating the results. Which procedure do you think will prove more accurate? The right data set is the one most directly relevant to the tasks at hand, not necessarily the biggest one.\\n\\nTake-Home Lesson: Do not blindly aspire to analyze large data sets. Seek the right data to answer a given question, not necessarily the biggest thing you can get your hands on.\\n\\n\\\\subsection*{1.4 Classification and regression\\\\index{regression}}\\nTwo types of problems arise repeatedly in traditional data science and pattern recognition applications, the challenges of classification and regression. As this book has developed, I have pushed discussions of the algorithmic approaches to solving these problems toward the later chapters, so they can benefit from a solid understanding of core material in data munging, statistics, visualization, and mathematical modeling.\\n\\nStill, I will mention issues related to classification and regression as they arise, so \\\\index{si\\\\index{correlation and causation}gniï¬cance}it makes sense to pause here for a quick introduction to these problems, to help you recognize them when you see them.\\n\\n\\\\begin{itemize}\\n  \\\\item Classification: Often we seek to assign a label to an item from a discrete set of possibilities. Such problems as predicting the winner of a particular\\\\\\n%---- Page End Break Here ---- Page : 16\\n\\\\\\nsporting contest (team $A$ or team $B$ ?) or deciding the genre of a given movie (comedy, drama, or animation?) are classification problems, since each entail selecting a label from the possible choices.\\n  \\\\item Regression: Another common task is to forecast a given numerical quantity. Predicting a person\\'s weight or how much snow we will get this year is a regression problem, where we forecast the future value of a numerical function in terms of previous values and other relevant features.\\n\\\\end{itemize}\\n\\nPerhaps the best way to see the intended distinction is to look at a variety of data science problems and label (classify) them as regression or classification. Different algorithmic methods are used to solve these two types of problems, although the same questions can often be approached in either way:\\n\\n\\\\begin{itemize}\\n  \\\\item Will the price of a particular stock be higher or lower tomorrow? (classification)\\n  \\\\item What will the price of a particular stock be tomorrow? (regression)\\n  \\\\item Is this person a good risk to sell an insurance policy to? (classification)\\n  \\\\ite\\\\\\\\index{cicada}index{autocorrelation}m How long do we expect this person to live? (regression)\\n\\\\end{itemize}\\n\\nKeep your eyes open for classification and regression problems as you encounter them in your life, and in this book.\\n\\n\\\\subsection*{1.5 data science television\\\\index{data science television}: The Quant Shop\\\\index{Quant Shop}}\\nI believe that hands-on experience is necessary to internalize basic principles. Thus when I teach data science, I like to give each student team an interesting but messy forecasting challenge, and demand that they build and evaluate a predictive model for the task.\\n\\nThese forecasting challenges are associated with events where the students must make testable predictions. They start from scratch: finding the relevant data sets, building their own evaluation environments, and devising their model. Finally, I make them watch the event as it unfolds, so as to witness the vindication or collapse of their prediction.\\n\\nAs an experiment, we documented the evolution of each group\\'s project on video in Fall 2014. Professionally edited, this became The Quant Shop, a television-like data science series for a general audience. The eight episodes of this first season are available at \\\\href{http://www.quant-shop.com}{http://www.quant-shop.com} and include:\\n\\n\\\\begin{itemize}\\n  \\\\item Finding Miss Universe - The annual Miss Universe competition aspires to identify the most beautiful woman in the world. Can computational models predict who will win a beauty contest? Is beauty just subjective, or can algorithms tell who is the fairest one of all?\\n \\n%---- Page End Break Here ---- Page : 17\\n \\\\item Modeling the Movies - The business of movie making involves a lot of high-stakes data analysis. Can we build models to predict which film will gross the most on Christmas day? How about identifying which actors will receive awards for their performance?\\n  \\\\item Winning the Baby Pool - Birth weight is an important factor in assessing the health of a newborn child. But how accurately can we predict junior\\'s weight before the actual birth? How can data clarify environmental risks to developing pregnancies?\\n  \\\\item The Art of the Auction - The world\\'s most valuable artworks sell at auctions to the highest bidder. But can we predict how many millions a particular J.W. Turner painting will sell for? Can computers develop an artistic sense of what\\'s worth buying?\\n  \\\\item White Christmas - Weather forecasting is perhaps the most familiar domain of predictive modeling. Short-term forecasts are generally accurate, but what about longer-term prediction? What places will wake up to a snowy Christmas this year? And can you tell one month in advance?\\n  \\\\item Predicting the Playoffs - Sports events have winners and losers, and bookies are happy to take your bets on the outcome of any match. How well can statistics help predict which football team will win the Super Bowl? Can Google\\'s PageRank algorithm pick the winners on the field as accurately as it does on the web?\\n  \\\\item The Ghoul Pool - Death comes to all men, but when? Can we apply actuarial models to celebrities, to decide who will be the next to die? Similar analysis underlies the workings of the life insurance industry, where accurate predictions of lifespan are necessary to set premiums which are both sustainable and affordable.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-036}\\n\\\\end{itemize}\\n\\nFigure 1.8: Exciting scenes from data science television: The Quant Shop.\\n\\n\\\\begin{itemize}\\n  \\\\item Playing the Market - Hedge fund quants get rich when guessing right about tomorrow\\'s prices, and poor when wrong. How accurately can we predict future prices of gold and oil using histories of price data? What other information goes into building a successful price model?\\n\\\\end{itemize}\\n\\nI encourage you to watch some episodes of The Quant Shop in tandem with reading this book. We try to make it fun, although I am sure you will find plenty of things to cringe at. Each show runs for thirty minutes, and maybe will inspire you to tackle a prediction challenge of your own.\\n\\nThese programs will certainly give you more insight into these eight specific challenges. I will use these projects throughout this book to illustrate important lessons in how to do data science, both as positive and negative examples. These projects provide a laboratory to see how intelligent but inexperienced people not wildly unlike yourself thought about a data science problem, and what happened when they did.\\n\\n\\\\subsection*{1.5.1 Kaggle Challenges}\\nAnother source of inspiration are challenges from Kaggle (\\\\href{http://www.kaggle.com}{www.kaggle.com}), which provides a competitive forum for data scientists. New challenges are posted on a regular basis, providing a problem definition, training data, and a scoring function over hidden evaluation data. A leader board displays the scores of the strongest competitors, so you can see how well your model stacks up in comparison with your opponents. The winners spill their modeling secrets during post-contest interviews, to help you improve your modeling skills.\\n\\nPerforming well on Kaggle challenges is an excellent credential to put on your resume to get a good job as a data scientist. Indeed, potential employers will track you down if you are a real Kaggle star. But the real reason to participate is that the problems are fun and inspiring, and practice helps make you a better data scientist.\\n\\nThe exercises at the end of each chapter point to expired Kaggle challenges, loosely connected to the material in that chapter. Be forewarned that Kaggle provides a misleading glamorous view of data science as applied machine learning, because it presents extremely well-defined problems with the hard work of data collection and cleaning already done for you. Still, I encourage you to check it out for inspiration, and as a source of data for new projects.\\n\\n\\\\subsection*{1.6 About the War Stories}\\ngenius\\\\index{genius} and wisdom\\\\index{wisdom} are two distinct intellectual gifts. Genius shows in discovering the right answer, making imaginative mental leaps which overcome obstacles and challenges. Wisdom shows in avoiding obstacles in the first place, providing a sense of direction or guiding light that keeps us moving soundly in the right direction.\\n\\n%---- Page End Break Here ---- Page : 19\\n\\nGenius is manifested in technical strength and depth, the ability to see things and do things that other people cannot. In contrast, wisdom comes from experience and general knowledge. It comes from listening to others. Wisdom comes from humility, observing how often you have been wrong in the past and figuring out why you were wrong, so as to better recognize future traps and avoid them.\\n\\nData science, like most things in life, benefits more from wisdom than from genius. In this book, I seek to pass on wisdom that I have accumulated the hard way through war stories, gleaned from a diverse set of projects I have worked on:\\n\\n\\\\begin{itemize}\\n  \\\\item Large-scale text analytics and NLP: My Data Science Laboratory at Stony Brook University works on a variety of projects in big data, including sentiment analysis\\\\index{analysis} from social media, historical trends analysis, deep learning approaches to natural language processing (NLP), and feature extraction from networks.\\n  \\\\item Start-up companies: I served as co-founder and chief scientist to two data analytics companies: General Sentiment and Thrivemetrics. General Sentiment analyzed large-scale text streams from news, blogs, and social media to identify trends in the sentiment (positive or negative) associated with people, places, and things. Thrivemetrics applied this type of analysis to internal corporate communications, like email and messaging systems.\\\\\\\\\\nNeither of these ventures left me wealthy enough to forgo my royalties from this book, but they did provide me with experience on cloud-based computing systems, and insight into how data is used in industry.\\n  \\\\item Collaborating with real scientists: I have had several interesting collaborations with biologists and social scientists, which helped shape my understanding of the complexities of working with real data. Experimental data is horribly noisy and riddled with errors, yet you must do the best you can with what you have, in order to discover how the world works.\\n  \\\\item Building gambling systems: A particularly amusing project was building a system to predict the results of jai-alai matches so we could bet on them, an experience recounted in my book Calculated Bets: Computers, Gambling, and Mathematical Modeling to Win Ski01. Our system relied on web scraping for data collection, statistical analysis, simulation/modeling, and careful evaluation. We also have developed and evaluated predictive models for movie grosses [ZS09], stock prices [ZS10], and football games HS10 using social media analysis.\\n  \\\\item Ranking historical figures: By analyzing Wikipedia to extract meaningful variables on over 800,000 historical figures, we developed a scoring function to rank them by their strength as historical memes. This ranking does a great job separating the greatest of the great (Jesus, Napoleon, Shakespeare, Mohammad, and Lincoln round out the top five) from lesser\\\\\\n%---- Page End Break Here ---- Page : 20\\n\\\\\\nmortals, and served as the basis for our book Who\\'s Bigger?: Where Historical Figures Really Rank SW13.\\n\\\\end{itemize}\\n\\nAll this experience drives what I teach in this book, especially the tales that I describe as war stories. Every one of these war stories is true. Of course, the stories improve somewhat in the retelling, and the dialogue has been punched up to make them more interesting to read. However, I have tried to honestly trace the process of going from a raw problem to a solution, so you can watch how it unfolded.\\n\\n\\\\subsection*{1.7 War Story: Answering the Right Question}\\nOur research group at Stony Brook University developed an NLP-based system\\\\index{NLP-based system} for analyzing millions of news, blogs and social media messages, and reducing this text to trends concerning all the entities under discussion. Counting the number of mentions each name receives in a text stream (volume) is easy, in principle. Determining whether the connotation of a particular reference is positive or negative (sentiment analysis) is hard. But our system did a pretty good job, particularly when aggregated over many references.\\n\\nThis technology served as the foundation for a social media analysis company named General Sentiment. It was exciting living through a start-up starting up, facing the challenges of raising money, hiring staff, and developing new products.\\n\\nBut perhaps the biggest problem we faced was answering the right question. The General Sentiment system recorded trends about the sentiment and volume for every person, place, and thing that was ever mentioned in news, blogs, and social media: over 20 million distinct entities. We monitored the reputations of celebrities and politicians. We monitored the fates of companies and products. We tracked the performance of sports teams, and the buzz about movies. We could do anything!\\n\\nBut it turns out that no one pays you to do anything. They pay you to do something, to solve a particular problem they have, or eliminate a specific pain point in their business. Being able to do anything proves to be a terrible sales strategy, because it requires you to find that need afresh for each and every customer.\\n\\nFacebook\\\\index{Facebook} didn\\'t open up to the world until September 2006. So when General Sentiment started in 2008, we were at the very beginning of the social media era. We had lots of interest from major brands and advertising agencies which knew that social media was ready to explode. They knew this newfangled thing was important, and that they had to be there. They knew that proper analysis of social media data could give them fresh insights into what their customers were thinking. But they didn\\'t know exactly what it was they really wanted to know.\\n\\nOne aircraft engine manufacturer was very interested in learning how much the kids talked about them on Facebook. We had to break it to them gently that the answer was zero. Other potential customers demanded proof that we\\\\\\n%---- Page End Break Here ---- Page : 21\\n\\\\\\nwere more accurate than the Nielsen television ratings. But of course, if you wanted Nielsen ratings then you should buy them from Nielsen. Our system provided different insights from a completely different world. But you had to know what you wanted in order to use them.\\n\\nWe did manage to get substantial contracts from a very diverse group of customers, including consumer brands like Toyota and Blackberry, governmental organizations like the Hawaii tourism office, and even the presidential campaign of Republican nominee Mitt Romney in 2012. Our analysts provided them insights into a wide variety of business issues:\\n\\n\\\\begin{itemize}\\n  \\\\item What did people think about Hawaii? (Answer: they think it is a very nice place to visit.)\\n  \\\\item How quickly would Toyota\\'s sentiment recover after news of serious brake problems in their cars? (Answer: about six months.)\\n  \\\\item What did people think about Blackberry\\'s new phone models? (Answer: they liked the iPhone much better.)\\n  \\\\item How quickly would Romney\\'s sentiment recover after insulting $47 \\\\%$ of the electorate in a recorded speech? (Answer: never.)\\n\\\\end{itemize}\\n\\nBut each sale required entering a new universe, involving considerable effort and imagination on the part of our sales staff and research analysts. We never managed to get two customers in the same industry, which would have let us benefit from scale and accumulated wisdom.\\n\\nOf course, the customer is always right. It was our fault that we could not explain to them the best way to use our technology. The lesson here is that the world will not beat a path to your door just for a new source of data. You must be able to supply the right questions before you can turn data into money.\\n\\n\\\\subsection*{1.8 Chapter Notes}\\nThe idea of using historical records from baseball players to establish that lefthanders have shorter lifespans is due to Halpern and Coren HC88, HC91, but their conclusion remains controversial. The percentage of left-handers in the population has been rapidly growing, and the observed effects may be a function of survivorship bias [McM04]. So lefties, hang in there! Full disclosure: I am one of you.\\n\\nThe discipline of quantitative baseball analysis is sometimes calle\\\\in\\\\index{notebook environments}dex{Mathematica}d sabermetrics\\\\index{sabermetrics}\\\\index{Java}, and its leading light is \\\\index{Wolfram Alpha}a fello\\\\index{Excel}w named Bill James. I recommend budding data scientists read his Historical Baseball Abstract Jam10 as an excellent example of how one turns numbers into knowled\\\\index{Câlanguage}ge and understanding. Time Magazine once said of James: \"Much of the joy of reading him comes from the extravagant spectacle of a first-rate mind wasting itself on baseball.\" I thank \\\\href{http://sports-reference.com}{http://sports-reference.com} for permission to use images of their website in this book. Ditto to Amazon, the owner of IMDb.\\n\\n%---- Page End Break Here ---- Page : 22\\n\\nThe potential of ride-sharing systems in New York was studied by Santi et. al. SRS $^{+} 14$, who showed that almost $95 \\\\%$ of the trips could have been shared with no more than five minutes delay per trip.\\n\\nThe Lydia system for sentiment analysis is described in GSS07. Methods to identify changes in word meaning through analysis of historical text corpora like Google Ngram are reported in KARPS15.\\n\\n\\\\subsection*{1.9 exercises\\\\index{exercises}}\\n\\\\section*{Identifying Data Sets}\\n1-1. [3] Identify where interesting data sets relevant to the following domains can be found on the web:\\\\\\\\\\n(a) Books.\\\\\\\\\\n(b) Horse racing.\\\\\\\\\\n(c) Stock prices.\\\\\\\\\\n(d) Risks of diseases.\\\\\\\\\\n(e) Colleges and universities.\\\\\\\\\\n(f) Crime rates.\\\\\\\\\\n(g) Bird watching.\\n\\nFor each of these data sources, explain what you must do to turn this data into a usable format on your computer for analysis.\\\\\\\\[0pt]\\n1-2. [3] Propose relevant data sources for the following The Quant Shop prediction challenges. Distinguish between sources of data that you are sure somebody must have, and those where the data is clearly available to you.\\\\\\\\\\n(a) Miss Universe.\\\\\\\\\\n(b) Movie gross.\\\\\\\\\\n(c) Baby weight.\\\\\\\\\\n(d) Art auction price.\\\\\\\\\\n(e) White Christmas.\\\\\\\\\\n(f) Football champions.\\\\\\\\\\n(g) Ghoul pool.\\\\\\\\\\n(h) Gold/oil prices.\\n\\n1-3. [3] Visit \\\\href{http://data.gov}{http://data.gov}, and identify five data sets that sound interesting to you. For each write a brief description, and propose three interesting things you might do with them.\\n\\n\\\\section*{Asking Questions}\\n1-4. [3] For each of the following data sources, propose three interesting questions you can answer by analyzing them:\\\\\\\\\\n(a) Credit card billing data.\\\\\\n%---- Page End Break Here ---- Page : 23\\n\\\\\\n(b) Click data from \\\\href{http://www.Amazon.com}{http://www.Amazon.com}\\\\\\\\\\n(c) White Pages residential/commercial telephone directory.\\n\\n1-5. [5] Visit Entrez, the National Center for Biotechnology Information (NCBI) portal. Investigate what data sources are available, particularly the Pubmed and Genome resources. Propose three interesting projects to explore with each of them.\\n\\n1-6. [5] You would like to conduct an experiment to establish whether your friends prefer the taste of regular Coke or Diet Coke. Briefly outline a design for such a study.\\\\\\\\[0pt]\\n1-7. [5] You would like to conduct an experiment to see whether students learn better if they study without any music, with instrumental music, or with songs that have lyrics. Briefly outline the design for such a study.\\n\\n1-8. [5] Traditional polling operations like Gallup use a procedure called random digit dialing, which dials random strings of digits instead of picking phone numbers from the phone book. Suggest why such polls are conducted using random digit dialing.\\n\\n\\\\section*{Implementation Projects}\\n1-9. [5] Write a program to scrape the best-seller rank for a book on \\\\href{http://Amazon.com}{Amazon.com}. Use this to plot the rank of all of Skiena\\'s books over time. Which one of these books should be the next item that you purchase? Do you have friends for whom they would make a welcome and appropriate gift? :-)\\n\\n1-10. [5] For your favorite sport (baseball, football, basketball, cricket, or soccer) identify a data set with the historical statistical records for all major participants. Devise and implement a ranking system to identify the best player at each position.\\n\\n\\\\section*{Interview Questions}\\n1-11. [3] For each of the following questions: (1) produce a quick guess based only on your understanding of the world, and then (2) use Google to find supportable numbers to produce a more principled estimate from. How much did your two estimates differ by?\\\\\\\\\\n(a) How many piano tuners are there in the entire world?\\\\\\\\\\n(b) How much does the ice in a hockey rink weigh?\\\\\\\\\\n(c) How many gas stations are there in the United States?\\\\\\\\\\n(d) How many people fly in and out of LaGuardia Airport every day?\\\\\\\\\\n(e) How many gallons of ice cream are sold in the United States each year?\\\\\\\\\\n(f) How many basketballs are purchased by the National Basketball Association (NBA) each year?\\\\\\\\\\n(g) How many fish are there in all the world\\'s oceans?\\\\\\\\\\n(h) How many people are flying in the air right now, all over the world?\\\\\\\\\\n(i) How many ping-pong balls can fit in a large commercial jet?\\\\\\\\\\n(j) How many miles of paved road are there in your favorite country?\\\\\\n%---- Page End Break Here ---- Page : 24\\n\\\\\\n(k) How many dollar bills are sitting in the wallets of all people at Stony Brook University?\\\\\\\\\\n(1) How many gallons of gasoline does a typical gas station sell per day?\\\\\\\\\\n(m) How many words are there in this book?\\\\\\\\\\n(n) How many cats live in New York city?\\\\\\\\\\n(o) How much would it cost to fill a typical car\\'s gas tank with Starbuck\\'s coffee?\\\\\\\\\\n(p) How much tea is there in China?\\\\\\\\\\n(q) How many checking accounts are there in the United States?\\n\\n1-12. [3] What is the difference between regression and classification?\\\\\\\\[0pt]\\n1-13. [8] How would you build a data-driven recommendation system? What are the limitations of this approach?\\\\\\\\[0pt]\\n1-14. [3] How did you become interested in data science?\\\\\\\\[0pt]\\n1-15. [3] Do you think data science is an art or a science?\\n\\n\\\\section*{Kaggle Challenges}\\n1-16. Who survived the shipwreck of the Titanic?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/titanic}{https://www.kaggle.com/c/titanic}\\\\\\\\\\n$1-17$. Where is a particular taxi cab going?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i}{https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i}\\\\\\\\\\n1-18. How long will a given taxi trip take?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii}\\n%---- Page End Break Here ---- Page : 25\\n{https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii}\\n\\n\\\\section*{Chapter 2}\\n\\\\section*{Mathematical Preliminaries}\\nA data scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician.\\n\\n\\\\begin{itemize}\\n  \\\\item Josh Blumenstock\\n\\\\end{itemize}\\n\\nYou must walk before you can run. Similarly, there is a certain level of mathematical maturity which is necessary before you should be trusted to do anything meaningful with numerical data.\\n\\nIn writing this book, I have assumed that the reader has had some degree of exposure to probability\\\\index{probability} and statistics, linear algebra, and continuous mathematics. I have also assumed that they have probably forgotten most of it, or perhaps didn\\'t always see the forest (why things are important, and how to use them) for the trees (all the details of definitions, proofs, and operations).\\n\\nThis chapter will try to refresh your understanding of certain basic mathematical concepts. Follow along with me, and pull out your old textbooks if necessary for future reference. Deeper concepts will be introduced later in the book when we need them.\\n\\n\\\\subsection*{2.1 Probability}\\nProbability theory provides a formal framework for reasoning about the likelihood of events. Because it is a formal discipline, there are a thicket of associated definitions to instantiate exactly what we are reasoning about:\\n\\n\\\\begin{itemize}\\n  \\\\item An experiment\\\\index{experiment} is a procedure which yields one of a set of possible outcomes. As our ongoing example, consider the experiment of tossing two six-sided dice, one red and one blue, with each face baring a distinct integer $\\\\{1, \\\\ldots, 6\\\\}$.\\n  \\\\item A sample space\\\\index{sample space} $S$ is the set of possible outcomes of an experiment. In our\\\\\\\\\\ndice example, there are 36 possible outcomes, namely\\n\\\\end{itemize}\\n\\n$$\\n\\\\begin{aligned}\\nS=\\\\{ & (1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,1),(2,2),(2,3),(2,4),(2,5),(2,6), \\\\\\\\\\n& (3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(4,1),(4,2),(4,3),(4,4),(4,5),(\\\\index{Blumenstock, Josh}4,6) \\\\\\\\\\n& (5,1),(5,2),(5,3),(5,4),(5,5),(5,6),(6,1),(6,2),(6,3),(6,4),(6,5),(6,6)\\\\} .\\n\\\\end{aligned}\\n$$\\n\\n\\\\begin{itemize}\\n  \\\\item An event $E$ is a specified subset of the outcomes of an experiment. The event that the sum of the dice equals 7 or 11 (the conditions to win at craps on the first roll) is the subset\\n\\\\end{itemize}\\n\\n$$\\nE=\\\\{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1),(5,6),(6,5)\\\\}\\n$$\\n\\n\\\\begin{itemize}\\n  \\\\item The probability of an outcome $s$, denoted $p(s)$ is a number with the two properties:\\n  \\\\item For each outcome $s$ in sample space $S, 0 \\\\leq p(s) \\\\leq 1$.\\n  \\\\item The sum of probabilities of all outcomes adds to one: $\\\\sum_{s \\\\in S} p(s)=1$.\\n\\\\end{itemize}\\n\\nIf we assume two distinct fair dice, the probability $p(s)=(1 / 6) \\\\times(1 / 6)=$ $1 / 36$ for all outcomes $s \\\\in S$.\\n\\n\\\\begin{itemize}\\n  \\\\item The probability of an event $E$ is the sum of the probabilities of the outcomes of the experiment. Thus\\n\\\\end{itemize}\\n\\n$$\\np(E)=\\\\sum_{s \\\\in E} p(s)\\n$$\\n\\nAn alternate formulation is in terms of the complement of the event $\\\\bar{E}$, the case when $E$ does not occur. Then\\n\\n$$\\nP(E)=1-P(\\\\bar{E}) .\\n$$\\n\\nThis is useful, because often it is easier to analyze $P(\\\\bar{E})$ than $P(E)$ directly.\\n\\n\\\\begin{itemize}\\n  \\\\item A random variable $V$ is a numerical function on the outcomes of a probability space. The function \"sum the values of two dice\" $(V((a, b))=a+b)$ produces an integer result between 2 and 12. This implies a probability distribution of the values of the random variable. The probability $P(V(s)=7)=1 / 6$, as previously shown, while $P(V(s)=12)=1 / 36$.\\n  \\\\item The expected value of a random variable $V$ defined on a sample space $S$, $E(V)$ is defined\\n\\\\end{itemize}\\n\\n$$\\nE(V)=\\\\sum_{s \\\\in S} p(s) \\\\cdot V(s)\\n$$\\n\\nAll this you have presumably seen before. But it provides the language we will use to connect between probability and statistics. The data we see usually comes from measuring properties of observed events. The theory of probability and statistics provides the tools to analyze this data.\\n\\n%---- Page End Break Here ---- Page : 28\\n\\n\\\\subsection*{2.1.1 Probability vs. Statistics}\\nProbability and statistics are related areas of mathematics which concern themselves with analyzing the relative frequency of events. Still, there are fundamental differences in the way they see the world:\\n\\nProbability deals with predicting the likelihood of future events, while statistics involves the analysis of the frequency of past events.\\n\\n\\\\begin{itemize}\\n  \\\\item Probability is primarily a theoretical branch of mathematics, which studies the consequences of mathematical definitions. Statistics is primarily an applied branch of mathematics, which tries to make sense of observations in the real world.\\n\\\\end{itemize}\\n\\nBoth subjects are important, relevant, and useful. But they are different, and understanding the distinction is crucial in properly interpreting the relevance of mathematical evidence. Many a gambler has gone to a cold and lonely grave for failing to make the proper distinction between probability and statistics.\\n\\nThis distinction will perhaps become clearer if we trace the thought process of a mathematician encountering her first craps game:\\n\\n\\\\begin{itemize}\\n  \\\\item If this mathematician were a probabilist, she would see the dice and think \"Six-sided dice? Each side of the dice is presumably equally likely to land face up. Now assuming that each face comes up with probability $1 / 6$, I can figure out what my chances are of crapping out.\"\\n  \\\\item If instead a statistician wandered by, she would see the dice and think \"How do I know that they are not loaded? I\\'ll watch a while, and keep track of how often each number comes up. Then I can decide if my observations are consistent with the assumption of equal-probability faces. Once I\\'m confident enough that the dice are fair, I\\'ll call a probabilist to tell me how to bet.\"\\n\\\\end{itemize}\\n\\nIn summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to measure the extent to which our world is ideal. This constant tension between theory and practice is why statisticians prove to be a tortured group of individuals compared with the happy-go-lucky probabilists.\\n\\nModern probability theory first emerged from the dice tables of France in 1654. Chevalier de MÃ©rÃ©, a French nobleman, wondered whether the player or the house had the advantage in a particular betting game. ${ }^{1}$ In the basic version, the player rolls four dice, and wins provided none of them are a 6 . The house collects on the even money bet if at least one 6 appears.\\n\\nDe MÃ©rÃ© brought this problem to the attention of the French mathematicians Blaise Pascal and Pierre de Fermat, most famous as the source of Fermat\\'s Last Theorem. Together, these men worked out the basics of probability theory,\\n\\n\\\\footnotetext{${ }^{1}$ He really shouldn\\'t have wondered. The house always has the advantage.\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-047}\\n\\nFigure 2.1: Venn diagrams illustrating set difference (left), intersection (middle), and union (right).\\\\\\\\\\nalong the way establishing that the house wins this dice game with probability $p=1-(5 / 6)^{4} \\\\approx 0.517$, where the probability $p=0.5$ would denote a fair game where the house wins exactly half the time.\\n\\n\\\\subsection*{2.1.2 Compound Events and independence\\\\index{independence}}\\nWe will be interested in complex events computed from simpler events $A$ and $B$ on the same set of outcomes. Perhaps event $A$ is that at least one of two dice be an even number, while event $B$ denotes rolling a total of either 7 or 11. Note that there exist certain outcomes of $A$ which are not outcomes of $B$, specifically\\n\\n$$\\n\\\\begin{aligned}\\nA-B=\\\\{ & (1,2),(1,4),(2,1),(2,2),(2,3),(2,4),(2,6),(3,2),(3,6),(4,1) \\\\\\\\\\n& (4,2),(4,4),(4,5),(4,6),(5,4),(6,2),(6,3),(6,4),(6,6)\\\\}\\n\\\\end{aligned}\\n$$\\n\\nThis is the set difference operation. Observe that here $B-A=\\\\{ \\\\}$, because every pair adding to 7 or 11 must contain one odd and one even number.\\n\\nThe outcomes in common between both events $A$ and $B$ are called the intersection, denoted $A \\\\cap B$. This can be written as\\n\\n$$\\nA \\\\cap B=A-(S-B) .\\n$$\\n\\nOutcomes which appear in either $A$ or $B$ are called the union, denoted $A \\\\cup B$. With the complement operation $\\\\bar{A}=S-A$, we get a rich language for combining events, shown in Figure 2.1. We can readily compute the probability of any of these sets by summing the probabilities of the outcomes in the defined sets.\\n\\nThe events $A$ and $B$ are independent if and only if\\n\\n$$\\nP(A \\\\cap B)=P(A) \\\\times P(B)\\n$$\\n\\nThis means that there is no special structure of outcomes shared between events $A$ and $B$. Assuming that half of the students in my class are female, and half the students in my class are above average, we would expect that a quarter of my students are both female and above average if the events are independent.\\n\\n%---- Page End Break Here ---- Page : 30\\n\\nProbability theorists love independent events, because it simplifies their calculations. But data scientists generally don\\'t. When building models to predict the likelihood of some future event $B$, given knowledge of some previous event $A$, we want as strong a dependence of $B$ on $A$ as possible.\\n\\nSuppose I always use an umbrella if and only if it is raining. Assume that the probability it is raining here (event $B$ ) is, say, $p=1 / 5$. This implies the probability that I am carrying my umbrella (event $A$ ) is $q=1 / 5$. But even more, if you know the state of the rain you know exactly whether I have my umbrella. These two events are perfectly correlated.\\n\\nBy contrast, suppose the events were independent. Then\\n\\n$$\\nP(A \\\\mid B)=\\\\frac{P(A \\\\cap B)}{P(B)}=\\\\frac{P(A) P(B)}{P(B)}=P(A)\\n$$\\n\\nand whether it is raining has absolutely no impact on whether I carry my protective gear.\\n\\nCorrelations are the driving force behind predictive models, so we will discuss how to measure them and what they mean in Section 2.3\\n\\n\\\\subsection*{2.1.3 conditional probability\\\\index{conditional probability}}\\nWhen two events are correlated, there is a dependency between them which makes calculations more difficult. The conditional probability of $A$ given $B$, $P(A \\\\mid B)$ is defined:\\n\\n$$\\nP(A \\\\mid B)=\\\\frac{P(A \\\\cap B)}{P(B)}\\n$$\\n\\nRecall the dice rolling events from Section 2.1.2 namely:\\n\\n\\\\begin{itemize}\\n  \\\\item Event $A$ is that at least one of two dice be an even number.\\n  \\\\item Event $B$ is the sum of the two dice is either a 7 or an 11 .\\n\\\\end{itemize}\\n\\nObserve that $P(A \\\\mid B)=1$, because any roll summing to an odd value must consist of one even and one odd number. Thus $A \\\\cap B=B$, analogous to the umbrella case above. For $P(B \\\\mid A)$, note that $P(A \\\\cap B)=9 / 36$ and $P(A)=$ $25 / 36$, so $P(B \\\\mid A)=9 / 25$.\\n\\nConditional probability will be important to us, because we are interested in the likelihood of an event $A$ (perhaps that a particular piece of email is spam) as a function of some evidence $B$ (perhaps the distribution of words within the document). Classification problems generally reduce to computing conditional probabilities, in one way or another.\\n\\nOur primary tool to compute conditional probabilities will be Bayes theorem, which reverses the direction of the dependencies:\\n\\n$$\\nP(B \\\\mid A)=\\\\frac{P(A \\\\mid B) P(B)}{P(A)}\\n$$\\n\\nOften it proves easier to compute probabilities in one direction than another, as in this problem. By Bayes theorem $P(B \\\\mid A)=(1 \\\\cdot 9 / 36) /(25 / 36)=9 / 25$, exactly\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-049}\\n\\nFigure 2.2: The probability density function\\\\index{probability density function} (pdf) of the sum of two dice contains exactly the same information as the cumulative density function (cdf), but looks very different.\\\\\\\\\\nwhat we got before. We will revisit Bayes theorem in Section 5.6. where it will establish the foundations of computing probabilities in the face of evidence.\\n\\n\\\\subsection*{2.1.4 probability distribution\\\\index{probability distribution}s}\\nRandom variables are numerical functions where the values are associated with probabilities of occurrence. In our example where $V(s)$ the sum of two tossed dice, the function produces an integer between 2 and 12. The probability of a particular value $V(s)=X$ is the sum of the probabilities of all the outcomes which add up to $X$.\\n\\nSuch random variables can be represented by their probability density function, or pdf. This is a graph where the $x$-axis represents the range of values the random variable can take on, and the $y$-axis denotes the probability of that given value. Figure 2.2 (left) presents the pdf of the sum of two fair dice. Observe that the peak at $X=7$ corresponds to the most frequent dice total, with a probability of $1 / 6$.\\n\\nSuch pdf plots have a strong relationship to histograms\\\\index{histograms} of data frequency, where the $x$-axis again represents the range of value, but $y$ now represents the observed frequency of exactly how many event occurrences were seen for each given value $X$. Converting a histogram to a pdf can be done by dividing each bucket by the total frequency over all buckets. The sum of the entries then becomes 1 , so we get a probability distribution.\\n\\nHistograms are statistical: they reflect actual observations of outcomes. In contrast, pdfs are probabilistic: they represent the underlying chance that the next observation will have value $X$. We often use the histogram of observations $h(x)$ in practice to estimate the probabilities ${ }^{2}$ by normalizing counts by the total\\n\\n\\\\footnotetext{${ }^{2} \\\\mathrm{~A}$ technique called discounting offers a better way to estimate the frequency of rare events, and will be discussed in Section 11.1 .2\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-050}\\n\\nFigure 2.3: iPhone quarterly sales data presented as cumulative and incremental (quarterly) distributions. Which curve did Apple CEO Tim Cook choose to present?\\\\\\\\\\nnumber of observations:\\n\\n$$\\nP(k=X)=\\\\frac{h(k=X)}{\\\\sum_{x} h(x=X)}\\n$$\\n\\nThere is another way to represent random variables which often proves useful, called a cumulative density function or cdf. The cdf is the running sum of the probabilities in the pdf; as a function of $k$, it reflects the probability that $X \\\\leq k$ instead of the probability that $X=k$. Figure 2.2 (right) shows the cdf of the dice sum distribution. The values increase monotonically from left to right, because each term comes from adding a positive probability to the previous total. The rightmost value is 1 , because all outcomes produce a value no greater than the maximum.\\n\\nIt is important to realize that the pdf $P(V)$ and $\\\\mathrm{cdf} C(V)$ of a given random variable $V$ contain exactly the same information. We can move back and forth between them because:\\n\\n$$\\nP(k=X)=C(X \\\\leq k+\\\\delta)-C(X \\\\leq k)\\n$$\\n\\nwhere $\\\\delta=1$ for integer distributions. The cdf is the running sum of the pdf, so\\n\\n$$\\nC(X \\\\leq k)=\\\\sum_{x \\\\leq k} P(X=x)\\n$$\\n\\nJust be aware of which distribution you are looking at. Cumulative distributions always get higher as we move to the right, culminating with a probability of $C(X \\\\leq \\\\infty)=1$. By contrast, the total area under the curve of a pdf equals 1 , so the probability at any point in the distribution is generally substantially less.\\n\\n%---- Page End Break Here ---- Page : 33\\n\\nAn amusing example of the difference between cumulative and incremental distributions is shown in Figure 2.3 Both distributions show exactly the same data on Apple iPhone sales, but which curve did Apple CEO Tim Cook choose to present at a major shareholder event? The cumulative distribution (red) shows that sales are exploding, right? But it presents a misleading view of growth rate, because incremental change is the derivative of this function, and hard to visualize. Indeed, the sales-per-quarter plot (blue) shows that the rate of iPhone sales actually had declined for the last two periods before the presentation.\\n\\n\\\\subsection*{2.2 Descriptive Statistics}\\nDescriptive statistics provide ways of capturing the properties of a given data set or sample. They summarize observed data, and provide a language to talk about it. Representing a group of elements by a new derived element, like mean, min, count, or sum reduces a large data set to a small summary statistic: aggregation as data reduction.\\n\\nSuch statistics can become features in their own right when taken over natural groups or clusters in the full data set. There are two main types of descriptive statistics:\\n\\n\\\\begin{itemize}\\n  \\\\item Central tendency measures, which capture the center around which the data is distributed.\\n  \\\\item Variation or variability measures, which describe the data spread, i.e. how far the measurements lie from the center.\\n\\\\end{itemize}\\n\\nTogether these statistics tell us an enormous amount about our distribution.\\n\\n\\\\subsection*{2.2.1 Centrality Measures}\\nThe first element of statistics we are exposed to in school are the basic centrality measures: mean, median\\\\index{median}, and mode. These are the right place to start when thinking of a single number to characterize a data set.\\n\\n\\\\begin{itemize}\\n  \\\\item Mean: You are probably quite comfortable with the use of the arithmetic\\\\index{arithmetic} mean, where we sum values and divide by the number of observations:\\n\\\\end{itemize}\\n\\n$$\\n\\\\mu_{X}=\\\\frac{1}{n} \\\\sum_{i=1}^{n} x_{i}\\n$$\\n\\nWe can easily maintain the mean under a stream of insertions and deletions, by keeping the sum of values separate from the frequency count, and divide only on demand.\\\\\\\\\\nThe mean is very meaningful to characterize symmetric distributions without outliers, like height and weight. That it is symmetric means the number of items above the mean should be roughly the same as the number\\\\\\n%---- Page End Break Here ---- Page : 34\\n\\\\\\nbelow. That it is without outliers means that the range of values is reasonably tight. Note that a single MAXINT creeping into an otherwise sound set of observations throws the mean wildly off. The median is a centrality measure which proves more appropriate with such ill-behaved distributions.\\n\\n\\\\begin{itemize}\\n  \\\\item geometric\\\\index{geometric} mean\\\\index{geometric mean}: The geometric mean is the $n$th root of the product of $n$ values:\\n\\\\end{itemize}\\n\\n$$\\n\\\\left(\\\\prod_{i=1}^{n} a_{i}\\\\right)^{1 / n}=\\\\sqrt[n]{a_{1} a_{2} \\\\ldots a_{n}}\\n$$\\n\\nThe geometric mean is always less than or equal to the arithmetic mean. For example, the geometric mean of the sums of 36 dice rolls is 6.5201 , as opposed to the arithmetic mean of 7 . It is very sensitive to values near zero. A single value of zero lays waste to the geometric mean: no matter what other values you have in your data, you end up with zero. This is somewhat analogous to having an outlier of $\\\\infty$ in an arithmetic mean.\\\\\\\\\\nBut geometric means prove their worth when averaging ratios. The geometric mean of $1 / 2$ and $2 / 1$ is 1 , whereas the mean is 1.25 . There is less available \"room\" for ratios to be less than 1 than there is for ratios above 1 , creating an asymmetry that the arithmetic mean overstates. The geometric mean is more meaningful in these cases, as is the arithmetic mean of the logarithms of the ratios.\\n\\n\\\\begin{itemize}\\n  \\\\item Median: The median is the exact middle value among a data set; just as many elements lie above the median as below it. There is a quibble about what to take as the median when you have an even number of elements. You can take either one of the two central candidates: in any reasonable data set these two values should be about the same. Indeed in the dice example, both are 7 .\\\\\\\\\\nA nice property of the median as so defined is that it must be a genuine value of the original data stream. There actually is someone of median height to you can point to as an example, but presumably no one in the world is of exactly average height. You lose this property when you average the two center elements.\\\\\\\\\\nWhich centrality measure is best for applications? The median typically lies pretty close to the arithmetic mean in symmetrical distributions, but it is often interesting to see how far apart they are, and on which side of the mean the median lies.\\\\\\\\\\nThe median generally proves to be a better statistic for skewed distributions or data with outliers: like wealth and income. Bill Gates adds $\\\\$ 250$ to the mean per capita wealth in the United States, but nothing to the median. If he makes you personally feel richer, then go ahead and use the mean. But the median is the more informative statistic here, as it will be for any power law distribution.\\\\\\n%---- Page End Break Here ---- Page : 35\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-053}\\n\\\\end{itemize}\\n\\nFigure 2.4: Two distinct probability distributions with $\\\\mu=3000$ for the lifespan of light bulbs: normal (left) and with zero variance\\\\index{variance} (right).\\n\\n\\\\begin{itemize}\\n  \\\\item mode\\\\index{mode}: The mode is the most frequent element in the data set. This is 7 in our ongoing dice example, because it occurs six times out of thirty-six elements. Frankly, I\\'ve never seen the mode as providing much insight as centrality measure, because it often isn\\'t close to the center. Samples measured over a large range should have very few repeated elements or collisions at any particular value. This makes the mode a matter of happenstance. Indeed, the most frequently occurring elements often reveal artifacts or anomalies in a data set, such as default values or error codes that do not really represent elements of the underlying distribution.\\\\\\\\\\nThe related concept of the peak in a frequency distribution (or histogram) is meaningful, but interesting peaks only get revealed through proper bucketing. The current peak of the annual salary distribution in the United States lies between $\\\\$ 30,000$ and $\\\\$ 40,000$ per year, although the mode presumably sits at zero.\\n\\\\end{itemize}\\n\\n\\\\subsection*{2.2.2 variability measures\\\\index{variability measures}}\\nThe most common measure of variability is the standard deviation\\\\index{standard deviation} $\\\\sigma$, which measures sum of squares differences between the individual elements and the mean:\\n\\n$$\\n\\\\sigma=\\\\sqrt{\\\\frac{\\\\sum_{i=1}^{n}\\\\left(a_{i}-\\\\bar{a}\\\\right)^{2}}{n-1}}\\n$$\\n\\nA related statistic, the variance $V$, is the square of the standard deviation, i.e. $V=\\\\sigma^{2}$. Sometimes it is more convenient to talk about variance than standard deviation, because the term is eight characters shorter. But they measure exactly the same thing.\\n\\nAs an example, consider the humble light bulb, which typically comes with an expected working life, say $\\\\mu=3000$ hours, derived from some underlying distribution shown in Figure 2.4 In a conventional bulb, the chance of it lasting longer than $\\\\mu$ is presumably about the same as that of it burning out quicker, and this degree of uncertainty is measured by $\\\\sigma$. Alternately, imagine a \"printer\\\\\\n%---- Page End Break Here ---- Page : 36\\n\\\\\\ncartridge bulb,\" where the evil manufacturer builds very robust bulbs, but includes a counter so they can prevent it from ever glowing after 3000 hours of use. Here $\\\\mu=3000$ and $\\\\sigma=0$. Both distributions have the same mean, but substantially different variance.\\n\\nThe sum of squares penalty in the formula for $\\\\sigma$ means that one outlier value $d$ units from the mean contributes as much to the variance as $d^{2}$ points each one unit from the mean, so the variance is very sensitive to outliers.\\n\\nAn often confusing matter concerns the denominator in the formula for standard deviation. Should we divide by $n$ or $n-1$ ? The difference here is technical. The standard deviation of the full population divides by $n$, whereas the standard deviation of the sample divides by $n-1$. The issue is that sampling just one point tells us absolutely nothing about the underlying variance in any population, where it is perfectly reasonable to say there is zero variance in weight among the population of a one-person island. But for reasonable-sized data sets $n \\\\approx(n-1)$, so it really doesn\\'t matter.\\n\\n\\\\subsection*{2.2.3 Interpreting Variance}\\nRepeated observations of the same phenomenon do not always produce the same results, due to random noise or error. Sampling errors result when our observations capture unrepresentative circumstances, like measuring rush hour traffic on weekends as well as during the work week. Measurement errors reflect the limits of precision inherent in any sensing device. The notion of signal to noise ratio\\\\index{signal to noise ratio} captures the degree to\\\\index{interpretation} which a series of observations reflects a quantity of interest as opposed to data variance. As data scientists, we care about changes in the signal instead of the noise, and such variance often makes this problem surprisingly difficult.\\n\\nI think of variance as an inherent property of the universe, akin to the speed of light or the time-value of money. Each morning you weigh yourself on a scale you are guaranteed to get a different number, with changes reflecting when you last ate (sampling error), the flatness of the floor, or the age of the scale (both measurement error) as much as changes in your body mass (actual variation). So what is your real weight?\\n\\nEvery measured quantity is subject to some level of variance, but the phenomenon cuts much deeper than that. Much of what happens in the world is just random fluctuations or arbitrary happenstance causing variance even when the situation is unchanged. Data scientists seek to explain the world through data, but distressingly often there is no real phenomena to explain, only a ghost created by variance. Examples include:\\n\\n\\\\begin{itemize}\\n  \\\\item The stock market\\\\index{stock market}: Consider the problem of measuring the relative \"skill\" of different stock market investors. We know that Warren Buffet is much better at investing than we are. But very few professional investors prove consistently better than others. Certain investment vehicles wildly outperform the market in any given time period. However, the hot fund one\\n\\n%---- Page End Break Here ---- Page : 37\\n\\\\end{itemize}\\n\\n\\\\begin{verbatim}\\nln[28]:= Season[p_Real, n_Integer] :=\\n    Count[Table[If[RandomReal[1] }\\\\leqslantP,1,0],{n}], 1]/(1.0*n\\nIn[29]:= Histogram[d = Table[Season[0.300, 500], {100 000}], 100]\\n\\\\end{verbatim}\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-055}\\n\\\\end{center}\\n\\nFigure 2.5: Sample variance on hitters with a real $30 \\\\%$ success rate results in a wide range of observed performance even over 500 trials per season.\\\\\\\\\\nyear usually underperforms the market the year after, which shouldn\\'t happen if this outstanding performance was due to skill rather than luck.\\n\\nThe fund managers themselves are quick to credit profitable years to their own genius, but losses to unforeseeable circumstances. However, several studies have shown that the performance of professional investors is essentially random, meaning there is little real difference in skill. Most investors are paying managers for previously-used luck. So why do these entrail-readers get paid so much money?\\n\\n\\\\begin{itemize}\\n  \\\\item sports performance\\\\index{sports performance}: Students have good semesters and bad semesters, as reflected by their grade point average (GPA). Athletes have good and bad seasons, as reflected by their performance and statistics. Do such changes reflect genuine differences in effort and ability, or are they just variance?\\\\\\\\\\nIn baseball, .300 hitters (players who hit with a $30 \\\\%$ success rate) represent consistency over a full season. Batting .275 is not a noteworthy season, but hit .300 and you are a star. Hit . 325 and you are likely to be the batting champion.\\n\\\\end{itemize}\\n\\nFigure 2.5 shows the results of a simple simulation, where random numbers were used to decide the outcome of each at-bat over a 500 at-bats/season. Our synthetic player is a real .300 hitter, because we programmed it to report a hit with probability $300 / 1000$ (0.3). The results show that a real . 300 hitter has a $10 \\\\%$ chance of hitting .275 or below, just by chance. Such a season will typically be explained away by injuries or maybe the inevitable effects of age on athletic performance. But it could just be natural variance. Smart teams try to acquire a good hitter after a lousy season, when the price is cheaper, trying to take advantage of this variance.\\n\\nOur . 300 hitter also has a $10 \\\\%$ chance of batting above .325, but you\\\\\\n%---- Page End Break Here ---- Page : 38\\n\\\\\\ncan be pretty sure that they will ascribe such a breakout season to their improved conditioning or training methods instead of the fact they just got lucky. Good or bad season, or lucky/unlucky: it is hard to tell the signal from the noise.\\n\\n\\\\begin{itemize}\\n  \\\\item Model performance: As data scientists, we will typically develop and evaluate several models for each predictive challenge. The models may range from very simple to complex, and vary in their training conditions or parameters.\\n\\\\end{itemize}\\n\\nTypically the model with the best accuracy on the training corpus will be paraded triumphantly before the world as the right one. But small differences in the performance between models is likely explained by simple variance rather than wisdom: which training/evaluation pairs were selected, how well parameters were optimized, etc.\\\\\\\\\\nRemember this when it comes to training machine learning models. Indeed, when asked to choose between models with small performance differences between them, I am more likely to argue for the simplest model than the one with the highest score. Given a hundred people trying to predict heads and tails on a stream of coin tosses, one of them is guaranteed to end up with the most right answers. But there is no reason to believe that this fellow has any better predictive powers than the rest of us.\\n\\n\\\\subsection*{2.2.4 characterizing distributions\\\\index{characterizing distributions}}\\nDistributions do not necessarily have much probability mass exactly at the mean. Consider what your wealth would look like after you borrow $\\\\$ 100$ million, and then bet it all on an even money coin flip. Heads you are now $\\\\$ 100$ million in clear, tails you are $\\\\$ 100$ million in hock. Your expected wealth is zero, but this mean does not tell you much about the shape of your wealth distribution.\\n\\nHowever, taken together the mean and standard deviation do a decent job of characterizing any distribution. Even a relatively small amount of mass positioned far from the mean would add a lot to the standard deviation, so a small value of $\\\\sigma$ implies the bulk of the mass must be near the mean.\\n\\nTo be precise, regardless of how your data is distributed, at least ( $1-$ $\\\\left.\\\\left(1 / k^{2}\\\\right)\\\\right)$ th of the mass must lie within $\\\\pm k$ standard deviations of the mean. This means that at least $75 \\\\%$ of all the data must lie within $2 \\\\sigma$ of the mean, and almost $89 \\\\%$ within $3 \\\\sigma$ for any distribution.\\n\\nWe will see that even tighter bounds hold when we know the distribution is well-behaved, like the Gaussian or normal distribution. But this is why it is a great practice to report both $\\\\mu$ and $\\\\sigma$ whenever you talk about averages. The average height of adult women in the United States is $63.7 \\\\pm 2.7$ inches, meaning $\\\\mu=63.7$ and $\\\\sigma=2.7$. The average temperature in Orlando, Fl is 60.3 degrees Fahrenheit. However, there have been many more 100 degree days at Disney World than 100 inch ( 8.33 foot) women visiting to enjoy them.\\n\\n%---- Page End Break Here ---- Page : 39\\n\\nTake-Home Lesson: Report both the mean and standard deviation to characterize your distribution, written as $\\\\mu \\\\pm \\\\sigma$.\\n\\n\\\\subsection*{2.3 Correlation Analysis}\\nSuppose we are given two variables $x$ and $y$, represented by a sample of $n$ points of the form $\\\\left(x_{i}, y_{i}\\\\right)$, for $1 \\\\leq i \\\\leq n$. We say that $x$ and $y$ are correlated when the value of $x$ has some predictive power on the value of $y$.\\n\\nThe correlation coefficient $r(X, Y)$ is a statistic that measures the degree to which $Y$ is a function of $X$, and vice versa. The value of the correlation coefficient ranges from -1 to 1 , where 1 means fully correlated and 0 implies no relation, or independent variables. Negative correlations imply that the variables are anti-correlated, meaning that when $X$ goes up, $Y$ goes down.\\n\\nPerfectly anti-correlated variables have a correlation of -1 . Note that negative correlations are just as good for predictive purposes as positive ones. That you are less likely to be unemployed the more education you have is an example of a negative correlation, so the level of education can indeed help predict job status. Correlations around 0 are useless for forecasting.\\n\\nObserved correlations drives many of the predictive models we build in data science. Representative strengths of correlations include:\\n\\n\\\\begin{itemize}\\n  \\\\item Are taller people more likely to remain lean? The observed correlation between height and BMI is $r=-0.711$, so height is indeed negatively correlated with body mass index (BMI) ${ }^{3}$\\n  \\\\item Do standardized tests predict the performance of students in college? The observed correlati\\\\index{bad uses}on between SAT scores and freshmen GPA is $r=0.47$, so yes, there is some degree of predictive power. But social economic status is just as strongly correlated with SAT scores $(r=0.42) \\\\bigsqcup^{4}$\\n  \\\\item Does financial status affect health? The observed correlation between household income and the prevalence of coronary artery disease is $r=$ -0.717 , so there is a strong negative correlation. So yes, the wealthier you are, the lower your risk of having a heart attack. $5^{5}$\\n  \\\\item Does smoking affect health? The observed correlation between a group\\'s propensity to smoke and their mortality rate is $r=0.716$, so for G-d\\'s sake, don\\'t smoke ${ }^{6}$\\n\\\\end{itemize}\\n\\n\\\\footnotetext{$\\\\sqrt[3]{\\\\text { https://onlinecourses.science.psu.edu/stat500/node/60 }}$\\\\\\\\\\n${ }^{4}$ \\\\href{https://research.collegeboard.org/sites/default/files/publications/2012/9/}{https://research.collegeboard.org/sites/default/files/publications/2012/9/} researchreport-2009-1-socioeconomic-status-sat-freshman-gpa-analysis-data.pdf\\\\\\\\\\n${ }^{\\\\circ}$ \\\\href{http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3457990/}{http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3457990/}\\\\\\\\\\n6 \\\\href{http://lib.stat.cmu.edu/DASL/Stories/SmokingandCancer.html}{http://lib.stat.cmu.edu/DASL/Stories/SmokingandCancer.html}\\n}\\\\begin{itemize}\\n  \\\\item Do violent video games increase aggressive behavior? The observed correlation between play and violence is $r=0.19$, so there is a weak but significant correlation. 7\\n\\\\end{itemize}\\n\\nThis section will introduce the primary measurements of correlation. Further, we study how to appropriately determine the strength and power of any observed correlation, to help us understand when the connections between variables are real.\\n\\n\\\\subsection*{2.3.1 Correlation Coefficients: Pearson and Spearman Rank}\\nIn fact, there are two primary statistics used to measure correlation. Mercifully, both operate on the same -1 to 1 scale, although they measure somewhat different things. These different statistics are appropriate in different situations, so you should be aware of both of them.\\n\\n\\\\section*{The Pearson Correlation Coefficient}\\nThe more prominent of the two statistics is Pearson correlation, defined as\\n\\n$$\\nr=\\\\frac{\\\\sum_{i=1}^{n}\\\\left(X_{i}-\\\\bar{X}\\\\right)\\\\left(Y_{i}-\\\\bar{Y}\\\\right)}{\\\\sqrt{\\\\sum_{i=1}^{n}\\\\left(X_{i}-\\\\bar{X}\\\\right)^{2}} \\\\sqrt{\\\\sum_{i=1}^{n}\\\\left(Y_{i}-\\\\bar{Y}\\\\right)^{2}}}=\\\\frac{\\\\operatorname{Cov}(X, Y)}{\\\\sigma(X) \\\\sigma(Y)}\\n$$\\n\\nLet\\'s parse this equation. Suppose $X$ and $Y$ are strongly correlated. Then we would expect that when $x_{i}$ is greater than the mean $\\\\bar{X}$, then $y_{i}$ should be bigger than its mean $\\\\bar{Y}$. When $x_{i}$ is lower than its mean, $y_{i}$ should follow. Now look at the numerator. The sign of each term is positive when both values are above $(1 \\\\times 1)$ or below $(-1 \\\\times-1)$ their respective means. The sign of each term is negative $((-1 \\\\times 1)$ or $(1 \\\\times-1))$ if they move in opposite directions, suggesting negative correlation. If $X$ and $Y$ were uncorrelated, then positive and negative terms should occur with equal frequency, offsetting each other and driving the value to zero.\\n\\nThe numerator\\'s operation determining the sign of the correlation is so useful that we give it a name, covariance, computed:\\n\\n$$\\n\\\\operatorname{Cov}(X, Y)=\\\\sum_{i=1}^{n}\\\\left(X_{i}-\\\\bar{X}\\\\right)\\\\left(Y_{i}-\\\\bar{Y}\\\\right)\\n$$\\n\\nRemember covariance: we will see it again in Section 8.2.3\\\\\\\\\\nThe denominator of the Pearson formula reflects the amount of variance in the two variables, as measured by their standard deviations. The covariance between $X$ and $Y$ potentially increases with the variance of these variables, and this denominator is the magic amount to divide it by to bring correlation to a -1 to 1 scale.\\n\\n\\\\footnotetext{${ }^{7}$ \\\\href{http://webspace.pugetsound.edu/facultypages/cjones/chidev/Paper/Articles/}{http://webspace.pugetsound.edu/facultypages/cjones/chidev/Paper/Articles/}\\n} Anderson-Aggression.pdf\\\\\\\\\\n%---- Page End Break Here ---- Page : 41\\n\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-059}\\n\\nFigure 2.6: The function $y=|x|$ does not have a linear model, but seems like it should be easily fitted despite weak correlations.\\n\\n\\\\section*{The Spearman Rank Correlation Coefficient}\\nThe Pearson correlation coefficient defines the degree to which a linear predictor of the form $y=m \\\\cdot x+b$ can fit the observed data. This generally does a good job measuring the similarity between the variables, but it is possible to construct pathological examples where the correlation coefficient between $X$ and $Y$ is zero, yet $Y$ is completely dependent on (and hence perfectly predictable from) $X$.\\n\\nConsider points of the form $(x,|x|)$, where $x$ is uniformly (or symmetrically) sampled from the interval $[-1,1]$ as shown in Figure 2.6 The correlation will be zero because for every point $(x, x)$ there will be an offsetting point $(-x, x)$, yet $y=|x|$ is a perfect predictor. Pearson correlation measures how well the best linear predictors can work, but says nothing about weirder functions like absolute value.\\n\\nThe Spearman rank correlation coefficient essentially counts the number of pairs of input points which are out of order. Suppose that our data set contains points $\\\\left(x_{1}, y_{1}\\\\right)$ and $\\\\left(x_{2}, y_{2}\\\\right)$ where $x_{1}<x_{2}$ and $y_{1}<y_{2}$. This is a vote that the values are positively correlated, whereas the vote would be for a negative correlation if $y_{2}<y_{1}$.\\n\\nSumming up over all pairs of points and normalizing properly gives us Spearman rank correlation. Let $\\\\operatorname{rank}\\\\left(x_{i}\\\\right)$ be the rank position of $x_{i}$ in sorted order among all $x_{i}$, so the rank of the smallest value is 1 and the largest value $n$. Then\\n\\n$$\\n\\\\rho=1-\\\\frac{6 \\\\sum d_{i}^{2}}{n\\\\left(n^{2}-1\\\\right)}\\n$$\\n\\nwhere $d_{i}=\\\\operatorname{rank}\\\\left(x_{i}\\\\right)-\\\\operatorname{rank}\\\\left(y_{i}\\\\right)$.\\\\\\\\\\nThe relationship between our two coefficients is better delineated by the example in Figure 2.7. In addition to giving high scores to non-linear but monotonic functions, Spearman correlation is less sensitive to extreme outlier elements than Pearson. Let $p=\\\\left(x_{1}, y_{\\\\max }\\\\right)$ be the data point with largest value\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-060(2)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-060}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-060(1)}\\n\\nFigure 2.7: A monotonic but not linear point set has a Spearman coefficient $r=1$ even though it has no good linear fit (left). Highly-correlated sequences are recognized by both coefficients (center), but the Pearson coefficient is much more sensitive to outliers (right).\\\\\\\\\\nof $y$ in a given data set. Suppose we replace $p$ with $p^{\\\\prime}=\\\\left(x_{1}, \\\\infty\\\\right)$. The Pearson correlation will go crazy, since the best fit now becomes the vertical line $x=x_{1}$. But the Spearman correlation will be unchanged, since all the points were under $p$, just as they are now under $p^{\\\\prime}$.\\n\\n\\\\subsection*{2.3.2 The Power and Significance of Correlation}\\nThe correlation coefficient $r$ reflects the degree to which $x$ can be used to predict $y$ in a given sample of points $S$. As $|r| \\\\rightarrow 1$, these predictions get better and better.\\n\\nBut the real question is how this correlation will hold up in the real world, outside the sample. Stronger correlations have larger $|r|$, but also involve samples of enough points to be significant. There is a wry saying that if you want to fit your data by a straight line, it is best to sample it at only two points. Your correlation becomes more impressive the more points it is based on.\\n\\nThe statistical limits in interpreting correlations are presented in Figure 2.8 based on strength and size:\\n\\n\\\\begin{itemize}\\n  \\\\item Strength of correlation: $R^{2}$ : The square of the sample correlation coefficient $r^{2}$ estimates the fraction of the variance in $Y$ explained by $X$ in a simple linear regression. The correlation between height and weight is approximately 0.8 , meaning it explains about two thirds of the variance.\\\\\\\\\\nFigure 2.8 (left) shows how rapidly $r^{2}$ decreases with $r$. There is a profound limit to how excited we should get about establishing a weak correlation. A correlation of 0.5 possesses only $25 \\\\%$ of the maximum predictive power, and a correlation of $r=0.1$ only $1 \\\\%$. Thus the predictive value of correlations decreases rapidly with $r$.\\n\\\\end{itemize}\\n\\nWhat do we mean by \"explaining the variance\"? Let $f(x)=m x+c$ be the\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96\\\\index{Body Mass Index}dcf8e028g-061(1)}\\n\\nFigure 2.8: Limits in interpreting significance. The $r^{2}$ value shows that weak correlations explain only a small fraction of the variance (left). The level of correlation necessary to be statistically significance decreases rapidly with sample size $n$ (right).\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf\\\\index{robustness}8e028g-061}\\n\\nFigure 2.9: Plotting $r_{i}=y_{i}-f\\\\left(x_{i}\\\\right)$ shows that the residual values have lower variance and mean zero. The original data points are on the left, with the corresponding residuals on the right.\\\\\\\\\\npredictive value of $y$ from $x$, with the parameters $m$ and $c$ corresponding to the best possible fit. The residual values $r_{i}=y_{i}-f\\\\left(x_{i}\\\\right)$ will have mean zero, as shown in Figure 2.9 . Further, the variance of the full data set $V(Y)$ should be much larger than $V(r)$ if there is a good linear fit $f(x)$. If $x$ and $y$ are perfectly correlated, there should be no residual error, and $V(r)=0$. If $x$ and $y$ are totally uncorrelated, the fit should contribute nothing, and $V(y) \\\\approx V(r)$. Generally speaking, $1-r^{2}=V(r) / V(y)$.\\\\\\\\\\nConsider Figure 2.9, showing a set of points (left) admitting a good linear fit, with correlation $r=0.94$. The corresponding residuals $r_{i}=y_{i}-f\\\\left(x_{i}\\\\right)$ are plotted on the right. The variance of the $y$ values on the left $V(y)=$ 0.056 , substantially greater than the variance $V(r)=0.0065$ on the right. Indeed,\\n\\n$$\\n1-r^{2}=0.116 \\\\longleftrightarrow V(r) / V(y)=0.116\\n$$\\n\\n\\\\begin{itemize}\\n  \\\\item Statistical significance: The statistical significance of a correlation depends upon its sample size $n$ as well as $r$. By tradition, we say that a correlation of $n$ points is significant if there is an $\\\\alpha \\\\leq 1 / 20=0.05$ chance that we would observe a correlation as strong as $r$ in any random set of $n$ points.\\\\\\\\\\nThis is not a particularly strong standard. Even small correlations become significant at the 0.05 level with large enough sample sizes, as shown in Figure 2.8 (right). A correlation of $r=0.1$ becomes significant at $\\\\alpha=$ 0.05 around $n=300$, even though such a factor explains only $1 \\\\%$ of the variance.\\\\index{scatter plots}\\n\\\\end{itemize}\\n\\nWeak but significant correlations can have value in big data models involving large numbers of features. Any single feature/correlation might explain/predict only small effects, but taken together a large number of weak but independent correlations may have strong predictive power. Maybe. We will discuss significance again in greater detail in Section 5.3\\n\\n\\\\subsection*{2.3.3 Correlation Does Not Imply Causation!}\\nYou have heard this before: correlation does not imply causation:\\n\\n\\\\begin{itemize}\\n  \\\\item The number of police active in a precinct correlate strongly with the local crime rate, but the police do not cause the crime.\\n  \\\\item The amount of medicine people take correlates with the probability they are sick, but the medicine does not cause the illness.\\n\\\\end{itemize}\\n\\nAt best, the implication works only one way. But many observed correlations are completely spurious, with neither variable having any real impact on the other.\\n\\nStill, correlation implies causation is a common error in thinking, even among those who understand logical reasoning. Generally speaking, few statistical tools are available to tease out whether $A$ really causes $B$. We can conduct controlled experiments, if we can manipulate one of the variables and watch the effect on\\\\\\n%---- Page End Break Here ---- Page : 45\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-063(2)}\\n\\nFigure 2.10: Correlation does not imply causation. (Source \\\\href{https://www.xkcd}{https://www.xkcd}. com/552.)\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-063}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-063(1)}\\n\\nFigure 2.11: Cyclic trends in a time series (left) are revealed through correlating it against shifts of itself (right).\\\\\\\\\\nthe other. For example, the fact that we can put people on a diet that makes them lose weight without getting shorter is convincing evidence that weight does not cause height. But it is often harder to do these experiments the other way, e.g. there is no reasonable way to make people shorter other than by hacking off limbs.\\n\\n\\\\subsection*{2.3.4 Detecting Periodicities by Autocorrelation}\\nSuppose a space alien was hired to analyze U.S. sales at a toy company. Instead of a nice smooth function showing a consistent trend, they would be astonished to see a giant bump every twelfth month, every year. This alien would have discovered the phenomenon of Christmas.\\n\\nSeasonal trends reflect cycles of a fixed duration, rising and falling in a regular pattern. Many human activities proceed with a seven-day cycle associated with the work week. Large populations of a type of insect called a cicada emerge on a 13 -year or 17 -year cycle, in an effort to prevent predators from learning to\\\\\\n%---- Page End Break Here ---- Page : 46\\n\\\\\\neat them.\\\\\\\\\\nHow can we recognize such cyclic patterns in a sequence $S$ ? Suppose we correlate the values of $S_{i}$ with $S_{i+p}$, for all $1 \\\\leq\\\\index{PageRank} i \\\\leq \\\\index{cl\\\\index{sear\\\\index{top s\\\\index{university rank\\\\index{scores vs. rankings}ings}ports teams}ch results}ass rank}n-p$. If the values are in sync for a particular period length $p$, then this correlation with itself will be unusually high relative to other possible lag values. Comparing a sequence to itself is called an autocorrelation, and the series of correlations for all $1 \\\\leq k \\\\leq n-1$ is called the autocorrelation function. Figure 2.11 presents a time series of daily sales, and the associated autocorrelation function for this data. The peak at a shift of seven days (and every multiple of seven days) establishes that there is a weekly periodicity in sales: more stuff gets sold on weekends.\\n\\nAutocorrelation is an important concept in predicting future events, because it means we can use previous observations as features in a model. The heuristic that tomorrow\\'s weather will be similar to today\\'s is based on autocorrelation, with a lag of $p=1$ days. Certainly we would expect such a model to be more accurate than predictions made on weather data from six months ago (lag $p=180$ days).\\n\\nGenerally speaking, the autocorrelation function for many quantities tends to be highest for very short lags. This is why long-term predictions are less accurate than short-term forecasts: the autocorrelations are generally much weaker. But periodic cycles do sometimes stretch much longer. Indeed, a weather forecast based on a lag of $p=365$ days will be much better than one of $p=180$, because of seasonal effects.\\n\\nComputing the full autocorrelation function requires calculating $n-1$ different correlations on points of the time series, which can get expensive for large $n$. Fortunately, there is an efficient algorithm based on the fast Fourier transform\\\\index{fast Fourier transform} (FFT\\\\index{FFT}), which makes it possible to construct the autocorrelation function even for very long sequences.\\n\\n\\\\subsection*{2.4 logarithm\\\\index{logarithm}s}\\nThe logarithm is the inverse exponential function $y=b^{x}$, an equation that can be rewritten as $x=\\\\log _{b} y$. This definition is the same as saying that\\n\\n$$\\nb^{\\\\log _{b} y}=y .\\n$$\\n\\nExponential functions grow at a very fast rate: consider $b=\\\\left\\\\{2^{1}, 2^{2}, 2^{3}, 2^{4}, \\\\ldots\\\\right\\\\}$. In contrast, logarithms grow a very slow rate: these are just the exponents of the previous series $\\\\{1,2,3,4, \\\\ldots\\\\}$. They are associated with any process where we are repeatedly multiplying by some value of $b$, or repeatedly dividing by $b$. Just remember the definition:\\n\\n$$\\ny=\\\\log _{b} x \\\\longleftrightarrow b^{y}=x\\n$$\\n\\nLogarithms are very useful things, and arise often in data analysis. Here I detail three important roles logarithms play in data science. Surprisingly, only one of them is related to the seven algorithmic applications of logarithms\\n\\n%---- Page End Break Here ---- Page : 47\\n\\nI present in The Algorithm Design Manual [Ski08]. Logarithms are indeed very useful things.\\n\\n\\\\subsection*{2.4.1 Logarithms and multiplying probabilities\\\\index{mult\\\\index{Social Networkâmovie}iplying probabilities}}\\nLogarithms were first invented as an aide to computation, by reducing the problem of multiplication to that of addition. In particular, to compute the product $p=x \\\\cdot y$, we could compute the sum of the logarithms $s=\\\\log _{b} x+\\\\log _{b} y$ and then take the inverse of the logarithm (i.e. raising $b$ to the $s$ th power) to get $p$, because:\\n\\n$$\\np=x \\\\cdot y=b^{\\\\left(\\\\log _{b} x+\\\\log _{b} y\\\\right)} .\\n$$\\n\\nThis is the trick that powered the mechanical slide rules that geeks used in the days before pocket calculators.\\n\\nHowever, this idea remains important today, particularly when multiplying long chains of probabilities. Probabilities are small numbers. Thus multiplying long chains of probability yield very small numbers that govern the chances of very rare events. There are serious numerical stability problems with floating point multiplication on real computers. Numerical errors will creep in, and will eventually overwhelm the true value of small-enough numbers.\\n\\nSumming the logarithms of probabilities is much more numerically stable than multiplying them, but yields an equivalent result because:\\n\\n$$\\n\\\\prod_{i=1}^{n} p_{i}=b^{P}, \\\\text { where } P=\\\\sum_{i=1}^{n} \\\\log _{b}\\\\left(p_{i}\\\\right)\\n$$\\n\\nWe can raise our sum to an exponential if we need the real probability, but usually this is not necessary. When we just need to compare two probabilities to decide which one is larger we can safely stay in log world, because bigger logarithms correspond to bigger probabilities.\\n\\nThere is one quirk to be aware of. Recall that the $\\\\log _{2}\\\\left(\\\\frac{1}{2}\\\\right)=-1$. The logarithms of probabilities are all negative numbers except for $\\\\log (1)=0$. This is the reason why equations with logs of probabilities often feature negative signs in strange places. Be on the lookout for them.\\n\\n\\\\subsection*{2.4.2 Logarithms and ratio\\\\index{ratio}s}\\nRatios are quantities of the form $a / b$. They occur often in data sets either as elementary features or values derived from feature pairs. Ratios naturally occur in normalizing data for conditions (i.e. weight after some treatment over the initial weight) or time (i.e. today\\'s price over yesterday\\'s price).\\n\\nBut ratios behave differently when reflecting increases than decreases. The ratio $200 / 100$ is $200 \\\\%$ above baseline, but $100 / 200$ is only $50 \\\\%$ below despite being a similar magnitude change. Thus doing things like averaging ratios is committing a statistical sin. Do you really want a doubling followed by a halving to average out as an increase, as opposed to a neutral change?\\\\\\n%---- Page End Break Here ---- Page : 48\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-066}\\n\\nFigure 2.12: Plotting ratios on a scale cramps the space allocated to small ratios relative to large ratios (left). Plotting the logarithms of ratios better represents the underlying data (right).\\n\\nOne solution here would have been to use the geometric mean. But better is taking the logarithm of these ratios, so that they yield equal displacement, since $\\\\log _{2} 2=1$ and $\\\\log _{2}(1 / 2)=-1$. We get the extra bonus that a unit ratio maps to zero, so positive and negative numbers correspond to improper and proper ratios, respectively.\\n\\nA rookie mistake my students often make involves plotting the value of ratios instead of their logarithms. Figure 2.12 (left) is a graph from a student paper, showing the ratio of new score over old score on data over 24 hours (each red dot is the measurement for one hour) on four different data sets (each given a row). The solid black line shows the ratio of one, where both scores give the same result. Now try to read this graph: it isn\\'t easy because the points on the left side of the line are cramped together in a narrow strip. What jumps out at you are the outliers. Certainly the new algorithm does terrible on 7UM917 in the top row: that point all the way to the right is a real outlier.\\n\\nExcept that it isn\\'t. Now look at Figure 2.12 (right), where we plot the logarithms of the ratios. The space devoted to left and right of the black line can now be equal. And it shows that this point wasn\\'t really such an outlier at all. The magnitude of improvement of the leftmost points is much greater than that of the rightmost points. This plot reveals that new algorithm generally makes things better, only because we are showing logs of ratios instead of the ratios themselves.\\n\\n\\\\subsection*{2.4.3 Logarithms and normalizing skewed distribution\\\\index{normalizing skewed distribution}s}\\nVariables which follow symmetric, bell-shaped distributions tend to be nice as features in models. They show substantial variation, so they can be used to discriminate between things, but not over such a wide range that outliers are overwhelming.\\n\\nBut not every distribution is symmetric. Consider the one in Figure 2.13\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-067}\\n\\nFigure 2.13: Hitting a skewed data distribution (left) with a $\\\\log$ often yields a more bell-shaped distribution (right).\\\\\\\\\\n(left). The tail on the right goes much further than the tail on the left. And we are destined to see far more lopsided distributions when we discuss power laws, in Section 5.1.5 Wealth is representative of such a distribution, where the poorest human has zero or perhaps negative wealth, the average person (optimistically) is in the thousands of dollars, and Bill Gates is pushing $\\\\$ 100$ billion as of this writing.\\n\\nWe need a normalization to convert such distributions into something easier to deal with. To ring the bell of a power law distribution we need something non-linear, that reduces large values to a disproportionate degree compared to more modest values.\\n\\nThe logarithm is the transformation of choice for power law variables. Hit your long-tailed distribution with a log and often good things happen. The distribution in Figure 2.13 happened to be the log normal distribution, so taking the logarithm yielded a perfect bell-curve on right. Taking the logarithm of variables with a power law distribution brings them more in line with traditional distributions. For example, as an upper-middle class professional, my wealth is roughly the same number of logs from my starving students as I am from Bill Gates!\\n\\nSometimes taking the logarithm proves too drastic a hit, and a less dramatic non-linear transformation like the square root works better to normalize a distribution. The acid test is to plot a frequency distribution of the transformed values and see if it looks bell-shaped: grossly-symmetric, with a bulge in the middle. That is when you know you have the right function.\\n\\n\\\\subsection*{2.5 War Story: Fitting Designer Genes}\\nThe word bioinformatician\\\\index{bioinformatician} is life science speak for \"data scientist,\" the practitioner of an emerging discipline which studies massive collections of DNA sequence data looking for patterns. Sequence data is very interesting to work\\\\\\\\\\nwith, and I have played bioinformatician in re\\\\index{Bordaâs method}search projects since the very beginnings of the human genome project.\\n\\nDNA sequences are strings on the four letter alphabet $\\\\{A, C, G, T\\\\}$. Proteins form the stuff that we are physical\\\\index{coeï¬cient}l\\\\index{merging}y constructed from, and are composed of strings of 20 different types of molecular units, called amino acids. Genes are the DNA sequences which describe exactly how to make specific proteins, with the units each described by a triplet of $\\\\{A, C, G, T\\\\}$ s called codons.\\n\\nFor our purposes, it suffices to know that there are a huge number of possible DNA sequences describing genes which could code for any particular desired protein sequence. But only one of them is used. My biologist collaborators and I wanted to know why.\\n\\nOriginally, it was assumed that all of these different synonymous encodings were essentially identical, but statistics performed on sequence data made it clear that certain codons are used more often than others. The biological conclusion is that \"codons matter,\" and there are good biological reasons why this should be.\\n\\nWe became interested in whether \"neighboring pairs of codon matter.\" Perhaps certain pairs of triples are like oil and water, and hate to mix. Certain letter pairs in English have order preferences: you see the bigram $g h$ far more often than $h g$. Maybe this is true of DNA as well? If so, there would be pairs of triples which should be underrepresented in DNA sequence data.\\n\\nTo test this, we needed a score comparing the number of times we actually see a particular triple (say $x=C A T$ ) next to another particular triple (say $y=G A G)$ to what we would expect by chance. Let $F(x y)$ be the frequency of $x y$, number of times we actually see codon $x$ followed by codon $y$ in the DNA sequence database. These codons code for specific amino acids, say $a$ and $b$ respectively. For amino acid $a$, the probability that it will be coded by $x$ is $P(x)=F(x) / F(a)$, and similarly $P(y)=F(y) / F(b)$. Then the expected number of times of seeing $x y$ is\\n\\n$$\\n\\\\text { Expected }(x y)=\\\\left(\\\\frac{F(x)}{F(a)}\\\\right)\\\\left(\\\\frac{F(y)}{F(b)}\\\\right) F(a b)\\n$$\\n\\nBased on this, we can compute a codon pair score for any given hexamer $x y$ as follows:\\n\\n$$\\nC P S(x y)=\\\\ln \\\\left(\\\\frac{\\\\text { Observed }(x y)}{E x p e c t e d}(x y)\\\\right)=\\\\ln \\\\left(\\\\frac{F(x y)}{\\\\frac{F(x) F(y)}{F(a) F(b)} F(a b)}\\\\right)\\n$$\\n\\nTaking the logarithm of this ratio produced very nice properties. Most importantly, the sign of the score distinguished over-represented pairs from underrepresented pairs. Because the magnitudes were symmetric ( +1 was just as impressive as -1 ) we could add or average these scores in a sensible way to give a score for each gene. We used these scores to design genes that should be bad for viruses, which gave an exciting new technology for making vaccines. See the chapter notes (Section 2.6) for more details.\\n\\n%---- Page End Break Here ---- Page : 51\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|l|l|l|l|l|}\\n\\\\hline\\nFr. Dep. & Score &  & Fr. Ind. & Score \\\\\\\\\\n\\\\hline\\nCATAGG & -1.74 &  & GGGGGG & -1.01 \\\\\\\\\\n\\\\hline\\nTCTAGC & -1.61 &  & CCCCCC & -0.95 \\\\\\\\\\n\\\\hline\\nGTTAGG & -1.58 &  & GGCGCC & -0.66 \\\\\\\\\\n\\\\hline\\nGCTAGT & -1.48 &  & GGGGGT & -0.63 \\\\\\\\\\n\\\\hline\\nCCTAGT & -1.44 &  & CGGGGG & -0.59 \\\\\\\\\\n\\\\hline\\nGGTAGG & -1.41 &  & AGGGGG & -0.58 \\\\\\\\\\n\\\\hline\\nCTTAGG & -1.40 &  & CACGTG & -0.58 \\\\\\\\\\n\\\\hline\\nACTAGC & -1.38 &  & ACCCCC & -0.56 \\\\\\\\\\n\\\\hline\\nGCTAGC & -1.37 &  & GGGCCC & -0.56 \\\\\\\\\\n\\\\hline\\nGCTAGA & -1.36 &  & CCCCCT & -0.53 \\\\\\\\\\n\\\\hline\\nCCTAGC & -1.35 &  & CGCCCC & -0.52 \\\\\\\\\\n\\\\hline\\nGATAGG & -1.35 &  & CCCCCG & -0.51 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 2.14: Patterns in DNA sequences with the lowest codon pair scores become obvious on inspection. When interpreted in-frame, the stop symbol TAG is substantially depleted (left). When interpreted in the other two frames, the most avoided patterns are all very low complexity, like runs of a single base (right)\\n\\nKnowing that certain pairs of codons were bad did not explain why they were bad. But by computing two related scores (details unimportant) and sorting the triplets based on them, as shown in Figure 2.14 certain patterns popped out. Do you notice the patterns? All the bad sequences on the left contain $T A G$, which turns out to be a special codon that tells the gene to stop. And all the bad sequences on the right consist of $C$ and $G$ in very simple repetitive sequences. These explain biologically why patterns are avoided by evolution, meaning we discovered something very meaningful about life.\\n\\nThere are two take-home lessons from this story. First, developing numerical scoring functions which highlight specific aspects of items can be very useful to reveal patterns. Indeed, Chapter 4 will focus on the development of such systems. Second, hitting such quantities with a logarithm can make them even more useful, enabling us to see the forest for the trees.\\n\\n\\\\subsection*{2.6 Chapter Notes}\\nThere are many excellent introductions to probability theory available, including [Tij12, BT08. The same goes for elementary statistics, with good introductory texts including JWHT13, Whe13. The brief history of probability theory in this chapter is based on Weaver Wea82.\\n\\nIn its strongest form, the efficient market hypothesis states that the stock market is essentially unpredictable using public information. My personal advice is that you should invest in index funds that do not actively try to predict the direction of the market. Malkiel\\'s A Random Walk Down Wall Street [Mal99]\\\\\\n%---- Page End Break Here ---- Page : 52\\n\\\\\\nis an excellent introduction to such investment thinking.\\\\\\\\\\nThe Fast Fourier Transform (FFT) provides an $O(n \\\\log n)$ time algorithm to compute the full autocorrelation function of an $n$-element sequence, where the straightforward computation of $n$ correlations takes $O\\\\left(n^{2}\\\\right)$. Bracewell [Bra99] and Brigham [Bri88] are excellent introductions to Fourier transforms and the FFT. See also the exposition in Press \\\\href{http://et.al}{et.al}. PFTV07.\\n\\nThe comic strip in Figure 2.10 comes from Randall Munroe\\'s webcomic $x k c d$, specifically \\\\href{https://xkcd.com/552}{https://xkcd.com/552}, and is reprinted with permission.\\n\\nThe war story of Section 2.5 revolves around our work on how the phenomenon of codon pair bias affects gene translation. Figure 2.14 comes from my collaborator Justin Gardin. See [CPS ${ }^{+} 08, \\\\mathrm{MCP}^{+} 10$, Ski12] for discussions of how we exploited codon pair bias to design vaccines for viral diseases like polio and the flu.\\n\\n\\\\subsection*{2.7 exercises\\\\index{exercises}}\\n\\\\section*{Probability}\\n2-1. [3] Suppose that $80 \\\\%$ of people like peanut butter, $89 \\\\%$ like jelly, and $78 \\\\%$ like both. Given that a randomly sampled person likes peanut butter, what is the probability that she also likes jelly?\\\\\\\\[0pt]\\n2-2. [3] Suppose that $P(A)=0.3$ and $P(B)=0.7$.\\\\\\\\\\n(a) Can you compute $P(A$ and $B)$ if you only know $P(A)$ and $P(B)$ ?\\\\\\\\\\n(b) Assuming that events $A$ and $B$ arise from independent random processes:\\n\\n\\\\begin{itemize}\\n  \\\\item What is $P(A$ and $B)$ ?\\n  \\\\item What is $P(A$ or $B)$ ?\\n  \\\\item What is $P(A \\\\mid B)$ ?\\n\\\\end{itemize}\\n\\n2-3. [3] Consider a game where your score is the maximum value from two dice. Compute the probability of each event from $\\\\{1, \\\\ldots, 6\\\\}$.\\\\\\\\[0pt]\\n2-4. [8] Prove that the cumulative distribution function of the maximum of a pair of values drawn from random variable $X$ is the square of the original cumulative distribution function of $X$.\\\\\\\\[0pt]\\n2-5. [5] If two binary random variables $X$ and $Y$ are independent, are $\\\\bar{X}$ (the complement of $X$ ) and $Y$ also independent? Give a proof or a counterexample.\\n\\n\\\\section*{Statistics}\\n2-6. [3] Compare each pair of distributions to decide which one has the greater mean and the greater standard deviation. You do not need to calculate the actual values of $\\\\mu$ and $\\\\sigma$, just how they compare with each other.\\\\\\\\\\n(a) i. $3,5,5,5,8,11,11,11,13$.\\\\\\\\\\nii. $3,5,5,5,8,11,11,11,20$.\\\\\\\\\\n(b) i. $-20,0,0,0,15,25,30,30$.\\\\\\\\\\nii. $-40,0,0,0,15,25,30,30$.\\\\\\\\\\n(c) i. $0,2,4,6,8,10$.\\\\\\\\\\nii. $20,22,24,26,28,30$.\\\\\\\\\\n(d) i. $100,200,300,400,500$.\\\\\\\\\\nii. $0,50,300,550,600$.\\n\\n2-7. [3] Construct a probability distribution where none of the mass lies within one $\\\\sigma$ of the mean.\\n\\n2-8. [3] How does the arithmetic and geometric mean compare on random integers?\\\\\\\\\\n$2-9$. [3] Show that the arithmetic mean equals the geometric mean when all terms are the same.\\n\\n\\\\section*{Correlation Analysis}\\n2-10. [3] True or false: a correlation coefficient of -0.9 indicates a stronger linear relationship than a correlation coefficient of 0.5 . Explain why.\\\\\\\\[0pt]\\n2-11. [3] What would be the correlation coefficient between the annual salaries of college and high school graduates at a given company, if for each possible job title the college graduates always made:\\\\\\\\\\n(a) $\\\\$ 5,000$ more than high school grads?\\\\\\\\\\n(b) $25 \\\\%$ more than high school grads?\\\\\\\\\\n(c) $15 \\\\%$ less than high school grads?\\n\\n2-12. [3] What would be the correlation between the ages of husbands and wives if men always married woman who were:\\\\\\\\\\n(a) Three years younger than themselves?\\\\\\\\\\n(b) Two years older than themselves?\\\\\\\\\\n(c) Half as old as themselves?\\n\\n2-13. [5] Use data or literature found in a Google search to estimate/measure the strength of the correlation between:\\\\\\\\\\n(a) Hits and walks scored for hitters in baseball.\\\\\\\\\\n(b) Hits and walks allowed by pitchers in baseball.\\n\\n2-14. [5] Compute the Pearson and Spearman Rank correlations for uniformly drawn samples of points $\\\\left(x, x^{k}\\\\right)$. How do these values change as a function of increasing $k$ ?\\n\\n\\\\section*{Logarithms}\\n$2-15$. [3] Show that the logarithm of any number less than 1 is negative.\\\\\\\\\\n$2-16$. [3] Show that the logarithm of zero is undefined.\\\\\\\\[0pt]\\n2-17. [5] Prove that\\n\\n$$\\nx \\\\cdot y=b^{\\\\left(\\\\log _{b} x+\\\\log _{b} y\\\\right)}\\n$$\\n\\n2-18. [5] Prove the correctness of the formula for changing a base- $b$ logarithm to base$a$, that\\n\\n$$\\n\\\\log _{a}(x)=\\\\log _{b}(x) / \\\\log _{b}(a)\\n$$\\n\\n\\\\section*{Implementation Projects}\\n2-19. [3] Find some interesting data sets, and compare how similar their means and medians are. What are the distributions where the mean and median differ on the most?\\n\\n2-20. [3] Find some interesting data sets and search all pairs for interesting correlations. Perhaps start with what is available at \\\\href{http://www.data-manual.com/}{http://www.data-manual.com/} data What do you find?\\n\\n\\\\section*{Interview Questions}\\n2-21. [3] What is the probability of getting exactly $k$ heads on $n$ tosses, where the coin has a probability of $p$ in coming up heads on each toss? What about $k$ or more heads?\\n\\n2-22. [5] Suppose that the probability of getting a head on the $i$ th toss of an everchanging coin is $f(i)$. How would you efficiently compute the probability of getting exactly $k$ heads in $n$ tosses?\\\\\\\\[0pt]\\n2-23. [5] At halftime of a basketball game you are offered two possible challenges:\\\\\\\\\\n(a) Take three shots, and make at least two of them.\\\\\\\\\\n(b) Take eight shots, and make at least five of them.\\n\\nWhich challenge should you pick to have a better chance of winning the game?\\\\\\\\[0pt]\\n2-24. [3] Tossing a coin ten times resulted in eight heads and two tails. How would you analyze whether a coin is fair? What is the $p$-value?\\n\\n2-25. [5] Given a stream of $n$ numbers, show how to select one uniformly at random using only constant storage. What if you don\\'t know $n$ in advance?\\\\\\\\[0pt]\\n2-26. [5] A $k$-streak starts at toss $i$ in a sequence of $n$ coin flips when the outcome of the $i$ th flip and the next $k-1$ flips are identical. For example, sequence HTTTHH contains 2-streaks starting at the second, third, and fifth tosses. What are the expected number of $k$-streaks that you will see in $n$ tosses of a fair coin ?\\n\\n2-27. [5] A person randomly types an eight-digit number into a pocket calculator. What is the probability that the number looks the same even if the calculator is turned upside down?\\\\\\\\[0pt]\\n2-28. [3] You play a dice rolling game where you have two choices:\\\\\\\\\\n(a) Roll the dice once and get rewarded with a prize equal to the outcome number (e.g, $\\\\$ 3$ for number \" 3 \") and then stop the game.\\\\\\\\\\n(b) You can reject the first reward according to its outcome and roll the dice a second time, and get rewarded in the same way.\\n\\nWhich strategy should you choose to maximize your reward? That is, for what outcomes of the first roll should you chose to play the second game? What is the statistical expectation of reward if you choose the second strategy?\\\\\\\\[0pt]\\n2-29. [3] What is A/B testing and how does it work?\\\\\\\\[0pt]\\n2-30. [3] What is the difference between statistical independence and correlation?\\\\\\\\[0pt]\\n2-31. [3] We often say that correlation does not imply causation. What does this mean?\\n\\n%---- Page End Break Here ---- Page : 55\\n\\n2-32. [5] What is the difference between a skewed distribution and a uniform one?\\n\\n\\\\section*{Kaggle Challenges}\\n2-33. Cause-effect pairs: correlation vs. causation.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/cause-effect-pairs}{https://www.kaggle.com/c/cause-effect-pairs}\\\\\\\\\\n2-34. Predict the next \"random number\" in a sequence.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/random-number-grand-challenge}{https://www.kaggle.com/c/random-number-grand-challenge}\\\\\\\\\\n2-35. Predict the fate of animals at a pet shelter.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/shelter-animal-outcomes}\\n%---- Page End BR\\\\index{R}eak Here ---- Page : 56\\n{https://www.kaggle.com/c/shelter-animal-outcomes}\\n\\n\\\\section*{Chapter 3}\\n\\\\section*{data munging\\\\index{data munging}}\\nOn two occasions I have been asked, \"Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?\" ... I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.\\n\\n\\\\begin{itemize}\\n  \\\\item Charles Babbage\\n\\\\end{itemize}\\n\\nMost data scientists spend much of their time cleaning and formatting data. The rest spend most of their time complaining that there is no data available to do what they want to do.\\n\\nIn this chapter, we will work through some of the basic mechanics of computing with data. Not the high-faluting stuff like statistics or machine learning, but the grunt work of finding data and cleaning it that goes under the moniker of data munging.\\n\\nWhile practical questions like \"What is the best library or programming language available?\" are clearly important, the answers change so rapidly that a book like this one is the wrong place to address them. So I will stick at the level of general principles, instead of shaping this book around a particular set of software tools. Still, we will discuss the landscape of available resources in this chapter: why they exist, what they do, and how best to use them.\\n\\nThe first step in any data science project is getting your hands on the right data. But this is often distressingly hard. This chapter will survey the richest hunting grounds for data resources, and then introduce techniques for cleaning what you kill. Wrangling your data so you that can safely analyze it is critical for meaningful results. As Babbage himself might have said more concisely, \"garbage in, garbage out.\"\\n\\n\\\\subsection*{3.1 languages\\\\index{languages} for Data Science}\\nIn theory, every sufficiently powerful programming language is capable of expressing any algorithm worth computing. But in practice, certain programming\\\\\\\\\\nlanguages prove much better than others at specific tasks. Better here might denote easier for the programmer or perhaps more computationally effi\\\\index{Babbage, Charles}cient, depending upon the mission at hand.\\n\\nThe primary data science programming languages\\\\index{programming languages} to be aware of are:\\n\\n\\\\begin{itemize}\\n  \\\\item Python\\\\index{Python}: This is today\\'s bread-and-butter programming language for data science. Python contains a variety of language features to make basic data munging easier, like regular expressions. It is an interpreted language, making the development process quicker and enjoyable. Python is supported by an enormous variety of libraries, doing everything from scraping to visualization to linear algebra and machine learning.\\n\\\\end{itemize}\\n\\nPerhaps the biggest strike against Python is efficiency: interpreted languages cannot compete with compiled ones for speed. But Python compilers exist in a fashion, and support linking in efficient C/assembly language libraries for computationally-intensive tasks. Bottom line, Python should probably be your primary tool in working through the material we present in this book.\\n\\n\\\\begin{itemize}\\n  \\\\item Perl\\\\index{Perl}: This used to be the go to language for data munging on the web, before Python ate it for lunch. In the TIOBE programming language popularity index (\\\\href{http://www.tiobe.com/tiobe-index}{http://www.tiobe.com/tiobe-index}), Python first exceeded Perl in popularity in 2008 and hasn\\'t looked back. There are several reasons for this, including stronger support for object-oriented programming and better available libraries, but the bottom line is that there are few good reasons to start projects in Perl at this point. Don\\'t be surprised if you encounter it in some legacy project, however.\\n  \\\\item $R$ : This is the programming language of statisticians, with the deepest libraries available for data analysis and visualization. The data science world is split between R and Python camps, with R perhaps more suitable for exploration and Python better for production use. The style of interaction with R is somewhat of an acquired taste, so I encourage you to play with it a bit to see whether it feels natural to you.\\n\\\\end{itemize}\\n\\nLinkages exist between $R$ and Python, so you can conveniently call $R$ library functions in Python code. This provides access to advanced statistical methods, which may not be supported by the native Python libraries.\\n\\n\\\\begin{itemize}\\n  \\\\item Matlab\\\\index{Matlab}: The Mat here stands for matrix, as Matlab is a language designed for the fast and efficient manipulation of matrices. As we will see, many machine learning algorithms reduce to operations on matrices, making Matlab a natural choice for engineers programming at a high-level of abstraction.\\n\\\\end{itemize}\\n\\nMatlab is a proprietary system. However, much of its functionality is available in GNU Octave, an open-source alternative.\\n\\n%---- Page End Break Here ---- Page : 58\\n\\n\\\\begin{itemize}\\n  \\\\item Java and $C / C++$ : These mainstream programming languages for the development of large systems are important in big data applications. Parallel processing systems like Hadoop and Spark are based on Java and C++, respectively. If you are living in the world of distributed computing, then you are living in a world of Java and C++ instead of the other languages listed here.\\n  \\\\item Mathematica/Wolfram Alpha: Mathematica is a proprietary system providing computational support for all aspects of numerical and symbolic mathematics, built upon the less proprietary Wolfram programming language. It is the foundation of the Wolfram Alpha computational knowledge engine, which processes natural language-like queries through a mix of algorithms and pre-digested data sources. Check it out at \\\\href{http://www}{http://www}. \\\\href{http://wolframalpha.com}{wolframalpha.com}.\\\\\\\\\\nI will confess a warm spot for Mathematica. It is what I tend to reach for when I am doing a small data analysis or simulation, but cost has traditionally put it out of the range of many users. The release of the Wolfram language perhaps now opens it up to a wider community.\\n  \\\\item Excel: Spreadsheet programs like Excel are powerful tools for exploratory data analysis, such as playing with a given data set to see what it contains. They deserve our respect for such applications.\\\\\\\\\\nFull featured spreadsheet programs contain a surprising amount of hidden functionality for power users. A student of mine who rose to become a Microsoft executive told me that $25 \\\\%$ of all new feature requests for Excel proposed functionality already present there. The special functions and data manipulation features you want probably are in Excel if you look hard enough, in the same way that a Python library for what you need probably will be found if you search for it.\\n\\\\end{itemize}\\n\\n\\\\subsection*{3.1.1 The Importance of Notebook Environments}\\nThe primary deliverable for a data science project should not be a program. It should not be a data set. It should not be the results of running the program on your data. It should not just be a written report.\\n\\nThe deliverable result of every data science project should be a computable notebook tying together the code, data, computational results, and written analysis of what you have learned in the process. Figure 3.1 presents an excerpt from a Jupyter/IPython notebook, showing how it integrates code, graphics, and documentation into a descriptive document which can be executed like a program.\\n\\nThe reason this is so important is that computational results are the product of long chains of parameter selections and design decisions. This creates several problems that are solved by notebook computing environments:\\n\\n\\\\footnotetext{${ }^{1}$ Full disclosure: I have known Stephen Wolfram for over thirty years. Indeed, we invented the iPad together Bar10 MOR ${ \\n%---- Page End Break Here ---- Page : 59\\n}^{+88}$.\\n}In [40]:\\n\\n\\\\begin{verbatim}\\ndegrees = range(1, 8)\\nerrors = np.array([regressor3(d) for d in degrees])\\nplt.plot(degrees, errors[:, 0], marker=\\'^\\', c=\\'r\\', label=\\'Testing samples\\nplt.plot(degrees, errors[:, 1], marker=\\'o\\', c=\\'b\\', label=\\'Training sample\\nplt.yscale(\\'log\\')\\nplt.xlabel(\"degree\"); plt.ylabel(\"Error\")\\n    = plt.legend(loc=\\'best\\')\\n\\\\end{verbatim}\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-077}\\n\\\\end{center}\\n\\nBy sweeping the degree we discover two regions of model performance:\\n\\n\\\\begin{itemize}\\n  \\\\item Underfitting (degree < 3): Characterized by the fact that the testing error will get lower if we increase the model capacity.\\n  \\\\item Overfitting (degree $>3$ ): Characterized by the fact the testing will get higher if we increase the model capacity. Note, that the training error is getting lower or just staying the same!.\\n\\\\end{itemize}\\n\\nFigure 3.1: Jupyter/IPython\\\\index{IPython} notebooks tie together code, computational results, and documentation.\\n\\n%---- Page End Break Here ---- Page : 60\\n\\n\\\\begin{itemize}\\n  \\\\item Computations need to be reproducible. We must be able to run the same programs again from scratch, and get exactly the same result. This means that data pipelines must be complete: taking raw input and producing the final output. It is terrible karma to start with a raw data set, do some processing, edit/format the data files by hand, and then do some more processing - because what you did by hand cannot be readily done again on another data set, or undone after you realize that you may have goofed up.\\n  \\\\item Computations must be tweakable. Often reconsideration or evaluation will prompt a change to one or more parameters or a\\\\index{Gaussian noise}lgorithms. This requires rerunning the notebook to produce the new comp\\\\index{measurement error}utatio\\\\index{mean}n.\\\\\\\\index{standard deviation}index{normal} There is nothing more disheartening to be given a big data product without provenance and told that this is the final result and you can\\'t change anything. A notebook is never finished until after the entire project is done.\\n  \\\\item Data pipelines need to be documented. That notebooks permit you to integrate text and visualizations with your code provides a powerful way to communicate what you are doing and why, in ways that traditional programming environments cannot match.\\n\\\\end{itemize}\\n\\nTake-Home Lesson: Use a notebook environment like IPython or Mathematica\\\\index{Mathematica} to build and report the results of any data science project.\\n\\n\\\\subsection*{3.1.2 Standard data formats\\\\index{data formats}}\\nData comes from all sorts of places, and in all kinds of formats. Which representation is best depends upon who the ultimate consumer is. Charts and graphs are marvelous ways to convey the meaning of numerical data to people. Indeed, Chapter 6 will focus on techniques for visualizing data. But these pictures are essentially useless as a source of data to compute with. There is a long way from printed maps to Google Maps.\\n\\nThe best computational data formats have several useful properties:\\n\\n\\\\begin{itemize}\\n  \\\\item They are easy for computers to parse: Data written in a useful format is destined to be used again, elsewhere. Sophisticated data formats are often supported by APIs that govern technical details ensuring proper format.\\n  \\\\item They are easy for people to read: Eyeballing data is an essential operation in many contexts. Which of the data files in this directory is the right one for me to use? What do we know about the data fields in this file? What is the gross range of values for each particular field?\\\\\\\\\\nThese use cases speak to the enormous value of being able to open a data file in a text editor to look at it. Typically, this means presenting the data in a human-readable text-encoded format, with records demarcated by separate lines, and fields separated by delimiting symbols.\\n \\n%---- Page End Break Here ---- Page : 61\\n \\\\item They are widely used by other tools and systems: The urge to invent proprietary data standard beats firmly in the corporate heart, and most software developers would rather share a toothbrush than a file format. But these are impulses to be avoided. The power of data comes from mixing and matching it with other data resources, which is best facilitated by using popular standard formats.\\n\\\\end{itemize}\\n\\nOne property I have omitted from this list is conciseness, since it is generally not a primary concern for most applications running on modern computing systems. The quest to minimize data storage costs often works against other goals. Cleverly packing multiple fields into the higher-order bits of integers saves space, but at the cost of making it incompatible and unreadable.\\n\\nGeneral compression utilities like gzip prove amazingly good at removing the redundancy of human-friendly formatting. Disk prices are unbelievably cheap: as I write this you can buy a 4TB drive for about $\\\\$ 100$, meaning less than the cost of one hour of developer time wasted programming a tighter format. Unless you are operating at the scale of Facebook or Google, conciseness does not have nearly the importance you are liable to think it d\\\\index{Zipfâs law}oes.$^{2}$\\n\\nThe most important data formats/representations to be aware of are discussed below:\\n\\n\\\\begin{itemize}\\n  \\\\item CSV (comma separated value) files: These files provide the simplest, most popular format to exchange data between programs. Tha\\\\index{CSV ï¬les}t each line represents a single record, with fields separated by commas, is obvious from inspection. But subtleties revolve around special characters and text strings: what if your data about names contains a comma, like \"Thurston Howell, Jr.\" The csv format provides ways to escape code such characters so they are not treated as delimiters, but it is messy. A better alternative is to use a rarer delimiter character, as in tsv or tab separated value files.\\\\\\\\\\nThe best test of whether your csv file is properly formatted is whether Microsoft Excel or some other spreadsheet program can read it without hassle. Make sure the results of every project pass this test as soon as the first csv file has been written, to avoid pain later.\\n  \\\\item XML\\\\index{XML} (eXtensible Markup Language): Structured but non-tabular data are often written as text with annotations. The natural output of a named-entity tagger for text wraps the relevant substrings of a text in brackets denoting person, place, or thing. I am writing this book in LaTex, a formatting language with bracketing commands positioned around mathematical expressions and italicized text. All webpages are written in HTML, the hypertext markup language which organizes documents using bracketing commands like  and  to enclose bold faced text.\\\\\\\\\\nXML is a language for writing specifications of such markup languages. A proper XML specification enables the user to parse any document complying with the specification. Designing such specifications and fully adhering\\n\\\\end{itemize}\\n\\n\\\\footnotetext{${ }^{2}$ Indeed, my friends at Google assure me that they are often slovenly about space even at the petabyte scale.\\n\\n%---- Page End Break Here ---- Page : 62\\n}\\nto them requires discipline, but is worthwhile. In the first version of our Lydia text analysis system, we wrote our markups in a \"pseudo-XML,\" read by ad hoc parsers that handled $99 \\\\%$ of the documents correctly but broke whenever we tried to extend them. After a painful switch to XML, everything worked more reliably and more efficiently, because we could deploy fast, open-source XML parsers to handle all the dirty work of enforcing our specifications.\\n\\n\\\\begin{itemize}\\n  \\\\item SQL (structured query language) databases: Spreadsheets are naturally structured around single tables of data. In contrast, relational databases prove excellent for manipulating multiple distinct but related tables, using SQL to provide a clunky but powerful query language.\\\\\\\\\\nAny reasonable database system imports and exports records as either csv or XML files, as well as an internal content dump. The internal representation in databases is opaque, so it really isn\\'t accurate to describe them as a data format. Still, I empha\\\\index{protocol buï¬ers}size them here because SQL databases\\\\index{SQL databases} generally prove a better and more powerful solution than manipulating multiple data files in an ad hoc manner.\\n  \\\\item JSON\\\\index{JSON} (JavaScript Object Notation): This is a format for transmitting data objects between programs. It is a natural way to communicate the state of variables/data structures from one system to another. This representation is basically a list of attribute-value pairs corresponding to variable/field names, and the associated values:\\n\\\\end{itemize}\\n\\n\\\\begin{verbatim}\\n{\"employees\":[\\n    {\"firstName\":\"John\", \"lastName\":\"Doe\"},\\n    {\"firstName\":\"Anna\", \"lastName\":\"Smith\"},\\n    {\"firstName\":\"Peter\", \"lastName\":\"Jones\"}\\n]}\\n\\\\end{verbatim}\\n\\nBecause library functions that support reading and writing JSON objects are readily available in all modern programming languages, it has become a very convenient way to store data structures for later use. JSON objects are human readable, but are quite cluttered-looking, representing arrays of records compared to CSV files. Use them for complex structured objects, but not simple tables of data.\\n\\n\\\\begin{itemize}\\n  \\\\item Protocol buffers: These are a language/platform-neutral way of serializing structured data for communications and storage across applications. They are essentially lighter weight versions of XML (where you define the format of your structured data), designed to communicate small amounts of data across programs like JSON. This data format is used for much of the intermachine communication at Google. Apache Thrift is a related standard, used at Facebook.\\n\\n%---- Page End Break Here ---- Page : 63\\n\\\\end{itemize}\\n\\n\\\\subsection*{3.2 collecting\\\\index{collecting} Data}\\nThe most critical issue in any data science or modeling project is finding the right data set. Identifying viable data sources\\\\index{data sources} is an art, one that revolves arou\\\\index{cumulative density function}nd three basic questions:\\n\\n\\\\begin{itemize}\\n  \\\\item Who might actually have the data I need?\\n  \\\\item Why might they decide to make\\\\index{median} it availa\\\\index{samp\\\\index{standard deviation}ling}ble to me?\\n  \\\\\\\\index{mean}\\\\inde\\\\index{scale invariant}x{inverse t\\\\index{probability density function}ransform sampling}item How can I get my hands on it?\\n\\\\end{itemize}\\n\\nIn this section, we will explore the answers to these questions. We look at common sources of data, and what you are likely to be able to find and why. We then review the primary mechanisms for getting access, including API\\\\index{API}s, scraping, and logging.\\n\\n\\\\subsection*{3.2.1 Hunting}\\nWho has the data, and how can you get it? Some of the likely suspects are reviewed below.\\n\\n\\\\section*{Companies and Proprietary Data Sources}\\nLarge companies like Facebook, Google, Amazon, American Express, and Blue Cross have amazing amounts of exciting data about users and transactions, data which could be used to improve how the world works. The problem is that getting outside access is usually impossible. Companies are reluctant to share data for two good reasons:\\n\\n\\\\begin{itemize}\\n  \\\\item Business issues, and the fear of helping their competition.\\n  \\\\item Privacy issues, and the fear of offending their customers.\\n\\\\end{itemize}\\n\\nA heartwarming\\\\index{company data} tale of what can happen with cor\\\\index{beyond one dimension}porate data release occurred when AOL\\\\index{AOL} provided academics with a data set of millions of queries to its search engine, carefully stripped of identifying information. The first thing the academics discovered was that the most frequently-entered queries were desperate attempts to escape to other search engines like Google. This did nothing to increase public confidence in the quality of AOL search.\\n\\nTheir second discovery was that it proved much harder to anonymize search queries than had previously been suspected. Sure you can replace user names with id numbers, but it is not that hard to figure out who the guy on Long Island repeatedly querying Steven Skiena, Stony Brook, and \\\\href{https://twitter.com/}{https://twitter.com/} search?q=Skiena\\\\&src=sprv is. Indeed, as soon as it became publicized that people\\'s identities had been revealed by this data release, the responsible party was fired and the data set disappeared. User privacy is important, and ethical issues around data science will be discussed in Section 12.7\\n\\n\\n%---- Page End Break Here ---- Page : 64\\nSo don\\'t think you are going to sweet talk companies into releasing confidential user data. However, many responsible companies like The New York Times, Twitter, Facebook, and Google do release certain data, typically by rate-limited application program interfaces\\\\index{application program interfaces} (APIs). They generally have two motives:\\n\\n\\\\begin{itemize}\\n  \\\\item Providing customers and third parties with data that can increase sales. For example, releasing data about query frequency and ad pricing can encourage more people to place ads on a given platform.\\n  \\\\item It is generally better for the company to provide well-behaved APIs than having cowboys repeatedly hammer and scrape their site.\\n\\\\end{itemize}\\n\\nSo hunt for a public API before reading Section 3.2 .2 on scraping. You won\\'t find exactly the content or volume that you dream of, but probably something that will suffice to get started. Be aware of limits and terms of use.\\n\\nOther organizations do provide bulk downloads of interesting data for offline analysis, as with the Google Ngrams, IMDb, and the taxi fare data sets discussed in Chapter 1 Large data sets often come with valuable metadata, such as book titles, image captions, and edit history, which can be re-purposed with proper imagination.\\n\\nFinally, most organizations have internal data sets of relevance to their business. As an employee, you should be able to get privileged access while you work there. Be aware that companies have internal data access policies, so you will still be subject to certain restrictions. Violating the terms of these policies is an excellent way to become an ex-employee.\\n\\n\\\\section*{government data\\\\index{government data} Sources}\\nCollecting data is one of the important things that governments do. Indeed, the requirement that the United States conduct a census of its population is mandated by our constitution, and has been running on schedule every ten years since 1790 .\\n\\nCity, state, and federal governments have become increasingly committed to open data, to facilitate novel applications and improve how government can fulfill its mission. The website \\\\href{http://Data.gov}{http://Data.gov} is an initiative by the federal government to centrally collect its data sources, and at last count points to over 100,000 data sets!\\n\\nGovernment data differs from industrial data in that, in principle, it belongs to the People. The Freedom of Information Act\\\\index{Freedom of Information Act} (FOI) enables any citizen to make a formal request for any government document or data set. Such a request triggers a process to determine what can be released without compromising the national interest or violating privacy.\\n\\nState gover\\\\index{signiï¬cance level}nments operate under fifty different sets of laws, so data that is tightly held in one jurisdiction may be freely available in others. Major cities like New York have larger data processing operations than many states, again with restrictions that vary by location.\\n\\n%---- Page End Break Here ---- Page : 65\\n\\nI recommend the following way of thinking about government records. If you cannot find what you need online after some snooping around, figure out which agency is likely to have it. Make a friendly call to them to see if they can help you find what you want. But if they stonewall you, feel free to try for a FOI act request. Preserving privacy is typically the biggest issue in deciding whether a particular government data set can be released.\\n\\n\\\\section*{academic data\\\\index{academic data} Sets}\\nThere is a vast world of academic scholarship, covering all that humanity has deemed worth knowing. An increasing fraction of academic research involves the creation of large data sets. Many journals now require making source data available to other researchers prior to publication. Expect to be able to find vast amounts of economic, medical, demographic, historical, and scientific data if you look hard enough.\\n\\nThe key to finding these data sets is to track down the relevant papers. There is an academic literature on just about any topic of interest. Google Scholar\\\\index{Google Scholar} is the most accessible source of research publications. Search by topic, and perhaps \"Open Science\" or \"data.\" Research publications will typically provide pointers to where its associated data can be found. If not, contacting the author directly with a request should quickly yield the desired result.\\n\\nThe biggest catch with using published data sets is that someone else has worked hard to analyze them before you got to them, so these previously mined sources may have been sucked dry of interesting new results. But bringing fresh questions to old data generally opens new possibilities.\\n\\nOften interesting data science projects involve collaborations between researchers from different disciplines, such as the social and natural sciences. These people speak different languages than you do, and may seem intimidating at first. But they often welcome collaboration, and once you get past the jargon it is usually possible to understand their issues on a reasonable level without specialized study. Be assured that people from other disciplines are generally not any smarter than you are.\\n\\n\\\\section*{sweat equity\\\\index{sweat equity}}\\nSometimes you will have to work for your data, instead of just taking it from others. Much historical data still exists only in books or other paper documents, thus requiring manual entry and curation. A graph or table might contain information that we need, but it can be hard to get numbers from a graphic locked in a PDF (portable document format) file.\\n\\nI have observed that computationally-oriented people vastly over-estimate the amount of effort it takes to do manual data entry. At one record per minute, you can easily enter 1,000 records in only two work days. Instead, computational people tend to devote massive efforts trying to avoid such grunt work, like hunting in vain for optical character recognition (OCR) systems that don\\'t make\\\\\\n%---- Page End Break Here ---- Page : 66\\n\\\\\\na mess of the file, or spending more time cleaning up a noisy scan than it would take to just type it in again fresh.\\n\\nA middle ground here comes in paying someone else to do the dirty work for you. crowdsourcing\\\\index{crowdsourcing} platforms like Amazon Turk\\\\index{Amazon Turk} and CrowdFlower\\\\index{CrowdFlower} enable you to pay for armies of people to help you extract data, or even collect it in the first place. Tasks requiring human annotation like labeling images or answering surveys are particularly good use of remote workers. Crowdsourcing will be discussed in greater detail in Section 3.5\\n\\nMany amazing open data resources have been built up by teams of contributors, like Wikipedia, Freebase, and IMDb. But there is an important concept to remember: people generally work better when you pay them.\\n\\n\\\\subsection*{3.2.2 scraping\\\\index{scraping}}\\nWebpages often contain valuable text and numerical data, which we would like to get our hands on. For example, in our project to build a gambling system for the sport of jai-alai, we needed to feed our system the results of yesterday\\'s matches and the schedule of what games were going on today. Our solution was to scrape the websites of jai-alai betting establishments, which posted this information for their fans.\\n\\nThere are two distinct steps to make this happen, spidering\\\\index{spidering} and scraping:\\n\\n\\\\begin{itemize}\\n  \\\\item Spidering is the process of downloading the right set of pages for analysis.\\n  \\\\item Scraping is the fine art of stripping this content from each page to prepare it for computational analysis.\\n\\\\end{itemize}\\n\\nThe first thing to realize is that webpages are generally written in simple-tounderstand formatting languages like HTML\\\\index{HTML} and/or JavaScript. Your browser knows these languages, and interprets the text of the webpage as a program to specify what to display. By calling a function that emulates/pretends to be a web browser, your program can download any webpage and interpret the contents for analysis.\\n\\nTraditionally, scraping programs were site-specific scripts hacked up to look for particular HTML patterns flanking the content of interest. This exploited the fact that large numbers of pages on specific websites are generated by programs themselves, and hence highly predictable in their format. But such scripts tend to be ugly and brittle, breaking whenever the target website tinkers with the internal structure of its pages.\\n\\nToday, libraries in languages like Python\\\\index{Python} (see BeautifulSoup) make it easier to write robust spiders and scrapers. Indeed, someone else probably has already written a spider/scraper for every popular website and made it available on SourceForge or Github, so search before you code.\\n\\nCertain spidering missions may be trivial, for example, hitting a single URL (uniform resource locator) at regular time intervals. Such patterns occur in monitoring, say, the sales rank of this book from its Amazon page. Somewhat more sophisticated approaches to spidering are based on the name regularity of the\\\\\\n%---- Page End Break Here ---- Page : 67\\n\\\\\\nunderlying URLs. If all the pages on a site are specified by the date or product ID number, for example \\\\href{http://www.amazon.com/gp/product/1107041376/}{http://www.amazon.com/gp/product/1107041376/}, iterating through the entire range of interesting values becomes just a matter of counting.\\n\\nThe most advanced form of spidering is web crawling\\\\index{web crawling}, where you systematically traverse all outgoing links from a given root page, continuing recursively until you have visited every page on the target website. This is what Google does in indexing the web. You can do it too, with enough patience and easy-to-find web crawling libraries in Python.\\n\\nPlease understand that politeness limits how rapidly you should spider/crawl a given website. It is considered bad form to hit a site more than once a second, and indeed best practices dictate that providers block access to the people who are hammering them.\\n\\nEvery major website contains a terms of service\\\\index{terms of service} document that restricts what you can legally do with any associated data. Generally speaking, most sites will leave you alone provided you don\\'t hammer them, and do not redistribute any data you scrape. Understand that this is an observation, not a legal opinion. Indeed, read about the Aaron Schwartz case\\\\index{Aaron Schwartz case}, where a well-known Internet figure was brought up on serious criminal charges for violating terms of services in spidering/scraping journal articles, and literally hounded to death. If you are attempting a web-scraping project professionally, be sure that management understands the terms of service before you get too creative with someone else\\'s property.\\n\\n\\\\subsection*{3.2.3 logging\\\\index{logging}}\\nIf you own a potential data source, treat it like you own it. Internal access to a web service, communications device, or laboratory instrument grants you the right and responsibility to log all activity for downstream analysis.\\n\\nAmazing things can be done with ambient data collection from weblogs and sensing devices, soon destined to explode with the coming \"Internet of Things\\\\index{Internet of Things}.\" The accelerometers in cell phones can be used to measure the strength of earthquakes, with the correlation of events within a region sufficient to filter out people driving on bumpy roads or leaving their phones in a clothes dryer. Monitoring the GPS data of a fleet of taxi cabs tracks traffic congestion on city streets. Computational analysis of image and video streams opens the door to countless applications. Another cool idea is to use cameras as weather instruments, by looking at the color of the sky in the background of the millions of photographs uploaded to photo sites daily.\\n\\nThe primary reason to instrument your system to collect data is because you can. You might not know exactly what to do with it now, but any wellconstructed data set is likely to become of value once it hits a certain critical mass of size.\\n\\nCurrent storage costs make clear just how low a barrier it is to instrument a system. My local Costco is currently selling three terabyte disk drive for under $\\\\$ 100$, which is Big O of nothing. If each transaction record takes 1 kilobyte (one\\\\\\n%---- Page End Break Here ---- Page : 68\\n\\\\\\nthousand characters), this device in principle has room for 3 billion records, roughly one for every two people on earth.\\n\\nThe important considerations in designing any logging system are:\\n\\n\\\\begin{itemize}\\n  \\\\item Build it to endure with limited maintenance. Set it and forget it, by provisioning it with enough storage for unlimited expansion, and a backup.\\n  \\\\item Store all fields of possible value, without going crazy.\\n  \\\\item Use a human-readable format or transactions database, so you can understand exactly what is in there when the time comes, months or years later, to sit down and analyze your data.\\n\\\\end{itemize}\\n\\n\\\\subsection*{3.3 cleaning\\\\index{cleaning} Data}\\n\"garbage in, garbage out\\\\index{garbage in, garbage out}\" is th\\\\index{errors vs. artifacts}e fundamental principle of data analysis. The road from raw data to a clean, analyzable data set can be a long one.\\n\\nMany potential issues can arise in cleaning data for analysis. In this section, we discuss identifying processing artifacts\\\\index{artifacts} and integrating diverse data sets. Our focus here is the processing before we do our real analysis, to make sure that the garbage never gets in in the first place.\\n\\nTake-Home Lesson: Savvy painting restorers only do things to the original that are reversible. They never do harm. Similarly, data cleaning is always done on a copy of the original data, ideally by a pipeline that makes changes in a systematic and repeatable way.\\n\\n\\\\subsection*{3.3.1 errors\\\\index{errors} vs. Artifacts}\\nUnder ancient Jewish law, if a suspect on trial was unanimously found guilty by all judges, then this suspect would be acquitted. The judges had noticed that unanimous agreement often indicates the presence of a systemic error in the judicial process. They reasoned that when something seems too good to be true, a mistake has likely been made somewhere.\\n\\nIf we view data items as measurements about some aspect of the world, data errors represent information that is fundamentally lost in acquisition. The Gaussian noise blurring the resolution of our sensors represents error, precision which has been permanently lost. The two hours of missing logs because the server crashed represents data error: it is information which cannot be reconstructed again.\\n\\nBy contrast, artifacts are generally systematic problems arising from processing done to the raw information it was constructed from. The good news is that processing artifacts can be corrected, so long as the original raw data set remains available. The bad news is that these artifacts must be detected before they can be corrected.\\\\\\n%---- Page End Break Here ---- Page : 69\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-087}\\n\\nFigure 3.2: What artifacts can you find in this time series, counting the number of author\\'s names first appearing in the scientific literature each year?\\n\\nThe key to detecting processing artifacts is the \"sniff test,\" examining the product closely enough to get a whiff of something bad. Something bad is usually something unexpected or surprising, because people are naturally optimists. Surprising observations are what data scientists live for. Indeed, such insights are the primary reason we do what we do. But in my experience, most surprises turn out to be artifacts, so we must look at them skeptically.\\n\\nFigure 3.2 presents computational results from a project where we investigated the process of scientific publication. It shows a time series of the 100,000 most prolific authors, binned according to the year of their first paper appearing in Pubmed\\\\index{Pubmed}, an essentially complete bibliography of the biomedical literature.\\n\\nStudy this figure closely, and see if you can discover any artifacts worth commenting on. I see at least two of them. Extra credit will be awarded if you can figure out what caused the problem.\\n\\nThe key to finding artifacts is to look for anomalies in the data, that contradict what you expect to see. What should the distribution in the number of virgin authors look like, and how should it change over time? First, construct a prior distribution of what you expect to see, so that you can then properly evaluate potential anomalies against it.\\n\\nMy intuition says that the distribution of new top scientists should be pretty flat, because new stars are born with every successive class of graduate students. I would also guess that there may be a gradual drift upward as population expands, and more people enter the scientific community. But that\\'s not what I see in Figure 3.2. So try to enumerate what the anomalies/potential arti\\\\index{Bayesâ theorem}facts are...\\n\\nI see two big bumps when I look at Figure 3.2 a left bump starting around\\\\\\n%---- Page End Break Here ---- Page : 70\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-088}\\n\\nFigure 3.3: The cleaned data removes these artifacts, and the resulting distribution looks correct.\\n\\n1965, and a peak which explodes in 2002. On reflection, the leftmost bump makes sense. This left peak occurs the year when Pubmed first started to systematically collect bibliographic records. Although there is some very incomplete data from 1960-1964, most older scientists who had been publishing papers for several years would \"emerge\" only with the start of systematic records in 1965. So this explains the left peak, which then settles down by 1970 to what looks like the flat distribution we expected.\\n\\nBut what about that giant 2002 peak? And the decline in new authors to almost zero in the years which precede it? A similar decline is also visible to the right of the big peak. Were all the world\\'s major scientists destined to be born in 2002?\\n\\nA careful inspection of the records in the big peak revealed the source of the anomaly: first names. In the early days of Pubmed, authors were identified by their initials and last names. But late in 2001, SS Skiena became Steven $S$. Skiena, so it looked like a new author emerging from the heavens.\\n\\nBut why the declines to nothingness to the left and right of this peak? Recall that we limited this study to the 100,000 most prolific scientists. A scientific rock star emerging in 1998 would be unlikely to appear in this ranking because their name was doomed to change a few years later, not leaving enough time to accumulate a full career of papers. Similar things happen at the very right of the distribution: newly created scientists in 2010 would never be able to achieve a full career\\'s work in only a couple of years. Both phenomena are neatly explained by this first name basis.\\n\\nCleaning this data to unify name references took us a few iterations to get right. Even after eliminating the 2002 peak, we still saw a substantial dip in prominent scientists starting their careers in the mid 1990s. This was because many people who had a great half career pre-first names and a second great half career post-first names did not rise to the threshold of a great full career\\\\\\n%---- Page End Break Here ---- Page : 71\\n\\\\\\nin either single period. Thus we had to match all the names in the full before identifying who were the top 100,000 scientists.\\n\\nFigure 3.3 shows our final distribution of authors, which matches the platonic ideal of what we expected the distribution to be. Don\\'t be too quick to rationalize away how your data looks coming out of the computer. My collaborators were at one point ready to write off the 2002 bump as due to increases in research funding or the creation of new scientific journals. Always be suspicious of whether your data is clean enough to trust.\\n\\n\\\\subsection*{3.3.2 Data compatibility\\\\index{compatibility}}\\nWe say that a comparison of two items is \"apples to apples\" when it is fair comparison, that the items involved are similar enough that they can be meaningfully stood up against each other. In contrast, \"apples to oranges\" comparisons are ultimately meaningless. For example:\\n\\n\\\\begin{itemize}\\n  \\\\item It makes no sense to compare weights of 123.5 against 78.9 , when one is in pounds and the other is in kilograms.\\n  \\\\item It makes no sense to directly compare the movie gross of Gone with the Wind against that of Avatar, because 1939 dollars are 15.43 times more valuable than 2009 dollars.\\n  \\\\item It makes no sense to compare the price of gold at noon today in New York and London, because the time zones are five hours off, and the prices affected by intervening events.\\n  \\\\item It makes no sense to compare the stock price of Microsoft on February 17, 2003 to that of February 18, 2003, because the intervening 2 -for- 1 stock split cut the price in half, but reflects no change in real value.\\n\\\\end{itemize}\\n\\nThese types of data comparability issues arise whenever data sets are merged. Here I hope to show you how insidious such comparability issues can be, to sensitize you as to why you need to\\\\index{exercises} be aware of them. Further, for certain important classes of conversions I point to ways to deal with them.\\n\\nTake-Home Lesson: Review the meaning of each of the fields in any data set you work with. If you do not understand what\\'s in there down to the units of measurement, there is no sensible way you can use it.\\n\\n\\\\section*{unit conversions\\\\index{unit conversions}}\\nQuantifying observations in physical systems requires standard units of measurement. Unfortunately there exist many functionally equivalent but incompatible systems of measurement. My 12-year old daughter and I both weigh about 70, but one of us is in pounds and the other in kilograms.\\n\\n%---- Page End Break Here ---- Page : 72\\n\\nDisastrous things like rocket explosions happen when measurements are entered into computer systems using the wrong units of measurement. In particular, NASA\\\\index{NASA} lost the $\\\\$ 125$ million Mars Climate Orbiter space mission on September 23, 1999 due to a metric-to-English conversion issue.\\n\\nSuch problems are best addressed by selecting a single system of measurements and sticking to it. The metric system offers several advantages over the traditional English system. In particular, individual measurements are naturally expressed as single decimal quantities (like 3.28 meters) instead of incomparable pairs of quantities ( 5 feet, 8 inches). This same issue arises in measuring angles (radians vs. degrees/seconds) and weight (kilograms vs. pounds/oz).\\n\\nSticking to the metric system does not by itself solve all comparability issues, since there is nothing to prevent you from mixing heights in meters and centimeters. But it is a good start.\\n\\nHow can you defend yourself against incompatible units when merging data sets? Vigilance has to be your main weapon. Make sure that you know the intended units for each numerical column in your data set, and verify compatibility when merging. Any column which does not have an associated unit or object type should immediately be suspect.\\n\\nWhen merging records from diverse sources, it is an excellent practice to create a new \"origin\" or \"source\" field to identify where each record came from. This provides at least the hope that unit conversion mistakes can be corrected later, by systematically operating on the records from the problematic source.\\n\\nA partially-automated procedure to detect such problems can be devised from statistical significance testing, to be discussed in Section 5.3 Suppose we were to plot the frequencies of human heights in a merged data set of English (feet) and metric (meter) measurements. We would see one peak in the distribution around 1.8 a\\\\index{numerical conversions}nd a second around 5.5. The existence of multiple peaks in a distribution should make us suspicious. The $p$-value resulting from significance testing on the two input populations provides a rigorous measurement of the degree to which our suspicions are validated.\\n\\n\\\\section*{Numerical Representation Conversions}\\nNumerical features are the easiest to incorporate into mathematical models. Indeed, certain machine learning algorithms such as linear regression and support vector machines work only with numerically-coded data. But even turning numbers into numbers can be a subtle problem. Numerical fields might be represented in different ways: as integers (123), as decimals (123.5), or even as fractions (123 1/2). Numbers can even be represented as text, requiring the conversion from \"ten million\" to 10000000 for numerical processing.\\n\\nNumerical representation issues can take credit for destroying another rocket ship. An Ariane 5 rocket launched at a cost of $\\\\$ 500$ million on June 4, 1996 exploded forty seconds after lift-off, with the cause ultimately ascribed to an unsuccessful conversion of a 64 -bit floating point number to a 16 -bit integer.\\n\\nThe distinction between integers and floating point (real) numbers is important to maintain. Integers are counting numbers: quantities which are really\\\\\\n%---- Page End Break Here ---- Page : 73\\n\\\\\\ndiscrete should be represented as integers. Physically measured quantities are never precisely quantified, because we live in a continuous world. Thus all measurements should be reported as real numbers. Integer approximations of real numbers are sometimes used in a misbegotten attempt to save space. Don\\'t do this: the quantification effects of rounding or truncation introduces artifacts.\\n\\nIn one particularly clumsy data set we encountered, baby weights were represented as two integer fields (pounds and the remaining ounces). Much better would have been to combine them into a single decimal quantity.\\n\\n\\\\section*{Name Unification}\\nIntegrating records from two distinct data sets requires them to share a common key field. Names are frequently used as key fields, but they are often reported inconsistently. Is JosÃ© the same fellow as Jose? Such diacritic marks are banned from the official birth records of several U.S. states, in an aggressive attempt to force them to be consistent.\\n\\nAs another case in point, databases show my publications as authored by the Cartesian product of my first (Steve, Steven, or S.), middle (Sol, S., or blank), and last (Skiena) names, allowing for nine different variations. And things get worse if we include misspellings. I can find mys\\\\index{character code uniï¬cation}elf on Goo\\\\index{name uniï¬cation}gle with a first name of Stephen and last names of Skienna and Skeina.\\n\\nUnifying records by key is a very ugly problem, which doesn\\'t have a magic bullet. This is exactly why ID numbers were invented, so use them as keys if you possibly can.\\n\\nThe best general technique is unification: doing simple text transformations to reduce each name to a single canonical version. Converting all strings to lower case increases the number of (usually correct) collisions. Eliminating middle names or at least reducing them to an abbreviation creates even more name matches/collisions, as does mapping first names to canonical versions (like turning all Steves into Stevens).\\n\\nAny such transformation runs the risk of creating Frankenstein-people, single records assembled from multiple bodies. Applications differ in whether the greater danger lies in merging too aggressively or too timidly. Figure out where your task sits on this spectrum and act accordingly.\\n\\nAn important concern in merging data sets is character code unification. Characters in text strings are assigned numerical representations, with the mapping between symbols and number governed by the character code standard. Unfortunately, there are several different character code standards in common usage, meaning that what you scrape from a webpage might not be in the same character code as assumed by the system which will process it.\\n\\nHistorically, the good old 7-bit ASCII code standard was expanded to the 8-bit ISO 8859-1 Latin alphabet code, which adds characters and punctuation marks from several European languages. UTF-8 is an encoding of all Unicode characters using variable numbers of 8-bit blocks, which is backwards compatible with ASCII. It is the dominant encoding for web-pages, although other systems remain in\\\\index{summary statistics} use.\\n\\n%---- Page End Break Here ---- Page : 74\\n\\nCorrectly unifying character codes after merging is pretty much impossible. You must have the discipline to pick a single code as a standard, and check the encoding of each input file on preprocessing, converting it to the target before further work.\\n\\n\\\\section*{Time/Date Unification}\\nData/time stamps are used to infer the relative order of events, and group events by relative simultaneity. Integrating event data from multiple sources requires careful cleaning to ensure meaningful results.\\n\\nFirst let us consider issues in measuring time. The clocks from two computers never exactly agree, so precisely aligning logs from different systems requires a mix of work and guesswork. There are also time zone issues when dealing with data from different regions, as well as diversities in local rules governing changes in daylight saving time.\\n\\nThe right answer here is to align all time measurements to Coordinated Universal Time (UTC), a modern standard subsuming the traditional Greenwich Mean Time (GMT). A related standard is UNIX time\\\\index{UNIX time}, which reports an event\\'s precise time in terms of the number of elapsed seconds since 00:00:00 UTC on Thursday, January 1, 1970.\\n\\nThe Gregorian cal\\\\index{ï¬nancial uniï¬cati\\\\index{time uniï¬cation}on}endar is common throughout the technology world, although many other calendar systems are in use in different countries. Subtle algorithms must be used to convert between calendar systems, as described in RD01. A bigger problem for date alignment concerns the proper interpretation of time zones and the international date line.\\n\\nTime series unification is often complicated by the nature of the business calendar. Financial markets are closed on weekends and holidays, making for questions of interpretation when you are correlating, say, stock prices to local temperature. What is the right moment over the weekend to measure temperature, so as to be consistent with other days of the week? Languages like Python contain extensive libraries to deal with financial time series data to get issues like this correct. Similar issues arise with monthly data, because months (and even years) have different lengths.\\n\\n\\\\section*{Financial Unification}\\nMoney makes the world go round, which is why s\\\\index{summary statistics}o many data science projects revolve around financial time series. But money can be dirty, so this data requires cleaning.\\n\\nOne issue here is currency conversion\\\\index{currency conversion}, representing international prices using a standardized financial unit. Currency exchange rates can vary by a few percent within a given day, so certain applications require time-sensiti\\\\index{Anscombeâs Quartet}ve conversions. Conversion rates are not truly standardized. Different markets will each have different rates and spreads, the gap between buying and selling prices that cover the cost of conversion.\\n\\n%---- Page End Break Here ---- Page : 75\\n\\nThe other important correction is for inflation. The time value of money implies that a dollar today is (generally) more valuable than a dollar a year from now, with interest rates providing the right way to discount future dollars. Inflation rates are estimated by tracking price changes over baskets of items, and provide a way to standardize the purchasing power of a dollar over time.\\n\\nUsing unadjusted prices in a model over non-trivial periods of time is just begging for trouble. A group of my students once got very excited by the strong correlation observed between stock prices and oil prices over a thirtyyear period, and so tried to use stock prices in a commodity prediction model. But both goods were priced in dollars, without any adjustment as they inflated. The time series of prices of essentially any pair of items will correlate strongly over time when you do not correct for inflation.\\n\\nIn fact, the most meaningful way to represent price changes over time is probably not differences but returns, which normalize the difference by the initial price:\\n\\n$$\\nr_{i}=\\\\frac{p_{i+1}-p_{i}}{p_{i}}\\n$$\\n\\nThis is more analogous to a percentage change, with the advantage here that taking the logarithm of this ratio becomes symmetric to gains and losses.\\n\\nFinancial time series contain many other subtleties which require cleaning. Many stocks give scheduled dividends to the shareholder on a particular date every year. Say, for exampl\\\\index{inï¬ation rates}e, that Microsoft will pay a $\\\\$ 2.50$ dividend on January 16. If you own a share of Microsoft at the start of business that day, you receive this check, so the value of the share then immediately drops by $\\\\$ 2.50$ the moment after the dividend is issued. This price decline reflects no real loss to the shareholder, but properly cleaned data needs to factor the dividend into the price of the stock. It is easy to imagine a model trained on uncorrected price data learning to sell stocks just prior to its issuing dividends, and feeling unjustly proud of itself for doing so.\\n\\n\\\\subsection*{3.3.3 Dealing with missing values\\\\index{missing values}}\\nNot all data sets are complete. An important aspect of data cleaning is identifying fields for which data isn\\'t there, and then properly compensating for them:\\n\\n\\\\begin{itemize}\\n  \\\\item What is the year of death of a living person?\\n  \\\\item What should you do with a survey question left blank, or filled with an obviously outlandish value?\\n  \\\\item What is the relative frequency of events too rare to see in a limited-size sample?\\n\\\\end{itemize}\\n\\nNumerical data sets expect a value for every element in a matrix. Setting missing values to zero is tempting, but generally wrong, because there is always some ambiguity as to whether these values should be interpreted as data or not.\\n\\n%---- Page End Break Here ---- Page : 76\\n\\nIs someone\\'s salary zero because he is unemployed, or did he just not answer the question?\\n\\nThe danger with using nonsense values as not-data symbols is that they can get misinterpreted as data when it comes time to build models. A linear regression model trained to predict salaries from age, education, and gender will have trouble with people who refused to answer the question.\\n\\nUsing a value like -1 as a no-data symbol has exactly the same deficiencies as zero. Indeed, be like the mathematician who is afraid of negative numbers: stop at nothing to avoid them.\\n\\nTake-Home Lesson: Separately maintain both the raw data and its cleaned version. The raw data is the ground truth, and must be preserved intact for future analysis. The cleaned data may be improved using imputation to fill in missing values. But keep raw data distinct from cleaned, so we can investigate different approaches to guessing.\\n\\nSo how should we deal with missing values? The simplest approach is to drop all records containing missing values. This works just fine when it leaves enough training data, provided the missing values are absent for non-systematic reasons. If the people refusing to state their salary were generally those above the mean, dropping these records will lead to biased results.\\n\\nBut typically we want to make use of records with missing fields. It can be better to estimate or impute missing values, instead of leaving them blank. We need general methods for filling in missing values. Candidates include:\\n\\n\\\\begin{item\\\\index{by mean value}ize\\\\index{by random value}}\\n  \\\\item heuristic-based\\\\index{heuristic-based} imputation: Given sufficient knowledge of the underlying domain, we should be able to make a reasonable guess for the value of certain fields. If I need to fill in a value for the year you will die, guessing birth year +80 will prove about right on average, and a lot faster than waiting for the final answer.\\n  \\\\item Mean value imputation: Using the mean value of a variable as a proxy for missing values is generally sensible. First, adding more values with the mean leaves the mean unchanged, so we do not bias our statistics by such imputation. Second, fields with mean values add a vanilla flavor to most models, so they have a muted impact on any forecast made using the data.\\n\\\\end{itemize}\\n\\nBut the mean might not be appropriate if there is a systematic reason for missing data. Suppose we used the mean death-year in Wikipedia to impute the missing value for all living people. This would prove disastrous, with many people recorded as dying before they were actually born.\\n\\n\\\\begin{itemize}\\n  \\\\item Random value imputation: Another approach is to select a random value from the column to replace the missing value. This would seem to set us up for potentially lousy guesses, but that is actually the point. Repeatedly selecting random values permits statistical evaluation of the impact of imputation. If we run the model ten times with ten different imputed\\\\\\n%---- Page End Break Here ---- Page : 77\\n\\\\\\nvalues and get widely varying results, then we probably shouldn\\'t have much confidence in the model. This accuracy check is particularly valuable when there is a substantial fraction of values missing from the data set.\\n  \\\\item Imputation by nearest neighbor\\\\index{by nearest neighbor}: What if we identify the complete record which matches most closely on all fields present, and use this nearest neighbor to infer the values of what is missing? Such predictions should be more accurate than the mean, when there are systematic reasons to explain variance among records.\\n\\\\end{itemize}\\n\\nThis approach requires a distance function to identify the most similar records. Nearest neighbor methods are an important technique in data science, and will be presented in greater detail in Section 10.2\\n\\n\\\\begin{itemize}\\n  \\\\item Imputation by interpolation\\\\index{by interpolation}: More generally, we can use a method like linear regression (see Section 9.1) to predict the values of the target column, given the other fields in the record. Such models can be trained over full records and then applied to those with missing values.\\n\\\\end{itemize}\\n\\nUsing linear regression to predict missing values works best when there is only one field missing per record. The potential danger here is creating significant outliers through lousy predictions. Regression models can easily turn an incomplete record into an outlier, by filling the missing fields in with unusually high or low values. This would lead downstream analysis to focus more attention on the records with missing values, exactly the opposite of what we want to do.\\n\\nSuch concerns emphasize the importance of outlier detection\\\\index{detection}, the final step in the cleaning process that will be considered here.\\n\\n\\\\subsection*{3.3.4 Outlier Detection}\\nMistakes in data collection can easily produce outliers\\\\index{scaling and labeling} that can interfere with proper analysis. An interesting example concerns the largest dinosaur vertebra\\\\index{dinosaur vertebra} ever discovered. Measured at 1500 millimeters, it implies an individual that was 188 feet long. This is amazing, particularly because the second largest specimen ever discovered comes in at only 122 feet.\\n\\nThe most likely explanation here (see [Gol16]) is that this giant fossil never actually existed: it has been missing from the American Museum of Natural History for over a hundred years. Perhaps the original measurement was taken on a conventionally-sized bone and the center two digits accidentally transposed, reducing the vertebra down to 1050 millimeters.\\n\\nOutlier elements are often created by data entry mistakes, as apparently was the case here. They can also result from errors in scraping, say an irregularity in formatting causing a footnote number to be interpreted as a numerical value. Just because something is written down doesn\\'t make it correct. As with the dinosaur example, a single outlier element can lead to major misinterpretations.\\n\\n%---- Page End Break Here ---- Page : 78\\n\\nGeneral sanity checking requires looking at the largest and smallest values in each variable/column to see whether they are too far out of line. This can best be done by plotting the frequency histogram and looking at the location of the extreme elements. Visual inspection can also confirm that the distribution looks the way it should, typically bell-shaped.\\n\\nIn normally distributed data, the probability that a value is $k$ standard deviations from the mean decreases exponentially with $k$. This explains why there are no 10 -foot basketball players, and provides a sound threshold to identify outliers. Power law distributions are less easy to detect outliers in: there really is a Bill Gates worth over 10,000 times as much as the average individual.\\n\\nIt is too simple to just delete the rows containing outlier fields and move on. Outliers often point to more systematic problems that one must deal with. Consider a data set of historical figures by lifespan. It is easy to finger the biblical Methuselah (at 969 years) as an outlier, and remove him.\\n\\nBut it is better to figure out whether he is \\\\index{repetition}indicative of other figures that we should consider removing. Observe that Methuselah had no firmly established birth and death dates. Perhaps the published ages of anybody without dates should be considered suspicious enough to prune. By contrast, the person with the shortest lifespan in Wikipedia\\\\index{Wikipedia} (John I, King of France) lived only five days. But his birth (November 15) and death (November 20) dates in 1316 convinces me that his lifespan was accurate.\\n\\n\\\\\\\\index{normal distribution}subsection*{3.4 War Story: Beating the Market}\\nEvery time we met, my graduate student Wenbin told me we were making money. But he sounded less and less confident every time I asked.\\n\\nOur Lydia sentiment analysis system took in massive text feeds of news and social media, reducing them to daily time series of frequency and sentiment for the millions of different people, places, and organizations mentioned within. When somebody wins a sports championship, many articles get written describing how great an athlete they are. But when this player then gets busted on drug charges, the tone of the articles about them immediately changes. By keeping count of the relative frequency of association with positive words (\"victorious\") vs. negative words (\"arrested\") in the text stream, we can construct sentiment signals for any news-worthy entity.\\n\\nWenbin studied how sentiment signals could be used to predict future events like the gross for a given movie, in response to the quality of published reviews or buzz. But he particularly wanted to use this data to play the stock market\\\\index{stock market}. Stocks move up and down according to news. A missed earnings report is bad news for a company, so the price goes down. Food and Drug Administration (FDA) approval of a new drug is great news for the company which owns it, so the price goes up. If Wenbin could use our sentiment signal to predict future stock prices, well, let\\'s just say I wouldn\\'t have to pay him as a research assistant anymore.\\n\\nSo he simulated a strategy of buying the stocks that showed the highest\\\\\\n%---- Page End Break Here ---- Page : 79\\n\\\\\\nsentiment in that day\\'s news, and then shorting those with the lowest sentiment. He got great results. \"See,\" he said. \"We are making money.\"\\n\\nThe numbers looked great, but I had one quibble. Using today\\'s news results to predict current price movements wasn\\'t really fair, because the event described in the article may have already moved the price before we had any chance to read about it. Stock prices should react very quickly to important news.\\n\\nSo Wenbin simulated the strategy of buying stocks based on sentiment from the previous day\\'s news, to create a gap between the observed news and price changes. The return rate went down substantially, but was still positive. \"See,\" he said. \"We are still making money.\"\\n\\nBut I remained a little uncomfortable with this. Many economists believe that the financial markets are efficient, meaning that all public news is instantly reflected in changing prices. Prices certainly changed in response to news, but you would not be able to get in fast enough to exploit the information. We had to remain skeptical enough to make sure there were no data/timing problems that could explain our results.\\n\\nSo I pressed Wenbin about exactly how he had performed his simulation. His strategy bought and sold at the closing price every day. But that left sixteen hours until the next day\\'s open, plenty of time for the world to react to events that happened while I slept. He switched his simulated purchase to the opening price. Again, the return rate went down substantially, but was still positive. \"See,\" he said. \"We are still making some money.\"\\n\\nBut might there still be other artifacts in how we timed our data, giving us essentially tomorrow\\'s newspaper today? In good faith, we chased down all other possibilities we could think of, such as whether the published article dates reflected when they appeared instead of when they were written. After doing our best to be skeptical, his strategies still seemed to show positive returns from news sentiment.\\n\\nOur paper on this analysis ZS10 has been well received, and Wenbin has gone on to be a successful quant, using sentiment among other signals to trade in the financial markets. But I remain slightly queasy about this result. Cleaning our data to precisely time-stamp each news article was very difficult to do correctly. Our system was originally designed to produce daily time series in a batch mode, so it is hard to be sure that we did everything right in the millions of articles downloaded over several years to now perform finer-scale analysis.\\n\\nThe take-home lesson is that cleanliness is important when there is money on the line. Further, it is better to design a clean environment at the beginning of analysis instead of furiously washing up at the end.\\n\\n\\\\subsection*{3.5 crowdsourcing\\\\index{crowdsourcing}}\\nNo single person has all the answers. Not even me. Much of what passes for wisdom is how we aggregate expertise, assembling opinions from the knowledge and experience of others.\\\\\\n%---- Page End Break Here ---- Page : 80\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-098}\\n\\nFigure 3.4: Guess how many pennies I have in this jar? (left) The correct answer was determined using precise scientific methods (right).\\n\\nCrowdsourcing harnesses the insights and labor from large numbers of people towards a common goal. It exploits the wisdom of crowds\\\\index{wisdom of crowds}, that the collective knowledge of a group of people might well be greater than that of the smartest individual among them.\\n\\nThis notion began with an ox. Francis Galton, a founder of statistical science and a relative of Charles Darwin, attended a local livestock fair in 1906. As part of the festivities, villagers were invited to guess the weight of this particular ox, with the person whose guess proved closest to the mark earning a prize. Almost 800 participants took a whack at it. No one picked the actual weight of 1,178 pounds, yet Galton observed th\\\\index{Darw\\\\index{Galton, Francis}in, Charles}at the average guess was amazingly close: 1,179 pounds! Galton\\'s experiment suggests that for certain tasks one can get better results by involving a diverse collection of people, instead of just asking the experts.\\n\\nCrowdsourcing serves as an important source of data in building models, especially for tasks associated with human perception. Humans remain the state-of-the-art system in natural language processing and computer vision, achieving the highest level of performance. The best way to gather training data often requires asking people to score a particular text or image. Doing this on a large enough scale to build substantial training data typically requires a large number of annotators, indeed a crowd.\\n\\nSocial media and other new technologies have made it easier to collect and aggregate opinions on a massive scale. But how can we separate the wisdom of crowds from the cries of the rabble?\\n\\n\\\\subsection*{3.5.1 The Penny Demo}\\nLet\\'s start by performing a little wisdom of crowds experiment of our own. Figure 3.4 contains photos of a jar of pennies I accumulated in my office over many years. How many pennies do I have in this jar? Make your own guess now, because I am going to tell you the answer on the next page.\\n\\n%---- Page End Break Here ---- Page : 81\\n\\nTo get the right answer, I had my biologist-collaborator Justin Garden weigh the pennies on a precision laboratory scale. Dividing by the weight of a single penny gives the count. Justin can be seen diligently performing his task in Figure 3.4 (right).\\n\\nSo I ask again: how many pennies do you think I have in this jar? I performed this experiment on students in my data science class. How will your answer compare to theirs?\\n\\nI first asked eleven of my students to write their opinions on cards and quietly pass them up to me at the front of the room. Thus these guesses were completely independent of each other. The results, sorted for convenience, were:\\n\\n$$\\n537,556,600,636,1200,1250,2350,3000,5000,11,000,15,000 .\\n$$\\n\\nI then wrote then wrote these numbers on the board, and computed some statistics. The median\\\\index{median} of these guesses was 1250 , with a mean\\\\index{mean} of 3739 . In fact, there were exactly 1879 pennies in the jar. The median score among my students was closer to the right amount than any single guess.\\n\\nBut before revealing the actual total, I then asked another dozen students to guess. The only difference was that this cohort had seen the guesses from the first set of students written on the board. Their choices were:\\\\\\\\\\n$750,750,1000,1000,1000,1250,1400,1770,1800,3500,4000,5000$.\\\\\\\\\\nExposing the cohort to other people\\'s guesses strongly conditioned the distribution by eliminating all outliers: the minimum among the second grou\\\\index{Oh G-d}p was greater than four of the previous guesses\\\\index{line hatchings}, and the maximum less than or equal to three o\\\\index{Surowiecki, James}f the previous round. Within this cohort, the median was 1325 and the mean 1935. Both happen to be somewhat closer to the actual answer, but it is clear that group-think had settled in to make it happen.\\n\\nanchoring\\\\in\\\\index{scatter plots}dex{anch\\\\index{Body Mass Index}oring} is the well-known cognit\\\\index{best practices}ive bias that people\\'s judgments get irrationally fixated on the first number they hear. Car dealers exploit this all the time, initially giving an inflated cost for the vehicle so that subsequent prices sound like a bargain.\\n\\nI then did one final test before revealing the answer. I allowed my students to bid on the jar, meaning that they had to be confident enough to risk money on the result. This yielded exactly two bids from brave students, at 1500 and 2000 pennies respectively. I pocketed $\\\\$ 1.21$ from the sucker with the high bid, but both proved quite close. This is not a surprise: people willing to bet their own money on an event are, by definition, confident in their selection.\\n\\n\\\\subsection*{3.5.2 When is the Crowd Wise?}\\nAccording to James Surowiecki in his book The Wisdom of Crowds Sur05, crowds are wise when four conditions are satisfied:\\n\\n%---- Page End Break Here ---- Page : 82\\n\\n\\\\begin{itemize}\\n  \\\\item When the opinions are independent: Our experiment highlighted how easy it is for a group to lapse into group-think. People naturally get influenced by others. If you want someone\\'s true opinion, you must ask them in isolation.\\n  \\\\item When crowds are peo\\\\index{heatmaps}ple with diverse knowledge and methods: Crowds only add information when there is disagreement. A committee composed of perfectly-correlated experts contributes nothing more than you could learn from any one of them. In the penny-guessing problem, some people estimated the volume of the container, while others gauged the sag of my arm as I lifted the heavy mass. Alternate approaches might have estimated how many pennies I could have accumulated in twenty years of occasionally emptying my pockets, or recalled their own hoarding experiences.\\n  \\\\item When the problem is in a domain that does not need specialized knowledge: I trust the consensus of the crowd in certain important decisions, like which type of car to buy or who should serve as the president of my country (gulp). But when it comes to deciding whether my tumor sample is cancerous or benign, I will trust the word of one doctor over a cast of 1,000 names drawn at random from the phone book.\\\\\\\\\\nWhy? Because the question at hand benefits greatly from specialized knowledge and experience. There is a genuine reason why the doctor should know more than all the others. For\\\\index{aggregation mechanisms} simpler perceptual tasks the mob rules, but one must be careful not to ask the crowd something they have no way of knowing.\\n  \\\\item Opinions can be fairly aggregated: The least useful part of any mass survey form is the open response field \"Tell us what you think!\". The problem here is that there is no way to combine these opinions to form a consensus, because different people have different issues and concerns. Perhaps these texts could be put into buckets by similarity, but this is hard to do effectively.\\\\\\\\\\nThe most common use of such free-form responses are anecdotal. People cherry-pick the most positive-sounding ones, then put them on a slide to impress the boss.\\n\\\\end{itemize}\\n\\nTake-Home Lesson: Be an incomparable element on the partial order of life. Diverse, independent thinking contributes the most wisdom to the crowd.\\n\\n\\\\subsection*{3.5.3 Mechanisms for Aggregation}\\nCollecting wisdom from a set of responses requires using the right aggregation mechanism. For estimating numerical quantities, standard techniques like plotting the frequency distribution and computing summary statistics are appropriate. Both the mean and median implicitly assume that the errors are\\\\\\n%---- Page End Break Here ---- Page : 83\\n\\\\\\nsymmetrically distributed. A quick look at the shape of the distribution can generally confirm or reject that hypothesis.\\n\\nThe median is, generally speaking, a more appropriate choice than the mean in such aggregation problems. It reduces the influence of outliers, which is a particular problem in the case of mass experiments where a certain fraction of your participants are likely to be bozos. On our penny guessing data, the mean produced a ghastly over-estimate of 3739 , which reduced to 2843 after removing the largest and smallest guess, and then down to 2005 once trimming the two outliers on each end (recall the correct answer was 1879).\\n\\nRemoving outliers is a very good strategy, but we may have other grounds to judge the reliability of our subjects, such as their performance on other tests where we do know the answer. Taking a weighted average\\\\index{weighted average}, where we give more weight to the scores deemed more reliable, provides a way to take such confidence measures into account.\\n\\nFor classification\\\\index{Arrowâs impossibility theorem} problems, voting is the basic aggregation mechanism. The Condorcet jury theorem\\\\index{Condorcet jury theorem} justifies our faith in democracy. It states that if the probability of each voter being correct on a given issue is $p>0.5$, the probability that a majority of the voters are correct $(P(n))$ is greater than $p$. In fact, it is exactly:\\n\\n$$\\nP(n)=\\\\sum_{i=(n+1) / 2}^{n}\\\\binom{n}{i} p^{i}(1-p)^{n-i}\\n$$\\n\\nLarge voter counts give statistical validity even to highly contested elections. Suppose $p=0.51$, meaning the forces of right are a bare majority. A jury of 101 members would reach the correct decision $57 \\\\%$ of the time, while $P(1001)=0.73$ and $P(10001)=0.9999$. The probability of a correct decision approaches 1 as $n \\\\rightarrow \\\\infty$.\\n\\nThere are natural limitations to the power of electoral systems, however. Arrow\\'s impossibility theorem states that no electoral system for summing permutations of preferences as votes satisfies four natural conditions for the fairness of an election. This will be discussed in Section 4.6, in the context of scores and rankings.\\n\\n\\\\subsection*{3.5.4 crowdsourcing services\\\\index{crowdsourcing services}}\\nCrowdsourcing services like Amazon Turk\\\\index{Amazon Turk} and CrowdFlower\\\\index{CrowdFlower} provide the opportunity for you to hire large numbers of people to do small amounts of piecework. They help you to wrangle people, in order to create data for you to wrangle.\\n\\nThese crowdsourcing services maintain a large stable of freelance workers, serving as the middleman between them and potential employers. These workers, generally called Turkers\\\\index{Turkers}, are provided with lists of available jobs and what they will pay, as shown in Figure 3.5. Employers generally have some ability to control the location and credentials of who they hire, and the power to reject a worker\\'s efforts without pay, if they deem it inadequate. But statistics on employers\\' acceptance rates are published, and good workers are unlikely to labor for bad actors.\\\\\\n%---- Page End Break Here ---- Page : 84\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-102}\\n\\nFigure 3.5: Representative tasks on Mechanical Turk.\\n\\nThe tasks assigned\\\\index{tasks assigned} to Turkers generally involve simple cognitive efforts that cannot currently be performed well by computers. Good applications of Turkers include:\\n\\n\\\\begin{itemize}\\n  \\\\item Measuring aspects of human perception: Crowdsourcing systems provide efficient ways to gather representative opinions on simple tasks. One nice application was establishing link\\\\index{probability density function}ages between colors in red-green-blue space, and the names by which people typically identify them in a language. This is important to know when writing descriptions of products an\\\\index{cumulative density function}d images.\\\\\\\\\\nSo where is the boundary i\\\\index{classiï¬ers}n color space between \"blue\" and \"light blue,\" or \"robin\\'s egg blue\" and \"teal\"? The right names are a function of culture and convention, not physics. To find out, you must ask people, and crowdsourcing permits you to easily query hundreds or thousands of different people.\\n  \\\\item Obtaining training data for machine learning classifiers: Our primary interest in crowdsourcing will be to produce human annotations that serve as training data. Many machine learning problems seek to do a particular task \"as well as people do.\" Doing so requi\\\\index{U.S. presidential elections}res a large number of training instances to establish what people did, when given the chance.\\\\\\\\\\nFor example, suppose we sought to build a sentiment analysis system capable of reading a written review and deciding whether its opinion of a product is favorable or unfavorable. We will need a large number of reviews labeled by annotators to serve as testing/training data. Further, we need the same reviews labeled repeatedly by different annotators, so as to\\\\\\n%---- P\\\\index{data maps}age End Break Here ---- Page : 85\\n\\\\\\nidentify any inter-annotator disagreements concerning the exact meaning of a text.\\n  \\\\item Obtaining evaluation data for computer systems: $A / B$ testing is a standard method for optimizing user interfaces: show half of the judges version $A$ of a given system and the other half version $B$. Then test which group did better according to some metric. Turkers can provide feedback on how interesting a given app is, or how well a new classifier is performing.\\\\\\\\\\nOne of my grad students (Yanqing Chen) used CrowdFlower\\\\index{CrowdFlower} to evaluate a system he built to identify the most relevant Wikipedia category for a particular entity. Which category better describes Barack Obama: Presidents of the United States or African-American Authors? For $\\\\$ 200$, he got people to answer a total of 10,000 such multiple-choice questions, enough for him to properly evaluate his system.\\n  \\\\item Putting humans into the machine: There still exist many cognitive tasks that people do much better than machines. A cleverly-designed interface can supply user queries to people sitting inside the computer, waiting to serve those in need.\\\\\\\\\\nSuppose you wanted to build an app to help the visually impaired, enabling the user to snap a picture and ask someone for help. Maybe they are in their kitchen, and need someone to read the label on a can to them. This app could call a Turker as a subroutine, to do such a task as it is needed.\\\\\\\\\\nOf course, these image-annotation pairs should be retained for future analysis. They could serve as training data for a machine learning program to take the people out of the loop, as much as possible.\\n  \\\\item Independent creative efforts: Crowdsourcing can be used to commission large numbers of creative works on demand. You can order blog posts or articles on demand, or written product reviews both good and bad. Anything that you might imagine can be created, if you just specify what you want.\\\\\\\\\\nHere are two silly examples that I somehow find inspiring:\\n  \\\\item T\\\\index{A/B testing}he Sheep Market\\\\index{Sheep Market} (\\\\href{http://www.thesheepmarket.com}{http://www.thesheepmarket.com}) commissioned 10,000 drawings of sheep for pennies each. As a conceptual art piece, it tries to sell them to the highest bidder. What creative endeavors can you think of that people will do for you at $\\\\$ 0.25$ a pop?\\n  \\\\item Emoji Dick\\\\index{Emoji Dick} (\\\\href{http://www.emojidick.com}{http://www.emojidick.com}) was a crowdsourced effort to translate the great American novel Moby Dick\\\\index{Moby Dick} completely into emoji images. Its creators partitioned the book into roughly 10,000 parts, and farmed out each part to be translated by three separate Turkers. Other Turkers were hired to select the best one of these to be incorporated into the final book. Over 800 Turkers were involved, with the total cost of $\\\\$ 3,676$ raised by the crowd-funding site Kickstarter.\\n \\n%---- Page End Break Here ---- Page : 86\\n \\\\item Economic/psychological experiments: Crowdsourcing has proven a boon to social scientists conducting experiments in behavioral economics and psychology. Instead of bribing local undergraduates to participate in their studies, these investigators can now expand their subject pool to the entire world. They get the power to harness larger populations, perform independent replications in different countries, and thus test whether there are cultural biases of their hypotheses.\\n\\\\end{itemize}\\n\\nThere are many exciting tasks that can be profitably completed using crowdsourcing. However, you are doomed to disappointment if you employ Turkers for the wrong task, in the wrong way. Bad uses of crowdsourcing include:\\n\\n\\\\begin{itemize}\\n  \\\\item Any task that requires advanced training: Although every person possesses unique skills and expertise, crowdsourcing workers come with no specific training. They are designed to be treated as interchangeable parts. You do not establish a personal relationship with these workers, and any sensible gig will be too short to allow for more than a few minutes training.\\\\\\\\\\nTasks requiring specific technical skills are not reasonably crowdsourced. However, they might be reasonably subcontracted, in traditional longerterm arrangements.\\n  \\\\item Any task you cannot specify clearly: You have no mechanism for back-and-forth communication with Turkers. Generally speaking, they have no way to ask you questions. Thus the system works only if you can specify your tasks clearly, concisely, and unambiguously.\\\\\\\\\\nThis is much harder than it looks. Realize that you are trying to program people instead of computers, with all the attendant bugs associated with \"do as I say\" trumping \"do what I mean.\" Test your specifications out on local people before opening up your job to the masses, and then do a small test run on your crowdsourcing platform to evaluate how it goes before cutting loose with the bulk of your budget. You may be in for some cultural surprises. Things that seem obvious to you might mean something quite different to a worker halfway around the world.\\n  \\\\item Any task where you cannot verify whether they are doing a good job: Turkers have a single motivation for taking on your piecework: they are trying to convert their time into money as efficiently as possible. They are looking out for jobs offering the best buck for their bang, and the smartest ones will seek to complete your task as quickly and thoughtlessly as possible.\\\\\\\\\\nCrowdsourcing platforms permit employers to withhold payment if the contracted work is unacceptable. Taking advantage of this requires some efficient way to check the quality of the product. Perhaps you should ask them to complete certain tasks where you already know the correct answer. Perhaps you can compare their responses to that of other independent workers, and throw out their work if it disagrees too often from the consensus.\\n\\n%---- Page End Break Here ---- Page : 87\\n\\\\end{itemize}\\n\\nIt is very important to employ some quality control mechanism. Some fraction of the available workers on any platform are bots, looking for multiple-choice tasks to attack through randomness. Others may be people with language skills wholly inadequate for the given task. You need to check and reject to avoid being a sucker.\\\\\\\\\\nHowever, you cannot fairly complain about results from poorly specified tasks. Rejecting too high a fraction of work will lower your reputation, with workers and the platform. It is particularly bad karma to refuse to pay people but use their work product anyway.\\n\\n\\\\begin{itemize}\\n  \\\\item Any illegal task, or one too inhuman to subject people to: You are not allowed to ask a Turker to do something illegal or unethical. The classic example is hiring someone to write bad reviews of your competitor\\'s products. Hiring a hit man makes you just as guilty of murder as the guy who fired the shots. Be aware that there are electronic trails that can be followed from the public placement of your ad directly back to you.\\\\\\\\\\nPeople at educational and research institutions are held to a higher standard than the law, through their institutional review board\\\\index{institutional review board} or IRB. The IRB is a committee of researchers and administrative officials who must approve any research on human subjects before it is undertaken. Benign crowdsourcing applications such as the ones we have discussed are routinely approved, after the researchers have undergone a short online training course to make sure they understand the\\\\index{gamiï¬cation} rules.\\\\\\\\\\nAlways realize that there is a person at the other end of the machine. Don\\'t assign them tasks that are offensive, degrading, privacy-violating, or too stressful. You will probably get better results out of your workers if you treat them like human beings.\\n\\\\end{itemize}\\n\\nGetting people to do your bidding requires proper incentives, not just clear instructions. In life, you generally get what you pay for. Be aware of the currently prevailing minimum hourly wage in your country, and price your tasks accordingly. This is not a legal requirement, but it is generally good business.\\n\\nThe sinister glow that comes from hiring workers at $\\\\$ 0.50$ per hour wears off quickly once you see the low quality of workers that your tasks attract. You can easily eat up all your savings by the need to rigorously correct their work product, perhaps by paying multiple workers do it repeatedly. Higher paying tasks find workers much more quickly, so be prepared to wait if you do not pay the prevailing rate. Bots and their functional equivalents are happier to accept slave wages than the workers you really want to hire.\\n\\n\\\\subsection*{3.5.5 Gamification}\\nThere is an alternative to paying people to annotate or transcribe your data. Instead, make things so much fun that people will work for you for free!\\n\\ngames with a purpose\\\\index{games with a purpose} (GWAP) are systems which disguise data collection as a game people want to play, or a task people themselves want done. With\\\\\\n%---- Page End Break Here ---- Page : 88\\n\\\\\\nthe right combination of game, motive, and imagination, amazing things can be done. Successful examples include:\\n\\n\\\\begin{itemize}\\n  \\\\item CAPTCHAs\\\\index{CAPTCHAs} for optical character recognition (OCR): CAPTCHAs are those distorted text images you frequently encounter when creating an account on the web. They demand that you type in the contents of text strings shown in the image to prove that you are a human, thus enabling them to deny access to bots and other programmed systems.\\\\\\\\\\nReCAPTCHAs were invented to get useful data from the over 100 million CAPTCHAs displayed each day. Two text strings are displayed in each, one of which the system checks in order to grant entry. The other represents a hard case for an OCR system that is digitizing old books and newspapers. The answers are mapped back to improve the digitization of archival documents, transcribing over 40 million words per day.\\n  \\\\item Psychological/IQ testing\\\\index{IQ testing} in games/apps: psychologists\\\\index{psychologists} have established five basic personality traits as important and reproducible aspects of personality. Academic psychologists use multiple-choice personality tests to measure where individuals sit along personality scales for each of the big five traits: openness, conscientiousness, extroversion, agreeableness, and neuroticism.\\n\\\\end{itemize}\\n\\nBy turning these surveys into game apps (\"What are your personality traits?\") psychologists have gathered personality measurements on over 75,000 different people, along with other data on preferences and behavior. This has created an enormous data set to study many interesting issues in the psychology of personality.\\n\\n\\\\begin{itemize}\\n  \\\\item The FoldIt\\\\index{FoldIt} game for predicting protein structures: Predicting the structures formed by protein molecules is one of the great computational challenges in science. Despite many years of work, what makes a protein fold into a particular shape is still not well understood.\\\\\\\\\\nFoldIt (\\\\href{https://fold.it}{https://fold.it}) is a game challenging non-biologists to design protein molecules that fold into a particular shape. Players are scored as to how closely their design approaches the given target, with the highest scoring players ranked on a leader board. Several scientific papers have been published on the strength of the winning designs.\\n\\\\end{itemize}\\n\\nThe key to success here is making a game that is playable enough to become popular. This is much harder than it may appear. There are millions of free apps in the app store, mostly games. Very few are ever tried by more than a few hundred people, which is nowhere near enough to be interesting from a data collection standpoint. Adding the extra constraint that the game generate interesting scientific data while being playable makes this task even harder.\\n\\nMotivational techniques should be used to improve playability. Keeping score is an important part of any game, and the game should be designed so that performance increases rapidly at first, in order to hook the player. Progress bars\\\\\\n%---- Page End Break Here ---- Page : 89\\n\\\\\\nprovide encouragement to reach the next level. Awarding badges and providing leader boards seen by others encourages greater efforts. Napoleon instituted a wide array of ribbons and decorations for his soldiers, observing that \"it is amazing what men will do for a strip of cloth.\"\\n\\nThe primary design principle of games such as FoldIt is to abstract the domain technicality away, into the scoring function. The game is configured so players need not really understand issues of molecular dynamics, just that certain changes make the scores go up while others make them go down. The player will build their own intuition about the domain as they play, resulting in designs which may never occur to experts skilled in the art.\\n\\n\\\\subsection*{3.6 Chapter Notes}\\nThe Charles Babbage quote from the start of this chapter is from his book Passages from the Life of a Philosopher Bab11. I recommend Padua\\'s graphic novel Pad15 for an amusing but meaningful (albeit fictitious) introduction to his work and relationship with Ada Lovelace.\\n\\nMany books deal with hands-on practical matters of data wrangling in particular programming languages. Particularly useful are the O\\'Reilly books for data science in Python, including Gru15, McK12.\\n\\nThe story of our jai-alai betting system, including the role of website scraping, is reported in my book Calculated Bets [Ski01. It is a quick and fun overview of how to build simulation models for prediction, and will be the subject of the war story of Section 7.8.\\n\\nThe failure of space missions due to numerical computing errors has been well chronicled in popular media. See Gleick [Gle96] and Stephenson et al. $\\\\left[\\\\mathrm{SMB}^{+} 99\\\\right.$ for discussions of the Ariane 5 and Mars Climate Orbiter space missions, respectively.\\n\\nThe clever idea of using accelerometers in cell phones to detect earthquakes comes from Faulkner et al. $\\\\left[\\\\mathrm{FCH}^{+} 14\\\\right]$. Representative studies of large sets of Flickr images includes Kisilevich et al. $\\\\mathrm{KKK}^{+} 10$.\\n\\nKittur [KCS08] reports on experiences with crowdsourcing user studies on Amazon Turk. Our use of CrowdFlower to identify appropriate descriptions of historical figures was presented in CPS15. Methods for gamification in instruction are discussed in DDKN11 Kap12. Recaptchas are introduced in Von Ahn, et al. $\\\\mathrm{VAMM}^{+} 08$. The large-scale collection of psychological trait data via mobile apps is due to Kosinski, et al. KSG13.\\n\\n\\\\subsection*{3.7 exercises\\\\index{exercises}}\\n\\\\section*{Data Munging}\\n3-1. [3] Spend two hours getting familiar with one of the following programming languages: Python, R, MatLab, Wolfram Alpha/Language. Then write a brief paper with your impressions on its characteristics:\\n\\n%---- Page End Break Here ---- Page : 90\\n\\n\\\\begin{itemize}\\n  \\\\item Expressibility.\\n  \\\\item Runtime speed.\\n  \\\\item Breadth of library functions.\\n  \\\\item Programming environment.\\n  \\\\item Suitability for algorithmically-intensive tasks.\\n  \\\\item Suitability for general data munging tasks.\\n\\\\end{itemize}\\n\\n3-2. [5] Pick two of the primary data science programming languages, and write programs to solve the following tasks in both of them. Which language did you find most suitable for each task?\\\\\\\\\\n(a) Hello World!\\\\\\\\\\n(b) Read numbers from a file, and print them out in sorted order.\\\\\\\\\\n(c) Read a text file, and count the total number of words.\\\\\\\\\\n(d) Read a text file, and count the total number of distinct words.\\\\\\\\\\n(e) Read a file of numbers, and plot a frequency histogram of them.\\\\\\\\\\n(f) Download a page from the web, and scrape it.\\n\\n3-3. [3] Play around for a little while with Python, R, and Matlab. Which do you like best? What are the strengths and weaknesses of each?\\\\\\\\[0pt]\\n3\\\\index{variance}-4. [5] C\\\\index{un\\\\index{Occ\\\\index{overï¬tting}am, William of}derï¬t}onstruct a data set of $n$ human heights, with $p \\\\%$ of them recording in English (feet) and the rest with metric (meter) measurements. Use statistical tests to test whether this distribution is distinguishable from one properly recorded in meters. What is the boundary as a function of $n$ and $p$ where it becomes clear there is a problem?\\n\\n\\\\section*{Data Sources}\\n3-5. [3] Find a table of storage prices over time. Analyze this data, and make a projection about the cost/volume of data storage five years from now. What will disk prices be in 25 or 50 years?\\\\\\\\[0pt]\\n3-6. [5] For one or mo\\\\index\\\\index{biasâvariance trade-oï¬}{bias}re of the following The Quant Shop challenges, find relevant data sources and assess their quality:\\n\\n\\\\begin{itemize}\\n  \\\\item Miss Universe.\\n  \\\\item Movie gross.\\n  \\\\item Baby weight.\\n  \\\\item Art auction price.\\n  \\\\item Snow on Christmas.\\n  \\\\item Super Bowl/college champion.\\n  \\\\item Ghoul pool?\\n  \\\\item Future gold/oil price?\\n\\\\end{itemize}\\n\\nData Cleaning\\n\\n3-7. [3] Find out what was weird about September 1752. What special steps might the data scientists of the day have had to take to normalize annual statistics?\\\\\\\\\\n$3-8$. [3] What types of outliers might you expect to occur in the following data sets:\\\\\\\\\\n(a) Student grades.\\\\\\\\\\n(b) Salary data.\\\\\\\\\\n(c) Lifespans in Wikipedia.\\n\\n3-9. [3] A health sensor produces a stream of twenty different values, including blood p\\\\index{U.S. presidential elections}ressure, heart rat\\\\index{Silver, Nate}e, and body tempe\\\\index{principles for eï¬ectiveness}rat\\\\index{overï¬t}ure. Describe two or more techniques you could use to check whether the stream \\\\index{probabilistic}of data coming from the sensor is valid.\\n\\n\\\\section*{Implementation Projects}\\n3-10. [5] Implement a function that extracts the set of hashtags from a data frame of tweets. Hashtags begin with the \"\\\\#\" character and contain any combination of upper and lowercase characters and digits. Assume the hashtag ends where there is a space or a punctuation mark, like a comma, semicolon, or period.\\n\\n3-11. [5] The laws governing voter registration records differ from state to state in the United States. Identify one or more states with very lax rules, and see what you must do to get your hands on the data. Hint: Florida.\\n\\n\\\\section*{Crowdsourcing}\\n3-12. [5] Describe how crowdsourced workers might have been employed to help gather data for The Quant Shop challenges:\\n\\n\\\\begin{itemize}\\n  \\\\item Miss Universe.\\n  \\\\item Movie gross.\\n  \\\\item Baby weight.\\n  \\\\item Art auction price.\\n  \\\\item Snow on Christmas.\\n  \\\\item Super Bowl/college champion.\\n  \\\\item Ghoul pool.\\n  \\\\item Future gold/oil price?:\\n\\\\end{itemize}\\n\\n3-13. [3] Suppose you are paying Turkers to read texts and annotate them based on the underlying sentiment (positive or negative) that each passage conveys. This is an opinion task, but how can we algorithmically judge whether the Turker was answering in a random or arbitrary manner instead of doing their job seriously?\\n\\n\\\\section*{Interview Questions}\\n3-14. [5] Suppose you built a system to predict stock prices. How would you evaluate it?\\\\\\\\[0pt]\\n3-15. [5] In general, how would you screen for outliers, and what should you do if you find one?\\n\\n3-16. [3] Why does data cleaning play a vital role in analysis?\\n\\n%---- Page End Break Here ---- Page : 92\\n\\n3-17. [5] During analysis, how do you treat missing values?\\\\\\\\[0pt]\\n3-18. [5] Explain selection bias. Why is it important? How can data management procedures like handling missing data make it worse?\\\\\\\\[0pt]\\n3 -19. [3] How do you efficiently scrape web data?\\n\\n\\\\section*{Kaggle Challenges}\\n3-20. Partially sunny, with a chance of hashtags. \\\\href{https://www.kaggle.com/c/crowdflower-weather-twitter}{https://www.kaggle.com/c/crowdflower-weather-twitter}\\\\\\\\\\n$3-21$. Predict end of day stock returns, without being deceived by noise. \\\\href{https://www.kaggle.com/c/the-winton-stock-market-challenge}{https://www.kaggle.com/c/the-winton-stock-market-challenge}\\\\\\\\\\n$3-22$. Data cleaning and the analysis of historical climate change. \\\\href{https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data}\\n%---- Page End Break Here ---- Page : 93\\n{https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data}\\n\\n\\\\section*{Chapter 4}\\n\\\\section*{scores\\\\index{scores} and rankings\\\\index{rankings}}\\nMoney is a scoreboard where you can rank how you\\'re doing against other people.\\n\\n\\\\begin{itemize}\\n  \\\\item Mark Cuban\\n\\\\end{itemize}\\n\\nscoring functions\\\\index{scoring functions} are measures that reduce multi-dimensional records to a single value, highlighting some particular property of the data. A familiar example of scoring functions are those used to assign student grades in courses such as mine. Students can then be ranked (sorted) according to these numerical scores, and later assigned letter grades based on this order.\\n\\nGrades are typically computed by functions over numerical features that reflect student performance, such as the points awarded on each homework and exam. Each student receives a single combined score, often scaled between 0 and 100 . These scores typically come from a linear combination of the input variables, perhaps giving $8 \\\\%$ weight to each of five homework assignments, and $20 \\\\%$ weight to each of three exams.\\n\\nThere are several things to observe about such grading rubrics, which we will use as a model for more general scoring and ranking functions:\\n\\n\\\\begin{itemize}\\n  \\\\item Degree of arbitrariness: Every teacher/professor uses a different trade-off between homework scores and exams when judging their students. Some weigh the final exam more than all the other variables. Some normalize each value to 100 before averaging, while others convert each score to a Zscore. They all differ in philosophy, yet every teacher/professor is certain that their grading system is the best way to do things.\\n  \\\\item Lack of validation data\\\\index{validation data}: There is no gold standard informing instructors of the \"right\" grade that their students should have received in the course. Students often complain that I should give them a better grade, but self-interest seems to lurk behind these requests more than objectivity. Indeed, I rarely hear students recommend that I lower their grade.\\n\\\\end{itemize}\\n\\nWithout objective feedback or standards to compare against, there is no rigorous way for me to evaluate my grading system and improve it.\\n\\n\\\\begin{itemize}\\n  \\\\item General Robustness: And yet, despite using widely-disparate and totally unvalidated approaches, different grading systems generally produce similar results. Every school has a cohort of straight-A students who monopolize a sizable chunk of the top grades in each course. This couldn\\'t happen if all these different grading systems were arbitrarily ordering student\\\\index{garbage in, garbage out} performance. C students generally muddle along in the middle-to-lower tiers of the bulk of their classes, instead of alternating As and Fs on the way to their final average. All grading systems are different, yet almost all are defensible.\\n\\\\end{itemize}\\n\\nIn this chapter, we will use scoring and ranking functions as our first foray into data analysis. Not everybody loves them as much as I do. Scoring functions often seem arbitrary and ad hoc, and in the wrong hands can produce impressive-looking numbers which are essentially meaningless. Because their effectiveness generally cannot be validated, these techniques are not as scientifically sound as the statistical and machine learning methods we will present in subsequent chapters.\\n\\nBut I think it is important to appreciate scoring functions for what they are: useful, heuristic ways to tease understanding from large data sets. A scoring function is sometimes called a statistic, which lends it greater dignity and respect. We will introduce several methods for getting meaningful scores from data.\\n\\n\\\\subsection*{4.1 The Body Mass Index (BMI)}\\nEverybody loves to eat, and our modern world of plenty provides numerous opportunities for doing so. The result is that a sizable percentage of the population are above their optimal body weight. But how can you tell whether you are one of them?\\n\\nThe body mass index (BMI) is a score or statistic designed to capture whether your weight is under control. It is defined as\\n\\n$$\\nB M I=\\\\frac{\\\\text { mass }}{\\\\text { height }^{2}}\\n$$\\n\\nwhere mass is measured in kilograms and height in meters.\\\\\\\\\\nAs I write this, I am 68 inches tall ( 1.727 meters) and feeling slightly pudgy at $150 \\\\mathrm{lbs}(68.0 \\\\mathrm{~kg})$. Thus my BMI is $68.0 /\\\\left(1.727^{2}\\\\right)=22.8$. This isn\\'t so terrible, however, because commonly accepted BMI ranges in the United States define:\\n\\n\\\\begin{itemize}\\n  \\\\item Underweight: below 18.5.\\n  \\\\item Normal weight: from 18.5 to 25.\\n  \\\\item Overweight: from 25 to 30 .\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-113}\\n\\\\end{itemize}\\n\\nFigure 4.1: Height-weight scatter plot, for 1000 representative Americans. Colors illustrate class labels in the BMI distribution.\\n\\n\\\\begin{itemize}\\n  \\\\item Obese: over 30.\\n\\\\end{itemize}\\n\\nThus I am considered to be in normal range, with another dozen pounds to gain before I officially become overweight. Figure 4.1 plots where a representative group of Americans sit in height-weight space according to this scale. Each point in this scatter plot is a person, colored according to their weight classification by BMI. Regions of seemingly solid color are so dense with people that the dots overlap. Outlier points to the right correspond to the heaviest individuals.\\n\\nThe BMI is an example of a very successful statistic/scoring function. It is widely used and generally accepted, although some in the public health field quibble that better statistics are available.\\n\\nThe logic for the BMI is almost sound. The square of height should be proportional to area. But mass should grow proportional to the volume, not area, so why is it not mass/height ${ }^{3}$ ? Historically, BMI was designed to correlate with the percentage of body fat in an individual, which is a much harder measurement to make than height and weight. Experiments with several simple scoring functions, including $m / l$ and $m / l^{3}$ revealed that BMI works best.\\n\\nIt is very interesting to look at BMI distributions for extreme populations. Consider professional athletes in American football (NFL) and basketball (NBA):\\n\\n\\\\begin{itemize}\\n  \\\\item Basketball players are notoriously tall individuals. They also have to run up and down the court all day, promoting superior fitness.\\n  \\\\item American football players are notoriously heavy individuals. In particular, linemen exist only to block or move other linemen, thus placing a premium on bulk.\\n\\\\end{itemize}\\n\\nLet\\'s look at some data. Figure 4.2 shows the BMI distributions of basketball and football players, by sport. And indeed, almost all of the basketball players\\\\\\n%---- Page End Break Here ---- Page : 97\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-114(2)}\\n\\nFigure 4.2: BMI distributions of professional basketball (left) and football (right) players.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-114(1)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-114}\\n\\nFigure 4.3: Position in basketball (left) and football (right) is largely determined by size.\\\\\\\\\\nhave normal BMI despite their very abnormal heights. And the football players are almost uniformly animals, with most scored as obese despite the fact that they are also well-conditioned athletes. These football players are generally optimized for strength, instead of cardiovascular fitness.\\n\\nIn Chapter 6, we will discuss visualization techniques to highlight the presentation of data, but let\\'s start to develop our aesthetic here. We use scatter plots to show each individual as a point in height-weight space, with labels (weight class or player position) shown as colors.\\n\\nThe breakdown of BMI by position is also revealing, and shown in Figure 4.3. In basketball, the guards are quick and sleek while the centers are tall and intimidating. So all of these positions segregate neatly by size. In football, the skill players (the quarterbacks, kickers, and punters) prove to be considerably smaller than the sides of beef on the line.\\n\\n\\\\subsection*{4.2 developing scoring systems\\\\index{developing scoring systems}}\\nScores are functions that map the features of each entity to a numerical value of merit. This section will look at the basic approaches for building effective scoring systems, and evaluating them.\\n\\n\\\\subsection*{4.2.1 gold standards\\\\index{gold standards} and proxies\\\\index{proxies}}\\nHistorically, paper currencies were backed with gold, meaning that one paper dollar could always be traded in for $\\\\$ 1$ worth of gold. This was why we knew that our money was worth more than the paper it was printed on.\\n\\nIn data science, a gold standard is a set of labels or answers that we trust to be correct. In the original formulation of BMI, the gold standard was the body fat percentages carefully measured on a small number of subjects. Of course, such measurements are subject to some error, but by defining these values to be the gold standard for fitness we accept them to be the right measure. In gold we trust.\\n\\nThe presence of a gold standard provides a rigorous way to develop a good scoring system. We can use curve-fitting technique like linear regression (to be discussed in Section 9.1) to weigh the input features so as to best approximate the \"right answers\" on the gold standard instances.\\n\\nBut it can be hard to find real gold standards. Proxies are easier-to-find data that should correlate well with the desired but unobtainable ground truth. BMI was designed to be a proxy for body fat percentages. It is easily computable from just height and weight, and does a pretty good job correlating with body fat. This means it is seldom necessary to test buoyancy in water tanks or \"pinch an inch\" with calipers, more intrusive measures that directly quantify the extent of an individual\\'s flab.\\n\\nSuppose I wanted to improve the grading system I use for next year\\'s data science course. I have student data from the previous year, meaning their scores on homework and tests, but I don\\'t really have a gold standard on what grades these students deserved. I have only the grade I gave them, which is meaningless if I am trying to improve the system.\\n\\nI need a proxy for their unknown \"real\" course merit. A good candidate for this might be each student\\'s cumulative GPA in their other courses. Generally speaking, student performance should be conserved across courses. If my scoring system hurts the GPA of the best students and helps the lower tier, I am probably doing something wrong.\\n\\nProxies are particularly good when evaluating scoring/ranking systems. In our book Who\\'s Bigger? SW13] we used Wikipedia to rank historical figures by \"significance.\" We did not have any gold standard significance data measuring how important these people really were. But we used several proxies to evaluate how we were doing to keep us honest:\\n\\n\\\\begin{itemize}\\n  \\\\item The prices that collectors will pay for autographs from celebrities should generally correlate with the celebrity\\'s significance. The higher the price people are willing to pay, the bigger the star.\\n \\n%---- Page End Break Here ---- Page : 99\\n \\\\item The statistics of how good a baseball player is should generally correlate with the player\\'s significance. The better the athlete, the more important they are likely to be.\\n  \\\\item Published rankings appearing in books and magazines list the top presidents, movie stars, singers, authors, etc. Presidents ranked higher by historians should generally be ranked higher by us. Such opinions, in aggregate, should generally correlate with the significance of these historical figures.\\n\\\\end{itemize}\\n\\nWe will discuss the workings of our historical significance scores in greater detail in Section 4.7.\\n\\n\\\\subsection*{4.2.2 Scores vs. Rankings}\\nRankings are permutations ordering $n$ entities by merit, generally constructed by sorting the output of some scoring system. Popular examples of rankings/rating systems include:\\n\\n\\\\begin{itemize}\\n  \\\\item Football/basketball top twenty: Press agencies generally rank the top college sports teams by aggregating the votes of coaches or sportswriters. Typically, each voter provides their own personal ranking of the top twenty teams, and each team gets awarded more points the higher they appear on the voter\\'s list. Summing up the points from each voter gives a total score for each team, and sorting these scores defines the ranking.\\n  \\\\item University academic rankings: The magazine U.S News and World Report publishes annual rankings of the top American colleges and universities. Their methodology is proprietary and changes each year, presumably to motivate people to buy the new rankings. But it is generally a score produced from statistics like faculty/student ratio, acceptance ratio, the standardized test scores of its students and applicants, and maybe the performance of its football/basketball teams :-). Polls of academic experts also go into the mix.\\n  \\\\item Google PageRank/search results: Every query to a search engine triggers a substantial amount of computation, implicitly ranking the relevance of every document on the web against the query. Documents are scored on the basis of how well they match the text of the query, coupled with ratings of the inherent quality of each page. The most famous page quality metric here is PageRank, the network-centrality algorithm that will be reviewed in Section 10.4\\n  \\\\item Class rank: Most High Schools rank students according to their grades, with the top ranked student honored as class valedictorian. The scoring function underlying these rankings is typically grade-point average (GPA), where the contribution of each course is weighted by its number of credits, and each possible letter grade is mapped to a number (typically\\\\\\n%---- Page End Break Here ---- Page : 100\\n\\\\\\n$A=4.0)$. But there are natural variants: many schools choose to weigh honors courses more heavily than lightweight classes like gym, to reflect the greater difficulty of getting good grades.\\n\\\\end{itemize}\\n\\nGenerally speaking, sorting the results of a scoring system yields a numerical ranking. But thinking the other way, each item\\'s ranking position (say, 493th out of 2196) yields a numerical score for the item as well.\\n\\nSince scores and rankings are duals of each other, which provides a more meaningful representation of the data? As in any comparison, the best answer is that it depends, on issues like:\\n\\n\\\\begin{itemize}\\n  \\\\item Will the numbers be presented in isolation? Rankings are good at providing context for interpreting scores. As I write this, Stony Brook\\'s basketball team ranks 111th among the nation\\'s 351 college teams, on the strength of our RPI (ratings percentage index) of 39.18. Which number gives you a better idea of whether we have a good or bad team, 111th or 39.18?\\n  \\\\item What is the underluing distribution\\\\index{distribution} of scores? By definition, the top ranked entity has a better score than the second ranked one, but this tells you nothing about the magnitude of the difference between them. Are they virtually tied, or is \\\\#1 crushing it?\\n\\\\end{itemize}\\n\\nDifferences in rankings appear to be linear: the difference between 1 and 2 seems the same as the difference between 111 and 112 . But this is not generally true in scoring systems. Indeed, small absolute scoring differences can often yield big ranking differences.\\n\\n\\\\begin{itemize}\\n  \\\\item Do vou care about the extremes or the middle? Well-designed scoring systems often have a bell-shaped distribution. With the scores concentrated around the mean, small differences in score can mean large differences in rank. In a normal distribution, increasing your score from the mean by one standard deviation ( $\\\\sigma$ ) moves you from the 50th percentile to the 84th percentile. But the same sized change from $1 \\\\sigma$ to $2 \\\\sigma$ takes you only from the 84 th to 92.5 th percentile.\\n\\\\end{itemize}\\n\\nSo when an organization slips from first to tenth, heads should roll. But when Stony Brook\\'s team slides from 111th to 120th, it likely represents an insignificant difference in score and should be discounted. Rankings are good at highlighting the very best and very worst entities among the group, but less so the differences near the median.\\n\\n\\\\subsection*{4.2.3 Recognizing good scoring functions\\\\index{good scoring functions}}\\nGood scoring functions are good because they are easily interpretable and generally believable. Here we review the properties of statistics which point in these directions:\\n\\n%---- Page End Break Here ---- Page : 101\\n\\n\\\\begin{itemize}\\n  \\\\item Easily computable: Good statistics can be easily described and presented. BMI is an excellent example: it contains only two parameters, and is evaluated using only simple algebra. It was found as the result of a search through all simple functional forms on a small number of easily obtained, relevant variables. It is an excellent exercise to brainstorm possible statistics from a given set of features on a data set you know well, for practice.\\n  \\\\item Easily understandable: It should be clear from the description of the statistics that the ranking is relevant to the question at hand. \"Mass adjusted by height\" explains why BMI is associated with obesity. Clearly explaining the ideas behind your statistic is necessary for other people to trust it enough to use.\\n  \\\\item Monotonic interpretations of variables: You should have a sense of how each of the features used in your scoring function correlate with the objective. Mass should correlate positively with BMI, because being heavy requires that you weigh a lot. Height should correlate negatively, because tall people naturally weigh more than short people.\\\\\\\\\\nGenerally speaking, you are producing a scoring function without an actual gold standard to compare against. This requires understanding what your variables mean, so your scoring function will properly correlate with this mushy objective.\\n  \\\\item Produces generally satisfying results on outliers: Ideally you know enough about certain individual points to have a sense of where they belong in any reasonable scoring system. If I am truly surprised by the identity of the top entities revealed by the scoring system, it probably is a bug, not a feature. When I compute the grades of the students in my courses, I already know the names of several stars and several bozos from their questions in class. If my computed grades do not grossly correspond to these impressions, there is a potential bug that needs to be tracked down.\\\\\\\\\\nIf the data items really are completely anonymous to you, you probably should spend some time getting to know your domain better. At the very least, construct artificial examples (\"Superstar\" and \"Superdork\") with feature values so that they should be near the top and bottom of the ranking, and then see how they fit in with the real data.\\n  \\\\item Uses systematically normalized variables: Variables drawn from bellshaped distributions behave sensibly in scoring functions. There will be outliers at the tails of either end which correspond to the best/worst items, plus a peak in the middle of items whose scores should all be relatively similar.\\\\\\\\\\nThese normally-distributed variables should be turned into Z-score\\\\index{Z-score}s (see Section 4.3) before adding them together, so that all features have comparable means and variance. This reduces the scoring function\\'s dependence on magic constants to adjust the weights, so no single feature has too dominant an impact on the results.\\n\\n%---- Page End Break Here ---- Page : 102\\n\\\\end{itemize}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{l|cccccccccc}\\n$\\\\mu(B)=21.9$ & $\\\\sigma(B)=1.92$ &  &  &  &  &  &  &  &  &  \\\\\\\\\\n$\\\\mu(Z)=0$ & $\\\\sigma(Z)=1$ &  &  &  &  &  &  &  &  &  \\\\\\\\\\nB & 19 & 22 & 24 & 20 & 23 & 19 & 21 & 24 & 24 & 23 \\\\\\\\\\nZ & -1.51 & 0.05 & 1.09 & -0.98 & 0.57 & -1.51 & -0.46 & 1.09 & 1.09 & 0.57 \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 4.4: Taking the Z -scores of a set of values $B$ normalizes them to have mean $\\\\mu=0$ and $\\\\sigma=1$.\\n\\nGenerally speaking, summing up Z-scores using the correct signs (plus for positively correlated variables and minus for negative correlations) with uniform weights will do roughly the right thing. A better function might weigh these variables by importance, according to the strength of the correlation with the target. But it is unlikely to not make much difference.\\n\\n\\\\begin{itemize}\\n  \\\\item Breaks ties in meaningful ways: Ranking functions are of very limited value when there are bunches of ties. Ranking the handiness of people by how many fingers they have won\\'t be very revealing. There will be a very select group with twelve, a vast majority tied with ten, and then small groups of increasingly disabled accident victims until we get down to zero.\\\\\\\\\\nIn general, scores should be real numbers over a healthy range, in order to minimize the likelihood of ties. Introducing secondary features to break ties is valuable, and makes sense provided these features also correlate with the property you care about.\\n\\\\end{itemize}\\n\\n\\\\subsection*{4.3 Z-scores and normalization\\\\index{normalization}}\\nAn important principle of data science is that we must try to make it as easy as possible for our models to do the right thing. Machine learning techniques\\\\index{techniques} like linear regression purport to find the line optimally fitting to a given data set. But it is critical to normalize all the different variables to make their range/distribution comparable before we try to use them to fit something.\\n\\nZ-scores will be our primary method of normalization. The Z-score transform is computed:\\n\\n$$\\nZ_{i}=\\\\left(a_{i}-\\\\mu\\\\right) / \\\\sigma\\n$$\\n\\nwhere $\\\\mu$ is the mean of the distribution and $\\\\sigma$ the associated standard deviation.\\\\\\\\\\nZ-scores transform arbitrary sets of variables to a uniform range. The Zscores of height measured in inches will be exactly the same as that of the height measured in miles. The average value of a Z -score over all points is zero. Figure 4.4 shows a set of integers reduced to Z-scores. Values greater than the mean become positive, while those less than the mean become negative. The standard deviation of the Z-scores is 1 , so all distributions of Z-scores have similar properties.\\n\\nTransforming values to Z-scores accomplishes two goals. First, they aid in visualizing patterns and correlations, by ensuring that all fields have an identical\\\\\\n%---- Page End Break Here ---- Page : 103\\n\\\\\\nmean (zero) and operate over a similar range. We understand that a Z-score of 3.87 must represent basketball-player level height in a way that 79.8 does not, without familiarity with the measurement unit (say inches). Second, the use of Z-scores makes it easier on our machine learning algorithms, by making all different features of a comparable scale.\\n\\nIn theory, performing a linear transformation like the Z-score doesn\\'t really do anything that most learning algorithms couldn\\'t figure out by themselves. These algorithms generally find the best coefficient to multiply each variable with, which is free to be near $\\\\sigma$ if the algorithm really wants it to be.\\n\\nHowever, the realities of numerical computation kick in here. Suppose we were trying to build a linear model on two variables associated with U.S. cities, say, area in square miles and population. The first has a mean of about 5 and a max around 100. The second has a mean about 25,000 and a max of $8,000,000$. For the two variables to have a similar effect on our model, we must divide the second variable by a factor of 100,000 or so.\\n\\nThis causes numerical precision problems, because a very small change in the value of the coefficient causes a very large change in how much the population variable dominates the model. Much better would be to have the variables be grossly the same scale and distribution range, so the issue is whether one feature gets weighted, say, twice as strongly as another.\\n\\nZ-scores are best used on normally distributed variables, which, after all, are completely described by mean $\\\\mu$ and standard deviation $\\\\sigma$. But they work less well when the distribution is a power law. Consider the wealth distribution in the United States, which may have a mean of (say) $\\\\$ 200,000$, with a $\\\\sigma=\\\\$ 200,000$. The Z-score of $\\\\$ 80$ billion dollar Bill Gates would then be 4999, still an incredible outlier given the mean of zero.\\n\\nYour biggest data analysis sins will come in using improperly normalized variables in your analysis. What can we do to bring Bill Gates down to size? We can hit him with a log, as we discussed in Section 2.4\\n\\n\\\\subsection*{4.4 Advanced Ranking Techniques}\\nMost bread-and-butter ranking tasks are solved by computing scores as linear combinations of features, and then sorting them. In the absence of any gold standard, these methods produce statistics which are often revealing and informative.\\n\\nThat said, several powerful techniques have been developed to compute rankings from specific types of inputs: the results of paired comparisons, relationship networks, and even assemblies of other rankings. We review these methods here, for inspiration.\\n\\n\\\\subsection*{4.4.1 Elo rankings\\\\index{Elo rankings}}\\nRankings are often formed by analyzing sequences of binary comparisons, which arise naturally in competitions between entities:\\n\\n%---- Page End Break Here ---- Page : 104\\n\\n\\\\begin{itemize}\\n  \\\\item Sports contest results: Typical sporting events, be they football games or chess matches, pit teams $A$ and $B$ against each other. Only one of them will win. Thus each match is essentially a binary comparison of merit.\\n  \\\\item Votes and polls: Knowledgeable individuals are often asked to compare options and decide which choice they think is better. In an election, these comparisons are called votes. A major component of certain university rankings come from asking professors: which school is better, $A$ or $B$ ?\\n\\\\end{itemize}\\n\\nIn the movie The Social Network, Facebook\\'s Mark Zuckerberg is shown getting his start with FaceMash, a website showing viewers two faces and asking them to pick which one is more attractive. His site then ranked all the faces from most to least attractive, based on these paired comparisons.\\n\\n\\\\begin{itemize}\\n  \\\\item Implicit comparisons: From the right vantage point, feature data can be meaningfully interpreted as pairwise comparisons. Suppose a student has been accepted by both universities $A$ and $B$, but opts for $A$. This can be taken as an implicit vote that $A$ is better than $B$.\\n\\\\end{itemize}\\n\\nWhat is the right way to interpret collections of such votes, especially where there are many candidates, and not all pairs of players face off against each other? It isn\\'t reasonable to say the one with the most wins wins, because (a) they might have competed in more comparisons than other players, and (b) they might have avoided strong opponents and beaten up only inferior competition.\\n\\nThe Elo system starts by rating all players, presumably equally, and then incrementally adjusts each player\\'s score in response to the result of each match, according to the formula:\\n\\n$$\\nr^{\\\\prime}(A)=r(A)+k\\\\left(S_{A}-\\\\mu_{A}\\\\right)\\n$$\\n\\nwhere\\n\\n\\\\begin{itemize}\\n  \\\\item $r(A)$ and $r^{\\\\prime}(A)$ represent the previous and updated scores for player $A$.\\n  \\\\item $k$ is a fixed parameter reflecting the maximum possible score adjustment in response to a single match. A small value of $k$ results in fairly static rankings, while using too large a $k$ will cause wild swings in ranking based on the latest match.\\n  \\\\item $S_{A}$ is the scoring result achieved by player $A$ in the match under consideration. Typically, $S_{A}=1$ if $A$ won, and $S_{A}=-1$ if $A$ lost.\\n  \\\\item $\\\\mu_{A}$ was the expected result for $A$ when competing against $B$. If $A$ has exactly the same skill level as $B$, then presumably $\\\\mu_{A}=0$. But suppose that $A$ is a champion and $B$ is a beginner or chump. Our expectation is that $A$ should almost certainly win in a head-to-head matchup, so $\\\\mu_{A}>0$ and is likely to be quite close to 1 \\n%---- Page End Break Here ---- Page : 105\\n.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-122}\\n\\\\end{itemize}\\n\\nFigure 4.5: The shape of the logit function\\\\index{logit function}, for three different values for $c$.\\n\\nAll is clear here except how to determine $\\\\mu_{A}$. Given an estimate of the probability that $A$ beats $B\\\\left(P_{A>B}\\\\right)$, then\\n\\n$$\\n\\\\mu_{A}=1 \\\\cdot P_{A>B}+(-1) \\\\cdot\\\\left(1-P_{A>B}\\\\right) .\\n$$\\n\\nThis win probability clearly depends on the magnitude of the skill difference between players $A$ and $B$, which is exactly what is supposed to be measured by the ranking system. Thus $x=r(A)-r(B)$ represents this skill difference.\\n\\nTo complete the Elo ranking system, we need a way to take this real variable $x$ and convert it to a meaningful probability. This is an important problem we will repeatedly encounter in this book, solved by a bit of mathematics called the logit function.\\n\\n\\\\section*{The Logit Function}\\nSuppose we want to take a real variable $-\\\\infty<x<\\\\infty$ and convert it to a probability $0 \\\\leq p \\\\leq 1$. There are many ways one might imagine doing this, but a particularly simple transformation is $p=f(x)$, where\\n\\n$$\\nf(x)=\\\\frac{1}{1+e^{-c x}}\\n$$\\n\\nThe shape of the logit function $f(x)$ is shown in Figure 4.5. Particularly note the special cases at the mid and endpoints:\\n\\n\\\\begin{itemize}\\n  \\\\item When two players are of equal ability, $x=0$, and $f(0)=1 / 2$, reflects that both players have an equal probability of winning.\\n  \\\\item When player $A$ has a vast advantage, $x \\\\rightarrow \\\\infty$, and $f(\\\\infty)=1$, defining that $A$ is a lock to win the match.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-123}\\n\\\\end{itemize}\\n\\nFigure 4.6: Changes in ELO scores as a consequence of an unlikely chess tournament.\\n\\n\\\\begin{itemize}\\n  \\\\item When player $B$ has a vast advantage, $x \\\\rightarrow-\\\\infty$, and $f(-\\\\infty)=0$, denoting that $B$ is a lock to win the match.\\n\\\\end{itemize}\\n\\nThese are exactly the values we want if $x$ measures the skill difference between the players.\\n\\nThe logit function smoothly and symmetrically interpolates between these poles. The parameter $c$ in the logit function governs how steep the transition is. Do small differences in skill translate into large differences in the probability of winning? For $c=0$, the landscape is as flat as a pancake: $f(x)=1 / 2$ for all $x$. The larger $c$ is, the sharper the transition, as shown in Figure 4.5. Indeed, $c=\\\\infty$ yields a step function from 0 to 1 .\\n\\nSetting $c=1$ is a reasonable start, but the right choice is domain specific. Observing how often a given skill-difference magnitude results in an upset (the weaker party winning) helps specify the parameter. The Elo Chess ranking system was designed so that $r(A)-r(B)=400$ means that $A$ has ten times the probability of winning than $B$.\\n\\nFigure 4.6 illustrates Elo computations, in the context of a highly unlikely tournament featuring three of the greatest chess players in history, and one lowranked patzer. Here $k=40$, implying a maximum possible scoring swing of 80 points as a consequence of any single match. The standard logit function gave Kasparov a probability of 0.999886 of beating Skiena in the first round, but through a miracle akin to raising Lazarus the match went the other way. As a consequence, 80 points went from Kasparov\\'s ranking to mine.\\n\\nOn the other side of the bracket two real chess champions did battle, with the more imaginable upset by Polgar moving only 55 points. She wiped the floor with me the final round, an achievement so clearly expected that she gained essentially zero rating points. The Elo method is very effective at updating ratings in response to surprise, not just victory.\\n\\n%---- Page End Break Here ---- Page : 107\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{l|ccccll}\\n1 & A & B & A & A & $\\\\mathrm{~A}: 5$ \\\\\\\\\\n2 & C & A & B & B & $\\\\mathrm{B}: 8$ \\\\\\\\\\n3 & B & C & C & D & $\\\\mathrm{C}: 12$ \\\\\\\\\\n4 & D & D & E & C & $\\\\mathrm{D}: 16$ \\\\\\\\\\n5 & E & E & D & E & $\\\\mathrm{E}: 19$ \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 4.7: Borda\\'s method for constructing the consensus ranking of $\\\\{A, B, C, D, E\\\\}$ from a set of four input rankings, using linear weights.\\n\\n\\\\subsection*{4.4.2 Merging Rankings}\\nAny single numeric feature $f$, like height, can seed $\\\\binom{n}{2}$ pairwise comparisons among $n$ items, by testing whether $f(A)>f(B)$ for each pair of items $A$ and $B$. We could feed these pairs to the Elo method to obtain a ranking, but this would be a silly way to think about things. After all, the result of any such analysis would simply reflect the sorted order of $f$.\\n\\nIntegrating a collection of rankings by several different features makes for a more interesting problem, however. Here we interpret the sorted order of the $i$ th feature as defining a permutation $P_{i}$ on the items of interest. We seek the consensus permutation $P$, which somehow best reflects all of the component permutations $P_{1}, \\\\ldots, P_{k}$.\\n\\nThis requires defining a distance function to measure the similarity between two permutations. A similar issue arose in defining the Spearman rank correlation coefficient (see Section 2.3.1), where we compared two variables by the measure of agreement in the relative order of the elements. 1\\n\\nBorda\\'s method creates a consensus ranking from multiple other rankings by using a simple scoring system. In particular, we assign a cost or weight to each of the $n$ positions in the permutation. Then, for each of the $n$ elements, we sum up the weights of its positions over all of the $k$ input rankings. Sorting these $n$ scores determines the final consensus ranking.\\n\\nAll is now clear except for the mapping between positions and costs. The simplest cost function assign $i$ points for appearing in the $i$ th position in each permutation, i.e. we sum up the ranks of the element over all permutations. This is what we do in the example of Figure 4.7. Item $A$ gets $3 \\\\cdot 1+1 \\\\cdot 2=5$ points on the strength of appearing first in three rankings and second in one. Item $C$ finishes with 12 points by finishing $2,3,3$, and 4 . The final consensus ranking of $\\\\{A, B, C, D, E\\\\}$ integrates all the votes from all input rankings, even though the consensus disagrees at least in part with all four input rankings.\\n\\nBut it is not clear that using linear weights represents the best choice, because it assumes uniform confidence in our accuracy to position elements\\n\\n\\\\footnotetext{${ }^{1}$ Observe the difference between a similarity measure and a distance metric. In correlation, the scores get bigger as elements get more similar, while in a distance function the difference goes to zero. Distance metrics will be discussed more thoroughly in Section 10.1.1\\n}\\n%---- Page End Break Here ---- Page : 108\\n\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-125}\\n\\nFigure 4.8: Equally-spaced values by the normal distribution\\\\index{normal distribution} are closer in the middle than the ends, making appropriate weights for Borda\\'s method.\\\\\\\\\\nthroughout the permutation. Typically, we will know the most about the merits of our top choices, but will be fairly fuzzy about exactly how those near the middle order among themselves. If this is so, a better approach might be to award more points for the distinction between 1st and 2nd than between 110th and 111th.\\n\\nThis type of weighting is implicitly performed by a bell-shaped curve. Suppose we sample $n$ items at equal intervals from a normal distribution, as shown in Figure 4.8. Assigning these $x$ values as the positional weights produces more spread at the highest and lowest ranks than the center. The tail regions really are as wide as they appear for these 50 equally-spaced points: recall that $95 \\\\%$ of the probability mass sits within $2 \\\\sigma$ of the center.\\n\\nAlternately, if our confidence is not symmetric, we could sample from the half-normal distribution, so the tail of our ranks is weighted by the peak of the normal distribution. This way, there is the greatest separation among the highest-ranked elements, but little distinction among the elements of the tail.\\n\\nYour choice of weighting function here is domain dependent, so pick one that seems to do a good job on your problem. Identifying the very best cost function turns out to be an ill-posed problem. And strange things happen when we try to design the perfect election system, as will be shown in Section 4.6\\n\\n\\\\subsection*{4.4.3 digraph-based\\\\index{digraph-based} Rankings}\\nnetworks\\\\index{networks} provide an alternate way to think about a set of votes of the form \" $A$ ranks ahead of $B$.\" We can construct a directed graph/network where there is a vertex corresponding to each entity, and a directed edge $(A, B)$ for each vote that $A$ ranks ahead of $B$.\\n\\nThe optimal ranking would then be a permutation $P$ of the vertices which\\\\\\n%---- Page End Break Here ---- Page : 109\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-126}\\n\\nFigure 4.9: Consistently ordered preferences yield an acyclic graph or DAG (left). Inconsistent preferences result in directed cycles, which can be broken by deleting small sets of carefully selected edges, here shown dashed (right).\\\\\\\\\\nviolates the fewest number of edges, where edge $(A, B)$ is violated if $B$ comes before $A$ in the final ranking permutation $P$.\\n\\nIf the votes were totally consistent, then this optimal permutation would violate exactly zero edges. Indeed, this is the case when there are no directed cycles in the graph. A directed cycle like $(A, C),(C, E),(E, A)$ represents an inherent contradiction to any rank order, because there will always be an unhappy edge no matter which order you choose.\\n\\nA directed graph without cycles is called a directed acyclic graph\\\\index{directed acyclic graph} or DAG. An alert reader with a bit of algorithms background will recall that finding this optimal vertex order is called topologically sorting the DAG, which can be performed efficiently in linear time. Figure 4.9 (left) is a DAG, and has exactly two distinct orders consistent with the directed edges: $\\\\{A, B, C, D, E\\\\}$ and $\\\\{A, C, B, D, E\\\\}$.\\n\\nHowever, it is exceedingly unlikely that a real set of features or voters will all happen to be mutually consistent. The maximum acyclic subgraph problem seeks to find the smallest number of edges to delete to leave a DAG. Removing edge $(E, A)$ suffic\\\\index{topological sorting}es in Figure 4.9 (right). Unfortunately, the problem of finding the best ranking here is NP-complete, meaning that no efficient algorithm exists for finding the optimal solution.\\n\\nBut there are natural heuristics. A good clue as to where a vertex $v$ belongs is the difference $d_{v}$ between its in-degree and its out-degree. When $d_{v}$ is highly negative, it probably belongs near the front of the permutation, since it dominates many elements but is dominated by only a few. One can build a decent ranking permutation by sorting the vertices according to these differences. Even better is incrementally inserting the most negative (or most positive) vertex $v$ into its logical position, deleting the edges incident on $v$, and then adjusting the counts before positioning the next best vertex.\\n\\n%---- Page End Break Here ---- Page : 110\\n\\n\\\\subsection*{4.4.4 PageRank\\\\index{PageRank}}\\nThere is a different and more famous method to order the vertices in a network by importance: the PageRank algorithm underpinning Google\\'s search engine.\\n\\nThe web is constructed of webpages, most of which contain links to other webpages. Your webpage linking to mine is an implicit endorsement that you think my page is pretty good. If it is interpreted as a vote that \"you think my page is better than yours,\" we can construct the network of links and treat it as a maximum acyclic-subgraph problem, discussed in the previous subsection.\\n\\nBut dominance isn\\'t really the right interpretation for links on the web. PageRank instead rewards vertices which have the most in-links to it: if all roads lead to Rome, Rome must be a fairly important place. Further, it weighs these in-links by the strength of the source: a link to me from an important page should count for more than one from a spam site.\\n\\nThe details here are interesting, but I will defer a deeper discussion to Section 10.4, when we discuss network analysis. However, I hope this brief introduction to PageRank helps you appreciate the following tale.\\n\\n\\\\subsection*{4.5 War Story: Clyde\\\\index{Clyde}\\\\index{Clyde}\\'s Revenge}\\nDuring my sophomore year of high school, I had the idea of writing a program to predict the outcome of professional football\\\\index{football} games. I wasn\\'t all that interested in football as a sport, but I observed several of my classmates betting their lunch money on the outcome of the weekend football games. It seemed clear to me that writing a program which accurately predicted the outcome of football games could have significant value, and be a very cool thing to do besides.\\n\\nIn retrospect, the program I came up with now seems hopelessly crude. My program would average the points scored by team $x$ and the points\\\\index{game prediction} allowed by team $y$ to predict the number of points $x$ will score against $y$.\\n\\n$$\\n\\\\begin{aligned}\\n& P_{x}=\\\\frac{((\\\\text { points scored by team } x)+(\\\\text { points allowed by team } y))}{2 \\\\times(\\\\text { games played })} \\\\\\\\\\n& P_{y}=\\\\frac{((\\\\text { points scored by team } y)+(\\\\text { points allowed by team } x))}{2 \\\\times(\\\\text { games played })}\\n\\\\end{aligned}\\n$$\\n\\nI would then adjust these numbers up or down in response to other factors, particularly home field advantage, round the numbers appropriately, and call what was left my predicted score for the game.\\n\\nThis computer program, Clyde, was my first attempt to build a scoring function for some aspect of the real world. It had a certain amount of logic going for it. Good teams score more points than they allow, while bad teams allow more points than they score. If team $x$ plays a team $y$ which has given up a lot of points, then $x$ should score more points against $y$ than it does against\\\\\\n%---- Page End Break Here ---- Page : 111\\n\\\\\\nteams with better defenses. Similarly, the more points team $x$ has scored against the rest of the league, the more points it is likely to score against $y$.\\n\\nOf course, this crude model couldn\\'t capture all aspects of football reality. Suppose team $x$ has been playing all stiffs thus far in the season, while team $y$ has been playing the best teams in the league. Team $y$ might be a much better team than $x$ even though its record so far is poor. This model also ignores any injuries a team is suffering from, whether the weather is hot or cold, and whether the team is hot or cold. It disregards all the factors that make sports inherently unpredictable.\\n\\nAnd yet, even such a simple model can do a reasonable job of predicting the outcome of football games. If you compute the point averages as above, and give the home team an additional three points as a bonus, you will pick the winner in almost two-thirds of all football games. Compare this to the even cruder model of flipping a coin, which predicts only half the games correctly. That was the first major lesson Clyde taught me:\\n\\n\\\\section*{Even crude mathematical models can have real predictive power.}\\nA\\\\index{Rota, Gian-Carlo}s an au\\\\ind\\\\index{matrix}ex{line\\\\index{power of}ar algebra}dacious 16 year-old, I wrote to our local newspaper, The New Brunswick Home News, explaining that I had a computer program to predict football game results and was ready to offer them the exclusive opportunity to publish my predictions each week. Remember that this was back in 1977, well before personal computers had registered on the public consciousness. In those days, the idea of a high school kid actually using a computer had considerable gee-whiz novelty value. To appreciate how much times have changed, check out the article the paper published about Clyde and I in Figure 4.10.\\n\\nI got the job. Clyde predicted the outcome of each game in the 1977 National Football League\\\\index{National Football League}. As I recall, Clyde and I finished the season with the seemingly impressive record of 135-70. Each week, they would compare my predictions against those of the newspaper\\'s sportswriters. As I recall, we all finished within a few games of each other, although most of the sportswriters finished with better records than the computer.\\n\\nThe Home News was so impressed by my work that they didn\\'t renew me the following season. However, Clyde\\'s picks for the 1978 season were published in the Philadelphia Inquirer, a much bigger newspaper. I didn\\'t have the column to myself, though. Instead, the Inquirer included me among ten amateur and professional prognosticators, or touts. Each week we had to predict the outcomes of four games against the point spread\\\\index{point spread}.\\n\\nThe point spread in football is a way of handicapping stronger teams for betting purposes. The point spread is designed to make each game a $50 / 50$ proposition, and hence makes predicting the outcome of games much harder.\\n\\nClyde and I didn\\'t do very well against the spread during the 1978 National Football League season, and neither did most of the other Philadelphia Inquirer touts. We predicted only $46 \\\\%$ of our games correctly against the spread, a performance good (or bad) enough to finish 7th out of the ten published prognosticators. Picking against the spread taught me a second major life lesson:\\n\\n%---- Page End Break Here ---- Page : 112\\n\\n\\\\section*{Ð¼à¦¾à§°uant uses computers to predict football winners}\\nEAST BRUNSWTCK - A 16 -yearold East Brunswick High School student has found a way to combine an interest in foottall with a fascination for computers.\\\\\\\\\\nSteven Skiena says be can determine, with a hight degree of accuracy the outcome of professbinal football games by feeding a computer pertinent information about compeling teams.\\\\\\\\\\n\"The winners will almost al ways be correct,\" said the high sehool junior who lives at 5 Currier Road off Dunhams Comer Road. \"I had an $\\\\$ 5$ per cent aceuracy rate when I staried predicting at the end of last season.\"\\\\\\\\\\nHe does it by feeding the computer a myriad of slatistics that include team records, points seored and aliowed, average yards gained and allowed during a game, a breakdown of the yards pained and allowed into reshing and passing categories, performances at home and on the roed, and more\\\\\\\\\\nThe information is gathered from weekly compilations of footbsill statistics and standings. Skiena puts the facts on index cards and then types them into one of the six computer terminals at the high school or a terminal at The Library where be works part-lime after school.\\\\\\\\\\nII get a winning team, a decimal score for each team and a point spread,\" said the teet-ager who completed a cornputer programming course last year at the bigh school\\\\\\\\\\nHis first attempt at pleking winners involved a Monday night game between boo Oakiand Raiders, the eventual Super Bowi victors, and the Cincinnatl Bengals.\\\\\\\\\\nIt was a difficult game to analyze because Cinctnnati was fighting for a playoff berth while Oakland bad already clinched a spot in the post season compettion.\\\\\\\\\\n\"Nobody knew whether Oakland would be giving 100 per ceat.\" Skiena said. \"Bat my calculations indicated they would $w i n$ by $24-20$. The final score was $35-20$. they went all out.\"\\\\\\\\\\nSkiena sald he went oa to pick 12 of the 14 winners the following week and accurately preticted Oakland would defeat Mitnnesota in the Super Bowl.\\n\\nThe National Football League\\'s 1778 Record Book, *hich breaks down last year\\'s statistics for each of the eague\\'s 28 teams, will supply most of Skiena\\'s infornation for the first few weeks of the 1977 season. He vill also use statistics from the final two extibition lames played this year by each of the teams.\\\\\\\\\\nSkiena wrote a computer program based on 17 . itatistical variabies that might come into play during 1 football game.\\\\\\\\\\nThe compater, in essence, asks him questions and he ypes the answers\\\\\\\\\\n\"It starts out by asking for the names of the teams,\" e said. \"Then it will ask for records, points scored, tc....\"\\\\\\\\\\nThe computer program also attempts to include tch intangible variables as injuries.\\\\\\\\\\n\"The injuries are broken down into offense, defense nd quarterback,\" he explained. \"Obviously a quarterack injury is the most serious. It\\'s too difficult to reak doan injuries for every position. When the omputer asks for the number of injuries on detense. I1 type in one. two or whatever the figure is.\\n\\nWill Skiena use his compater results to eater the ariety of football pools and contests that are availar le during the season?\\\\\\\\\\n\"No.\" be said. \"I dont like to bet on my own redictions. Last season a fitiend bet on a game I cedicted and it happened to be one of the few that\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-129}\\n\\nSTEVEN SKIENA\\\\\\\\\\n...to test his accuracy\\n\\n\\\\section*{Predictions published}\\nSteven Skiena will get a chance to display his skill as a pro foothall prognosticator each Sunday in The Home News.\\\\\\\\\\nThe youngster\\'s seekly selections will be an \"added iagredient to our foothall eoverage,\" acconding to Home News Executive Editor Robert E. Rhodes.\\\\\\\\\\n\"I think it\\'s interesting enough for us to give it a shot,\" Rhodes said of the teen ager\\'s computer method of determining the outcome of games. ${ }^{6}$ He seeras We an earnest young man and we\\'ll stand bectind him.\"\\\\\\\\\\nSkiena will reccive a \"modest stipend\" for predicting the winners, scores for each team and briefly explaining the reasots for his conclasion:\\\\\\\\\\nHis column witl appear for the first time in Sunday\\'s sports sectiva when the National Football Ieague (AFL) opens its 1977 season with a slate of 13 games. Skiena will also predict the oatcome of the league\\'s Monday night games.\\\\\\\\\\nThe Home News is publishing the column in the sports section to test the younuster\\'s system and to offer football tans an entertaining feature. 1ts purpose is not to encouraze betting.\\\\\\\\\\n\"We\\'ll he printing lis selfections rlose enough th the time of the game to prevent belting. Mhodes said \"There\\'s bis interest in pro fontball and more than anything else we want to test his system. [ II be rooting for him.\"\\n\\nFigure 4.10: My first attempt at mathematical modeling.\\n\\nCrude mathematical models do not have real predictive power when there is real money on the line.\\n\\nSo Clyde was not destined to revolutionize the world of football prognostication. I pretty much forgot about it until I assigned the challenge of predicting the Super Bowl as a project in my data science class. The team that got the job was made up of students from \\\\index{Arrowâs impossibility theorem}India, meaning they knew much more about cricket than American football when they started.\\n\\nStill, they rose to the challenge, becoming fans as they built a large data set on the outcome of every professional and college game played over the past ten years. They did a logistic regression analysis over 142 different features including rushing, passing, and kicking yardage, time of possession, and number of punts. They then proudly reported to me the accuracy of their model: correct predictions on $51.52 \\\\%$ of NFL games.\\\\\\\\\\n\"What!\" I screamed, \"That\\'s terrible!\" \"Fifty percent is what you get by flipping a coin. Try averaging the points scored and yielded by the two teams, and give three points to the home team. How does that simple model do?\"\\n\\nOn their data set, this Clyde-light model picked $59.02 \\\\%$ of all games correctly, much much better than their sophisticated-looking machine learning model. They had gotten lost in the mist of too many features, which were not properly normalized, and built using statistics collected over too long a history to be representative of the current team composition. Eventually the students managed to come up with a PageRank-based model that did a little bit better ( $60.61 \\\\%$ ), but Clyde did almost as well serving as a baseline model.\\n\\nThere are several important lessons here. First, garbage in, garbage out. If you don\\'t prepare a clean, properly normalized data set, the most advanced machine learning algorithms can\\'t save you. Second, simple scores based on a modest amount of domain-specific knowledge can do surprisingly well. Further, they help keep you honest. Build and evaluate simple, understandable baselines before you invest in more powerful approaches. Clyde going baseline left their machine learning model defenseless.\\n\\n\\\\subsection*{4.6 Arrow\\'s Impossibility Theorem}\\nWe have seen several approaches to construct rankings or scoring functions from data. If we have a gold standard reporting the \"right\" relative order for at least some of the entities, then this could be used to train or evaluate our scoring function to agree with these rankings to the greatest extent possible.\\n\\nBut without a gold standard, it can be shown that no best ranking system exists. This is a consequence of Arrow\\'s impossibility theorem, which proves that no election system for aggregating permutations of preferences satisfies the following desirable and innocent-looking properties:\\n\\n\\\\begin{itemize}\\n  \\\\item The system should be complete, in that when asked to choose between alternatives $A$ and $B$, it should say (1) $A$ is preferred to $B$, (2) $B$ is preferred to $A$, or (3) there is equal preference between them.\\n\\n%---- Page End Break Here ---- Page : 114\\n\\\\end{itemize}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|ccc}\\nVoter & Red & Green & Blue \\\\\\\\\\n\\\\hline\\nx & 1 & 2 & 3 \\\\\\\\\\ny & 2 & 3 & 1 \\\\\\\\\\nz & 3 & 1 & 2 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 4.11: Preference rankings for colors highlighting the loss of transitivity. Red is preferred to green and green preferred to blue, yet blue is preferred to red.\\n\\n\\\\begin{itemize}\\n  \\\\item The results should be transitive, meaning if $A$ is preferred to $B$, and $B$ is preferred to $C$, then $A$ must be preferred to $C$.\\n  \\\\item If every individual prefers $A$ to $B$, then the system should prefer $A$ to $B$.\\n  \\\\item The system should not depend only upon the preferences of one individual, a dictator.\\n  \\\\item The preference of $A$ compared to $B$ should be independent of preferences for any other alternatives, like $C$.\\n\\\\end{itemize}\\n\\nFigure 4.11 captures some of the flavor of Arrow\\'s theorem, and the nontransitive nature of \"rock-paper-scissors\" type ordering. It shows three voters ( $x, y$, and $z$ ) ranking their preferences among colors. To establish the preference among two colors $a$ and $b$, a logical system might compare how many permutations rank $a$ before $b$ as opposed to $b$ before $a$. By this system, red is preferred to green by $x$ and $y$, so red wins. Similarly, green is preferred to blue by $x$ and $z$, so green wins. By transitivity, red should be preferred to blue by implication on these results. Yet $y$ and $z$, prefer blue to red, violating an inherent property we want our election system to preserve.\\n\\nArrow\\'s theorem is very surprising, but does it mean that we should give up on rankings as a tool for analyzing data? Of course not, no more than Arrow\\'s theorem means that we should give up on democracy. Traditional voting systems based on the idea that the majority rules generally do a good job of reflecting popular preferences, once appropriately generalized to deal with large numbers of candidates. And the techniques in this chapter generally do a good job of ranking items in interesting and meaningful ways.\\n\\nTake-Home Lesson: We do not seek correct rankings, because this is an illdefined objective. Instead, we seek rankings that are useful and interesting.\\n\\n\\\\subsection*{4.7 War Story: Who\\'s Bigger?}\\nMy students sometimes tell me that I am history. I hope this isn\\'t true quite yet, but I am very interested in history, as is my former postdoc Charles Ward. Charles and I got to chatting about who the most significant figures in history\\\\\\n%---- Page End Break Here ---- Page : 115\\n\\\\\\nwere, and how you might measure this. Like most people, we found our answers in Wikipedia\\\\index{Wikipedia}.\\n\\nWikipedia is an amazing thing, a distributed work product built by over 100,000 authors which somehow maintains a generally sound standard of accuracy and depth. Wikipedia captures an astonishing amount of human knowledge in an open and machine-readable form.\\n\\nWe set about using the English Wikipedia as a data source to base historical\\\\index{historical} rankings on. Our first step was to extract feature variables from each person\\'s Wikipedia page that should clearly correlate with historical significance. This included features like:\\n\\n\\\\begin{itemize}\\n  \\\\item Length: Most significant historical figures should have longer Wikipedia pages than lesser mortals. Thus article length in words provides a natural feature reflecting historical wattage, to at least some degree.\\n  \\\\item Hits: The most significant figures have their Wikipedia pages read more often than others, because they are of greater interest to a larger number of people. My Wikipedia page gets hit an average of twenty times per day, which is pretty cool. But Issac Newton\\'s page gets hit an average of 7700 times per day, which is a hell of a lot better.\\n  \\\\item PageRank: Significant historical figures interact with other significant historical figures, which get reflected as hyperlink references in Wikipedia articles. This defines a directed graph where the vertices are articles, and the directed edges hyperlinks. Computing the PageRank of this graph will measure the centrality of each historical figure, which correlates well with significance.\\n\\\\end{itemize}\\n\\nAll told, we extracted six features for each historical figure. Next, we normalized these variables before aggregating, essentially by combining the underlying rankings with normally-distributed weights, as suggested in Section 4.4.2 We used a technique called statistical factor analysis related to principal component analysis (discussed in Section 8.5.2, to isolate two factors that explained most of the variance in our data. A simple linear combination of these variables gave us a scoring function, and we sorted the scores to determine our initial ranking, something we called fame.\\n\\nThe top twenty figures by our fame score are shown in Figure 4.12 (right). We studied these rankings and decided that it didn\\'t really capture what we wanted it to. The top twenty by fame included pop musicians like Madonna and Michael Jackson, and three contemporary U.S. presidents. It was clear that contemporary figures ranked far higher than we thought they should: our scoring function was capturing current fame much more than historical significance.\\n\\nOur solution was to decay the scores of contemporary figures to account for the passage of time. That a current celebrity gets a lot of Wikipedia hits is impressive, but that we still care about someone who died 300 years ago is much more impressive. The top twenty figures after age correction are shown in Figure 4.12 (left).\\n\\n%---- Page End Break Here ---- Page : 116\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|r|l|c|l|}\\n\\\\hline\\nSignif & \\\\multicolumn{1}{|c|}{Name} & \\\\multicolumn{1}{|c|}{Fame} & \\\\multicolumn{1}{|c|}{Person} \\\\\\\\\\n\\\\hline\\n1 & Jesus & 1 & George W. Bush \\\\\\\\\\n2 & Napoleon &  &  \\\\\\\\\\n3 & William Shakespeare & 2 & Barack Obama \\\\\\\\\\n4 & Muhammad & 3 & Jesus \\\\\\\\\\n5 & Abraham Lincoln & 4 & Adolf Hitler \\\\\\\\\\n6 & George Washington & 5 & Ronald Reagan \\\\\\\\\\n7 & Adolf Hitler & 6 & Bill Clinton \\\\\\\\\\n8 & Aristotle & 7 & Napoleon \\\\\\\\\\n9 & Alexander the Great &  &  \\\\\\\\\\n10 & Thomas Jefferson & 8 & Michael Jackson \\\\\\\\\\n11 & Henry VIII & 9 & W. Shakespeare \\\\\\\\\\n12 & Elizabeth I & 10 & Elvis Presley \\\\\\\\\\n13 & Julius Caesar & 11 & Muhammad \\\\\\\\\\n14 & Charles Darwin & 12 & Joseph Stalin \\\\\\\\\\n15 & Karl Marx & 13 & Abraham Lincoln \\\\\\\\\\n16 & Martin Luther & 14 & G. Washington \\\\\\\\\\n17 & Queen Victoria & 15 & Albert Einstein \\\\\\\\\\n18 & Joseph Stalin & 16 & John F. Kennedy \\\\\\\\\\n19 & Theodore Roosevelt &  &  \\\\\\\\\\n20 & Albert Einstein & 17 & Elizabeth II \\\\\\\\\\n & 18 & John Paul II &  \\\\\\\\\\n & 19 & Madonna &  \\\\\\\\\\n20 & Britney Spears &  &  \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 4.12: The top 20 historical figures, ranked by significance (left) and contemporary fame (right).\\n\\nNow this was what we were looking for! We validated the rankings using whatever proxies for historical significance we could find: other published rankings, autograph prices, sports statistics, history textbooks, and Hall of Fame election results. Our rankings showed a strong correlation against all of these proxies.\\n\\nIndeed, I think these rankings are wonderfully revealing. We wrote a book describing all kinds of things that could be learned from them [SW13. I proudly encourage you to read it if you are interested in history and culture. The more we studied these rankings, the more I was impressed in their general soundness.\\n\\nThat said, our published rankings did not meet with universal agreement. Far from it. Dozens of newspaper and magazine articles were published about our rankings, many quite hostile. Why didn\\'t people respect them, despite our extensive validation? In retrospect, most of the flack we fielded came for three different reasons:\\n\\n\\\\begin{itemize}\\n  \\\\item Differing implicit notions of significance: Our methods were designed to measure meme-strength, how successfully these historical figures were propagating their names though history. But many readers thought our methods should capture notions of historical greatness. Who was most important, in terms of changing the world? And do we mean world or just the English-speaking world? How can there be no Chinese or Indian\\\\\\n%---- Page End Break Here ---- Page : 117\\n\\\\\\nfigures on the list when they represent over $30 \\\\%$ of the world\\'s population?\\\\\\\\\\nWe must agree on what we are trying to measure before measuring it. Height is an excellent measure of size, but it does not do a good job of capturing obesity. However, height is very useful to select players for a basketball team.\\n  \\\\item outlier\\\\index{outlier}s: Sniff tests are important to evaluating the results of an analysis. With respect to our rankings, this meant checking the placement of people we knew, to confirm that they fell in reasonable places.\\\\\\\\\\nI felt great about our method\\'s ranking of the vast majority of historical figures. But there were a few people who our method ranked higher than any reasonable person would, specifically President George W. Bush (36) and teenage TV star Hilary Duff (1626). One could look at these outliers and dismiss the entire thing. But understand that we ranked almost 850,000 historical figures, roughly the population of San Francisco. A few cherry-picked bad examples must be put in the proper context.\\n  \\\\item Pigeonhole constraints: Most reviewers saw only the rankings of our top 100 figures, and they complained about exactly where we placed people and who didn\\'t make the cut. The women\\'s TV show The View complained we didn\\'t have enough women. I recall British articles complaining we had Winston Churchill (37) ranked too low, South African articles that thought we dissed Nelson Mandela (356), Chinese articles saying we didn\\'t have enough Chinese, and even a Chilean magazine whining about the absence of Chileans.\\\\\\\\\\nSome of this reflects cultural differences. These critics had a different implicit notion of significance than reflected by English Wikipedia. But much of it reflects the fact that there are exactly one hundred places in the top 100. Many of the figures they saw as missing were just slightly outside the visible horizon. For every new person we moved into the top hundred, we had to drop somebody else out. But readers almost never suggested names that should be omitted, only those who had to be added.\\n\\\\end{itemize}\\n\\nWhat is the moral here? Try to anticipate the concerns of the audience for your rankings. We were encouraged to explicitly call our measure meme-strength instead of significance. In retrospect, using this less-loaded name would have permitted our readers to better appreciate what we were doing. We probably also should have discouraged readers from latching on to our top 100 rankings, and instead concentrate on relative orderings within groups of interest: who were the top musicians, scientists, and artists? This might have proved less controversial, better helping people build trust in what we were doing.\\n\\n\\\\subsection*{4.8 Chapter Notes}\\nLangville and Meyer LM12] provide a through introduction to most of the ranking methods discussed here, including Elo and PageRank.\\n\\n%---- Page End Break Here ---- Page : 118\\n\\nOne important topic not covered in this chapter is learning to rank\\\\index{learning to rank} methods, which exploit gold standard ranking data to train appropriate scoring functions. Such ground truth data is generally not available, but proxies can sometimes be found. When evaluating search engines, the observation that a user clicked the (say) fourth item presented to them can be interpreted as a vote that it should have been higher ranked than the three placed above it. SVMrank Joa02] presents a method for learning ranking functions from such data.\\n\\nThe heuristic proposed minimizing edge conflicts in a vertex order is due to Eades et. al. ELS93]. My presentation of Arrow\\'s impossibility theorem is based on notes from Watkins Wat16.\\n\\nThe war stories of this chapter were drawn very closely from my books Calculated Bets and Who\\'s Bigger? Don\\'t sue me for self-plagiarism.\\n\\n\\\\subsection*{4.9 exercises\\\\index{exercises}}\\n\\\\section*{Scores and Rankings}\\n4-1. [3] Let $X$ represent a random variable drawn from the normal distribution defined by $\\\\mu=2$ and $\\\\sigma=3$. Suppose we observe $X=5.08$. Find the Z-score of $x$, and determine how many standard deviations away from the mean that $x$ is.\\\\\\\\\\n$4-2$. [3] What percentage of the standard normal distribution $(\\\\mu=0, \\\\sigma=1)$ is found in each region?\\\\\\\\\\n(a) $Z>1.13$.\\\\\\\\\\n(b) $Z<0.18$.\\\\\\\\\\n(c) $Z>8$.\\\\\\\\\\n(d) $|Z|<0.5$.\\n\\n4-3. [3] Amanda took the Graduate Record Examination (GRE), and scored 160 in verbal reasoning and 157 in quantitative reasoning. The mean score for verbal reasoning was 151 with a standard deviation of 7 , compared with mean $\\\\mu=153$ and $\\\\sigma=7.67$ for quantitative reasoning. Assume that both distributions are normal.\\\\\\\\\\n(a) What were Amanda\\'s Z-scores on these exam sections? Mark these scores on a standard normal distribution curve.\\\\\\\\\\n(b) Which section did she do better on, relative to other students?\\\\\\\\\\n(c) Find her percentile scores for the two exams.\\n\\n4-4. [3] Identify three successful and well-used scoring functions in areas of personal interest to you. For each, explain what makes it a good scoring function and how it is used by others.\\\\\\\\[0pt]\\n4-5. [5] Find a data set on properties of one of the following classes of things:\\\\\\\\\\n(a) The countries of the world.\\\\\\\\\\n(b) Movies and movie stars.\\\\\\\\\\n(c) Sports stars.\\\\\\n%---- Page End Break Here ---- Page : 119\\n\\\\\\n(d) Universities.\\n\\nConstruct a sensible ranking function reflecting quality or popularity. How well is this correlated with some external measure aiming at a similar result?\\\\\\\\[0pt]\\n4-6. [5] Produce two substantially different but sensible scoring functions on the same set of items. How different are the resulting rankings? Does the fact that both have to be sensible constrain rankings to be grossly similar?\\n\\n4-7. [3] The scoring systems used by professional sports leagues to select the most valuable player award winner typically involves assigning positional weights to permutations specified by voters. What systems do they use in professional baseball, basketball, and football? Are they similar? Do you think they are sensible?\\n\\n\\\\section*{Implementation Projects}\\n4-8. [5] Use Elo ratings to rank all the teams in a sport such as baseball, football, or basketball, which adjusts the rating in response to each new game outcome. How accurately do these Elo ratings predict the results of future contests?\\\\\\\\[0pt]\\n4-9. [5] Evaluate the robustness of Borda\\'s method by applying $k$ random swaps to each of $m$ distinct copies of the permutation $p=\\\\{1,2, \\\\ldots, n\\\\}$. What is the threshold where Borda\\'s method fails to reconstruct $p$, as a function of $n, k$, and $m$ ?\\n\\n\\\\section*{Interview Questions}\\n4-10. [5] What makes a data set a gold standard?\\\\\\\\[0pt]\\n4-11. [5] How can you test whether a new credit risk scoring model works?\\\\\\\\[0pt]\\n4-12. [5] How would you forecast sales for a particular book, based on Amazon public data?\\n\\n\\\\section*{Kaggle Challenges}\\n4-13. Rating chess players from game positions.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/chess}{https://www.kaggle.com/c/chess}\\\\\\\\\\n4-14. Develop a financial credit scoring system.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/GiveMeSomeCredit}{https://www.kaggle.com/c/GiveMeSomeCredit}\\\\\\\\\\n4-15. Predict the salary of a job from its ad.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/job-salary-prediction}{https://www.kaggle.com/c/job-salary-prediction}\\n\\n\\\\section*{Chapter 5}\\n\\\\section*{statistical analysis\\\\index{statistical analysis}}\\nIt is easy to lie with statistics, but easier to lie without them.\\n\\n\\\\begin{itemize}\\n  \\\\item Frederick Mosteller\\n\\\\end{itemize}\\n\\nI will confess that I have never had a truly satisfying conversation with a statistician. This is not completely for want of trying. Several times over the years I have taken problems of interest to statisticians, but always came back with answers like \"You can\\'t do it that way\" or \"But it\\'s not independent,\" instead of hearing \"Here is the way you can handle it.\"\\n\\nTo be fair, these statisticians generally did not appreciate talking with me, either. Statisticians have been thinking seriously about data for far longer than computer scientists, and have many powerful methods and ideas to show for it. In this chapter, I will introduce some of these important tools, like the definitions of certain fundamental distributions and tests for statistical significance. This chapter will also introduce Baysian analysis, a way to rigorously assess how new data should affect our previous estimates of future events.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-137}\\n\\nFigure 5.1: The central dogma of statis\\\\index{Mosteller, Frederick}tics: analysis of a small random sample enables drawing rigorous inferences about the entire population.\\n\\nFigure 5.1 illustrates the process of statistical reasoning. There is an underlying population of possible things that we can potentially observe. Only a relatively small subset of them are actually sampled, ideally at random, meaning that we can observe properties of the sampled items. Probability theory describes what properties our sample should have, given the properties of the underlying population. But statistical inference works the other way, where we try to deduce what the full population is like given analysis of the sample.\\n\\nIdeally, we will learn to think like a statistician: enough so as to remain vigilant and guard against overinterpretation and error, while retaining our confidence to play with data and take it where it leads us.\\n\\n\\\\subsection*{5.1 Statistical Distributions}\\nEvery variable that we observe defines a particular frequency distribution, which reflects how often each particular value arises. The unique properties of variables like height, weight, and IQ are captured by their distributions. But the shapes of these distributions are themselves not unique: to a great extent, the world\\'s rich variety of data appear only in a small number of classical forms.\\n\\nThese classical distributions have two nice properties: (1) they describe \\\\index{principal components}shapes of frequency distributions that arise often in practice, and (2) they can often be described mathematically using closed-form expressions with very few parameters. Once abstracted from specific data observations, they become probability distributions, worthy of independent study.\\n\\nFamiliarity with the classical probability distributions is important. They arise often in practice, so you should be on the look out for them. They give us a vocabulary to talk about what our data looks like. We will review the most important statistical distributions (binomial, normal, Poisson, and power law) in the sections to follow, emphasizing the properties that define their essential character.\\n\\nNote that your observed data does not necessarily arise from a particular theoretical distribution just because its shape is similar. Statistical tests can be used to rigorously prove whether your experimentally-observed data reflects samples drawn from a particular distribution.\\n\\nBut I am going to save you the trouble of actually running any of these tests. I will state with high confidence that your real-world data does not precisely fit any of the famous theoretical distributions.\\n\\nWhy is that? Understand that the world is a complicated place, which makes measuring it a messy process. Your observations will probably be drawn from multiple sample populations, each of which has a somewhat different underlying distribution. Something funny generally happens at the tails of any observed distribution: a sudden burst of unusually high or low values. Measurements will have errors associated with them, sometimes in weird systematic ways.\\n\\nBut that said, understanding the basic distributions is indeed very important. Each classical distribution is classical for a reason. Understanding these reasons tells you a lot about observed data, so they will be reviewed here.\\\\\\n%---- Page End Break Here ---- Page : 122\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-139(1)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-139}\\n\\nFigure 5.2: The binomial distribution can be used to model the distribution of heads in 200 coin tosses with $p=0.5$ (left), and the number of blown lightbulbs in 1000 events with failure probability $p=0.001$ (right).\\n\\n\\\\subsection*{5.1.1 The Binomial Distribution}\\nConsider an experiment consisting of identical, independent trials which have two possible outcomes $P_{1}$ and $P_{2}$, with the respective probabilities of $p$ and $q=(1-p)$. Perhaps your experiment is flipping fair coins, where the probability of heads $(p=0.5)$ is the same as getting tails $(q=0.5)$. Perhaps it is repeatedly turning on a light switch, where the probability of suddenly discovering that you must change the bulb ( $p=0.001$ ) is much less than that of seeing the light ( $q=0.999$ ).\\n\\nThe binomial distribution reports the probability of getting exactly $x P_{1}$ events in the course of $n$ independent trials, in no particular order. Independence is important here: we are assuming the probability of failure of a bulb has no relation to how many times it has previously been used. The pdf for the binomial distribution is defined by:\\n\\n$$\\nP(X=x)=\\\\binom{n}{x} p^{x}(1-p)^{(n-x)}\\n$$\\n\\nThere are several things to observe about the binomial distribution:\\n\\n\\\\begin{itemize}\\n  \\\\item It is discrete: Both arguments to the binomial distribution ( $n$ and $x$ ) must be integers. The smoothness of Figure 5.2 (left) is an illusion, because $n=200$ is fairly large. There is no way of getting 101.25 heads in 200 coin tosses.\\n  \\\\item You probably can explain the theory behind it: You first encountered the binomial distribution in high school. Remember Pascal\\'s triangle? To end up with exactly $x$ heads in $n$ flips in a particular sequence occurs with probability $p^{x}(1-p)^{(n-x)}$, for each of the $\\\\binom{n}{x}$ distinct flip sequences.\\n  \\\\item It is sort of bell-shaped: For a fair coin $(p=0.5)$, the binomial distribution is perfectly symmetrical, with the mean in the middle. This is not true\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-140}\\n\\\\end{itemize}\\n\\nFigure 5.3: The probability density function (pdf) of the normal distribution\\\\index{normal distribution} (left) with its corresponding cumulative density function (cdf) on right.\\\\\\\\\\nin the lightbulb case: if we only turn on the bulb $n=1000$ times, the most likely number of failures will be zero. This rings just half the bell in Figure 5.2. That said, as $n \\\\rightarrow \\\\infty$ we will get a symmetric distribution peaking at the mean.\\n\\n\\\\begin{itemize}\\n  \\\\item It is defined using only two parameters: All we need are values of $p$ and $n$ to completely define a given binomial distribution.\\n\\\\end{itemize}\\n\\nMany things can be reasonably modeled by the binomial distribution. Recall the variance in the performance of a $p=0.300$ hitter discussed in Section 2.2.3. There the probability of getting a hit with each trial was $p=0.3$, with $n=500$ trials per season. Thus the number of hits per season are drawn from a binomial distribution.\\n\\nRealizing that it was a binomial distribution meant that we really didn\\'t have to use simulation to construct the distribution. Properties like the expected number of hits $\\\\mu=n p=500 \\\\times 0.3=150$ and its standard deviation $\\\\sigma=$ $\\\\sqrt{n p q}=\\\\sqrt{500 \\\\times 0.3 \\\\times 0.7}=10.25$ simply fall out of closed-form formulas that you can look up when needed.\\n\\n\\\\subsection*{5.1.2 The Normal Distribution}\\nA great many natural phenomenon are modeled by bell-shaped curves. Measured characteristics like height, weight, lifespan, and IQ all fit the same basic scheme: the bulk of the values lie pretty close to the mean, the distribution is symmetric, and no value is too extreme. In the entire history of the world, there has never been either a 12 -foot-tall man or a 140 -year-old woman.\\n\\nThe mother of all bell-shaped curves is the Gaussian or normal distribution, which is completely parameterized by its mean and standard deviation:\\n\\n$$\\nP(x)=\\\\frac{1}{\\\\sigma \\\\sqrt{2 \\\\pi}} e^{-(x-\\\\mu)^{2} / 2 \\\\sigma^{2}}\\n$$\\n\\nFigure 5.3 shows the pdf and cdf of the normal distribution. There are several things to note:\\n\\n\\\\begin{itemize}\\n  \\\\item It is continuous: The arguments to the normal distribution (mean $\\\\mu$ and standard deviation $\\\\sigma$ ) are free to arbitrary real numbers, with the lone constraint that $\\\\sigma>0$.\\n  \\\\item You probably can\\'t explain where it comes from: The normal distribution is a generalization of the binomial distribution, where $n \\\\rightarrow \\\\infty$ and the degree of concentration around the mean is specified by the parameter $\\\\sigma$. Take your intuition here from the binomial distribution, and trust that Gauss got his calculations right: the great mathematician worked out the normal distribution for his Ph.D. dissertation. Or consult any decent statistics book if you are really curious to see where it comes from.\\n  \\\\item It truly is bell-shaped: The Gaussian distribution\\\\index{Gaussian distribution} is the platonic example of a bell-shaped curve. Because it operates on a continuous variable (like height) instead of a discrete count (say, the number of events) it is perfectly smooth. Because it goes infinitely in both directions, there is no truncation of the tails at either end. The normal distribution is a theoretical construct, which helps explain this perfection.\\n  \\\\item It is also defined usi\\\\index{coeï¬cient vector}ng only two parameters: However, these are different parameters than the binomial distribution! The normal distribution is completely defined by its central point (given by the mean $\\\\mu$ ) and its spread (given by the standard deviation $\\\\sigma$ ). They are the only knobs we can use to tweak the distribution.\\n\\\\end{itemize}\\n\\n\\\\section*{What\\'s Normal?}\\nAn amazing number of naturally-occurring phenomenon are modeled by the normal distribution. Perhaps the most important one is measurement error. Every time you measure your weight on a bathroom scale, you will get a somewhat different answer, even if your weight has not changed. Sometimes the scale will read high and other times low, depending upon room temperature and the warping of the floor. Small errors are more likely than big ones, and slightly high is just \\\\index{duality}as likely as slightly low. Experimental error is generally normally distributed as Gaussian noise.\\n\\nPhysical phenomenon like height, weight, and lifespan all have bell-shaped distributions, by similar arguments. Yet the claim that such distributions are normal is usually made too casually, without precisely specifying the underlying population. Is human height normally distributed? Certainly not: men and women have different mean heights and associated distributions. Is male height normally distributed? Certainly not: by including children in the mix and shrinking senior citizens you again have the sum of several different underlying distributions. Is the height of adult males in the United States normal? No, probably not even then. There are non-trivial populations with growth disorders\\\\\\n%---- Page End Break Here ---- Page : 125\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-142}\\n\\nFigure 5.4: The normal distribution implies tight bounds on the probability of lying far from the mean. $68 \\\\%$ of the values must lie within one sigma of the mean, and $95 \\\\%$ within $2 \\\\sigma$, and $99.7 \\\\%$ within $3 \\\\sigma$.\\\\\\\\\\nlike dwarfism and acromegaly, that leave bunches of people substantially shorter and taller than could be explained by the normal distribution.\\n\\nPerhaps the most famous bell-shaped but non-normal distribution is that of daily returns (percentage price movements) in the financial markets. A big market crash is defined by a large percentage price drop: on October 10, 1987, the Dow Jones average lost $22.61 \\\\%$ of its value. Big stock market crashes occur with much greater frequency than can be accurately modeled by the normal distribution. Indeed, every substantial market crash wipes out a certain number of quants who assumed normality, and inadequately insured against such extreme events. It turns out that the logarithm of stock returns proves to be normally distributed, resulting in a distribution with far fatter tails than normal.\\n\\nAlthough we must remember that bell-shaped distributions are not always normal, making such an assumption is a reasonable way to start thinking in the absence of better knowledge.\\n\\n\\\\subsection*{5.1.3 Implications of the Normal Distribution}\\nRecall that the mean and standard deviation together always roughly characterize any frequency distribution, as discussed in Section 2.2.4 But they do a spectacularly good job of characterizing the normal distribution, because they define the normal distribution.\\n\\nFigure 5.4 illustrates the famous $68 \\\\%-95 \\\\%-99 \\\\%$ rule of the normal distribution. Sixty-eight percent of the probability mass must lie within the region $\\\\pm 1 \\\\sigma$ of the mean. Further, $95 \\\\%$ of the probability is within $2 \\\\sigma$, and $99.7 \\\\%$ within $3 \\\\sigma$.\\n\\nThis means that values far from the mean (in terms of $\\\\sigma$ ) are vanishingly rare in any normally distributed variable. Indeed the term six sigma is used to\\\\\\n%---- Page End Break Here ---- Page : 126\\n\\\\\\nconnote quality standards so high that defects are incredibly rare events. We want plane crashes to be six sigma events. The probability of a $6 \\\\sigma$ event on the normal distribution is approximately 2 parts per billion.\\n\\nIntelligence as measured by IQ is normally distributed, with a mean of 100 and standard deviation $\\\\sigma=15$. Thus $95 \\\\%$ of the population lies within $2 \\\\sigma$ of the mean, from 70 to 130 . This leaves only $2.5 \\\\%$ of people with IQs above 130, and another $2.5 \\\\%$ below 70 . A total of $99.7 \\\\%$ of the mass lies within $3 \\\\sigma$ of the mean, i.e. people with IQs between 55 and 145 .\\n\\nSo how smart is the smartest person in the world? If we assume a population of 7 billion people, the probability of a randomly-selected person being smartest is approximately $1.43 \\\\times 10^{-10}$. This is about the same probability of a single sample lying more than $6.5 \\\\sigma$ from the mean. Thus the smartest person in the world should have an IQ of approximately 197.5, according to this reckoning.\\n\\nThe degree to which you accept this depends upon how strongly you believe that IQ really is normally distributed. Such models are usually in grave danger of breaking down at the extremes. Indeed, by this model there is almost the same probability of there being someone dumb enough to earn a negative score on an IQ test.\\n\\n\\\\subsection*{5.1.4 Poisson distribution\\\\index{Poisson distribution}}\\nThe Poisson distribution measures the frequency of intervals between rare events. Suppose we model human lifespan by a sequence of daily events, where there is a small but constant probability $1-p$ that one happens to stop breathing today. A lifespan of exactly $n$ days means successfully breathing for each of the first $n-1$ days and then forever breaking the pattern on the $n$th \\\\index{regre\\\\index{removing outliers}ssion models}day. The probability of living exactly $n$ days is given by $\\\\operatorname{Pr}(n)=p^{n-1}(1-p)$, yielding an expected lifespan\\n\\n$$\\n\\\\mu=\\\\sum_{k\\\\index{removing}=0}^{\\\\infty} k \\\\cdot \\\\operatorname{Pr}(k) .\\n$$\\n\\nThe Poisson distribution basically follows from this analysis, but takes a more convenient argument than $p$. Instead it is based on $\\\\mu$, the average value of the distribution. Since each $p$ defines a particular value of $\\\\mu$, these parameters ar\\\\index{Ascombe quartet}e in some sense equivalent, but the average is much easier to estimate or measure. The Poisson distribution yields the very simple closed form:\\n\\n$$\\n\\\\operatorname{Pr}(x)=\\\\frac{e^{-\\\\mu} \\\\mu^{x}}{x!}\\n$$\\n\\nOnce you start thinking the right way, many distributions begin to look Poisson, because they represent intervals between rare events.\\n\\nRecall the binomial distribution lightbulb model from the previous section. This made it easy to compute the expected number of changes in Figure 5.2 (right), but not the lifespan distribution, which is Poisson. Figure 5.5 plots the associated Poisson distribution for $\\\\mu=1 / p=1000$, which shows that we should\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-144}\\n\\nFigure 5.5: The lifespan di\\\\index{ï¬tting}stribution of lightbulbs with an expected life of $\\\\mu=$ 1000 hours, as modeled by a Poisson distribution.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-144(1)}\\n\\nFigure 5.6: The observed fraction of families with $x$ kids (isolated points) is accurately modeled by Poisson distribution, defined by an average of $\\\\mu=2.2$ children per family (polyline).\\\\\\\\\\nexpect almost all bulbs to glow for between 900 and 1100 hours before the dying of the light.\\n\\nAlternately, suppose we model the number of children by a process where the family keeps having children until after one too many tantrums, bake sales, or loads of laundry, a parent finally cracks. \"That\\'s it! I\\'ve had enough of this. No more!\"\\n\\nUnder such a model, family size should be modeled as a Poisson distribution, where every day there is a small but non-zero probability of a breakdown that results in shutting down the factory.\\n\\nHow well does the \"I\\'ve had it\" model work to predict family size? The polygonal line in Figure 5.6 represents the Poisson distribution with the parameter $\\\\lambda=2.2$, meaning families have an average of 2.2 kids. The points represent the fraction of families with $k$ children, drawn from the 2010 U.S. General Social\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-145}\\n\\nFigure 5.7: The population of U.S. cities by decreasing rank (left). On the right is the same data, now including the very largest cities, but plotted on a log-log scale. That they sit on a line is indicative of a power law distribution.\\n\\nSurvey (GSS).\\\\\\\\\\nThere is excellent agreement over all family sizes except $k=1$, and frankly, my personal experience suggests there are more singleton kids than this data set represents. Together, knowing just the mean and the formula for Poisson distribution enables us to construct a reasonable estimate of the real family-size distribution.\\n\\n\\\\subsection*{5.1.5 Power Law Distributions}\\nMany data distributions exhibit much longer tails than could be possible under the normal or Poisson distributions. Consider, for example, the population of cities. There were exactly 297 U.S. cities in 2014 with populations greater than 100,000 people, according to Wikipedia. The population of the $k$ th largest city, for $1 \\\\leq k \\\\leq 297$ is presented in Figure 5.7 (left). It shows that a relatively small number of cities have populations wildly dominating the rest. Indeed, the seventeen largest cities have populations so large they have been clipped off this plot so that we can see the rest.\\n\\nThese cities have a mean population of 304,689 , with a ghastly standard deviation of 599,816 . Something is wrong when the standard deviation is so large relative to the mean. Under a normal distribution, $99.7 \\\\%$ of the mass lies within $3 \\\\sigma$ of the mean, thus making it unlikely that any of these cities would have a population above 2.1 million people. Yet Houston has a population of 2.2 million people, and New York (at 8.4 million people) is more than $13 \\\\sigma$ above the mean! City populations are clearly not normally distributed. In fact, they observe a different distribution, called a power law.\\n\\n%---- Page End Break Here ---- Page : 129\\n\\nFor a given variable $X$ defined by a power law distribution,\\n\\n$$\\nP(X=x)=c x^{-\\\\alpha}\\n$$\\n\\nThis is parameterized by two constants: the exponent $\\\\alpha$ and normalization constant $c$.\\n\\nPower law distributions require some thinking to properly parse. The total probability defined by this distribution is the area under the curve:\\n\\n$$\\nA=\\\\int_{x=-\\\\infty}^{\\\\infty} c x^{-\\\\alpha}=c \\\\int_{x=-\\\\infty}^{\\\\infty} x^{-\\\\alpha}\\n$$\\n\\nThe particular value of $A$ is defined by the parameters $\\\\alpha$ and $c$. The normalization constant $c$ is chosen specifically for a given $\\\\alpha$ to make sure that $A=1$, as demanded by the laws of probability. Other than that, $c$ is of no particular importance to us.\\n\\nThe real action happens with $\\\\alpha$. Note that when we double the value of the input (from $x$ to $2 x$ ), we decrease the probability by a factor of $f=2^{-\\\\alpha}$. This looks bad, but for any given $\\\\alpha$ it is just a constant. So what the power law is really saying is that the probability of a $2 x$-sized event is $2^{\\\\alpha}$ times less frequent than an $x$-sized event, for all $x$.\\n\\nPersonal wealth is well modeled by a power law, where $f \\\\approx 0.2=1 / 5$. This means that over a large range, if $Z$ people have $x$ dollars, then $Z / 5$ people have $2 x$ dollars. One fifth as many people have $\\\\$ 200,000$ than have $\\\\$ 100,000$. If there are 625 people in the world worth $\\\\$ 5$ billion, then there should be approximately 125 multi-billionaires each worth $\\\\$ 10$ billion. Further, there should be 25 superbillionaires each worth $\\\\$ 20$ billion, five hyper-billionaires at the $\\\\$ 40$ billion level, and finally a single Bill Gates worth $\\\\$ 80$ billion.\\n\\nPower laws define the \" $80 / 20$ \" rules which account for all the inequality of our world: the observation that the top $20 \\\\%$ of the $A$ gets fully $80 \\\\%$ of the $B$. Power laws tend to arise whenever the rich get richer, where there is an increasing probability you will get more based on what you already have. Big cities grow disproportionately \\\\index{tip\\\\index{taxi driver}ping rate}large because more people are attracted to cities when they are big. Because of his wealth, Bill Gates gets access to much better investment opportunities than I do, so his money grows faster than mine does.\\n\\nMany distributions are defined by such preferential growth or attachment models, including:\\n\\n\\\\\\\\index{dimension reduction}begin{itemize}\\n  \\\\item Internet sites wi\\\\index{New York}th $x$ users: Websites get more popular because they have more users. You are more likely to join Instagram \\\\index{highly-correlated}or Facebook because your friends have already joined Instagram or Facebook. Preferential attachment leads to a power law distribution.\\n  \\\\item Words used with a relative frequency of $x$ : There is a long tail of millions of words like algorist or defenestrat $\\\\rrbracket^{1}$ that are rarely used in the English\\n\\\\end{itemize}\\n\\n\\\\footnotetext{${ }^{1}$ Defenestrate means \"to throw someone out a window.\"\\n}\\nlanguage. On the other hand, a small set of words like the are used wildly more often than the rest.\\n\\nZipf\\'s law governs the distribution of word usage in natural languages, and states that the $k$ th most popular word (as measured by frequency rank) is used only $1 / k$ th as frequently as the most popular word. To gauge how well it works, consider the ranks of words based on frequencies from the English Wikipedia below:\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c}\\nRank & Word & Count \\\\\\\\\\n\\\\hline\\n1 & the & 25131726 \\\\\\\\\\n110 & even & 415055 \\\\\\\\\\n212 & men & 177630 \\\\\\\\\\n312 & least & 132652 \\\\\\\\\\n412 & police & 99926 \\\\\\\\\\n514 & quite & 79205 \\\\\\\\\\n614 & include & 65764 \\\\\\\\\\n714 & knowledge & 57974 \\\\\\\\\\n816 & set & 50862 \\\\\\\\\\n916 & doctor & 46091 \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c}\\nRank & Word & Count \\\\\\\\\\n\\\\hline\\n1017 & build & 41890 \\\\\\\\\\n2017 & essential & 21803 \\\\\\\\\\n3018 & sounds & 13867 \\\\\\\\\\n4018 & boards & 9811 \\\\\\\\\\n5018 & rage & 7385 \\\\\\\\\\n6019 & occupied & 5813 \\\\\\\\\\n7020 & continually & 4650 \\\\\\\\\\n8020 & delay & 3835 \\\\\\\\\\n9021 & delayed & 3233 \\\\\\\\\\n10021 & glances & 2767 \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c}\\nRank & Word & Count \\\\\\\\\\n\\\\hline\\n10021 & glances & 2767 \\\\\\\\\\n20026 & ecclesiastical & 881 \\\\\\\\\\n30028 & zero-sum & 405 \\\\\\\\\\n40029 & excluded & 218 \\\\\\\\\\n50030 & sympathizes & 124 \\\\\\\\\\n60034 & capon & 77 \\\\\\\\\\n70023 & fibs & 49 \\\\\\\\\\n80039 & conventionalized & 33 \\\\\\\\\\n90079 & grandmom & 23 \\\\\\\\\\n100033 & slum-dwellers & 17 \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nIt should be convincing that frequency of use drops rapidly with rank: recall that grandmom is only a slang form of grandma, not the real McCoy.\\n\\nWhy is this a power law? A word of rank $2 x$ has a frequency of $F_{2 x} \\\\sim$ $F_{1} / 2 x$, compared to $F_{x} \\\\sim F_{1} / x$. Thus halving the rank doubles the frequency, and this corresponds to the power law with $\\\\alpha=1$.\\\\\\\\\\nWhat is the mechanism behind the evolution of languages that lead to this distribution? A plausible explanation is that people learn and use words because they hear other people using them. Any mechanism that favors the already popular leads to a power law.\\n\\n\\\\begin{itemize}\\n  \\\\item Frequency of earthquakes of magnitude x: The Richter scale\\\\index{Richter scale} for measuring the strength of earthquakes is logarithmic, meaning a 5.3 quake is ten times stronger than a 4.3 scale event. Adding one to the magnitude multiplies the strength by a factor of ten.\\\\\\\\\\nWith such a rapidly increasing scale it makes sense that bigger events are rarer than smaller ones. I cause a 0.02 magnitude quake every time I flush a toilet. There are indeed billions of such events each day, but larger quakes get increasingly rare with size. Whenever a quantity grows in a potentially unbounded manner but the likelihood it does diminishes exponentially, you get a power law. Data shows this is as true of the energy released by earthquakes as it is with the casualties of wars: mercifully the number of conflicts which kill $x$ people decreases as a power law.\\n\\\\end{itemize}\\n\\nLearn to keep your eyes open for power law distributions. You will find them everywhere in our unjust world. They are revealed by the following properties:\\n\\n\\\\begin{itemize}\\n  \\\\item Power laws show as straight lines on log value, log frequency plots: Check out the graph of city populations in Figure 5.7(right). Although there are some gaps at the edges where data gets scarce, by and large the points lie\\\\\\\\\\nneatly on a line. This is the main characteristic of a power law. By the way, the slope of this line is determined by $\\\\alpha$, the constant defining the shape of the power law distribution.\\n  \\\\item The mean does not make sense: Bill Gates alone adds about $\\\\$ 250$ to the wealth of the average person in the United States. This is weird. Under a power law distribution there is a very small but non-zero probability that someone will have infinite wealth, so what does this do to the mean? The median does a much better job of capturing the bulk of such distributions than the observed mean.\\n  \\\\item The standard deviation does not make sense: In a power law distribution, the stand\\\\index{gradient descent search}ard deviation is typically as large or larger th\\\\index{derivative\\\\index{second}}an the mean. This means that the distribution is very poorly characterized by $\\\\mu$ and $\\\\sigma$, while the power law provides a very good description in terms of $\\\\alpha$ and $c$.\\n  \\\\item The distribution is scale invariant: Suppose we plotted the populations of the 300th through 600th largest U.S. cities, instead of the top 300 as in Figure 5.7 (left). The shape would look very much the same, with the population of the 300th largest city towering over the tail. Any exponential function is scale invariant, because it looks the same at any resolution. This is a consequence of it being a straight line on a log-log plot: any subrange is a straight line segment, which has the same parameters in its window as the full distribution.\\n\\\\end{itemize}\\n\\nTake-Home Lesson: Be on the lookout for power law distributions. They reflect the inequalities of the world, which means that they are everywhere.\\n\\n\\\\subsection*{5.2 Sampling from Distributions}\\nSampling points from a given probability distribution is a common operation, one which it is pays to know how to do. Perhaps you need test data from a power law distribution to run a simulation, or to verify that your program operates under extreme conditions. Testing whether your data in fact fits a particular distribution requires something to compare it against, and that should generally be properly-generated synthetic data drawn from the canonical distribution.\\n\\nThere is a general technique for sampling from any given probability distribution, called inverse transform sampling. Recall that we can move between the probability density function $P$ and the cumulative density function $C$ by integration and differentiation. We can move back and forth between them because:\\n\\n$$\\nP(k=X)=C^{\\\\prime}(k)=C(X \\\\leq k+\\\\delta)-C(X \\\\leq k), \\\\text { and }\\n$$\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-149}\\n\\\\end{center}\\n\\nFigure 5.8: The inverse transform sampling method enables us to convert a random number generated uniformly from $[0,1]$ (here 0.729 ) to a random sample drawn from any distribution, given its cdf.\\n\\n$$\\nC(X \\\\leq k)=\\\\int_{x=-\\\\infty}^{k} P(X=x)\\n$$\\n\\nSuppose I want to sample a point from this possibly very complicated distribution. I can use a uniform random number generator to select a value $p$ in the interval $[0, \\\\ldots, 1]$. We can interpret $p$ as a probability, and use it as an index on the cumulative distribution $C$. Precisely, we report the exact value of $x$ such that $C(X \\\\leq x)=p$.\\n\\nFigure 5.8 illustrates the approach, here sampling from the normal distribution. Suppose $p=0.729$ is the random number selected from our uniform generator. We return the $x$ value such that $y=0.729$, so $x=0.62$ as per this cdf.\\n\\nIf you are working with a popular probability distribution in a well-supported language like Python, there is almost certainly a library function to generate random samples already available. So look for the right library before you write your own.\\n\\n\\\\subsection*{5.2.1 Random Sampling beyond One Dimension}\\nCorrectly sampling from a given distribution becomes a very subtle problem once you increase the number of dimensions. Consider the task of sampling points uniformly from within a circle. Think for a moment about how you might do this before we proceed.\\\\\\n%---- Page End Break Here ---- Page : 133\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-150}\\n\\nFigure 5.9: Randomly generating 10,000 points by angle-radius pairs clearly oversamples near the origin of the circle (left). In contrast, Monte Carlo sampling generates points uniformly within the circle (right).\\n\\nThe clever among you may hit upon the idea of sampling the angle and distance from the center independently. The angle that any sampled point must make with respect to the origin and positive $x$-axis varies between 0 and $2 \\\\pi$. The distance from the origin must be a value between 0 and $r$. Select these coordinates uniformly at random and you have a random point in the circle.\\n\\nThis method is clever, but wrong. Sure, any point so created must lie within the circle. But the points are not selected with uniform frequency. This method will generate points where half of them will lie within a distance of at most $r / 2$ from the center. But most of the area of the circle is farther from the center than that! Thus we will oversample near the origin, at the expense of the mass near the boundary. This is shown by Figure 5.9 (left), a plot of 10,000 points generated using this method.\\n\\nA dumb technique that proves correct is Monte Carlo sampling. The $x$ and $y$ coordinates of every point in the circle range from $-r$ to $r$, as do many points outside the circle. Thus sampling these values uniformly at random gives us a point which lies in a bounding box of the circle, but not always within the circle itself. This can be easily tested: is the distance from $(x, y)$ to the origin at most $r$, i.e. is $\\\\sqrt{x^{2}+y^{2}} \\\\leq r$ ? If yes, we have found a random point in the circle. If not, we toss it out and try again. Figure 5.9 (right) plots 10,000 points constructed using this method: see how uniformly they cover the circle, without any obvious places of over- or under-sampling.\\n\\nThe efficiency here depends entirely upon the ratio of the desired region volume (the area of the circle) to the volume of the bounding box (the area of a square). Since $78.5 \\\\%$ of this bounded box is occupied by the circle, less than two trials on average suffice to find each new circle point.\\n\\n%---- Page End Break Here ---- Page : 134\\n\\nTotal revenue generated by arcades\\\\\\\\\\ncorrelates with\\\\\\\\\\nComputer science doctorates awarded in the US\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-151}\\n\\nFigure 5.10: Correlation vs. causation: the number of Computer Science Ph.Ds awarded each year in the United States strongly correlates with video/pinball arcade revenue. (from Vig15)\\n\\n\\\\subsection*{5.3 Statistical Significance}\\nStatisticians are largely concerned with whether observations on data are significant. Computational analysis will readily find a host of patterns and correlations in any interesting data set. But does a particular correlation reflect a real phenomena, as opposed to just chance? In other words, when is an observation really significant?\\n\\nSufficiently st\\\\index{classiï¬cation}rong correlations on large data sets may seem to be \"obviously\" meaningful, but the issues are often quite subtle. For one thing, correlation does not imply causation. Figure 5.10 convincingly demonstrates that the volume of advanced study in computer science correlates with how much video games are being played. I\\'d like to think I have driven more people to algorithms than Nintendo, but maybe this is just the same thing? The graphs of such spurious correlations literally fill a book Vig15, and a very funny one at that.\\n\\nThe discipline of statistics comes into its own i\\\\index{correlation and causation}n making su\\\\index{statistical signiï¬cance}btle distinctions about whether an observation is meaningful or not. The classical example comes from medical statistics, in determining the efficacy of drug treatments. A pharmaceutical company conducts an experiment comparing two drugs. Drug $A$ cured 19 of 34 patients. Drug $B$ cured 14 of 21 patients. Is drug $B$ really better than drug $A$ ? FDA approval of new drugs can add or subtract billions from the value of drug companies. But can you be sure that a new drug represents a real improvement? How do you tell?\\n\\n\\\\subsection*{5.3.1 The Significance of Significance}\\nStatistical significance measures our confidence that there is a genuine difference between two given distributions. This is important. But statistical significance does not measure the importance or magnitude of this difference. For large\\\\\\n%---- Page End Break Here ---- Page : 135\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-152}\\n\\nFigure 5.11: Pairs of normal distributions with the same variance, but decreasing difference in their means from left to right. As the means get closer, the greater overlap between the distributions makes it harder to tell them apart.\\\\\\\\\\nenough sample sizes, extremely small differences can register as highly significant on statistical tests.\\n\\nFor example, suppose I get suckered into betting tails on a coin which comes up heads $51 \\\\%$ of the time, instead of the $50 \\\\%$ we associate with a fair coin. After 100 tosses of a fair coin, I would expect to see $51 \\\\%$ or more heads $46.02 \\\\%$ of the time, so I have absolutely no grounds for complaint when I do. After 1,000 tosses, the probability of seeing at least 510 heads falls to 0.274 . By 10,000 tosses, the probability of seeing so many heads is only 0.0233 , and I should start to become suspicious of whether the coin is fair. After 100,000 tosses, the probability of fairness will be down to $1.29 \\\\times 10^{-10}$, so small that I must issue a formal complaint, even if I thought my opponent to be a gentleman.\\n\\nBut here is the thing. Although it is now crystal clear that I had been tricked into using a biased coin, the consequences of this act are not substantial. For almost any issue in life worth flipping over, I would be willing to take the short side of the coin, because t\\\\index{eï¬ect size}he stake\\\\index{Cohenâ\\\\index{Pearson correlation coeï¬cient}s d}s are just not high enough. At $\\\\$ 1$ bet per flip, my expected loss even after 100,000 tosses would only be $\\\\$ 1,000$ bucks.\\n\\nSignificance tells you how unlikely it is that something is due to chance, but not whether it is important. We really care about effect size, the magnitude of difference between the two groups. We informally categorize a medium-level effect size as visible to the naked eye by a careful observer. On this scale, large effects pop out, and small effects are not completely trivial [SF12]. There are several statistics which try to measure the effect size, including:\\n\\n\\\\begin{itemize}\\n  \\\\item Cohen\\'s $d$ : The importance of the difference between two means $\\\\mu$ and $\\\\mu^{\\\\prime}$ depends on the absolute magnitude of the change, but also the natural variation of the distributions as measured by $\\\\sigma$ or $\\\\sigma^{\\\\prime}$. This effect size can be measured by:\\n\\\\end{itemize}\\n\\n$$\\nd=\\\\left(\\\\left|\\\\mu-\\\\mu^{\\\\prime}\\\\right|\\\\right) / \\\\sigma\\n$$\\n\\nA reasonable threshold for a small effect size is $>0.2$, medium effect $>0.5$, and large effect size $>0.8$.\\n\\n\\\\begin{itemize}\\n  \\\\item Pearson\\'s correlation coefficient $r$ : Measures the degree of linear relationship between two variables, on a scale from -1 to 1 . The thresholds for effect sizes are comparable to the mean shift: small effects start at $\\\\pm \\n%---- Page End Break Here ---- Page : 136\\n0.2$,\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-153}\\n\\\\end{itemize}\\n\\nFigure 5.12: Pairs of normal distributions with the same difference in their mean\\\\index{mean}s but increasing variance, from left to right. As the variance increases, there becomes greater overlap between the distributions, making it harder to tell them apart.\\\\\\\\\\nmedium effects about $\\\\pm 0.5$, and large effect sizes require correlations of $\\\\pm 0.8$.\\n\\n\\\\begin{itemize}\\n  \\\\item The coefficient of variation\\\\index{binary} $r^{2}$ : \\\\index{for classiï¬cation}The square of the correlation coefficient reflects the proportion of the variance in one variable that is explained by the other. The thresholds follow from squaring those above. Small effects explain at least $4 \\\\%$ of the variance, medium effects $\\\\geq 25 \\\\%$ and large effect sizes at least $64 \\\\%$.\\n  \\\\item Percentage of overlap: The area under any single probability distribution is, by definition, 1. The area of intersection between two given distributions is a good measure of their similarity, as shown in Figure 5.11 Identical distributions overlap $100 \\\\%$, while disjoint intervals overlap $0 \\\\%$. Reasonable thresholds are: for small effects $53 \\\\%$ overlap, medium effect\\\\index{overlap percentage}s $67 \\\\\\\\index{variation coeï¬cient}%$ overlap, and \\\\index{AB testing}large effect sizes $85 \\\\%$ overlap.\\n\\\\end{itemize}\\n\\nOf course, any sizable effect which is not statistically significant is inherently suspect. The CS study vs. video game play correlation in Figure 5.10 was so high ( $r=0.985$ ) that the effect size would be huge, were the number of sample points and methodology sound enough to support the conclusion.\\n\\nTake-Home Lesson: Statistical significance depends upon the number of samples, while the effect size does not.\\n\\n\\\\subsection*{5.3.2 The T-test\\\\index{T-test}: Comparing Population Means}\\nWe have seen that large mean shifts between two populations suggest large effect sizes. But how many measurements do we need before we can safely believe tha\\\\index{logistic}t the phenomenon is real. Suppose we measure the IQs of twenty men and twenty women. Does the data show \\\\index{logit function}that one group is smarter, on average? Certainly the sample means will differ, at least a bit, but is this difference significant?\\n\\nThe t-test evaluates whether the population means of two samples are different. This problem commonly arises in $A B$ testing, associated with evaluating\\\\\\n%---- Page End Break Here ---- Page : 137\\n\\\\\\nwhether a product change makes a difference in performance. Suppose you show one group of users version A, and another group version B. Further, suppose you measure a system performance value for each user, such as the number of times they click on ads or the number of stars they give it when asked about the experience. The t-test measures whether the observed difference between the two groups is significant.\\n\\nTwo means differ significantly if:\\n\\n\\\\begin{itemize}\\n  \\\\item The mean difference is relatively large: This makes sense. One can conclude that men weigh more than women on average fairly easily, because the effect size is so large. According to the Center for Disease Contro ${ }^{2}$ the average American male weighed 195.5 pounds in 2010, whereas the average American woman weighted 166.2 pounds. This is huge. Proving that a much more subtle difference, like IQ, is real requires much more evidence to be equally convincing.\\n  \\\\item The standard deviation\\\\index{standard deviation}s are small enough: This also makes sense. It is easy to convince yourself that men and women have, on average, the same number of fingers because the counts we observe are very tightly bunched around the mean: $\\\\{10,10,10,10,9,10 \\\\ldots\\\\}$. The equal-finger count hypothesis would require much more evidence if the numbers jumped around a lot. I would be reluctant to commit to a true distributional average of $\\\\mu=10$ if I what I observed was $\\\\{3,15,6,14,17,5\\\\}$.\\n  \\\\item The number of samples are large enough: This again makes sense. The more data I see, the more solidly I become convinced that the sample will accurately represent its underlying distribution. For example, men undoubtedly have fewer fingers on average than women, as a consequence of more adventures with power tools $\\\\sqrt[\\\\index{Welchâs t-statistic}3]{3}$ But it would require a very large number of samples to observe and validate this relatively rare phenomenon.\\n\\\\end{itemize}\\n\\nThe t-test starts by computing a test statistic\\\\index{test statistic} on the two sets of observations. Welch\\'s t-statistic is defined as\\n\\n$$\\nt=\\\\frac{\\\\bar{x}_{1}-\\\\bar{x}_{2}}{\\\\sqrt{\\\\frac{\\\\sigma_{1}{ }^{2}}{n_{1}}+\\\\frac{\\\\sigma_{2}{ }^{2}}{n_{2}}}}\\n$$\\n\\nwhere $\\\\bar{x}_{i}, \\\\sigma_{i}$, and $n_{i}$ are the mean, standard deviation, and population size of sample $i$, respectively.\\n\\nLet us parse this equation carefully. The numerator is the difference between the means, so the bigger this difference, the bigger the value of the t-statistic. The standard deviations are in the denominator, so the smaller that $\\\\sigma_{i}$ is, the bigger the value of the t-statistic. If this is confusing, recall what happens when you divide $x$ by a number approaching zero. Increasing the sample sizes $n_{i}$\\n\\n\\\\footnotetext{$2^{2}$ \\\\href{http://www.cdc.gov/nchs/fastats/obesity-overweight.htm}{http://www.cdc.gov/nchs/fastats/obesity-overweight.htm}\\\\\\\\\\n${ }^{3}$ This observation alone may be sufficient to resolve the gender-IQ relationship, without the need for additional statistical evidence.\\n\\n%---- Page End Break Here ---- Page : 138\\n}\\nalso makes the denominator smaller, so the larger $n_{i}$ is, the bigger the value of the t-statistic. In all cases, the factors that make us more confident in there being a real difference between the two distributions increases the value of the t-statistic.\\n\\nInterpreting the meaning of a particular value of the t -statistic comes from looking up a number in an appropriate table. For a desired significance level $\\\\alpha$ and number of degrees of freedom (essentially the sample sizes), the table entry specifies the value $v$ that the t-statistic $t$ must exceed. If $t>v$, then the observation is significant to the $\\\\alpha$ level.\\n\\n\\\\section*{Why Does This Work?}\\nStatistical tests like the t-test often seem like voodoo to me, because we look up a number from some magic table and treat it like gospel. The oracle has spoken: the difference is significant! Of course there is real mathematics behind significance testing, but the derivation involves calculus and strange functions (like the gamma function $\\\\Gamma(n)$, a real numbered generalization of factorials). These complex calculations are\\\\index{stochastic gradient descent} why the convention arose to look things up in a precomputed table, instead of computing it yourself.\\n\\nYou can find derivations of the relevant formulae in any good statistics book, if you are interested. These tests are based on ideas like random sampling. We have seen how the mean and standard deviation constrain the shap\\\\index{overï¬tting}e of any underlying probability distribution. Getting a sample average very far from the mean implies bad luck. Randomly picking values several standard deviations away from the population mean is very unlikely, according to the theory. This makes it more likely that observing such a large difference is the result of drawing from a different distribution.\\n\\nMuch of the technicality here is a consequence of dealing with subtle phenomenon and small data sets. Historically, observed data was a very scarce resource, and it remains so in many situations. Recall our discussion of drug efficacy testing, where someone new must die for every single point we collect. The big data world you will likely inhabit generally features more observations (everybody visiting our webpage), lower stakes (do customers buy more when you show them a green background instead of a blue background?), and perhaps smaller effect sizes (how big an improvement do we really need to justify changing the background color?).\\n\\n\\\\subsection*{5.3.3 The Kolmogorov-Smirnov test\\\\index{Kolmogorov-Smirnov test}}\\nThe t-test compares two samples drawn from presumably normal distributions according to the distance between their respective means. Instead, the Kolmogorov-Smirnov (KS) test compares the cumulative distribution functions (cdfs) of the two sample distributions and assesses how similar they are.\\n\\nThis is illustrated in Figure 5.13 The cdfs of the two different samples are plotted on the same chart. If the two samples are drawn from the same distribution, the ranges of $x$ values should largely overlap. Further, since both\\\\\\n%---- Page End Break Here ---- Page : 139\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-156}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-156(1)}\\n\\nFigure 5.13: The Kolmogorov-Smirnov test quantifies the difference between two probability distributions by the maximum $y$-distance gap between the two cumulative distribution functions. On the left, two samples from the same normal distribution\\\\index{normal distribution}. On the right, comparison of samples from uniform and normal distributions drawn over the same $x$-range.\\\\\\\\\\ndistributions are represented as cdfs, the $y$-axis represents cumulative probability from 0 to 1 . Both functions increase monotonically from left to right, where $C(x)$ is the fraction of the sample $\\\\leq x$.\\n\\nWe seek to identify the value of $x$ for which the associated $y$ values of the two cdfs differ by as much as possible. The distance $D\\\\left(C_{1}, C_{2}\\\\right)$ between the distributions $C_{1}$ and $C_{2}$ is the difference of the $y$ values at this critical $x$, formally stated as\\n\\n$$\\nD\\\\left(C_{1}, C_{2}\\\\right)=\\\\max _{-\\\\infty \\\\leq x \\\\leq \\\\infty}\\\\left|C_{1}(x)-C_{2}(x)\\\\right|\\n$$\\n\\nThe more substantially that two sample distributions differ at some value, the more likely it is that they were drawn from different distributions. Figure 5.13 (left) shows two independent samples from the same normal distribution. Note the resulting tiny gap between them. In contrast, Figure 5.13 (right) compares a sample drawn from a normal distribution against one drawn from the uniform distribution. The KS-test is not fooled: observe the big gaps near the tails, where we would expect to see it.\\n\\nThe KS-test compares the value of $D\\\\left(C_{1}, C_{2}\\\\right)$ against a particular target, declaring that two distributions differ at the significance level of $\\\\alpha$ when:\\n\\n$$\\nD\\\\left(C_{1}, C_{2}\\\\right)>c(\\\\alpha) \\\\sqrt{\\\\frac{n_{1}+n_{2}}{n_{1} n_{2}}}\\n$$\\n\\nwhere $c(\\\\alpha)$ is a constant to look up in a table.\\\\\\\\\\nThe function of the sample sizes has some intuition behind it. Assume for simplicity that both samples have the same size, $n$. Then\\n\\n$$\\n\\\\sqrt{\\\\frac{n_{1}+n_{2}}{n_{1} n_{2}}}=\\\\sqrt{\\\\frac{2 n}{n^{2}}}=\\\\sqrt{\\\\frac{2}{n}}\\n$$\\n\\nThe quantity $\\\\sqrt{n}$ arises naturally in sampling problems, such as the standard deviation of the binomial distribution. The expected difference between the number of heads and tails in $n$ coin flips is on the order of $\\\\sqrt{n}$. In the context of the KS-test, it similarly reflects the expected deviation when two samples should be considered the same. The KS-test reflects what is happening in the meat of the distribution, where a robust determination can be made.\\n\\nI like the Kolmogorov-Smirnov test. It provides pictures of the distributions that I can understand, that identify the weakest point in the assumption that they are identical. This test has fewer technical assumptions and variants than the t-test, meaning we are less likely to make a mistake using it. And the KS-test can be applied to many problems, including testing whether points are drawn from a normal distribution.\\n\\n\\\\section*{normality testing\\\\index{normality testing}}\\nWhen plotted, the normal distribution yields a bell-shaped\\\\index{bell-shaped} curve. But not every bell-shaped distribution is normal, and it is sometimes important to know the difference.\\n\\nThere exist specialized statistical tests for testing the normality of a given distributional samples $f_{1}$. But we can use the general KS-test to do the job, provided we can identify a meaningful $f_{2}$ to compare $f_{1}$ to.\\n\\nThis is exactly why I introduced random sampling methods in Section 5.2 Using the cumulative distribution method we described, statistically-sound random samples of $n$ points can be drawn from any distribution that you know the cdf of, for any $n$. For $f_{2}$, we should pick a meaningful number of points to compare against. We can use $n_{2}=n_{1}$, or perhaps a somewhat larger sample if $n_{1}$ is very small. We want to be sure that we are capturing the shape of the desired distribution with our sample.\\n\\nSo if we construct our random sample for $f_{2}$ from the normal distribution, the KS-test should not be able to distinguish $f_{1}$ from $f_{2}$ if $f_{1}$ also comes from a normal distribution on the same $\\\\mu$ and $\\\\sigma$.\\n\\nOne word of caution. A sufficiently-sensitive statistical test will probably reject the normality of just about any observed distribution. The normal distribution is an abstraction, and the world is a complicated place. But looking at the plot of the KS-test shows you exactly where the deviations are happening. Are the tails too fat or too lean? Is the distribution skewed? With this understanding, you can decide whether the differences are big enough to matter to you.\\n\\n\\\\subsection*{5.3.4 The Bonferroni correction\\\\index{Bonferroni correction}}\\nIt has long been the convention in science to use $\\\\alpha=0.05$ as the cutoff between statistical significance and irrelevance. A statistical significance of 0.05 means there is a probability of $1 / 20$ that this result would have come about purely by chance.\\n\\n%---- Page End Break Here ---- Page : 141\\n\\nThis is not an unreasonable standard when collecting data to test a challenging hypothesis. Betting on a horse at 20 to 1 odds and winning your bet is a noteworthy accomplishment. Unless you simultaneously placed bets on millions of other horses. Then bragging about the small number of $20-1$ bets where you actually won would be misleading, to say the least.\\n\\nThus fishing expeditions which test millions of hypotheses must be held to higher standards. This is the fallacy that created the strong but spurious correlation between computer science Ph.Ds and video game activity in Figure 5.10. It was discovered in the course of comparing thousands of time series against each other, and retaining only the most amusing-sounding pairs which happened to show a high correlation score.\\n\\nThe Bonferroni correction ${ }^{4}$ provides an important balance in weighing how much we trust an apparently significant statistical result. It speaks to the fact that how you found the correlation can be as important as the strength of the correlation itself. Someone who buys a million lottery tickets and wins once has much less impressive mojo going than the fellow who buys a single ticket and wins.\\n\\nThe Bonferroni correction states that when testing $n$ different hypotheses simultaneously, the resulting $p$-value must rise to a level of $\\\\alpha / n$, in order to be considered as significant at the $\\\\alpha$ level.\\n\\nAs with any statistical test, there lurk many subtleties in properly applying the correction. But the big principle here is important to grasp. Computing people are particularly prone to running large-scale comparisons of all things against all things, or hunting for unusual outliers and patterns. After all, once you\\'ve written the analysis program, why not run it on all your data? Presenting only the best, cherry-picked results makes it easy to fool other people. The Bonferroni correction is the way to prevent you from fooling yourself.\\n\\n\\\\subsection*{5.3.5 False Discovery Rate}\\nThe Bonferroni correction safeguards us from being too quick to accept the significance of a lone successful hypothesis among many trials. But often when working with large, high-dimensional data we are faced with a different problem. Perhaps all $m$ of the variables correlate (perhaps weakly) with the target variable. If $n$ is large enough, many of these correlations will be statistically significant. Have we really made so many important discoveries?\\n\\nThe Benjamini-Hochberg procedure for minimizing false discovery rate (FDR) procedure gives a very simple way to draw the cutoff between interesting and uninteresting variables based on significance. Sort the variables by the strength of their $p$-value, so the more extreme variables lie on the left, and the least significant variables on the right. Now consider the $i$ th ranked variable in this ordering. We accept the significance of this variable at the $\\\\alpha$ level if\\n\\n$$\\n\\\\forall_{j=1}^{i}\\\\left(p_{j} \\\\leq \\\\frac{j}{m} \\\\alpha\\\\right)\\n$$\\n\\n\\\\footnotetext{${ }^{4}$ I have always thought that \"The Bonferroni Correction\" would make a fabulous title for an action movie. Dwayne Johnson as Bonferroni?\\n\\n%---- Page End Break Here ---- Page : 142\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-159}\\n\\nFigure 5.14: The Benjamini-Hochberg procedure minimizes false discovery rate, by accepting $p$-values only when $p_{i} \\\\leq \\\\alpha i / m$. The blue curve shows the sorted $p$ values, and the diagonal defines the cutoff for when such a $p$-value is significant.\\n\\nThe situation is illustrated in Figure 5.14 The $p$-values are sorted in increasing order from left to right, as denoted by the irregular blue curve. If we were accepting all $p$-values less than $\\\\alpha$, we accept too many. This is why Bonferroni developed his correction. B\\\\inde\\\\index{e\\\\index{manhat\\\\index{maximum component}tan distance}uclidean}x{Lk}ut requiring all $p$-values to meet the standard of the Bonferroni correction (where the curve crosses $\\\\alpha / m$ ) is too stringent.\\n\\nThe Benjamini-Hochberg procedure recognizes that if many values are really significant to a certain standard, a certain fraction of them should be significant to a much higher standard. The diagonal line in Figure 5.14 appropriately enforces this level of quality control.\\n\\n\\\\subsection*{5.4 War Story: Discovering the Fountain of Youth?}\\nIt had been a beautiful wedding. We were very happy for Rachel and David, the bride and groom. I had eaten like a king, danced with my lovely wife, and was enjoying a warm post-prime rib glow when it hit me that something was off. I looked around the room and did a double-take. Somehow, for the first time in many years, I had become younger than most of the people in the crowd.\\n\\nThis may not seem like a big deal to you, but that is because you the reader probably are younger than most people in many settings. But trust me, there will come a time when you notice such things. I remember when I first realized that I had been attending college at the time when most of my students were being born. Then they started being born when I was in graduate school. Today\\'s college students were not only born after I became a professor, but after I got tenure here. So how could I be younger than most of the people at this wedding?\\n\\nThere were two possibilities. Either it was by chance that so many older\\\\\\n%---- Page End Break Here ---- Page : 143\\n\\\\\\npeople entered the room, or there was a reason explaining this phenomenon, This is why statistical significance tests\\\\index{tests} and $p$-values were invented, to aid in distinguishing something from nothing.\\n\\nSo what was the probability that I, then at age 54, would have been younger than most of the 251 people at Rachel\\'s wedding? According to Wolfram Alpha\\\\index{Wolfram Alpha} (more precisely, the 2008-2012 American Community Survey five-year estimates), there were 309.1 million people in the United States, of whom 77.1 million were age 55 or older. Almost exactly $25 \\\\%$ of the population is older than I am as I write these words.\\n\\nThe probability that the majority of 251 randomly selected Americans would be older than 55 years is thus given by:\\n\\n$$\\np=\\\\sum_{i=126}^{251}\\\\binom{251}{i}(1-.75)^{i}(0.75)^{(251-i)}=8.98 \\\\times 10^{-18}\\n$$\\n\\nThis probability is impossibly small, comparable to pulling a fair coin out of your pocket and having it come up heads 56 times in a row. This could not have been the result of a chance event. There had to be a reason why I was junior to most of this crowd, and the answer wasn\\'t that I was getting any younger.\\n\\nWhen I asked Rachel about it, she mentioned that, for budgetary reasons, they decided against inviting children to the wedding. This seemed like it could be a reasonable explanation. After all, this rule excluded 73.9 million people unde\\\\index{higher dimensions}r the age of eighteen from attending the wedding, thus saving billions of dollars over what \\\\index{monotonic}it would have cost to invite them all. The fraction $f$ of people younger than me who are not children works out to $f=1-(77.1 /(309.1-$ $73.9))=0.672$. This is substantially larger than 0.5 , however. The probability of my being younger than the median in a random sample drawn from this cohort is:\\n\\n$$\\np=\\\\sum_{i=126}^{251}\\\\binom{251}{i}(1-.0672)^{i}(0.672)^{(251-i)}=9.118 \\\\times 10^{-9}\\n$$\\n\\nAlthough this is much larger than the previous $p$-value, it is still impossibly small: akin to tossing off 27 straight heads on your fair coin. Just forbidding children was not nearly powerful enough to make me young again.\\n\\nI went back to Rachel and made her fess up. It turns out her mother had an unusually large number of cousins growing up, and she was exceptionally good at keeping in touch with all of them. Recall Einstein\\'s theory of relativity\\\\index{theory of relativity}, where $E=m c^{2}$ denotes that everyone is my mother\\'s cousin, twice removed. All of these cousins were invited to the wedding. With Rachel\\'s family outpopulating the groom\\'s unusually tiny clan, this cohort of senior cousins came to dominate the dance floor.\\n\\nIndeed, we can compute the number of the older-cousins (c) that must be invited to yield a $50 / 50$ chance that I would be younger than the median guest, assuming the rest of the 251 guests were selected at random. It turns out that\\\\\\n%---- Page End Break Here ---- Page : 144\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-161}\\n\\nFigure 5.15: Permutation tests reveal the significance of the correlation between gender and the height (left). A random assignment of gender to height (center) results in a substantially different distribution of outcomes when sorted (right), validating the significance of the original relationship.\\\\\\\\\\n$c=65$ single cousins (or 32.5 married pairs) suffice, once the children have been excluded ( $f=0.672$ ).\\n\\nThe moral here is that it is important to compute the probability of any interesting observation before declaring it to be a miracle. Never stop with a partial explanation, if it does not reduce the surprise to plausible levels. There likely is a genuine phenomenon underlying any sufficiently rare event, and ferreting out what it is makes data science exciting.\\n\\n\\\\subsection*{5.5 Permutation Tests and $P$-values}\\nTraditional statistical significance tests prove quite effective at deciding whether two samples are in fact drawn from the same distribution. However, these tests must be properly performed in order to do their job. Many standard tests have subtleties like the issues of one- vs. two-sided tests, distributional assumptions, and more. Performing these tests correctly requires care and training.\\n\\nPermutation tests allow a more general and computationally idiot-proof way to establish significance. If your hypothesis is supported by the data, then\\\\\\n%---- Page End Break Here ---- Page : 145\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-162(1)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-162}\\n\\nFigure 5.16: Permutation tests score significance by the position of the score on actual data against a distribution of scores produced by random permutations. A position on the extreme tail (left) is highly significant, but one within the body of the distribution (left) is uninteresting.\\\\\\\\\\nrandomly shuffled data sets should be less likely to support it. By conducting many trials against randomized data, we can establish exactly how unusual the phenomenon is that you are testing for.\\n\\nConsider Figure 5.15, where we denote the independent variable (gender: male or female) and dependent variable (say, height) using colors. The original outcome color distribution (left) looks starkly different between men and women, reflective of the genuine differences in height. But how unusual is this difference? We can construct a new data set by randomly assigning gender to the original outcome variables (center). Sorting within each group makes clear that the pseudo-male/female distribution of outcomes is now much more balanced than in the original data (right). This demonstrates that gender was indeed a significant factor in determining height, a conclusion we would come to believe even more strongly after it happens again and again over 1,000 or $1,000,000$ trials.\\n\\nThe rank of the test statistic on the real data among the distribution of statistic values from random permutations determines the significance level or $p$-value. Figure 5.16 (left) shows what we are looking for. The real value lies on the very right of the distribution, attesting to significant. In the figure on right, the real value lies in the squishy middle of the distribution, suggesting no effect.\\n\\nPermutation tests require that you develop a statistic which reflects your hypothesis about the data. The correlation coefficient is a reasonable choice if you want to establish an important relationship between a specific pair of variables. Ideally the observed correlation in the real data will be stronger than in any random permutation of it. To validate the gender-height connection, perhaps our statistic could be the difference in the average heights of men and women. Again, we hope this proves larger in the real data than in most random permutations of it.\\n\\nBe creative in your choice of statistic: the power of permutation tests is\\\\\\\\\\nthat they can work with pretty much anything you can come up with to prove your case. It is best if your statistic minimizes the chance of ties, since you are honor-bound to count all ties against your hypothesis.\\n\\nTake-Home Lesson: Permutation tests give you the probability of your data given your hypothesis, namely that the statistic will be an outlier compared to the random sample distribution. This is not quite the same as proving your hypothesis given the data, which is the traditional goal of statistical significance testing. But it is much better than nothing.\\n\\nThe significance score or $p$-value of a permutation test depends upon how many random tries are run. Always try to do at least 1,000 random trials, and more if it is feasible. The more permutations you try, the more impressive your significance $p$-value can be, at least up to a point. If the given input is in fact the best of all $k$ ! permutations, the most extreme $p$-value you can get is $1 / k$ !, no matter how many random permutations you try.\\\\index{randomly generating} Oversampling will inflate your denominator without increasing your true confidence one bit.\\n\\nTake-Home Lesson: p-values\\\\index{p-values} are computed to increase your confidence that an observation is real and interesting. This only works when you do the permutation test honestly, by performing experiments that can provide a fair measure of surprise.\\n\\n\\\\subsection*{5.5.1 Generating Random Permutations}\\nGenerating random permutations is another impo\\\\index{analogies}rtant sampling problem that people often botch up. The two algorithms below both use sequences of random swaps to scramble up the initial permutation $\\\\{1,2, \\\\ldots, n\\\\}$.\\n\\nBut ensuring that all $n$ ! permutations are generated uniformly at random is a tricky business. Indeed only one of these algorithms gets it right. Is it this one,\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\text { for } i=1 \\\\text { to } n \\\\text { do } a[i]=i ; \\\\\\\\\\n& \\\\text { for } i=1 \\\\text { to } n-1 \\\\text { do } \\\\operatorname{swap}[a[i], a[\\\\operatorname{Random}[i, n]] ;\\n\\\\end{aligned}\\n$$\\n\\nor is it this:\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\text { for } i=1 \\\\text { to } n \\\\text { do } a[i]=i ; \\\\\\\\\\n& \\\\text { for } i=1 \\\\text { to } n-1 \\\\text { do } \\\\operatorname{swap}[a[i], a[\\\\operatorname{Random}[1, n]] ;\\n\\\\end{aligned}\\n$$\\n\\nThink about this carefully: the difference here is very subtle. It is so subtle you might not even notice it in the code. The critical difference is the 1 or $i$ in the call to Random. One of these algorithms is right and one of these algorithms is wrong. If you think you can tell, convincingly explain why one works and the other one doesn\\'t.\\n\\nIf you really must know, the first algorithm is the correct one. It picks a random element from 1 to $n$ for the first position, then leaves it alone and\\\\\\n%---- Page End Break Here ---- Page : 147\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-164}\\n\\nFigure 5.17: The generation frequency of all $4!=24$ permutations using two different algorithms. Algorithm 1 generates them with uniform frequency, while algorithm 2 is substantially biased.\\\\\\\\\\nrecurs on the rest. It generates permutations uniformly at random. The second algorithm gives certain elements a better chance to end up first, showing that the distribution is not uniform.\\n\\nBut if you can\\'t prove this theoretically, you can use the idea of a permutation test. Implement both of the algorithms, and perform 1,000,000 runs of each, constructing random permutations of, say, $n=4$ elements. Count how often each algorithm generates each one of the $4!=24$ distinct permutations. The results of such an experiment are shown in Figure 5.17 Algorithm 1 proves incredibly steady, with a standard deviation of 166.1 occurrences. In contrast, there is an eight-fold difference between the most and least frequent permutations under algorithm 2 , with $\\\\sigma=20,923.9$.\\n\\nThe moral here is that random generation can be very subtle. And that Monte Carlo-type experiments like permutation tests can eliminate the need for subtle reasoning. Verify, then trust.\\n\\n\\\\subsection*{5.5.2 DiMaggio\\'s Hitting Streak}\\nOne of baseball\\'s most amazing records is Joe DiMaggio\\'s 56-game hitting streak. The job of a batter is to get hits, and they receive perhaps four chances every game to get one. Even very good hitters often fail.\\n\\nBut back in 1941, Joe DiMaggio succeeded in getting hits in 56 straight games, a truly amazing accomplishment. No player in the seventy-five years since then has come close to this record, nor any one before him.\\n\\nBut how unusual was such a long streak in the context of his career? DiMag-\\\\\\n%---- Page End Break Here ---- Page : 148\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-165}\\n\\nFigure 5.18: The distribution of longest hitting streaks, over 100,000 simulated careers. DiMaggio\\'s actual 56 -game hitting streak stands at the very tail of this distribution, thus demonstrating the difficulty of this feat.\\\\\\\\\\ngio played 1736 games, with 2214 hits in 6821 at bats. Thus he should get hits in roughly $1-(1-(2214 / 6821))^{4}=79.2 \\\\%$ of his games with four at bats. What is the probability that someone with his skill level could manage such a consecutive game streak in the course of their career?\\n\\nFor those of you tired of my baseball analogies, let\\'s put this in another context. Suppose you are a student who averages a grade of 90 on tests. You are a very good student to be sure, but not perfect. What are the chances you could have a hot streak where you scored above 90 on ten straight tests? What about twenty straight? Could you possibly ace 56 tests in a row? ${ }^{5}$ If such a long streak happened, would that mean that you had taken your studies to another level, or did you just get lucky?\\n\\nSo when DiMaggio had his hitting streak, was it just an expected consequence of his undisputed skills and consistency, or did he just get lucky? He was one of the very best hitters of his or any time, an all-star every season of his thirteen-year career. But we also know that DiMaggio got lucky from time to time. After all, he was married to the movie star Marilyn Monroe.\\n\\nTo resolve this question, we used random numbers to simulate when he got hits over a synthetic \"career\" of 1736 games. Each game, the simulated Joe received four chances to hit, and succeeded with a probability of $p=$ $(2214 / 6821)=0.325$. We could then identify the longest hitting streak over the course of this simulated career. By simulating 100,000 DiMaggio careers, we get a frequency distribution of streaks that can put the rarity of his accomplishment in context, getting a $p$-value in the process.\\n\\nThe results are shown in Figure 5.18. In only 44 of 100,000 simulated careers ( $p=0.00044$ ) did DiMaggio manage a streak of at least 56 games. Thus the length is quite out of line with what would be expected from him. The second\\n\\n\\\\footnotetext{${ }^{5}$ Not if you are taking one of my classes, I tell you.\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-166}\\n\\nFigure 5.19: Bayes\\' Theorem in action.\\\\\\\\\\nlongest streak of any major league hitter is only 44 games, so it is out of line with everyone else as well. But he also once hit in 61 straight games at a lower level of competition, so he seems to have had an extraordinary capacity for consistency.\\n\\nHitting streaks can be thought of as runs between games without hits, and so can be modeled using a Poisson distribution. But Monte Carlo simulations provide answers without detailed mathematics. Permutation tests give us insight with minimal knowledge and intellectual effort.\\n\\n\\\\subsection*{5.6 Bayesian Reasoning}\\nThe conditional probability $P(A \\\\mid B)$ measures the likelihood of event $A$ given knowledge that event $B$ has occurred. We will rely on conditional probability throughout this book, because it lets us update our confidence in an event in response to fresh evidence, like observed data.\\n\\nBayes\\' Theorem is an important tool for working with conditional probabilities, because it lets us turn the conditionals around:\\n\\n$$\\nP(A \\\\mid B)=\\\\frac{P(B \\\\mid A) P(A)}{P(B)}\\n$$\\n\\nWith Bayes theorem, we can convert the question of $P$ (outcome|data) to $P($ data $\\\\mid$ outcome $)$, which is often much easier to compute. In some sense, Bayes\\' theorem is just a consequence of algebra, but it leads to a different way of thinking about probability.\\n\\nFigure 5.19 illustrates Bayes\\' theorem in action. The event space consists of picking one of four blocks. The complex events $A$ and $B$ represent sub-ranges of blocks, where $P(A)=3 / 4$ and $P(B)=2 / 4=1 / 2$. By counting blocks from the figure, we can see that $P(A \\\\mid B)=1 / 2$ and $P(B \\\\mid A)=1 / 3$. These also follow\\\\\\\\\\ndirectly from Bayes\\' theorem:\\n\\n$$\\n\\\\begin{aligned}\\n& P(A \\\\mid B)=\\\\frac{P(B \\\\mid A) P(A)}{P(B)}=\\\\frac{(1 / 3) \\\\cdot(3 / 4)}{(1 / 2)}=1 / 2 \\\\\\\\\\n& P(B \\\\mid A)=\\\\frac{P(A \\\\mid B) P(B)}{P(A)}=\\\\frac{(1 / 2) \\\\cdot(1 / 2)}{(3 / 4)}=1 / 3\\n\\\\end{aligned}\\n$$\\n\\nBayesian reasoning reflects how a prior probability $P(A)$ is updated to give the posterior probability $P(A \\\\mid B)$ in the face of a new observation $B$, according to the ratio of the likelihood $P(B \\\\mid A)$ and the marginal probability $P(B)$. The prior probability $P(A)$ reflects our initial assumption about the world, to be revised based on the additional evidence $B$.\\n\\nBayesian reasoning is an important way to look at the world. Walking into Rachel and David\\'s wedding, my prior assumption was that the age distribution would reflect that of the world at large. But my confidence weakened with every elderly cousin I encountered, until it finally crumbled.\\n\\nWe will use Bayesian reasoning to build classifiers in Section 11.1 But keep this philosophy in mind as you analyze data. You should come to each task with a prior conception of what the answers should be, and then revise in accordance with statistical evidence.\\n\\n\\\\subsection*{5.7 Chapter Notes}\\nEvery data scientist should take a good elementary statistics course. Representative texts include Freedman [FPP07] and James et al. [JWHT13. Wheelan Whe13 is a gentler introduction, with Huff Huf10 the classic treatise on how best to lie with statistics.\\n\\nDonoho Don15 presents a fascinating history of data science from the vantage point of a statistician. It makes an effective case that most of the major principles of today\\'s data science were originally developed by statisticians, although they were not quickly embraced by the discipline at large. Modern statisticians have begun having much more satisfying conversations with computer scientists on these matters as interests have mutually converged.\\n\\nVigen Vig15 presents an amusing collection of spurious correlations drawn from a large number of interesting time series. Figure 5.10 is representative, and is reprinted with permission.\\n\\nIt has been demonstrated that the size of American families is reasonably well fit by a Poisson distribution. In fact, an analysis of household size distributions from 104 countries suggests that the \"I\\'ve had enough\" model works around the world JLSI99.\\n\\n\\\\subsection*{5.8 Exercises}\\n5-1. [5] Explain which distribution seems most appropriate for the following phenomenon: binomial, normal, Poisson, or power law?\\\\\\\\\\n(a) The number of leaves on a fully grown oak tree.\\\\\\\\\\n(b) The age at which people\\'s hair turns grey.\\\\\\\\\\n(c) The number of hairs on the heads of 20 -year olds.\\\\\\\\\\n(d) The number of people who have been hit by lightning $x$ times.\\\\\\\\\\n(e) The number of miles driven before your car needs a new transmission.\\\\\\\\\\n(f) The number of runs a batter will get, per cricket over.\\\\\\\\\\n(g) The number of leopard spots per square foot of leopard skin.\\\\\\\\\\n(h) The number of people with exactly $x$ pennies sitting in drawers.\\\\\\\\\\n(i) The number of apps on people\\'s cell phones.\\\\\\\\\\n(j) The daily attendance in Skiena\\'s data science course.\\n\\n5-2. [5] Explain which distribution seems most appropriate for the following The Quant Shop phenomenon: binomial, normal, Poisson, or power law?\\\\\\\\\\n(a) The beauty of contestants at the Miss Universe contest.\\\\\\\\\\n(b) The gross of movies produced by Hollywood studios.\\\\\\\\\\n(c) The birth weight of babies.\\\\\\\\\\n(d) The price of art works at auction.\\\\\\\\\\n(e) The amount of snow New York will receive on Christmas.\\\\\\\\\\n(f) The number of teams that will win $x$ games in a given football season.\\\\\\\\\\n(g) The lifespans of famous people.\\\\\\\\\\n(h) The daily price of gold over a given year.\\n\\n5-3. [5] Assuming that the relevant distribution is normal, estimate the probability of the following events:\\\\\\\\\\n(a) That there will be 70 or more heads in the next hundred flips of a fair coin?\\\\\\\\\\n(b) That a randomly selected person will weight over 300 lbs ?\\n\\n5-4. [3] The average on a history exam was 85 out of 100 points, with a standard deviation of 15 . Was the distribution of the scores on this exam symmetric? If not, what shape would you expect this distribution to have? Explain your reasoning.\\\\\\\\[0pt]\\n5-5. [5] Facebook data shows that $50 \\\\%$ of Facebook users have a hundred or more friends. Further, the average user\\'s friend count is 190. What do these findings say about the shape of the distribution of number of friends of Facebook users?\\n\\n\\\\section*{Significance Testing}\\n5-6. [3] Which of the following events are likely independent and which are not?\\\\\\\\\\n(a) Coin tosses.\\\\\\n%---- Page End Break Here ---- Page : 152\\n\\\\\\n(b) Basketball shots.\\\\\\\\\\n(c) Party success rates in presidential elections.\\n\\n5-7. [5] The 2010 American Community Survey estimates that $47.1 \\\\%$ of women aged 15 years and over are married.\\\\\\\\\\n(a) Randomly select three women between these ages. What is the probability that the third woman selected is the only one that is married?\\\\\\\\\\n(b) What is the probability that all three women are married?\\\\\\\\\\n(c) On average, how many women would you expect to sample before selecting a married woman? What is the standard deviation?\\\\\\\\\\n(d) If the proportion of married women was actually $30 \\\\%$, how many women would you expect to sample before selecting a married woman? What is the standard deviation?\\\\\\\\\\n(e) Based on your answers to parts (c) and (d), how does decreasing the probability of an event affect the mean and standard deviation of the wait time until success?\\n\\n\\\\section*{Permutation Tests and P -values}\\n5-8. [5] Prove that the permutation generation algorithm of page 147 generates permutations correctly, meaning uniformly at random.\\\\\\\\[0pt]\\n5-9. [5] Obtain data on the heights of $m$ men and $w$ women.\\\\\\\\\\n(a) Use a t-test to establish the significance of whether the men are on average taller than the women.\\\\\\\\\\n(b) Perform a permutation test to establish the same thing: whether the men are on average taller than the women.\\n\\n\\\\section*{Implementation Projects}\\n5-10. [5] In sporting events, good teams tend to come back and win. But is this because they know how to win, or just because after a long-enough game the better team will generally prevail? Experiment with a random coin flip model, where the better team has a probability of $p>0.5$ of outplaying the other over a single period.\\\\\\\\\\nFor games of $n$ periods, how often does the better team win, and how often does it come from behind, for a given probability $p$ ? How does this compare to statistics from real sports?\\\\\\\\[0pt]\\n5-11. [8] February 2 is Groundhog Day in the United States, when it is said that six more weeks of winter follows if the groundhog sees its shadow. Taking whether it is sunny on February 2 as a proxy for the groundhog\\'s input, is there any predictive power to this tradition? Do a study based on weather records, and report the accuracy of the beast\\'s forecasts along with its statistical significance\\n\\n\\\\section*{Interview Questions}\\n$5-12$. [3] What is conditional probability?\\\\\\\\[0pt]\\n5-13. [3] What is Bayes Theorem? And why is it useful in practice?\\n\\n%---- Page End Break Here ---- Page : 153\\n\\n5-14. [8] How would you improve a spam detection algorithm that uses a naive Bayes classifier?\\n\\n5-15. [5] A coin is tossed ten times and the results are two tails and eight heads. How can you tell whether the coin is fair? What is the $p$-value for this result?\\\\\\\\[0pt]\\n5-16. [8] Now suppose that ten coins are each tossed ten times, for a total of 100 tosses. How would you test whether the coins are fair?\\\\\\\\\\n$5-17$. [8] An ant is placed on an infinitely long twig. The ant can move one step backward or one step forward with the same probability, during discrete time steps. What is the probability that the ant will return to its starting point after $2 n$ steps?\\\\\\\\[0pt]\\n5-18. [5] You are about to get on a plane to Seattle. Should you bring an umbrella? You call three random friends of yours who live there and ask each independently whether it is raining. Each of friends has a $2 / 3$ chance of telling you the truth and a $1 / 3$ chance of lying. All three friends tell you that it is raining. What is the probability that it is actually raining in Seattle?\\n\\n\\\\section*{Kaggle Challenges}\\n5-19. Decide whether a car bought at an auction is a bad buy.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/DontGetKicked}{https://www.kaggle.com/c/DontGetKicked}\\\\\\\\\\n5-20. Forecast the demand of a product in a given week.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/grupo-bimbo-inventory-demand}{https://www.kaggle.com/c/grupo-bimbo-inventory-demand}\\\\\\\\\\n$5-21$. How much rain we will get in the next hour?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/how-much-did-it-rain}{https://www.kaggle.com/c/how-much-did-it-rain}\\n\\n\\\\section*{Chapter 6}\\n\\\\section*{Visualizing Data}\\nAt their best, graphics are instruments for reasoning.\\n\\n\\\\begin{itemize}\\n  \\\\item Edward Tufte\\n\\\\end{itemize}\\n\\nEffective data visualization\\\\index{data visualization} is an important aspect of data science, for at least three distinct reasons:\\n\\n\\\\begin{itemize}\\n  \\\\item exploratory data analysis\\\\index{exploratory data analysis}: What does your data really look like? Getting a handle on what you are dealing with is the first step of any serious analysis. Plots and visualizations are the best way I know of to do this.\\n  \\\\item Error detection: Did you do something stupid in your analysis? Feeding unvisualized data to any machine learning algorithm is asking for trouble. Problems with outlier points, insufficient cleaning, and erroneous assumptions reveal themselves immediately when properly visualizing your data. Too often a summary statistic ( $77.8 \\\\%$ accurate!) hides what your model is really doing. Taking a good hard look what you are getting right vs. wrong is the first step to performing better.\\n  \\\\item Communication: Can you present what you have learned effectively to others? Meaningful results become actionable only after they are shared. Your success as a data scientist rests on convincing other people that you know what you are talking about. A picture is worth 1,000 words, especially when you are giving a presentation to a skeptical audience.\\n\\\\end{itemize}\\n\\nYou have probably been making graphs and charts since grade school. Ubiquitous software makes it easy to create prof\\\\index{Tufte, Edward}essional-looking images. So what is so hard about data visualization?\\n\\nTo answer, I offer a parable. A terrible incident during my youth concerned an attack on an ice skating champion. A thug hit her on the knee with a stick, hoping to knock her out of the upcoming Olympic games. Fortunately, he missed the knee, and the skater went on to win the silver medal.\\n\\nBut upon getting to know his client, the thug\\'s lawyer came up with an interesting defense. This crime, he said, was clearly too complex for his client to have conceived on his own. This left an impression on me, for it meant that I had underestimated the cognitive capacity necessary to rap someone on the leg with a stick.\\n\\nMy intended moral here is that many things are more complicated than they look. In particular, I speak of the problem of plotting data on a graph to capture what it is saying. An amazingly high fraction of the charts I have seen in presentations are terrible, either conveying no message or misconveying what the data actually showed. Bad charts can have negative value, by leading you in the wrong direction.\\n\\nIn this section, we will come to understand the principles that make standard plot designs work, and show how they can be misleading if not properly used. From this experience, we will try to develop your sense of when graphs might be lying, and how you can construct better ones.\\n\\n\\\\subsection*{6.1 Exploratory Data Analysis}\\nThe advent of massive data sets is changing in the way science is done. The traditional scientific method is hypothesis driven. The researcher formulates a theory of how the world works, and then seeks to support or reject this hypothesis based on data. By contrast, data-driven science starts by assembling a substantial data set, and then hunts for patterns that ideally will play the role of hypotheses for future analysis.\\n\\nExploratory data analysis is the search for patterns and trends in a given data set. Visualization techniques play an important part in this quest. Looking carefully at your data is important for several reasons, including identifying mistakes in collection/processing, finding violations of statistical assumptions, and suggesting interesting hypotheses.\\n\\nIn this section, we will discuss how to go about doing exploratory data analysis, and what visualization brings to the table as part of the process.\\n\\n\\\\subsection*{6.1.1 Confronting a New Data Set}\\nWhat should you do when encountering a new data set? This depends somewhat upon why you are interested in it in the first place, but the initial steps of exploration prove almost application-independent.\\n\\nHere are some basic steps that I encourage doing to get acquainted with any new data set, which I illustrate in exploring the body measurement data set NHANES, available at \\\\href{https://www.statcrunch.com/app/index.php?dataid=}{https://www.statcrunch.com/app/index.php?dataid=} 1406047. This is tabular data, but the general principles here are applicable to a broader class of resources:\\n\\n\\\\begin{itemize}\\n  \\\\item Answer the basic questions: There are several things you should know about your data set before you even open the file. Ask questions like:\\n \\n%---- Page End Break Here ---- Page : 156\\n \\\\item Who constructed this data set, when, and why? Understanding how your data was obtained provides clues as to how relevant it is likely to be, and whether we should trust it. It also points us to the right people if we need to know more about the data\\'s origin or provenance. With a little digging, I discovered that it came from the National Health and Nutrition Examination Survey 2009-2010, and who was responsible for posting it.\\n  \\\\item How big is it? How rich is the data set in terms of the number of fields or columns? How large is it as measured by the number of records or rows? If it is too big to explore easily with interactive tools, extract a small sample and do your initial explorations on that. This data set has 4978 records ( 2452 men and 2526 women), each with seven data fields plus gender.\\n  \\\\item What do the fields mean? Walk through each of the columns in your data set, and be sure you understand what they are. Which fields are numerical or categorical? What units were the quantities measured in? Which fields are IDs or descriptions, instead of data to compute with? A quick review shows that the lengths and weights here were measured using the metric system, in centimeters and kilograms respectively.\\n  \\\\item Look for familiar or interpretable records: I find it extremely valuable to get familiar with a few records, to the point where I know their names. Records are generally associated with a person, place, or thing that you already have some knowledge about, so you can put it in context and evaluate the soundness of the data you have on it. But if not, find a few records of special interest to get to know, perhaps the ones with the maximum or minimum values of the most important field.\\n\\\\end{itemize}\\n\\nIf familiar records do not exist, it sometimes pays to create them. A clever developer of a medical records database told me that he had used the top 5000 historical names from Who\\'s Bigger to serve as patient names while developing the product. This was a much more inspired idea than making up artificial names like \"Patient F1253.\" They were fun enough to encourage playing with the system, and memorable enough that outlier cases could be flagged and reported: e.g. \"There is something seriously wrong with Franz Kafka.\"\\n\\n\\\\begin{itemize}\\n  \\\\item Summary statistics: Look at the basic statistics of each column. Tukey\\'s five number summary is a great start for numerical values, consisting of the extreme values (max and min), plus the median and quartile elements.\\n\\\\end{itemize}\\n\\nApplied to the components of our height/weight data set, we get:\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c|c|c|c}\\n & Min & $25 \\\\%$ & Median & $75 \\\\%$ & Max \\\\\\\\\\n\\\\hline\\nAge & 241 & 418 & 584 & 748 & 959 \\\\\\\\\\nWeight & 32.4 & 67.2 & 78.8 & 92.6 & 218.2 \\\\\\\\\\nHeight & 140 & 160 & 167 & 175 & 204 \\\\\\\\\\nLeg Length & 23.7 & 35.7 & 38.4 & 41 & 55.5 \\\\\\\\\\nArm Length & 29.5 & 35.5 & 37.4 & 39.4 & 47.7 \\\\\\\\\\nArm Circumference & 19.5 & 29.7 & 32.8 & 36.1 & 141.1 \\\\\\\\\\nWaist & 59.1 & 87.5 & 97.95 & 108.3 & 172 \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nThis is very informative. First, what\\'s the deal that the median age is $584 ?$ Going back to the data, we learn that age is measured in months, meaning the median is 48.67 years. Arm and leg length seem to have about the same median, but leg length has much greater variability. I never knew that. But suddenly I realize that people are more often described as long/short legged than long/short armed, so maybe this is why.\\\\\\\\\\nFor categorical fields, like occupation, the analogous summary would be a report on how many different label types appear in the column, and what the three most popular categories are, with associated frequencies.\\n\\n\\\\begin{itemize}\\n  \\\\item pairwise correlations\\\\index{pairwise correlations}: A matrix of correlation coefficients between all pairs of columns (or at least the columns against the dependent variables of interest) gives you an inkling of how easy it will be to build a successful model. Ideally, we will have several features which strongly correlate with the outcome, while not strongly correlating with each other. Only one column from a set of perfectly correlated features has any value, because all the other features are completely defined from any single column.\\n\\\\end{itemize}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|rrrrrrr}\\n &  &  &  & \\\\begin{tabular}{r}\\nLeg \\\\\\\\\\nAge \\\\\\\\\\n\\\\end{tabular} & \\\\begin{tabular}{r}\\nArm \\\\\\\\\\nLength \\\\\\\\\\n\\\\end{tabular} & \\\\begin{tabular}{r}\\nArm \\\\\\\\\\nCircum \\\\\\\\\\n\\\\end{tabular} & Waist \\\\\\\\\\n\\\\hline\\nAge & 1.000 &  &  &  &  &  &  \\\\\\\\\\nWeight & 0.017 & 1.000 &  &  &  &  &  \\\\\\\\\\nHeight & -0.105 & 0.443 & 1.000 &  &  &  &  \\\\\\\\\\nLeg\\\\_Len & -0.268 & 0.238 & 0.745 & 1.000 &  &  &  \\\\\\\\\\nArm\\\\_Len & 0.053 & 0.583 & 0.801 & 0.614 & 1.000 &  &  \\\\\\\\\\nArm\\\\_Circ & 0.007 & 0.890 & 0.226 & 0.088 & 0.444 & 1.000 &  \\\\\\\\\\nWaist & 0.227 & 0.892 & 0.181 & -0.029 & 0.402 & 0.820 & 1.000 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nThese pairwise correlations are quite interesting. Why is height negatively correlated with age? The people here are all adults ( 241 months $=20.1$ years), so they are all fully grown. But the previous generation was shorter than the people of today. Further, people shrink when they get older, so together this probably explains it. The strong correlation between weight and waist size ( 0.89 ) reflects an unfortunate truth about nature.\\n\\n\\\\begin{itemize}\\n  \\\\item Class breakdowns: Are there interesting ways to break things down by major categorical variables, like gender or location? Through summary\\\\\\n%---- Page End Break Here ---- Page : 158\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-175}\\n\\\\end{itemize}\\n\\nFigure 6.1: The array of dot plots of variable pairs provides quick insight into the distributions of data values and their correlations.\\\\\\\\\\nstatistics, you can gauge whether there is a difference among the distributions when conditioned on the category. Look especially where you think there should be differences, based on your understanding of the data and application.\\\\\\\\\\nThe correlations were generally similar by gender, but there were some interesting differences. For example, the correlation between height and weight is stronger for men ( 0.443 ) than women (0.297).\\n\\n\\\\begin{itemize}\\n  \\\\item Plots of distributions: This chapter focuses on visualization techniques for data. Use the chart types we will discuss in Section 6.3 to eyeball the distributions, looking for patterns and outliers. What is the general shape of each distribution? Should the data be cleaned or transformed to make it more bell-shaped?\\\\\\\\\\nFigure 6.1 shows the power of a grid of dot plots of different variables. At a glance we see that there are no wild outliers, which pairs are correlated, and the nature of any trend lines. Armed with this single graphic, we are now ready to apply this data set to whatever is the challenge at hand.\\n\\\\end{itemize}\\n\\n\\\\subsection*{6.1.2 Summary Statistics and Anscombe\\'s Quartet}\\nThere are profound limits to how well you can understand data without visualization techniques. This is best depicted by Anscombe\\'s quartet: four twodimensional data sets, each with eleven points and shown in Figure 6.2 All four\\n\\n%---- Page End Break Here ---- Page : 159\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc|cc|cc|cc}\\n & \\\\multicolumn{2}{c|}{I} & \\\\multicolumn{2}{c|}{II} & \\\\multicolumn{2}{c|}{III} & \\\\multicolumn{2}{c}{IV} \\\\\\\\\\n & $\\\\mathbf{x}$ & $\\\\mathbf{y}$ & $\\\\mathbf{x}$ & $\\\\mathbf{y}$ & $\\\\mathbf{x}$ & $\\\\mathbf{y}$ & $\\\\mathbf{x}$ & $\\\\mathbf{y}$ \\\\\\\\\\n\\\\hline\\n & 10.0 & 8.04 & 10.0 & 9.14 & 10.0 & 7.46 & 8.0 & 6.58 \\\\\\\\\\n & 8.0 & 6.95 & 8.0 & 8.14 & 8.0 & 6.77 & 8.0 & 5.76 \\\\\\\\\\n & 13.0 & 7.58 & 13.0 & 8.74 & 13.0 & 12.74 & 8.0 & 7.71 \\\\\\\\\\n & 9.0 & 8.81 & 9.0 & 8.77 & 9.0 & 7.11 & 8.0 & 8.84 \\\\\\\\\\n & 11.0 & 8.33 & 11.0 & 9.26 & 11.0 & 7.81 & 8.0 & 8.47 \\\\\\\\\\n & 14.0 & 9.96 & 14.0 & 8.10 & 14.0 & 8.84 & 8.0 & 7.04 \\\\\\\\\\n & 6.0 & 7.24 & 6.0 & 6.13 & 6.0 & 6.08 & 8.0 & 5.25 \\\\\\\\\\n & 4.0 & 4.26 & 4.0 & 3.10 & 4.0 & 5.39 & 19.0 & 12.50 \\\\\\\\\\n & 12.0 & 10.84 & 12.0 & 9.31 & 12.0 & 8.15 & 8.0 & 5.56 \\\\\\\\\\n & 7.0 & 4.82 & 7.0 & 7.26 & 7.0 & 6.42 & 8.0 & 7.91 \\\\\\\\\\n & 5.0 & 5.68 & 5.0 & 4.74 & 5.0 & 5.73 & 8.0 & 6.89 \\\\\\\\\\n\\\\hline\\nMean & 9.0 & 7.5 & 9.0 & 7.5 & 9.0 & 7.5 & 9.0 & 7.5 \\\\\\\\\\nVar. & 10.0 & 3.75 & 10.0 & 3.75 & 10.0 & 3.75 & 10.0 & 3.75 \\\\\\\\\\nCorr. & 0.816 & \\\\multicolumn{2}{c}{0.816} & \\\\multicolumn{2}{c}{0.816} & 0.816 &  &  \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 6.2: Four data sets with identical statistical properties. What do they look like?\\\\\\\\\\ndata sets have identical means for the $x$ and $y$ values, identical variances for the $x$ and $y$ values, and the exact same correlation between the $x$ and $y$ values.\\n\\nThese data sets must all be pretty similar, right? Study the numbers for a little while so you can get an idea of what they look like.\\n\\nGot it? Now peak at the dot plots of these data sets in Figure 6.3 They all look different, and tell substantially different stories. One trends linear, while a second looks almost parabolic. Two others are almost perfectly linear modulo outliers, but with wildly different slopes.\\n\\nThe point here is that you can instantly appreciate these differences with a glance at the scatter plot. Even simple visualizations are powerful tools\\\\index{tools} for understanding what is going on in a data set. Any sensible data scientist strives to take full advantage of visualization techniques.\\n\\n\\\\subsection*{6.1.3 Visualization Tools}\\nAn extensive collection of software tools are available to support visualization. Generally speaking, visualization tasks fall into three categories, and the right choice of tools depends upon what your mission really is:\\n\\n\\\\begin{itemize}\\n  \\\\item Exploratory data analysis: Here we seek to perform quick, interactive explorations of a given data set. Spreadsheet programs like Excel and notebook-based programming environments like iPython, R, and Mathematica are effective at building the standard plot types. The key here is hiding the complexity, so the plotting routines default to doing something reasonable but can be customized if necessary.\\\\\\n%---- Page End Break Here ---- Page : 160\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-177}\\n\\\\end{itemize}\\n\\nFigure 6.3: Plots of the Ascombe quartet. These data sets are all dramatically different, even though they have identical summary statistics.\\n\\n\\\\begin{itemize}\\n  \\\\item Publication/presentation quality charts: Just because Excel is very popular does not mean it produces the best possible graphs/plots. The best visualizations are an interaction between scientist and software, taking full advantage of the flexibility of a tool to maximize the information content of the graphic.\\n\\\\end{itemize}\\n\\nPlotting libraries like MatPlotLib or Gnuplot support a host of options enabling your graph to look exactly like you want it to. The statistical language R has a very extensive library of data visualizations. Look through catalogs of the plot types supported by your favorite library, to help you find the best representation for your data.\\n\\n\\\\begin{itemize}\\n  \\\\item Interactive visualization for external applications: Building dashboards that facilitate user interaction with proprietary data sets is a typical task for data science-oriented software engineers. The typical mission here is to build tools that support exploratory data analysis for less technicallyskilled, more application-oriented personnel.\\n\\\\end{itemize}\\n\\nSuch systems can be readily built in programming languages like Python, using standard plotting libraries. There is also a class of third-party systems for building dashboards, like Tableau. These systems are programmable at a higher-level than other tools, supporting particular interaction paradigms and linked-views across distinct views of the data.\\\\\\n%---- Page End Break Here ---- Page : 161\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-178}\\n\\nFigure 6.4: Which painting do you like better? Forming intelligent preferences in art or visualizations depend upon having a distinctive visual aesthetic.\\n\\n\\\\subsection*{6.2 Developing a visualization aesthetic\\\\index{visualization aesthetic}}\\nSensible appreciation of art or wine requires developing a particular taste or aesthetic. It isn\\'t so much about whether you like something, but figuring out why you like it. Art experts talk about the range of a painter\\'s palate, use of light, or the energy/tension of a composition. Wine connoisseurs testify to the fragrance, body, acidity, and clarity of their favorite plonk, and how much oak or tannin it contains. They always have something better to say than \"that tastes good.\"\\n\\nDistinguishing good/bad visualizations requires developing a design aesthetic, and a vocabulary to talk about data representations. Figure 6.4 presents two famous landmarks in Western painting. Which one is better? This question is meaningless without a sense of aesthetics and a vocabulary to describe it.\\n\\nMy visual aesthetic and vocabulary is largely derived from the books of Edward Tufte Tuf83, Tuf90, Tuf97. He is an artist: indeed I once got to meet him at his former art gallery across\\\\index{Tufte, Edward} from Chelsea \\\\index{scaling and labeling}Piers in Manhattan. He has thought long and hard about what makes a chart or graph informative and beautiful, basing a design aesthetic on the following principles:\\n\\n\\\\begin{itemize}\\n  \\\\item Maximize data-ink ratio\\\\index{data-ink ratio}\\\\index{data-ink ratio}: Your visualization is supposed to show off your data. So why is so much of what you see in charts the background grids, shading, and tic-marks?\\n  \\\\item Minimize the lie factor\\\\index{lie factor}: As a scientist, your data should reveal the truth, ideally the truth you want to see revealed. But are you being honest with your audience, or using graphical devices that mislead them into seeing something that isn\\'t really there?\\n  \\\\item Minimize chartjunk\\\\index{chartjunk}: Modern visualization software often adds cool visual effects that have little to do with your data set. Is your graphic interesting because of your data, or in spite of it?\\n  \\\\item Use proper scales and clear labeling: Accurate interpretation of data depends upon non-data elements like scale and labeling. Are your descriptive\\\\\\n%---- Page End Break Here ---- Page : 162\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-179}\\n\\\\end{itemize}\\n\\nFigure 6.5: Three-dimensional monoliths casting rendered shadows (l) may look impressive. But they really are just chartjunk, which serves to reduce the clarity and data-ink ratio from just showing the data (r).\\\\\\\\\\nmaterials optimized for clarity and precision?\\n\\n\\\\begin{itemize}\\n  \\\\item Make effective use of color: The human eye has the power to discriminate between small gradations in hue and color saturation. Are you using color to highlight important properties of your data, or just to make an artistic statement?\\n  \\\\item Exploit the power of repetition\\\\index{repetition}: Arrays of similar graphics with different but related data elements provide a concise and powerful way to enable visual comparisons. Are your chart multiples facilitating comparisons, or merely redundant?\\n\\\\end{itemize}\\n\\nEach of these principles will be detailed in the subsections below.\\n\\n\\\\subsection*{6.2.1 Maximizing Data-Ink Ratio}\\nIn any graphic, some of the ink is used to represent the actual underlying data, while the rest is employed on graphic effects. Generally speaking, visualizations should focus on showing the data itself. We define the data-ink ratio to be:\\n\\n$$\\n\\\\text { Data-Ink Ratio }=\\\\frac{\\\\text { Data-Ink }}{\\\\text { Total ink used in graphic }}\\n$$\\n\\nFigure 6.5 presents average salary by gender (Bureau of Labor Statistics, 2015), and helps clarify this notion. Which data representation do you prefer? The image on the left says \"Cool, how did you make those shadows and that three-dimensional perspective effect?\" The image on the right says \"Wow, women really are underpaid at all points on the income spectrum. But why is the gap smallest for counselors?\"\\n\\nMaximizing the data-ink ratio lets the data talk, which is the entire point of the visualization exercise in the first place. The flat perspective on the right permits a fairer comparison of the heights of the bars, so the males do not look\\\\\\n%---- Page End Break Here ---- Page : 163\\n\\\\\\nlike pipsqueaks to the women. The colors\\\\index{colors} do a nice job enabling us to compare apples-to-apples.\\n\\nThere are more extreme ways to increase the data-ink ratio. Why do we need bars at all? The same information could be conveyed by plotting a point of the appropriate height, and would clearly be an improvement were we plotting much more than the eight points shown here. Be aware that less can be more in visualizing data.\\n\\n\\\\subsection*{6.2.2 Minimizing the lie factor\\\\index{lie factor}}\\nA visualization seeks to tell a true story about what the data is saying. The baldest form of lie is to fudge your data, but it remains quite possible to report your data accurately, yet deliberately mislead your audience about what it is saying. Tufte defines the lie factor of a chart as:\\n\\n$$\\n\\\\text { lie factor }=\\\\frac{(\\\\text { size of an effect in the graphic })}{(\\\\text { size of the effect in the data })}\\n$$\\n\\nGraphical integrity requires minimizing this lie factor, by avoiding the techniques which tend to mislead. Bad practices include:\\n\\n\\\\begin{itemize}\\n  \\\\item Presenting means without variance: The data values $\\\\{100,100,100,100,100\\\\}$ and $\\\\{200,0,100,200,0\\\\}$ tell different stories, even though both means are 100. If you cannot plot the actual points with the mean, at least show the variance, to make clear the degree to which the mean reflects the distribution.\\n  \\\\item Presenting interpolations without the actual data: Regression lines and fitted curves are effective at communicating trends and simplifying large data sets. But without showing the data points it is based on, it is impossible to ascertain the quality of the fit.\\n  \\\\item Distortions of scale: The aspect ratio of a figure can have a huge effect on how we interpret what we are seeing. Figure 6.6 presents three renderings of a given financial time series, identical except for the aspect ratio of the chart.\\n\\\\end{itemize}\\n\\nIn the bottom rendering, the series looks flat: there is nothing to worry about here. On the right, profits have fallen off a cliff: the sky is falling! The left corner plot presents a serious decline, but with signs of an autumn rebound.\\n\\nWhich plot is right? People are generally used to seeing plots presented according to the Golden ratio, implying that the width should be about 1.6 times the height. Give this shape to them, unless you have well-developed reasons why it is inappropriate. Psychologists inform us that 45 degree lines are the most readily interpretable, so avoid shapes that substantially amplify or mute lines from this objective.\\\\\\n%---- Page End Break Here ---- Page : 164\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-181}\\n\\nFigure 6.6: Three renderings of the same financial time series. Which most accurately represents the situation?\\n\\n\\\\begin{itemize}\\n  \\\\item Eliminating tick labels from numerical axes: Even the worst scale distortions can be completely concealed by not printing numerical reference labels on axes. Only with the numerical scale markings can the actual data values be reconstructed from the plot.\\n  \\\\item Hide the origin point from the plot: The implicit assumption in most graphs is that the range of values on the $y$-axis goes from zero to $y_{\\\\max }$. We lose the ability to visually compare magnitudes if the $y$-range instead goes from $y_{\\\\min }-\\\\epsilon$ to $y_{\\\\max }$. The largest value suddenly looks many times larger than the smallest value, instead of being scaled to the proper proportion.\\\\\\\\\\nIf Figure 6.5 (right) were drawn with a tight $y$-range [900, 2500], the message would be that counselors were starving, instead of earning salaries as close to teachers as software developers are to pharmacists. Such deceptions can be recognized provided the scales are marked on the axis, but are hard to catch.\\n\\\\end{itemize}\\n\\nDespite Tufte\\'s formula, the lie factor cannot be computed mechanically, because it requires understanding the agenda that is behind the distortion. In reading any graph, it is important to know who produced it and why. Understanding their agenda should sensitize you to potentially misleading messages encoded in the graphic.\\n\\n\\\\subsection*{6.2.3 Minimizing chartjunk\\\\index{chartjunk}}\\nExtraneous visual elements distract from the message the data is trying to tell. In an exciting graphic, the data tells the story, not the chartjunk.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-182}\\n\\nFigure 6.7: A monthly time series of sales. How can we improve/simplify this bar chart time series?\\n\\nFigure 6.7 presents a monthly time series of sales at a company beginning to encounter bad times. The graphic in question is a bar plot, a perfectly sound way to represent time series data, and is drawn using conventional, perhaps default, options using a reasonable plotting package.\\n\\nBut can we simplify this plot by removing elements to make the data stand out better? Think about this for a minute before peeking at Figure 6.8 which presents a series of four successive simplifications to this chart. The critical operations are:\\n\\n\\\\begin{itemize}\\n  \\\\item Jailbreak your data (upper left): Heavy grids imprison your data, by visually dominating the contents. Often graphs can be improved by removing the grid, or at least lightening it.\\\\\\\\\\nThe potential value of the data grid is that it facilitates more precise interpretation of numerical quantities. Thus grids tend to be most useful on plots with large numbers of values which may need to be accurately quoted. Light grids can adequately manage such tasks.\\n  \\\\item Stop throwing shade (upper right): The colored background here contributes nothing to the interpretation of the graphic. Removing it increases the data-ink ratio, and makes it less obtrusive.\\n  \\\\item Think outside the box (lower left): The bounding box does not really contribute information, particularly the upper and rightmost boundaries which do not define axes. Take them out, and let more air into your plots.\\n  \\\\item Make missing ink work for you (lower right): The effect of the reference grid can be recovered by removing lines from the bars instead of adding elements. This makes it easier to compare the magnitude of biggest numbers, by focusing attention on big changes in the relatively small top piece, instead of small changes in the long bar.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-183}\\n\\\\end{itemize}\\n\\nFigure 6.8: Four successive simplifications of Figure 6.7 by removing extraneous non-data elements.\\n\\nThe architect Mies van der Rohe famously said that \"less is more.\" Removing elements from plots often improves them far more than adding things. Make this part of your graphical design philosophy.\\n\\n\\\\subsection*{6.2.4 Proper Scaling and Labeling}\\nDeficiencies in scaling and labeling are the primary source of intentional or accidental misinformation in graphs. Labels need to report the proper magnitude of numbers, and scale needs to show these numbers to the right resolution, in a way to facilitate comparison. Generally speaking, data should be scaled so as to fill the space allotted to it on the chart.\\n\\nReasonable people can differ as to whether to scale the axes over the full theoretical range of the variable, or cut it down to reflect only the observed values. But certain decisions are clearly unreasonable.\\n\\nFigure 6.9 (left) was produced by a student of mine, presenting the correlation between two variables for almost a hundred languages. Because the correlation ranges between $[-1,1]$, he forced the plot to respect this interval. The vast sea of white in this plot captures only the notion that we might have done better, by getting the correlation closer to 1.0. But the chart is otherwise unreadable.\\n\\nFigure 6.9 (right) presents exactly the same data, but with a truncated scale. Now we can see where there are increases in performance as we move from left to right, and read off the score for any given language. Previously, the bars\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-184}\\n\\nFigure 6.9: Scaling over the maximum possible range (left) is silly when all it shows is white space. Better scaling permits more meaningful comparisons (right).\\\\\\\\\\nwere so far removed from the labels that it was difficult to make the name-bar correspondence at all.\\n\\nThe biggest sin of truncated scales comes when you do not show the whole of each bar, so the length of the bar no longer reflects the relative value of the variable. We show the $y=0$ line here, helping the reader to know that each bar must be whole. Getting the data out of its prison grid would also have helped.\\n\\n\\\\subsection*{6.2.5 Effective Use of Color and Shading}\\nColors are increasingly assumed as part of any graphical communication. Indeed, I was pleased to learn that my publisher\\'s printing costs are now identical for color and black-and-white, so you the reader are not paying any more to see my color graphics here.\\n\\nColors play two major roles in charts, namely marking class distinctions and encoding numerical values. Representing points of different types, clusters, or classes with different colors encodes another layer of information on a conventional dot plot. This is a great idea when we are trying to establish the extent of differences in the data distribution across cl\\\\index{Schaumann, Jan}asses. The most critical thing is that the classes be easily distinguishable from each other, by using bold primary colors.\\n\\nIt is best when the colors are selected to have mnemonic values to link naturally to the class at hand. Losses should be printed in red ink, environmental causes associated with green, nations with their flag colors, and sports teams with their jersey colors. Coloring points to represent males as blue and females as red offers a subtle clue to help the viewer interpret a scatter plot, as shown in Figure 9.17\\n\\nSelecting colors to represent a numerical scale is a more difficult problem. Rainbow color maps are perceptually non-linear, meaning it is not obvious to anyone whether purple lies before or after green. Thus while plotting numbers in rainbow colors groups similar numbers in similar colors, the relative magnitudes are imperceptible without explicitly referencing the color scale. Figure 6.10 presents several color scales from Python\\'s MatPlotLib, for comparison.\\\\\\n%---- Page End Break Here ---- Page : 168\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-185}\\n\\nFigure 6.10: Color scales from Python\\'s MatPlotLib, varying hue, saturation, and brightness. Rainbow color maps are perceptually non-linear, making it difficult to recognize the magnitudes of differences.\\n\\nMuch better are color scales based on a varying either brightness or saturation. The brightness of a color is modulated by blending the hue with a shade of gray, somewhere between white and black. Saturation is controlled by mixing in a fraction of gray, where 0 produces the pure hue, and 1 removes all color.\\n\\nAnother popular color scale features distinct positive/negative colors (say, blue and red, as in the seismic color scale of Figure 6.10p reflected around a white or gray center at zero. Thus hue tells the viewer the polarity of the number, while brightness/saturation reflects magnitude. Certain color scales are much better for color-blind people, particularly those avoiding use of red and green.\\n\\nAs a general rule, large areas on plots should be shown with unsaturated colors. The converse is true for small regions, which stand out better with saturated colors. Color systems are a surprisingly technical and complicated matter, which means that you should always use well established color scales, instead of inventing your own.\\n\\n\\\\subsection*{6.2.6 The Power of Repetition}\\nSmall multiple plots and tables are excellent ways to represent multivariate data. Recall the power of grids showing all bivariate distributions in Figure 6.1\\n\\nThere are many applications of small multiple charts. We can use them to break down a distribution by classes, perhaps plotting separate but comparable charts by region, gender, or time period. Arrays of plots facilitate comparisons: what has changed between different distributions.\\n\\nTime series plots enable us to compare the same quantities at different calendar points. Even better is to compare multiple time series, either as lines on the same plot, or multiple plots in a logical array reflecting their relationship.\\n\\n\\\\subsection*{6.3 chart types\\\\index{chart types}}\\nIn this section, we will survey the rationale behind the primary types of data visualizations. For each chart, I present best practices for using them, and outline the degrees of freedom you have to make your presentation as effective as possible.\\n\\nNothing says \"Here\\'s a plot of some data\" like a thoughtlessly-produced graphic, created using the default settings of some software tool. My students present me with such undigested data products way too often, and this section is somewhat of a personal reaction against it.\\n\\nTake-Home Lesson: You have the power and responsibility to produce meaningful and interpretable presentations of your work. Effective visualization involves an iterative process of looking at the data, deciding what story it is trying to tell, and then improving the display to tell the story better.\\n\\nFigure 6.11 presents a handy decision tree to help select the right data representation, from Abela Abe13]. The most important charts will be reviewed in this section, but use this tree to better understand why certain visualizations are more appropriate in certain contexts. We need to produce the right plot for a given data set, not just the first thing that comes to mind.\\n\\n\\\\subsection*{6.3.1 tabular data\\\\index{tabular data}}\\nTables of numbers can be beautiful things, and are very effective ways to present data. Although they may appear to lack the visual appeal of graphic presentations, tables have several advantages over other representations, including:\\n\\n\\\\begin{itemize}\\n  \\\\item Representation of precision: The resolution of a number tells you something about the process of how it was obtained: an average salary of $\\\\$ 79,815$ says something different than $\\\\$ 80,000$. Such subtleties are generally lost on plots, but nakedly clear in numerical tables.\\n  \\\\item Representation of scale: The digit lengths of numbers in a table can be likened to bar charts, on a logarithmic scale. Right-justifying numbers best communicates order-of-magnitude differences, as (to a lesser extent) does scanning the leading digits of numbers in a column.\\n\\\\end{itemize}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{lcr}\\nleft & center & right \\\\\\\\\\n\\\\hline\\n1 & 1 & 1 \\\\\\\\\\n10 & 10 & 10 \\\\\\\\\\n100 & 100 & 100 \\\\\\\\\\n1000 & 1000 & 1000 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nLeft justifying numbers prevents such comparisons, so always be sure to right justify them.\\\\\\n%---- Page End Break Here ---- Page : 170\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-187}\\n\\nFigure 6.11: A clever decision tree to help identify the best visual representation for representing data. Reprinted with permission from Abela [Abe13].\\n\\n%---- Page End Break Here ---- Page : 171\\n\\n\\\\begin{itemize}\\n  \\\\item Multivariate visualization: Geometry gets complicated to understand once we move beyond two dimensions. But tables can remain manageable even for large numbers of variables. Recall Babe Ruth\\'s baseball statistics from Figure 1.1. a table of twenty-eight columns which is readily interpretable by any knowledgeable fan.\\n  \\\\item Heterogeneous data: Tables generally are the best way to present a mix of numerical and categorical attributes, like text and labels. Glyphs like emojis can even be used to represent the values of certain fields.\\n  \\\\item Compactness: Tables are particularly useful for representing small numbers of points. Two points in two dimensions can be drawn as a line, but why bother? A small table is generally better than a sparse visual.\\n\\\\end{itemize}\\n\\nPresenting tabular data seems simple to do (\"just put it in a table\"), akin to rapping a leg with a stick. But subtleties go into producing the most informative tables. Best practices include:\\n\\n\\\\begin{itemize}\\n  \\\\item Order rows to invite comparison: You have the freedom to order the rows in a table any way you want, so take advantage of it. Sorting the rows according to the values of an important column is generally a good idea. Thus grouping the rows is valuable to facilitate comparison, by putting likes with likes.\\\\\\\\\\nSorting by size or date can be more revealing than the name in many contexts. Using a canonical order of the rows (say, lexicographic by name) can be helpful for looking up items by name, but this is generally not a concern unless the table has many rows.\\n  \\\\item Order columns to highlight importance, or pairwise relationships: Eyes darting from left-to-right across the page cannot make effective visual comparisons, but neighboring fields are easy to contrast. Generally speaking, columns should be organized to group similar fields, hiding the least important ones on the right.\\n  \\\\item Right-justify uniform-precision numbers: Visually comparing 3.1415 with 39.2 in a table is a hopeless task: the bigger number has to look bigger. Best is to right justify them, and set all to be the same precision: 3.14 vs. 39.20 .\\n  \\\\item Use emphasis, font, or color to highlight important entries: Marking the extreme values in each column so they stand out reveals important information at a glance. It is easy to overdo this, \\\\index{deci\\\\index{Zipfâs law}sion \\\\index{decision trees}tree classiï¬ers}however, so strive for subtlety.\\n  \\\\item Avoid excessive-length column descriptors: White ribbons in tables are d\\\\index{classiï¬cation and regression trees}istracting, and usually result from column labels that are longer than the values they represent. Use abbreviations or multiple-line word stacking to mini\\\\index{add-one discounting}mize the problem, and clarify any ambiguity in the caption attached to the table.\\n\\n%---- Page End Break Here ---- Page : 172\\n\\\\end{itemize}\\n\\nTo help illustrate these possible sins, here is a table recording six properties of fifteen different nations, with the row and column orders given at random. Do you see any possible ways of improving it?\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{l|llllll}\\nCountry & Area & Density & Birthrate & Population & Mortality & GDP \\\\\\\\\\n\\\\hline\\nRussia & 17075200 & 8.37 & 99.6 & 142893540 & 15.39 & 8900.0 \\\\\\\\\\nMexico & 1972550 & 54.47 & 92.2 & 107449525 & 20.91 & 9000.0 \\\\\\\\\\nJapan & 377835 & 337.35 & 99.0 & 127463611 & 3.26 & 28200.0 \\\\\\\\\\nUnited Kingdom & 244820 & 247.57 & 99.0 & 60609153 & 5.16 & 27700.0 \\\\\\\\\\nNew Zealand & 268680 & 15.17 & 99.0 & 4076140 & 5.85 & 21600.0 \\\\\\\\\\nAfghanistan & 647500 & 47.96 & 36.0 & 31056997 & 163.07 & 700.0 \\\\\\\\\\nIsrael & 20770 & 305.83 & 95.4 & 6352117 & 7.03 & 19800.0 \\\\\\\\\\nUnited States & 9631420 & 30.99 & 97.0 & 298444215 & 6.5 & 37800.0 \\\\\\\\\\nChina & 9596960 & 136.92 & 90.9 & 1313973713 & 24.18 & 5000.0 \\\\\\\\\\nTajikistan & 143100 & 51.16 & 99.4 & 7320815 & 110.76 & 1000.0 \\\\\\\\\\nBurma & 678500 & 69.83 & 85.3 & 47382633 & 67.24 & 1800.0 \\\\\\\\\\nTanzania & 945087 & 39.62 & 78.2 & 37445392 & 98.54 & 600.0 \\\\\\\\\\nTonga & 748 & 153.33 & 98.5 & 114689 & 12.62 & 2200.0 \\\\\\\\\\nGermany & 357021 & 230.86 & 99.0 & 82422299 & 4.16 & 27600.0 \\\\\\\\\\nAustralia & 7686850 & 2.64 & 100.0 & 20264082 & 4.69 & 29000.0 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nThere are many possible orderings of the rows (countries). Sorting by any single column is an improvement over random, although we could also group them by region/continent. The order of the columns can be made more understandable by putting like next to like. Finally, tricks like right-justifying numbers, removing uninformative digits, adding commas, and highlighting the biggest value in each column makes the data easier to read:\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{l|rrrrrr}\\n &  &  &  &  &  & \\\\begin{tabular}{r}\\nBirth \\\\\\\\\\n\\\\end{tabular} \\\\\\\\\\nCountry & Population & Area & Density & Mortality & GDP & \\\\begin{tabular}{r}\\nRate \\\\\\\\\\n\\\\end{tabular} \\\\\\\\\\n\\\\hline\\nAfghanistan & $31,056,997$ & 647,500 & 47.96 & $\\\\mathbf{1 6 3 . 0 7}$ & 700 & 36.0 \\\\\\\\\\nAustralia & $20,264,082$ & $7,686,850$ & 2.64 & 4.69 & 29,000 & $\\\\mathbf{1 0 0 . 0}$ \\\\\\\\\\nBurma & $47,382,633$ & 678,500 & 69.83 & 67.24 & 1,800 & 85.3 \\\\\\\\\\nChina & $\\\\mathbf{1 , 3 1 3 , 9 7 3 , 7 1 3}$ & $9,596,960$ & 136.92 & 24.18 & 5,000 & 90.9 \\\\\\\\\\nGermany & $82,422,299$ & 357,021 & 230.86 & 4.16 & 27,600 & 99.0 \\\\\\\\\\nIsrael & $6,352,117$ & 20,770 & 305.83 & 7.03 & 19,800 & 95.4 \\\\\\\\\\nJapan & $127,463,611$ & 377,835 & $\\\\mathbf{3 3 7 . 3 5}$ & 3.26 & 28,200 & 99.0 \\\\\\\\\\nMexico & $107,449,525$ & $1,972,550$ & 54.47 & 20.91 & 9,000 & 92.2 \\\\\\\\\\nNew Zealand & $4,076,140$ & 268,680 & 15.17 & 5.85 & 21,600 & 99.0 \\\\\\\\\\nRussia & $142,893,540$ & $\\\\mathbf{1 7 , 0 7 5 , 2 0 0}$ & 8.37 & 15.39 & 8,900 & 99.6 \\\\\\\\\\nTajikistan & $7,320,815$ & 143,100 & 51.16 & 110.76 & 1,000 & 99.4 \\\\\\\\\\nTanzania & $37,445,392$ & 945,087 & 39.62 & 98.54 & 600 & 78.2 \\\\\\\\\\nTonga & 114,689 & 748 & 153.33 & 12.62 & 2,200 & 98.5 \\\\\\\\\\nUnited Kingdom & $60,609,153$ & 244,820 & 247.57 & 5.16 & 27,700 & 99.0 \\\\\\\\\\nUnited States & $298,444,215$ & $9,631,420$ & 30.99 & 6.50 & $\\\\mathbf{3 7 , 8 0 0}$ & 97.0 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-190}\\n\\\\end{center}\\n\\nFigure 6.12: Many of the line chart styles that we have seen are supported by Python\\'s MatPlotLib package.\\n\\n\\\\subsection*{6.3.2 dot and line plots\\\\index{dot and line plots}}\\nDot and line plots are the most ubiquitous forms of data graphic, providing a visual representation of a function $y=f(x)$ defined by a set of $(x, y)$ points. Dot plots just show the data points, while line plots connect them or interpolate to define a continuous function $f(x)$. Figure 6.12 shows several different styles of line plots, varying in the degree of emphasis they give the points vs. the interpolated curve. Advantages of line charts\\\\index{line charts} include:\\n\\n\\\\begin{itemize}\\n  \\\\item Interpolation and fitting: The interpolation curve derived from the points provides a prediction for $f(x)$ over the full range of possible $x$. This enables us to sanity check or reference other values, and make explicit the trends shown in the data.\\\\\\\\\\nOverlaying a fitted or smoothed curve on the same graph as the source data is a very powerful combination. The fit provides a model explaining what the data says, while the actual points enable us to make an educated judgment of how well we trust the model.\\n  \\\\item Dot plots: A great thing about line plots is that you don\\'t actually have to show the line, resulting in a dot plot. Connecting points by line segments (polylines) proves misleading in many situations. If the function is only defined at integer points, or the $x$-values represent distinct conditions, then it makes no sense at all to interpolate between them.\\\\\\\\\\nFurther, polylines dramatically swing out of their way to capture outliers, thus visually encouraging us to concentrate on exactly the points we should most ignore. High frequency up-and-down movement distracts us from\\\\\\n%---- Page End Break Here ---- Page : 174\\n\\\\\\nseeing the broader trend, which i\\\\index{exclusive or}s the primary reason for staring at a chart.\\n\\\\end{itemize}\\n\\nBest practices with line charts include:\\n\\n\\\\begin{itemize}\\n  \\\\item Show data points, not just fits: It is generally important to show the actual data, instead of just the fitted or interpolated lines.\\\\\\\\\\nThe key is to make sure that one does not overwhelm the other. To represent large numbers of points unobtrusively, we can (a) reduce the size of the points, possibly to pinpricks, and/or (b) lighten the shade of the points so they sit in the background. Remember that there are fifty shades of gray, and that subtlety is the key.\\n  \\\\item Show the full variable range if possible: By default, most graphic software plots from $x_{\\\\text {min }}$ to $x_{\\\\text {max }}$ and $y_{\\\\text {min }}$ to $y_{\\\\max }$, where the mins and maxes are defined over the input data values. But the logical min and max are context specific, and it can reduce the lie factor to show the full range. Counts should logically start from zero, not $y_{\\\\text {min }}$.\\\\\\\\\\nBut sometimes showing the full range is uninformative, by completely flattening out the effect you are trying to illustrate. This was the case in Figure 6.9. One possible solution is to use a log scale for the axis, so as to embed a wider range of numbers in a space efficient way. But if you must truncate the ran\\\\index{ensembles of}ge, make clear what you are doing, by using axis labels with tick marks, and clarifying any ambiguity in the associated caption.\\n  \\\\item Admit uncertainty when plotting averages: The points appearing on a line or dot plot are often obtained by averaging \\\\index{bagging}multiple observations. The resulting mean better captures the distribution than any single observation. But means have differing interpretations based on variance. Both $\\\\{8.5,11.0,13.5,7.0,10.0\\\\}$ and $\\\\{9.9,9.6,10.3,10.1,10.1\\\\}$ average to be 10.0 , but the degree to which we trust them to be precise differs substantially.\\\\\\\\\\nThere are several ways to confess the level of measurement uncertainty in our plots. My favorite simply plots all the underlying data values on the same graph as the means, using the same $x$-value as the associated mean. These points will be visually unobtrusive relative to the heavier trend line, provided they are drawn as small dots and lightly shaded, but they are now available for inspection and analysis.\\\\\\\\\\nA second approach plots the standard deviation $\\\\sigma$ around $y$ as a whisker, showing the interval $[y-\\\\sigma, y+\\\\sigma]$. This interval representation is honest, denoting the range with $68 \\\\%$ of the values under a normal distribution. Longer whiskers means you should be more suspicious of the accuracy of the means, while short whiskers implies greater precision.\\\\\\\\\\nBox plots concisely record the range and distribution of values at a point with a box. This box shows the range of values from the quartiles ( $25 \\\\%$ and $75 \\\\%$ ), and is cut at the median (50th percentile). Typically whiskers\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-192}\\n\\\\end{itemize}\\n\\nFigure 6.13: Box and whisker plots concisely show the range/quartiles (i.e. median and variance) of a distribution.\\\\\\\\\\n(hairs) are added to show the range of the highest and lowest values. Figure 6.13 shows a box-and-whisker plot of weight as a function of height in a population sample. The median weight increases with height, but not the maximum, because fewer points in the tallest bucket reduces the chance for an outlier maximum value.\\n\\nReal scientists seem to love box-and-whisker plots\\\\index{box-and-whisker plots}, but I personally find them to be overkill. If you really can\\'t represent the actual data points, perhaps just show the contour lines flanking your mean/median at the 25 th and 75 th percentiles. This conveys exactly the same information as the box in the box plot, with less chartjunk.\\n\\n\\\\begin{itemize}\\n  \\\\item Never connect points for categorical data: Suppose you measure some variable (perhaps, median income) for several different classes (say, the fifty states, from Alabama to Wyoming). It might make sense to display this as a dot plot, with $1 \\\\leq x \\\\leq 50$, but it would be silly and misleading to connect the points. Why? Because there is no meaningful adjacency between state $i$ and state $i+1$. Indeed, such graphs are better off thought of as bar charts, discussed in Section 6.3.4\\n\\\\end{itemize}\\n\\nIndeed, connecting points by polylines is very often chartjunk. Trend or fit lines are often more revealing and informative. Try to show the raw data points themselves, albeit lightly and unobtrusively.\\n\\n\\\\begin{itemize}\\n  \\\\item Use color and hatching to distinguish lines/classes: Often we are faced representing the same function $f(x)$ drawn over two or more classes, perhaps income as a function of schooling, separately for men and women.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-193}\\n\\\\end{itemize}\\n\\nFigure 6.14: Smaller dots on scatter plots (left) reveal more detail than the default dot size (right).\\n\\nThese are best handled by assigning distinct colors to the line/points for each class. Line hatchings (dotted, dashed, solid, and bold) can also be used, but these are often harder to distinguish than colors, unless the output media is black and white. In practice, two to four such lines can be distinguished on a single plot, before the visual collapses into a mess. To visualize a large number of groups, partition them into logical clusters and use multiple line plots, each with few enough lines to be uncluttered.\\n\\n\\\\subsection*{6.3.3 Scatter Plots}\\nMassive data sets are a real challenge to present in an effective manner, because large numbers of points easily overwhelm graphic representations, resulting in an image of the black ball of death. But when properly drawn, scatter plots are capable of showing thousands of bivariate (two-dimensional) points in a clear understandable manner.\\n\\nScatter plots show the values of every $(x, y)$ point in a given data set. We used scatter plots in Section 4.1 to represent the body mass status of individuals\\\\index{gradient boosted decision trees} by representing them as points in height-weight space. The color of each point reflected their classification as normal, overweight, or obese. Best \\\\in\\\\index{support vector machines}dex{logistic regression}practices associated with scatter plots include:\\n\\n\\\\begin{itemize}\\n  \\\\item Scatter the right-sized dots: In the movie Oh G-d, George Burns as the creator looks back on the avocado as his biggest mistake. Why? Because he made the pit too large. Most people\\'s biggest mistake with scatter plots is that they mak\\\\index{non-linear classiï¬ers}e the points too large.\\n\\\\end{itemize}\\n\\nThe scatter plots in Figure 6.14 show the BMI distribution for over 1,000 Americans, with two different sizes of dots. Observe how the smaller dots show finer structure, because they are less likely to overlap and obscure other data points.\\n\\nNow we see a fine structure of a dense core, while still being able to detect the light halo of outliers. The default dot size for most plotting programs\\\\\\n%---- Page End Break Here ---- Page : 177\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-194}\\n\\nFigure 6.15: Overlapping dots can obscure scatter plots, particularly for large data sets. Reducing the opacity of the dots (left) shows some of the fine structure of the data (left). But a colored heatmap more dramatically reveals the distribution of the points (right).\\\\\\\\\\nis appropriate for about fifty points. But for larger data sets, use smaller dots.\\n\\n\\\\begin{itemize}\\n  \\\\item Color or jiggle integer points before scatter-plotting them: Scatter plots reveal grid patterns when the $x$ and $y$ values have integer values, because there are no smooth gradations between them. These scatter plots look unnatural, but even worse tend to obscure data because often multiple points will share exactly the same coordinates.\\\\\\\\\\nThere are two reasonable solutions. The first is to color each point based on its frequency of occurrence. Such plots are called heatmaps, and concentration centers become readily visible provided that a sensible color scale is used. Figure 6.15 shows a heatmap of height-weight data, which does a much better job of revealing point concentrations than the associated simple dot plot.\\\\\\\\\\nA related idea is to reduce the opacity (equivalently, increase the transparency) of the points we scatter plot. By default, points are generally drawn to be opaque, yielding a mass when there are overlapping points. But now suppose we permit these points to be lightly shaded and transparent. Now the overlapping points show up as darker than singletons, yielding a heatmap in multiple shades of gray.\\\\\\\\\\nThe second approach is to add a small amount of random noise to each point, to jiggle it within a sub-unit radius circle around its original position. Now we will see the full multiplicity of points, and break the distracting regularity of the grid.\\n  \\\\item Project multivariate data down to two dimensions, or use arrays of pairwise plots: Beings from our universe find it difficult to visualize data sets in four or more dimensions. Higher-dimensional data sets can often be projected down to two dimensions before rendering them on scatter plots, using techniques such as principal component analysis and self-organizing\\\\\\n%---- Page End Break Here ---- Page : 178\\n\\\\\\nmaps. A pretty example appears a few chapters ahead in Figure 11.16 where we project a hundred dimensions down to two, revealing a very coherent view of this high-dimensional data set.\\\\\\\\\\nThe good thing about such plots is that they can provide effective views of things we couldn\\'t otherwise see. The bad thing is that the two dimensions no longer mean anything. More specifically, the new dimensions do not have variable names that can co\\\\index{bar charts}nvey meaning, because each of the two \"new\" dimensions encodes properties of all the original dimensions.\\\\\\\\\\nAn alternative representation is to plot a grid or lattice of all pairwise projections, each showing just two of the original dimensions. This is a wonderful way to get a sense of which pairs of dimensions correlate with each other, as we showed in Figure 6.1\\n  \\\\item three-dimensional\\\\index{three-dimensional}-scatter plots help only when there is real structure to show: TV news stories about data science always feature some researcher gripping a three-dimensional point cloud and rotating it through space, striving for some important scientific insight. They never find it, because the view of a cloud from any given direction looks pretty much the same as the view from any other direction. There generally isn\\'t a vantage point where it suddenly becomes clear how the dimensions interact.\\\\\\\\\\nThe exception is when the data was actually derived from structured threedimensional objects, such as laser scans of a given scene. Most of the data we encounter in data science doesn\\'t fit this description, so have low expectations for interactive visualization. Use the grid of all twodimensional projections technique, which essentially views the cloud from all orthogonal directions.\\n  \\\\item bubble plots\\\\index{bubble plots} vary color and size to represent additional dimensions: Modulating the color, shape, size, and shading of dots enables dot plots to represent additional dimensions, on bubble plots. This generally works better than plotting points in three dimensions.\\\\\\\\\\nIndeed Figure 6.16 neatly shows four dimensions (GDP, life expectancy, population, and geographic region) using $x, y$, size, and color, respectively. There is much to read into such a bubble chart: it clearly reveals the correlation between GDP and health (by the straight line fit, although note that the x -values are not linearly-spaced), the new world is generally richer than the old world, and that the biggest countries (China and India) generally sit in the middle of the pack. Certain rich but sick nations are doing abysmally by their people (e.g. South Africa), while countries like Cuba and Vietnam seem to be punching above their weight.\\n\\\\end{itemize}\\n\\n\\\\subsection*{6.3.4 Bar Plots and pie charts\\\\index{pie charts}}\\nBar plots and pie charts are tools for presenting the relative proportions of categorical variables. Both work by partitioning a geometric whole, be it a\\\\\\n%---- Page End Break Here ---- Page : 179\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-196}\\n\\nFigure 6.16: Adding size and color to a scatter plot leads to natural fourdimensional visualizations, here illustrating properties of the world\\'s nations. Based on a free chart from \\\\href{http://www.gapminder.org}{\\n%---- Page End Break Here ---- Page : 180\\nhttp://www.gapminder.org}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-197}\\n\\nFigure 6.17: Voter data from three U.S. presidential elections. Bar plots and pie charts display the frequency of proportion of categorical variables. Relative magnitudes in a time series can be displayed by modulating the area of the line or circle.\\\\\\\\\\nbar or a circle, into areas proportional to the frequency of each group. Both elements are effective in multiples, to enable comparisons. Indeed, partitioning each bar into pieces yields the stacked\\\\index{stacked} bar chart.\\n\\nFigure 6.17 shows voter data from three years of U.S. presidential elections, presented as both pie and bar charts. The blue represents Democratic votes, the red Republican votes. The pies more clearly shows which side won each elections, but the bars show the Republican vote totals have stayed fairly constant while the Democrats were generally growing. Observe that these bars can be easily compared because they are left-justified.\\n\\nCertain critics get whipped into an almost religious fever against pie charts, because they take up more space than necessary and are generally harder to read and compare. But pie charts are arguably better for showi\\\\index{AlphaGo}ng percentages of totality. Many people seem to like them, so they are probably harmless in s\\\\index{supervi\\\\ind\\\\index{unsupervised learning}ex{su\\\\index{degrees of}pervision}sed learning}mall quantities. Best practices for bar plots and pie charts include:\\n\\n\\\\begin{itemize}\\n  \\\\item Directly label slices of the pie: Pie charts are of\\\\index{reinforcement learning}ten accompanied by legend keys labeling what each color slice corresponds to. This is very \\\\index{position evaluation function}distracting, because your eyes must move back and forth between the key and the pie to interpret this.\\\\\\\\\\nMuch better is to label each slice directly, inside the slice or just beyond the rim. This has a secondary benefit of discouraging the use of too many slices, because slivers generally become uninterpretable. It helps to group the slivers into a single slice called other, and then perhaps present a\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-198}\\n\\\\end{itemize}\\n\\nFigure 6.18: Small multiple bar plots/tables are excellent ways to represent multivariate data for comparison.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-198(1)}\\n\\nFigure 6.19: Stacked bar charts illustrating the survivorship rate on the doomed ship Titanic, by ticket class. The histogram (left) informs us of the size of each class, but scaled bars (right) better capture proportions. Primary conclusion: you were better off not to be traveling steerage (third class).\\\\\\\\\\nsecond pie chart decomposed into the major other components.\\n\\n\\\\begin{itemize}\\n  \\\\item Use bar charts to enable precise comparisons: When anchored on a fixed line, arrays of bars make it easy to identify the minimum and maximum values in a series, and whether a trend is increasing or decreasing.\\\\\\\\\\nStacked bar charts are concise, but harder to use for such purposes. Presenting an array of small bar charts, here one for each gender/ethnic group, empowers us to make such fine comparisons, as shown in Figure 6.18\\n  \\\\item Scale appropriately, depending upon whether you seek to highlight absolute magnitude or proportion: Pie charts exist to represent fractions of the whole. In presenting a series of pie or bar charts, your most critical decision is whether you want to show the size of the whole, or instead the fractions of each subgroup.\\n\\\\end{itemize}\\n\\nFigure 6.19 shows two stacked bar charts presenting survivorship statistics\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-199}\\n\\nFigure 6.20: Pie charts of delegates to the 2016 Republican convention by candidate. Which one is better and why?\\\\\\\\\\non the doomed ship Titanic, reported by ticket class. The histogram (left) precisely records the sizes of each class and the resulting outcomes. The chart with equal length bars (right) better captures how the mortality rate increased for lower classes.\\\\\\\\\\nPie charts can also be used to show changes in magnitude, by varying the area of the circle defining the pie. But it is harder for the eye to calculate area than length, making comparisons difficult. Modulating the radius to reflect magnitude instead of area is even more deceptive, because doubling the radius of a circle multiplies the area by four.\\n\\n\\\\section*{Bad Pie Charts}\\nFigure 6.20 shows two pie charts reporting the distribution of delegates to the 2016 Republican convention, by candidate. The left pie is two-dimensional, while the chart on right has thick slices neatly separated to show off this depth. Which one is better at conveying the distribution of votes?\\n\\nIt should be clear that the three-dimensional effects and separation are pure chartjunk, that only obscures the relationship between the size of the slices. The actual data values disappeared as well, perhaps because there wasn\\'t enough space left for them after all those shadows. But why do we need a pie chart at all? A little table of labels/colors with an extra column with percentages would be more concise and informative.\\n\\n\\\\subsection*{6.3.5 Histograms}\\nThe interesting properties of variables or features are defined by their underlying frequency distribution. Where is the peak of the distribution, and is the mode near the mean? Is the distribution symmetric or skewed? Where are the tails? Might it be bimodal, suggesting that the distribution is drawn from a mix of two or more underlying populations?\\\\\\n%---- Page End Break Here ---- Page : 183\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-200}\\n\\nFigure 6.21: Time series of vote totals in U.S. presidential elections by party enable us to see the changes in magnitude and distribution. Democrats are shown in blue, and Republicans in red. It is hard to visualize changes, particularly in the middle layers of the stack.\\n\\nOften we are faced with a large number of observations of a particular variable, and seek to plot a representation for them. Histograms are plots of the observed frequency distributions. When the variable is defined over a large range of possible values relative to the $n$ observations, it is unlikely we will ever see any exact duplication. However, by partitioning the value range into an appropriate number of equal-width bins, we can accumulate different counts per bin, and approximate the underlying probability distribution.\\n\\nThe biggest issue in building a histogram is deciding on the right number of bins to use. Too many bins, and there will be only a few points in even the most popular bucket. We turned to binning to solve exactly this problem in the first place. But use too few bins, and you won\\'t see enough detail to understand the shape of the distribution.\\n\\nFigure 6.22 illustrates the consequences of bin size on the appearance of a histogram. The plots in the top row bin 100,000 points from a normal distribution into ten, twenty, and fifty buckets respectively. There are enough points to fill fifty buckets, and the distribution on right looks beautiful. The plots in the bottom row bin have only 100 points, so the thirty-bucket plot on right is sparse and scraggy. Here seven bins (shown on the left) seems to produce the most representative plot.\\n\\nIt is impossible to giv\\\\index{Z-score}e hard and fast rules to select the best bin count $b$ for showing off your data. Realize you will never be able to discrimin\\\\index{dimension reduction}ate between more than a hundred bins by eye, so this provides a logical upper \\\\index{data cleaning}bound. In general, I like to see an average of 10 to 50 points per bin to make things smooth, so $b=\\\\lceil n / 25\\\\rceil$ gives a reasonable first guess. But experime\\\\index{regularization}nt with different values of $b$, because the right bin count will work much better than the others. You will know it \\\\index{missing values}when you see it.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf\\\\index{normalization}8e028g-201}\\n\\nFigure 6.22: Histograms of a given distribution can look wildly different depending on the number of bins. A large data set benefits from many bins (top). But the structure of a smaller data set is best shown when each bin a non-trivial number of element.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-202(1)}\\n\\nFigure 6.23: Dividing counts by the total yields a probability density plot, which is more generally interpretable even though the shapes are identical.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-202}\\n\\nFigure 6.24: Generally speaking, histograms are better for displaying peaks in a distribution, but cdfs are better for showing tails.\\n\\nBest practices for histograms include:\\n\\n\\\\begin{itemize}\\n  \\\\item Turn your histogram into a pdf: Typically, we interpret our data as observations that approximate the probability density function (pdf) of an underlying random variable. If so, it becomes more interpretable to label the $y$-axis by the fraction of elements in each bucket, instead of the total count. This is particularly true in large data sets, where the buckets are full enough that we are unconcerned about the exact level of support.\\\\\\\\\\nFigure 6.23 shows the same data, plotted on the right as a pdf instead of a histogram. The shape is exactly the same in both plots: all that changes is the label on the $y$-axis. Yet the result is easier to interpret, because it is in terms of probabilities instead of counts.\\n  \\\\item Consider the cdf: The cumulative density function (cdf) is the integral of the pdf, and the two functions contain exactly the same information. So consider using a cdf instead of a histogram to represent your distribution,\\\\\\n%---- Page End Break Here ---- Page : 186\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-203}\\n\\\\end{itemize}\\n\\nFigure 6.25: Maps summarizing the results of the 2012 U.S. presidential election. The recipient of each state\\'s electoral votes is effectively presented on a data map (left), while a cartogram making the area of each state proportional to the number of votes (right) better captures the magnitude of Obama\\'s victory. Source: Images from Wikipedia.\\\\\\\\\\nas shown in Figure 6.24\\\\\\\\\\nOne great thing about plotting the cdf is that it does not rely on a bin count parameter, so it presents a true, unadulterated view of your data. Recall how great the cdfs looked in the Kolmogorov-Smirnov test, Figure 5.13. We draw a cdf as a line \\\\index{rectiï¬ed\\\\index{rectiï¬er} linear units}plot with $n+2$ points for $n$ observations. The first and last points are $\\\\left(x_{\\\\min }-\\\\epsilon, 0\\\\right)$ and $\\\\left(x_{\\\\max }+\\\\epsilon, 1\\\\right)$. We then sort the observations to yield $S=\\\\left\\\\{s_{1}, \\\\ldots, s_{n}\\\\right\\\\}$, and plot $\\\\left(s_{i}, i / n\\\\right)$ for all $i$.\\n\\nCumulative distributions require slightly more sophistication to read than histograms. The cdf is monotonically increasing, so there are no peaks in the distribution. Instead, the mode is marked by the longest vertical line segment. But cdfs are much better at highlighting the tails of a distribution. The reason is clear: the small counts at the tails are obscured by the axis on a histogram, but accumulate into visible stuff in the cdf.\\n\\n\\\\subsection*{6.3.6 Data Maps}\\nMaps use the spatial arrangement of regions to represent places, concepts, or things. We have all mastered skills for navigating the world through maps, skills which translate into understanding related visualizations.\\n\\nTraditional data maps use color or shading to highlight properties of the regions in the map. Figure 6.25 (left) colors each state according to whether they voted for Barack Obama (blue) or his opponent Mitt Romney (red) in the 2012 U.S. presidential election. The map makes clear the country\\'s political divisions. The northeast and west coasts are as solidly blue as the Midwest and south are red.\\\\\\n%---- Page End Break Here ---- Page : 187\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-204}\\n\\nFigure 6.26: The periodic table\\\\index{periodic table} maps the elements into logical groupings, reflecting chemical properties. Source: \\\\href{http://sciencenotes.org}{http://sciencenotes.org}\\n\\nMaps are not limited to geographic regions. The most powerful map in the history of scientific visualization is chemistry\\'s periodic table of the elements. Connected regions show where the metals and Nobel gases reside, as well as spots where undiscovered elements had to lie. The periodic table is a map with enough detail to be repeatedly referenced by working chemists, yet is easily understood by school children.\\n\\nWhat gives the periodic table such power as a visualization?\\n\\n\\\\begin{itemize}\\n  \\\\item The map has a story to tell: Maps are valuable when they encode information worth referencing or assimilating. The periodic table is the right visualization of the elements because of the structure of electron shells, and their importance on chemical properties and bonding. The map bears repeated scrutiny because it has important things to tell us.\\n\\\\end{itemize}\\n\\nData maps are fascinating, because breaking variables down by region so often leads to interesting stories. Regions on maps generally reflect cultural, historical, economic, and linguistic continuities, so phenomena deriving from any of these factors generally show themselves clearly on data maps.\\n\\n\\\\begin{itemize}\\n  \\\\item Regions are contiguous, and adjacency means something: The continuity of regions in Figure 6.26 is reflected by its color scheme, grouping elements sharing similar properties, like the alkali metals and the Nobel gases. Two elements sitting next to each other usually means that they share something important in common.\\n \\n%---- Page End Break Here ---- Page : 188\\n \\\\item The squares are big enough to see: A critical decision in canonizing the periodic table was the placement of the Lanthanide (elements 57-71) and Actinide metals (elements 89-103). They are conventionally presented as the bottom two rows, but logically belong within the body of the table in the unlabeled green squares.\\\\\\\\\\nHowever, the conventional rendering avoids two problems. To do it \"right,\" these elements would either be compressed into hopelessly thin slivers, or else the table would need to become twice as wide to accommodate them.\\n  \\\\item It is not too faithful to reality: Improving reality for the sake of better maps is a long and honorable tradition. Recall that the Mercator projection distorts the size of landmasses near the poles (yes, Greenland, I am looking at you) in order to preserve their shape.\\n\\\\end{itemize}\\n\\ncartograms\\\\index{cartograms} are maps distorted such that regions reflect some underlying variable, like population. Figure 6.25 (right) plots the electoral results of 2012 on a map where the area of each state is proportional to its population/electoral votes. Only now does the magnitude of Obama\\'s victory become clear: those giant red Midwestern states shrink down to an appropriate size, yielding a map of more blue than red.\\n\\n\\\\subsection*{6.4 Great Visualizations}\\nDeveloping your own visualization aesthetic gives you a language to talk about what you like and what you don\\'t like. I now encourage you to apply your judgment to assess the merits and demerits of c\\\\index{stochastic gradient descent}ertain charts and graphs.\\n\\nIn this section, we will look at a select group of classic visualizations which I consider great. There are a large number of terrible graphics I would love to contrast them against, but I was taught that it is not nice to make fun of people.\\n\\nThis is especially true when the\\\\index{backpropagation}re are copyright restrictions over the use of such images in a book like this. However, I strongly encourage you to visit \\\\href{http://wtfviz.net/to}{http://wtfviz.net/to} see a collection of startling charts and graphics. Many are quite amusing, for reasons you should now be able to articulate using the ideas in this chapter.\\n\\n\\\\subsection*{6.4.1 Marey\\'s Train Schedule}\\nTufte points to E.J. Marey\\'s railroad schedule as a landmark in graphical design. It is shown in Figure 6.27. The hours of the day are represented on the $x$-axis. Indeed, this rectangular plot is really a cylinder cut at 6 AM and laid down flat. The $y$-axis represents all the stations on the Paris to Lyon line. Each line represents the route of a particular train, reporting where it should be at each moment in time.\\n\\nNormal train schedules are tables, with a column for each train, a row for each station, and entry $(i, j)$ reporting the time train $j$ arrives at station $i$. Such tables are useful to tell us what time to arrive to catch our train. But Marey\\'s\\\\\\n%---- Page End Break Here ---- Page : 189\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-206}\\n\\nFigure 6.27: Marey\\'s train schedule plots the position of each train as a function of time.\\\\\\\\\\ndesign provides much more information. What else can you see here that you cannot with conventional time tables?\\n\\n\\\\begin{itemize}\\n  \\\\item How fast is the train moving? The slope of a line measures how steep it is. The faster the train, the greater the absolute slope. Slower trains are marked by flatter lines, because they take more time to cover the given ground.\\\\\\\\\\nA special case here is identifying periods when the train is idling in the station. At these times, the line is horizontal, indicating no movement down the line.\\n  \\\\item When do trains pass each other? The direction of a train is given by the slope of the associated line. Northbound trains have a positive slope, and southbound trains a negative slope. Two trains pass each other at the intersection points of two lines, letting passengers know when to look out the window and wave.\\n  \\\\item When is rush hour? There is a concentration of trains leaving both Paris and Lyon around 7PM, which tells me that must have been the most popular time to travel. The trip typically took around eleven hours, so this must have been a sleeper train, where travelers would arrive at their destination bright and early the next day.\\n\\\\end{itemi\\\\index{word2vec}ze}\\n\\nThe departure times for trains at a station are also there, of course. Each station is marked by a horizontal line, so look for the time when trains cross your station in the proper direction.\\\\\\n%---- Page End Break Her\\\\index{graph embeddings}e ---- Page : 1\\\\index{dimensions}90\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-207}\\n\\nFigure 6.28: Cholera deaths center around a pump on Broad street, revealing the source of the epidemic.\\n\\nMy only quibble here is that it would have been even better with a lighter data grid. Never imprison your data!\\n\\n\\\\subsection*{6.4.2 Snow\\'s Cholera Map}\\nA particularly famous data map changed the course of medical history. Cholera was a terrible disease which killed large numbers of people in 19th-century cities. The plague would come suddenly and strike people dead, with the cause a mystery to the science of the day.\\n\\nJohn Snow plotted cholera cases from an epidemic of 1854 on a street map of London, hoping to see a pattern. Each dot in Figure 6.28 represented a household struck with the disease. What do you see?\\n\\nSnow noticed a cluster of the cases centered on Broad Street. Further, at the center of the cluster was a cross, denoting a well where the residents got their drinking water. The source of the epidemic was traced to the handle of a single water pump. They changed the handle, and suddenly people stopped getting sick. This proved that cholera was an infectious disease caused by contaminated water, and pointed the way towards preventing it.\\\\\\n%---- Page End Break Here ---- Page : 191\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-208}\\n\\nFigure 6.29: This weather year in review displays a clear story in over 2000 numbers.\\n\\n\\\\subsection*{6.4.3 New York\\'s Weather Year}\\nIt is almost worth enduring a New York winter to see a particular graphic which appears in The New York Times each January, summarizing the weather of the previous year. Figure 6.29 presents an independent rendition of the same data, which captures why this chart is exciting. For every day of the year, we see the high and low temperature plotted on a graph, along with historical data to put it in context: the average daily high and low, as well as the highest/lowest temperatures ever recorded for that date.\\n\\nWhat is so great about that? First, it shows $6 \\\\times 365=2190$ numbers in a coherent manner, which facilitates comparisons on the sine curve of the seasons:\\n\\n\\\\begin{itemize}\\n  \\\\item We can tell when there were hot and cold spells, and how long they lasted.\\n  \\\\item We can tell what days had big swings in temperature, and when the thermometer hardly moved at all.\\n  \\\\item We can tell whether the weather was unusual this year. Was it an unusually hot or cold year, or both? When were record high/lows set, and when did they get close to setting them?\\n\\\\end{itemize}\\n\\nThis single graphic is rich, clear, and informative. Be inspired by it.\\n\\n\\\\subsection*{6.5 Reading Graphs}\\nWhat you see isn\\'t always what you get. I have seen many graphs brought to me by my students over the years. Some have been amazing, and most others good enough to get the job done.\\\\\\n%---- Page End Break Here ---- Page : 192\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-209}\\n\\nFigure 6.30: A dotplot of word frequency by rank. What do you see?\\n\\nBut I also repeatedly see plots with the same basic problems. In this section, for a few of my pet peeves, I present the original plot along with the way to remedy the problem. With experience, you should be able to identify these kinds of problems just by looking at the initial plot.\\n\\n\\\\subsection*{6.5.1 The Obscured Distribution}\\nFigure 6.30 portrays the frequency of 10,000 English words, sorted by frequency. It doesn\\'t look very exciting: all you can see is a single point at $(1,2.5)$. What happened, and how can you fix it?\\n\\nIf you stare at the figure long enough, you will see there are actually a lot more points. But they all sit on the line $y=0$, and overlap each other to the extent that they form an undifferentiated mass.\\n\\nThe alert reader will realize that this single point is in fact an outlier, with magnitude so large as to shrink all other totals toward zero. A natural reaction would delete the biggest point, but curiously the remaining points will look much the same.\\n\\nThe problem is that this distribution is a power law, and plotting a power law on a linear scale shows nothing. The key here is to plot it on log scale, as in Figure 6.31 (left). Now you can see the points, and the mapping from ranks to frequency. Even better is plotting it on a log-log scale, as in Figure 6.31 (right). The straight line here confirms that we are dealing with a power law.\\n\\n\\\\subsection*{6.5.2 Overinterpreting Variance}\\nIn bioinformatics, one seeks to discover how life works by looking at data. Figure 6.32 (left) presents a graph of the folding energy of genes as a function of their length. Look at this graph, and see if you can make a discovery.\\n\\nIt is pretty clear that something is going on in there. For gene lengths above 1500 , the plot starts jumping around, producing some very negative values. Did\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-210}\\n\\nFigure 6.31: Frequency of 10,000 English words. Plotting the word frequencies on a $\\\\log$ scale (left), or even better on a log-log scale reveals that it is power law (right).\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-210(1)}\\n\\nFigure 6.32: Folding energy of genes as a function of their length. Mistaking variance for signal: the extreme values in the left figure are artifacts from averaging small numbers of samples.\\\\\\\\\\nwe just discover that energy varies inversely with gene length?\\\\\\\\\\nNo. We just overinterpreted variance. The first clue is that what looked very steady starts to go haywire as the length increases. Most genes are very short in length. Thus the points on the right side of the left plot are based on very little data. The average of a few points is not as robust as the average of many points. Indeed, a frequency plot of the number of genes by length (on right) shows that the counts drop off to nothing right where it starts jumping.\\n\\nHow could we fix it? The right thing to do here is to threshold the plot by only showing the values with enough data support, perhaps at length 500 or a bit more. Beyond that, we might bin them by length and take the average, in order to prove that the jumping effect goes away.\\n\\n\\\\subsection*{6.6 Interactive Visualization}\\nThe charts and graphs we have discussed so far were all static images, designed to be studied by the viewer, but not manipulated. Interactive visualization techniques are becoming increasingly important for exploratory data analysis.\\n\\nMobile apps, notebooks, and webpages with interactive visualization widgets can be especially effective in presenting data, and disseminating it to larger audiences to explore. Providing viewers with the power to play with the actual data helps ensure that the story presented by a graphic is the true and complete story.\\n\\nIf you are going to view your data online, it makes sense to do so using interactive widgets. These are generally extensions of the basic plots that we have described in this chapter, with features like offering pop-ups with more information when the user scrolls over points, or encouraging the user to change the scale ranges with sliders.\\n\\nThere are few potential downsides to interactive visualization. First, it is harder to communicate exactly what you are seeing to others, compared with static images. They might not be seeing exactly the same thing as you. Screenshots of interactive systems generally cannot compare to publication-quality graphics optimized on traditional systems. The problem with what you see is what you get (WYSIWYG) systems is that, generally, what you see is all you get. Interactive systems are best for exploration, not presentation.\\n\\nThere are also excesses which tend to arise in interactive visualization. Knobs and features get added because they can, but the visual effects they add can distract rather than add to the message. Rotating three-dimensional point clouds always looks cool, but I find them hard to interpret and very seldom find such views insightful.\\n\\nMaking data tell a story requires some insight into how to tell stories. Films and television represent the state-of-the-art in narrative presentation. Like them, the best interactive presentations show a narrative, such as moving through time or rifling through alternative hypotheses. For inspiration, I encourage you to watch the late Han Rosling\\'s TED talk $\\\\square$ using animated bubble\\n\\n\\\\footnotetext{1 \\\\href{https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen}{https://www.ted.com/talks/hans\\\\_rosling\\\\_shows\\\\_the\\\\_best\\\\_stats\\\\_you\\\\_ve\\\\_ever\\\\_seen}\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-212}\\n\\nFigure 6.33: The TextMap dashboard for Barack Obama, circa 2008.\\\\\\\\\\ncharts to present the history of social and economic development for all the world\\'s nations.\\n\\nRecent trends in cloud-based visualization encourage you to upload your data to a site like Google Charts \\\\href{https://developers.google.com/chart/}{https://developers.google.com/chart/}, so you can take advantage of interactive visualization tools and widgets that they provide. These tools produce very nice interactive plots, and are easy to use.\\n\\nThe possible sticking point is security, since you are giving your data to a third party. I hope and trust that the CIA analysts have access to their own inhouse solutions which keep their data confidential. But feel free to experiment with these tools in less sensitive situations.\\n\\n\\\\subsection*{6.7 War Story: TextMapping the World}\\nMy biggest experience in data visualization came as a result of our large scale news/sentiment analysis system. TextMap presented a news analysis dashboard, for every entity whose name appeared in news articles of the time. Barack Obama\\'s page is shown in Figure 6.33. Our dashboard was made up of a variety of subcomponents:\\n\\n\\\\begin{itemize}\\n  \\\\item Reference time series:\\\\index{unrepresentative participation} Here we reported how often the given entity appeared in the news, as a function of time. Spikes corresponded to more important news events. Further, we partitioned these counts according\\\\\\n%---- Page End B\\\\index\\\\index{spam ï¬ltering}{spam}reak Here ---- Page : 196\\n\\\\\\nto appearanc\\\\index{redundancy}es in each section of the newspaper: news, sports, entertainment, or business.\\n  \\\\item News sector distribution: This graph contains exactly the same data as in the reference time series, but presented as a stacked area plot to show the fraction of entity references in each section of the paper. Obama clearly presents as a news figure. But other people exhibited interesting transitions, such as Arnold Schwarzenegger when he shifted from acting to politics.\\n  \\\\item Sentiment analysis: Here we present a time series measuring the sentiment of the entity, by presenting the normalized difference of the number of positive news mentions to total references. Thus zero represented a neutral reputation, and we provided a central reference line to put the placement into perspective. Here Obama\\'s sentiment ebbs and flows with events, but generally stays on the right side of the line.\\\\\\\\\\nIt was nice to see bad things happen to bad people, by watching their news sentiment drop. As I recall, the lowest news sentiment ever achieved was by a mom who achieved notoriety by cyberbullying one of her daughter\\'s social rivals into committing suicide.\\n  \\\\item Heatmap: Here we presented a data map showing the relative reference frequency of the entity. Obama\\'s great red patch around Illinois is because he was serving as a senator from there at the time he first ran for president. Many classes entities showed strong regional biases, like sports figures and politicians, but less so entertainment figures like movie stars and singers.\\n  \\\\item Juxtapositions and relational network: Our system built a network on news entities, by linking two entities whenever they were mentioned in the same article. The strength of this relationship could be measured by the number of articles linking them together. The strongest of these associations are reported as juxtapositions, with their frequency and strength shown by a bar chart on a logarithmic scale.\\\\\\\\\\nWe have not really talked so far about network visualization, but the key idea is to position vertices so neighbors are located near each other, meaning the edges are short. Stronger friendships are shown by thicker lines.\\n  \\\\item Related articles: We provided links to representative news articles mentioning the given entity.\\n\\\\end{itemize}\\n\\nOne deficiency of our dashboard was that it was not interactive. In fact, it was anti-interactive: the only animation was a gif of the world spinning forlornly in the upper-left corner. Many of the plots we rendered required large amounts of data access from our clunky database, particularly the heatmap. Since it could not be rendered interactively, we precomputed these maps offline and showed them on demand.\\n\\n%---- Page End Break Here ---- Page : 197\\n\\nAfter Mikhail updated our infrastructure (see the war story of Section 12.2, it became possible for us to support some interactive plots, particularly time series. We developed a new user interface called TextMap Access that let users play with our data.\\n\\nBut when General Sentiment licensed the technology, the first thing it disposed of was our user interface. It didn\\'t make sense to the business analysts who were our customers. It was too complicated. There is a substantial difference between surface appeal and the real effectiveness of an interface for users. Look carefully at our TextMap dashboard: it had \"What is this?\" buttons in several locations. This was a sign of weakness: if our graphics were really intuitive enough we would not have needed them.\\n\\nAlthough I grumbled about the new interface, I was undoubtedly wrong. General Sentiment employed a bunch of analysts who used our system all day long, and were available to talk to our developers. Presumably the interface evolved to serve them better. The best interfaces are built by a dialog between developers and users.\\n\\nWhat are the take-home lessons from this experience? There is a lot I still find cool about this dashboard: the presentation was rich enough to really expose interesting things about how the world worked. But not everyone agreed. Different customers preferred different interfaces, because they did different things with them.\\n\\nOne lesson here is the power of providing alternate views of the same data. The reference time series and news sector distribution were exactly the same data, but provided quite different insights when presented in different ways. All songs are made from the same notes, but how you arrange them makes for different kinds of music.\\n\\n\\\\subsection*{6.8 Chapter Notes}\\nI strongly recommend Edward Tufte\\'s books Tuf83, Tuf90, Tuf97 to anyone interested in the art of scientific visualization. You don\\'t even have to read them. Just look at the pictures for effective contrasts between good and bad graphics, and plots that really convey stories in data.\\n\\nSimilarly good books on data visualization are Few [Few09] and far between. Interesting blogs about data visualization are \\\\href{http://flowingdata.com}{http://flowingdata.com} and \\\\href{http://viz.wtf}{http://viz.wtf}. The first focuses on great visualizations, the second seeking out the disasters. The story of Snow\\'s cholera map is reported in Johnson Joh07.\\n\\nThe chartjunk removal example from Section 6.2.3 was inspired by an example by Tim Bray at \\\\href{http://www.tbray.org}{http://www.tbray.org}. Anscombe\\'s quartet was first presented in Ans73. The basic architecture of our Lydia/TextMap news analysis system is reported in Lloyd et al. LKS05], with additional papers describing heatmaps [MBL $\\\\left.{ }^{+} 06\\\\right]$ and the Access user interface [BWPS10].\\n\\n\\\\subsection*{6.9 exercises\\\\index{exercises}}\\n\\\\section*{Exploratory Data Analysis}\\n6-1. [5] Provide answers to the questions associated with the following data sets, available at \\\\href{http://www.data-manual.com/data}{http://www.data-manual.com/data}.\\\\\\\\\\n(a) Analyze the movie data set. What is the range of movie gross in the United States? Which type of movies are most likely to succeed in the market? Comedy? PG-13? Drama?\\\\\\\\\\n(b) Analyze the Manhattan rolling sales data set. Where in Manhattan is the most/least expensive real estate located? What is the relationship between sales price and gross square feet?\\\\\\\\\\n(c) Analyze the 2012 Olympic data set. What can you say about the relationship between a country\\'s population and the number of medals it wins? What can you say about the relationship between the ratio of female and male counts and the GDP of that country?\\\\\\\\\\n(d) Analyze the GDP per capita data set. How do countries from Europe, Asia, and Africa compare in the rates of growth in GDP? When have countries faced substantial changes in GDP, and what historical events were likely most responsible for it?\\n\\n6-2. [3] For one or more of the data sets from \\\\href{http://www.data-manual.com/data}{http://www.data-manual.com/data}, answer the following basic questions:\\\\\\\\\\n(a) Who constructed it, when, and why?\\\\\\\\\\n(b) How big is it?\\\\\\\\\\n(c) What do the fields mean?\\\\\\\\\\n(d) Identify a few familiar or interpretable records.\\\\\\\\\\n(e) Provide Tukey\\'s five number summary for each column.\\\\\\\\\\n(f) Construct a pairwise correlation matrix for each pair of columns.\\\\\\\\\\n(g) Construct a pairwise distribution plot for each interesting pair of columns.\\n\\n\\\\section*{Interpreting Visualizations}\\n6-3. [5] Search your favorite news websites until you find ten interesting charts/plots, ideally half good and half bad. For each, please critique along the following dimensions, using the vocabulary we have developed in this chapter:\\\\\\\\\\n(a) Does it do a good job or a bad job of presenting the data?\\\\\\\\\\n(b) Does the presentation appear to be biased, either deliberately or accidentally?\\\\\\\\\\n(c) Is there chartjunk in the figure?\\\\\\\\\\n(d) Are the axes labeled in a clear and informative way?\\\\\\\\\\n(e) Is the color used effectively?\\\\\\\\\\n(f) How can we make the graphic better?\\n\\n%---- Page End Break Here ---- Page : 199\\n\\n6-4. [3] Visit \\\\href{http://www.wtfviz.net}{http://www.wtfviz.net}. Find five laughably bad visualizations, and explain why they are both bad and amusing.\\n\\n\\\\section*{Creating Visualizations}\\n6-5. [5] Construct a revealing visualization of some aspect of your favorite data set, using:\\\\\\\\\\n(a) A well-designed table.\\\\\\\\\\n(b) A dot and/or line plot.\\\\\\\\\\n(c) A scatter plot.\\\\\\\\\\n(d) A heatmap.\\\\\\\\\\n(e) A bar plot or pie chart.\\\\\\\\\\n(f) A histogram.\\\\\\\\\\n(g) A data map.\\n\\n6-6. [5] Create ten different versions of line charts for a particular set of $(x, y)$ points. Which ones are best and which ones worst? Explain why.\\\\\\\\\\n$6-7$. [3] Construct scatter plots for sets of $10,100,1000$, and 10,000 points. Experiment with the point size to find the most revealing value for each data set.\\n\\n6-8. [5] Experiment with different color scales to construct scatter plots for a particular set of $(x, y, z)$ points, where color is used to represent the $z$ dimension. Which color schemes work best? Which are the worst? Explain why.\\n\\n\\\\section*{Implementation Projects}\\n6-9. [5] Build an interactive exploration widget for your favorite data set, using appropriate libraries and tools. Start simple, but be as creative as you want to be.\\n\\n6-10. [5] Create a data video/movie, by recording/filming an interactive data exploration. It should not be long, but how interesting/revealing can you make it?\\n\\n\\\\section*{Interview Questions}\\n6-11. [3] Describe some good practices in data visualization?\\\\\\\\[0pt]\\n6-12. [5] Explain Tufte\\'s concept of chart junk.\\\\\\\\\\n$6-13$. [8] How would you to determine whether the statistics published in an article are either wrong or presented to support a biased view?\\n\\n\\\\section*{Kaggle Challenges}\\n6-14. Analyze data from San Francisco Bay Area bike sharing. \\\\href{https://www.kaggle.com/benhamner/sf-bay-area-bike-share}{https://www.kaggle.com/benhamner/sf-bay-area-bike-share}\\\\\\\\\\n$6-15$. Predict whether West Nile virus is present in a given time and place.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/predict-west-nile-virus}{https://www.kaggle.com/c/predict-west-nile-virus}\\\\\\\\\\n$6-16$. What type of crime\\\\index{duplicate removal} is most likely at a given time an\\\\index{uninvertible}d place?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/sf-crime}{https://www.kaggl\\\\index{frequency counting}e.com/c/sf-crime}\\n\\n\\\\section*{Chapter 7}\\n\\\\section*{Mathematical Models}\\n\\\\begin{displayquote}\\nAll\\\\index{cryptographic hashing} models are wrong, but some models are u\\\\index{canonical\\\\index{canonization} representation}seful. $$ - \\\\text { George Box } $$\\n\\\\end{displayquote}\\n\\nSo far in this book, a variety of tools have been developed to manipulate and interpret data. But we haven\\'t really dealt with modeling\\\\index{modeling}, which is the process of encapsulating information into a tool which can forecast and make predictions.\\n\\nPredictive models are structured around some idea of what causes future events to happen. Extrapolating from recent trends and observations assumes a world view that the future will be like the past. More sophisticated models, such as the laws of physics, provide principled notions of causation; fundamental explanations of why things happen.\\n\\nThis chapter will concentrate on designing and validating models. Effectively formulating models requires a detailed understanding of the space of possible choices.\\n\\nAccurately evaluating the performance of a model can be surprisingly hard, but it is essential for knowing how to interpret the resulting predictions. The best forecasting system is not necessarily the\\\\index{Occamâs razor} most accurate one, but the model with the best sense\\\\index{Box, George} of its boundaries and limitations.\\n\\n\\\\subsection*{7.1 philosophies of\\\\index{philosophies of} Modeling}\\nEngineers and scientists are often leery of the p-word (philosophy). But it pays to think in some fundamental way about what we are trying to do, and why. Recall that people turn to data scientists for wisdom, instead of programs.\\n\\nIn this section, we will turn to different ways of thinking about models to help shape the way we build them.\\n\\n\\\\subsection*{7.1.1 Occam\\'s Razor}\\nOccam\\'s razor is the philosophical principle that \"the simplest explanation is\\\\\\\\\\nthe best explanation.\" According to William of Occam, a 13th-century theologian, given two models or theories which do an equally accurate job of making predictions, we should opt for the simpler one as sounder and more robust. It is more likely to be making the right decision for the right reasons.\\n\\nOccam\\'s notion of simpler generally refers to reducing the number of assumptions employed in developing the model. With respect to statistical modeling, Occam\\'s razor speaks to the need to minimize the parameter count of a model. Overfitting occurs when a model tries too hard to achieve accurate performance on its training data. This happens when there are so many parameters that the model can essentially memorize its training set, instead of generalizing appropriately to minimize the effects of error and outliers.\\n\\nOverfit models tend to perform extremely well on training data, but much less accurately on independent test data. Invoking Occam\\'s razor requires that we have a meaningful way to evaluate how accurately our models are performing.\\n\\nSimplicity is not an absolute virtue, when it leads to poor performance. Deep learning is a powerful technique for building models with millions of parameters, which we will discuss in Section 11.6 Despite the danger of overfitting, these models perform extremely well on a variety of complex tasks. Occam would have been suspicious of such models, but come to accept those that have substantially more predictive power than the alternatives.\\n\\nAppreciate the inherent trade-off between accuracy and simplicity. It is almost always possible to \"improve\" the performance of any model by kludgingon extra parameters and rules to govern exceptions. Complexity has a cost, as explicitly captured in machine learning methods like LASSO/ridge regression. These techniques employ penalty functions to minimize the features used in the model.\\n\\nTake-Home Lesson: Accuracy is not the best metric to use in judging the quality of a model. Simpler models tend to be more robust and understandable than complicated alternatives. Improved performance on specific tests is often more attributable to variance or overfitting than insight.\\n\\n\\\\subsection*{7.1.2 Bias-Variance Trade-Offs}\\nThis tension between model complexity and performance shows up in the statistical notion of the bias-variance trade-off:\\n\\n\\\\begin{itemize}\\n  \\\\item Bias is error from incorrect assumptions built into the model, such as restricting an interpolating function to be linear instead of a higher-order curve.\\n  \\\\item Variance is error from sensitivity to fluctuations in the training set. If our training set contains sampling or measurement error, this noise introduces variance into the resulting model.\\n\\\\end{itemize}\\n\\nErrors of bias produce underfit models. They do not fit the training data as tightly as possible, were they allowed the freedom to do so. In popular discourse,\\n\\n%---- Page End Break Here ---- Page : 202\\n\\nI associate the word \"bias\" with prejudice, and the correspondence is fairly apt: an apriori assumption that one group is inferior to another will result in less accurate predictions than an unbiased one. Models that perform lousy on both training and testing data are underfit.\\n\\nErrors of variance result in overfit models: their quest for accuracy causes them to mistake noise for signal, and they adjust so well to the training data that noise leads them astray. Models that do much better on testing data than training data are overfit ${ }^{1}$\\n\\nTake-Home Lesson: Models based on first principles or assumptions are likely to suffer from bias, while data-driven models are in greater danger of overfitting.\\n\\n\\\\subsection*{7.1.3 What Would Nate Silver Do?}\\nNate Silver is perhaps the most prominent public face of data science today. A quantitative fellow who left a management consulting job to develop baseball forecasting methods, he rose to fame through his election forecast website http: \\\\href{//www.fivethirtyeight.com}{//www.fivethirtyeight.com}. Here he used quantitative methods to analyze poll results to predict the results of U.S. presidential elections. In the 2008 election, he accurately called the winner of 49 of the 50 states, and improved in 2012 to bag 50 out of 50 . The results of the 2016 election proved a shock to just about everyone, but alone among public commentators Nate Silver had identified a substantial chance of Trump winning the electoral college while losing the popular vote. This indeed proved to be the case.\\n\\nSilver wrote an excellent book The Signal and the Noise: Why so many predictions fail - but some don\\'t [Sil12]. There he writes sensibly about state-of-the-art forecasting in several fields, including sports, weather and earthquake prediction, and financial modeling. He outlines principles for effective modeling, including:\\n\\n\\\\begin{itemize}\\n  \\\\item Think probabilistically: Forecasts which make concrete statements are less meaningful than those that are inherently probabilistic. A forecast that Trump has only $28.3 \\\\%$ chance of winning is more meaningful than one that categorically states that he will lose.\\n\\\\end{itemize}\\n\\nThe real world is an uncertain place, and successful models recognize this uncertainty. There are always a range of possible outcomes that can occur with slight perturbations of reality, and this should be captured in your model. Forecasts of numerical quantities should not be single numbers, but instead report probability distributions. Specifying a standard deviation $\\\\sigma$ along with the mean prediction $\\\\mu$ suffices to describe such a distribution, particularly if it is assumed to be normal.\\n\\n\\\\footnotetext{${ }^{1}$ To complete this taxonomy, models that do better on testing data than the training data are said to be cheating.\\n\\n%---- Page End Break Here ---- Page : 203\\n}Several of the machine learning techniques we will study naturally provide probabilistic answers. Logistic regression provides a confidence along with each classification it makes. Methods that vote among the labels of the $k$ nearest neighbors define a natural confidence measure, based on the consistency of the labels in the neighborhood. Collecting ten of eleven votes for blue means something stronger than seven out of eleven.\\n\\n\\\\begin{itemize}\\n  \\\\item Change your forecast in response to new information: live\\\\index{live} models are much more interesting than dead ones. A model is live if it is continually updating predictions in response to new information. Building an infrastructure that maintains a live model is more intricate than that of a one-off computation, but much more valuable.\\\\\\\\\\nLive models are more intellectually honest than dead ones. Fresh information should change the result of any forecast. Scientists should be open to changing opinions in response to new data: indeed, this is what separates scientists from hacks and trolls.\\n\\\\end{itemize}\\n\\nDynamically-changing forecasts provide excellent opportunities to evaluate your model. Do they ultimately converge on the correct answer? Does uncertainty diminish as the event approaches? Any live model should track and display its predictions over time, so the viewer can gauge whe\\\\index{Googleâs forecasting}ther changes accurately reflected the impact of new information.\\n\\n\\\\begin{itemize}\\n  \\\\item Look for consensus: A good forecast comes from multiple distinct sources of evidence. Data should derive from as many different sources as possible. Ideally, multiple models should be built, each trying to predict the same thing in different ways. You should have an opinion as to which model is the best, but be concerned when it substantially differs from the herd.\\\\\\\\\\nOften third parties produce competing forecasts, which you can monitor and compare against. Being different doesn\\'t mean that you are wrong, but it does provide a reality check. Who has been doing better lately? What explains the differences in the forecast? Can your model be improved?\\\\\\\\\\nGoogle\\'s Flu Trends forecasting model predicted disease outbreaks by monitoring key words on search: a surge in people looking for aspirin or fever might suggest that illness is spreading. Google\\'s forecasting model proved quite consistent with the Center for Disease Control\\\\index{Center for Disease Control}\\'s (CDC) statistics on actual flu cases for several years, until they embarrassingly went astray.\\\\\\\\\\nThe world changes. Among the changes was that Google\\'s search interface began to suggest search queries in response to a user\\'s history. When offered the suggestion, many more people started searching for aspirin after searching for fever. And the old model suddenly wasn\\'t accurate anymore. Google\\'s sins lay in not monitoring its performance and adjusting over time.\\n\\n%---- Page End Break Here ---- Page : 204\\n\\\\end{itemize}\\n\\nCertain machine learning methods explicitly strive for consensus. Boosting algorithms combine large numbers of weak classifiers to produce a strong one. Ensemble decision tree methods build many independent classifiers, and vote among them to make the best decision. Such methods can have a robustness which eludes more single-track models.\\n\\n\\\\begin{itemize}\\n  \\\\item Employ Baysian reasoning: Bayes\\' theorem has several interpretations, but perhaps most cogently provides a way to calculate how probabilities change in response to new evidence. When stated as\\n\\\\end{itemize}\\n\\n$$\\nP(A \\\\mid B)=\\\\frac{P(B \\\\mid A) P(A)}{P(B)}\\n$$\\n\\nit provides a way to calculate how the probability of event $A$ changes in response to new evidence $B$.\\\\\\\\\\nApplying Bayes\\' theorem requires a prior probability $P(A)$, the likelihood of event $A$ before knowing the status of a particular event $B$. This might be the result of running a classifier to predict the status of $A$ from other features, or background knowledge about event frequencies in a population. Without a good estimate for this prior, it is very difficult to know how seriously to take the classifier.\\\\\\\\\\nSuppose $A$ is the event that person $x$ is actually a terrorist, and $B$ is the result of a feature-based classifier that decides if $x$ looks like a terrorist. When trained/evaluated on a data\\\\index{Bayesâ theorem} set of 1,000 people, half of whom were terrorists, the classifier achieved an enviable accuracy of, say, $90 \\\\%$. The classifier now says that Skiena looks like a terrorist. What is the probability that Skiena really is a terrorist?\\\\\\\\\\nThe key insight here is that the prior probability of \" $x$ is a terrorist\" is really, really low. If there are a hundred terrorists operating in the United States, then $P(A)=100 / 300,000,000=3.33 \\\\times 10^{-7}$. The probability of the terrorist detector saying yes, $P(B)=0.5$, while the probability of the detector being right when it says yes $P(B \\\\mid A)=0.9$. Multiplying this out gives a still very tiny probability that I am a bad guy,\\n\\n$$\\nP(A \\\\mid B)=\\\\frac{P(B \\\\mid A) P(A)}{P(B)}=\\\\frac{(0.9)\\\\left(3.33 \\\\times 10^{-7}\\\\right)}{(0.5)}=6 \\\\times 10^{-7}\\n$$\\n\\nalthough admittedly now greater than that of a random citizen.\\\\\\\\\\nFactoring in prior probabilities is essential to getting the right interpretation from this classifier. Bayesian reasoning starts from the prior distribution, then weighs further evidence by how strongly it should impact the probability of the event.\\n\\n\\\\subsection*{7.2 A taxonomy of\\\\index{taxonomy of} Models}\\nModels come in, well, many different models. Part of developing a philosophy of modeling is understanding your available degrees of freedom in design and\\\\\\n%---- Page End Break Here ---- Page : 205\\n\\\\\\nimplementation. In this section, we will look at model types along several different dimensions, reviewing the primary technical issues which arise to distinguish each class.\\n\\n\\\\subsection*{7.2.1 linear\\\\index{linear} vs. non-linear\\\\index{non-linear} Models}\\nLinear models are governed by equations that weigh each feature variable by a coefficient reflecting its importance, and sum up these values to produce a score. Powerful machine learning techniques, such as linear regression, can be used to identify the best possible coefficients to fit training data, yielding very effective models.\\n\\nBut generally speaking, the world is not linear. Richer mathematical descriptions include higher-order polynomials, logarithms, and exponentials. These permit models that fit training data much more tightly than linear functions can. Generally speaking, it is much harder to find the best possible coefficients to fit non-linear models. But we don\\'t have to find the best possible fit: deep learning methods, based on neural network\\\\index{neural network}s, offer excellent performance despite inherent difficulties in optimization.\\n\\nModeling cowboys often sneer in contempt at the simplicity of linear models. But linear models offer substantial benefits. They are readily understandable, generally defensible, easy to build, and avoid overfitting on modest-sized data sets. Occam\\'s razor tells us that \"the simplest explanation is the best explanation.\" I am generally happier with a robust linear model, yielding an accuracy of $x \\\\%$, than a complex non-linear beast only a few percentage points better on limited testing data.\\n\\n\\\\subsection*{7.2.2 blackbox\\\\index{blackbox} vs. descriptive\\\\index{descriptive} Models}\\nBlack boxes are devices that do their job, but in some unknown manner. Stuff goes in and stuff comes out, but how the sausage is made is completely impenetrable to outsiders.\\n\\nBy contrast, we prefer models that are descriptive, meaning they provide some insight into why they are making their decisions. Theory-driven models are generally descriptive, because they are explicit implementations of a particular well-developed theory. If you believe the theory, you have a reason to trust the underlying model, and any resulting predictions.\\n\\nCertain machine learning models prove less opaque than others. Linear regression models are descriptive, because one can see exactly which variables receive the most weight, and measure how much they contribute to the resulting prediction. Decision tree models enable you to follow the exact decision path used to make a classification. \"Our model denied you a home mortgage because your income is less than $\\\\$ 10,000$ per year, you have greater than $\\\\$ 50,000$ in credit card debt, and you have been unemployed over the past year.\"\\n\\nBut the unfortunate truth is that blackbox modeling techniques such as deep learning can be extremely effective. Neural network models are generally completely opaque as to why they do what they do. Figure 7.1 makes this\\\\\\n%---- Page End Break Here ---- Page : 206\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-223}\\n\\nFigure 7.1: Synthetic images that are mistakenly recognized as objects by state-of-the-art Deep Learning neural networks, each with a confidence greater than $99.6 \\\\%$. Source: NYC15.\\\\\\\\\\nclear. It shows images which were very carefully constructed to fool state-of-the-art neural networks. They succeeded brilliantly. The networks in question had $\\\\geq 99.6 \\\\%$ confidence that they had found the right label for every image in Figure 7.1.\\n\\nThe scandal here is not that the network got the labels wrong on these perverse images, for these recognizers are very impressive systems. Indeed, they were much more accurate than dreamed possible only a year or two before. The problem is that the creators of these classifiers had no idea why their programs made such terrible errors, or how they could prevent them in the future.\\n\\nA similar story is told of a system built for the military to distinguish images of cars from trucks. It performed well in training, but disastrously in the field. Only later was it realized that the training \\\\index{ï¬rst-principle}images for cars were shot on a sunny day and those of trucks on a cloudy day, so the system had learned to link the sky in the background with the class of the vehicle.\\n\\nTales like these highlight why visualizing the training data and using descriptive models\\\\index{models} can be so important. You must be convinced that your model has the information it needs to make the decisions you are asking of it, particularly in situations where the stakes are high.\\n\\n\\\\subsection*{7.2.3 First-Principle vs. data-driven\\\\index{data-driven} Models}\\nFirst-principle models are based on a belief of how the system under investigation really works. It might be a theoretical explanation, like Newton\\'s laws of motion. Such models can employ the full weight of classical mathematics: calculus, algebra, geometry, and more. The model might be a discrete event simulation, as will be discussed in Section 7.7 It might be seat-of-the-pants reasoning from an understanding of the domain: voters are unhappy if the economy is bad, therefore variables which measure the state of the economy should help us predict who will win the election.\\n\\n%---- Page End Break Here ---- Page : 207\\n\\nIn contrast, data-driven models are based on observed correlations between input parameters and outcome variables. The same basic model might be used to predict tomorrow\\'s weather or the price of a given stock, differing only on the data it was trained on. machine learning\\\\index{machine learning} methods make it possible to build an effective model on a domain one knows nothing about, provided we are given a good enough training set.\\n\\nBecause this is a book on data science, you might infer that my heart lies more on the side of data-driven models. But this isn\\'t really true. Data science is also about science, and things that happen for understandable reasons. Models which ignore this are doomed to fail embarrassingly in certain circumstances.\\n\\nThere is an alternate way to frame this discussion, however. ad hoc\\\\index{ad hoc} models are built using domain-specific knowledge to guide their structure and design. These tend to be brittle in response to changing conditions, and difficult to apply to new tasks. In contrast, machine learning models for classification and regression are general, because they employ no problem-specific ideas, only specific data. Retrain the models on fresh data, and they adapt to changing conditions. Train them on a different data set, and they can do something completely different. By this rubric, general models sound much better than ad hoc ones.\\n\\nThe truth is that the best models are a mixture of both theory and data. It is important to understand your domain as deeply as possible, while using the best data you can in order to fit and evaluate your models.\\n\\n\\\\subsection*{7.2.4 stochastic\\\\index{stochastic} vs. deterministic\\\\index{deterministic} Models}\\nDemanding a single deterministic \"prediction\" from a model can be a fool\\'s errand. The world is a complex place of many realities, with events that generally would not unfold in exactly the same way if time could be run over again. Good forecasting models incorporate such thinking, and produce probability distributions over all possible events.\\n\\nStochastic is a fancy word meaning \"randomly determined.\" Techniques that explicitly build some notion of probability into the model include logistic regression and Monte Carlo simulation. It is important that your model observe the basic properties of probabilities, including:\\n\\n\\\\begin{itemize}\\n  \\\\item Each probability is a value between 0 and 1: Scores that are not constrained to be in this range do not directly estimate probabilities. The solution is often to put the values through a logit function (see Section 4.4.1 to turn them into probabilities in a principled way.\\n  \\\\item That they must sum to 1 : Independently generating values between 0 and 1 does not mean that they together add up to a unit probability, over the full event space. The solution here is to scale these values so that they do, by dividing each by the partition function. See Section 9.7.4 Alternately, rethink your model to understand why they didn\\'t add up in the first place.\\n \\n%---- Page End Break Here ---- Page : 208\\n \\\\item Rare events do not have probability zero: Any event that is possible must have a greater than zero probability of occurrence. discounting\\\\index{discounting} is a way of evaluating the likelihood of unseen but possible events, and will be discussed in Section 11.1.2\\n\\\\end{itemize}\\n\\nProbabilities are a measure of humility about the accuracy of our model, and the uncertainty of a complex world. Models must be honest in what they do and don\\'t know.\\n\\nThere are certain advantages of deterministic models, however. First-principle models often yield only one possible answer. Newton\\'s laws of motion will tell you exactly how long a mass takes to fall a given distance.\\n\\nThat deterministic models always return the same answer helps greatly in debugging their implementation. This speaks to the need to optimize repeatability during model development. Fix the initial seed if you are using a random number generator, so you can rerun it and get the same answer. Build a regression test suite for your model, so you can confirm that the answers remain identical on a given input after program modifications.\\n\\n\\\\subsection*{7.2.5 Flat vs. hierarchical\\\\index{hierarchical} Models}\\nInteresting problems often exist on several different levels, each of which may require independent submodels. Predicting the future price for a particular stock really should involve submodels for analyzing such separate issues as (a) the general state of the economy, (b\\\\index{ï¬at}) the company\\'s balance sheet, and (c) the performance of other companies in its industrial sector.\\n\\nImposing a hierarchical structure on a model permits it to be built and evaluated in a logical and transparent way, instead of as a black box. Certain subproblems lend themselves to theory-based, first-principle models, which can then be used as features in a general data-driven model. Explicitly hierarchical models are descriptive: one can trace a final decision back to the appropriate top-level subproblem, and report how strongly it contributed to making the observed result.\\n\\nThe first step to build a hierarchical model is explicitly decomposing our problem into subproblems. Typically these represent mechanisms governing the underlying process being modeled. What should the model depend on? If data and resources exist to make a principled submodel for each piece, great! If not, it is OK to leave it as a null model or baseline\\\\index{baseline}, and explicitly describe the omission when documenting the results.\\n\\ndeep learning\\\\index{deep learning} models can be thought of as being both flat and hierarchical, at the same time. They are typically trained on large sets of unwashed data, so there is no explicit definition of subproblems to guide the subprocess. Looked at as a whole, the network does only one thing. But because they are built from multiple nested layers (the deep in deep learning), these models presume that there are complex features there to be learned from the lower level inputs.\\n\\nI am always reluctant to believe that machine learning models prove better than me at inferring the basic organizing principles in a domain that I un-\\\\\\n%---- Page End Break Here ---- Page : 209\\n\\\\\\nderstand. Even when employing deep learning, it pays to sketch out a rough hierarchical structure that likely exists for your network to find. For example, any image processing network should generalize from patches of pixels to edges, and then from boundaries to sub-objects to scene analysis as we move to higher layers. This influences the architecture of your network, and helps you validate it. Do you see evidence that your network is making the right decisions for the right reasons?\\n\\n\\\\subsection*{7.3 baseline models\\\\index{baseline models}}\\nA wise man once observed that a broken clock is right twice a day. As modelers we strive to be better than this, but proving that we are requires some level of rigorous evaluation.\\n\\nThe first step to assess the complexity of your task involves building baseline models: the simplest reasonable models that produce answers we can compare against. More sophisticated models should do better than baseline models, but verifying that they really do and, if so by how much, puts its performance into the proper context.\\n\\nCertain forecasting tasks are inherently harder than others. A simple baseline (\"yes\") has proven very accurate in predicting whether the sun will rise to\\\\index{classiï¬cation}morrow. By contrast, you could get rich predicting whether the stock market will go up or down $51 \\\\%$ of the time. Only after you decisively beat your baselines can your models really be deemed effective.\\n\\n\\\\subsection*{7.3.1 Baseline Models for Classification}\\nThere are two common tasks for data science\\\\index{data science} models: classification and value prediction\\\\index{value prediction}. In classification tasks, we are given a small set of possible labels for any given item, like (spam or not spam), (man or woman), or (bicycle, car, or truck). We seek a system that will generate a label accurately describing a particular instance of an email, person, or vehicle.\\n\\nRepresentative baseline models for classification include:\\n\\n\\\\begin{itemize}\\n  \\\\item Uniform or random selection among labels: If you have absolutely no prior distribution on the objects, you might as well make an arbitrary selection using the broken watch method. Comparing your stock market prediction model against random coin flips will go a long way to showing how hard the problem is.\\\\\\\\\\nI think of such a blind classifier as the monkey, because it is like asking your pet to make the decision for you. In a prediction problem with twenty possible labels or classes, doing substantially better than $5 \\\\%$ is the first evidence that you have some insight into the problem. You first have to show me that you can beat the monkey before I start to trust you.\\n  \\\\item The most common label appearing in the training data: A large training set usually provides some notion of a prior distribution on the classes.\\n\\n%---- Page End Break Here ---- Page : 210\\n\\\\end{itemize}\\n\\nSelecting the most frequent label is better than selecting them uniformly or randomly. This is the theory behind the sun-will-rise-tomorrow baseline model.\\n\\n\\\\begin{itemize}\\n  \\\\item The most accurate single-feature model: Powerful models strive to exploit all the useful features present in a given data set. But it is valuable to know what the best single feature can do. Building the best classifier on a single numerical feature $x$ is easy: we are declaring that the item is in class 1 if $x \\\\geq t$, and class 2 if otherwise. To find the best threshold $t$, we can test all $n$ possible thresholds of the form $t_{i}=x_{i}+\\\\epsilon$, where $x_{i}$ is the value of the feature in the $i$ th of $n$ training instances. Then select the threshold which yields the most accurate classifier on your training data.\\n\\\\end{itemize}\\n\\nOccam\\'s razor deems the simplest model to be best. Only when your complicated model beats all single-factor models does it start to be interesting.\\n\\n\\\\begin{itemize}\\n  \\\\item Somebody else\\'s model: Often we are not the first person to attempt a particular task. Your company may have a legacy model that you are charged with updating or revising. Perhaps a close variant of the problem has been discussed in an academic paper, and maybe they even released their code on the web for you to experiment with.\\n\\\\end{itemize}\\n\\nOne of two things can happen when you compare your model against someone else\\'s wor\\\\index{Occamâs razor}k: either you beat them or you don\\'t. If you beat them, you now have something worth bragging about. If you don\\'t, it is a chance to learn and improve. Why didn\\'t you win? The fact that you lost gives you certainty that your model can be improved, at least to the level of the other guy\\'s model.\\n\\n\\\\begin{itemize}\\n  \\\\item Clairvoyance: There are circumstances when even the best possible model cannot theoretically reach $100 \\\\%$ accuracy. Suppose that two data records are exactly the same in feature space, but with contradictory labels. There is no deterministic classifier that could ever get both of these problems right, so we\\'re doomed to less than perfect performance. But the tighter upper bound from an optimally clairvoyant predictor might convince you that your baseline model is better than you thought.\\n\\\\end{itemize}\\n\\nThe need for better upper bounds often arises when your training data is the result of a human annotation process, and multiple annotators evaluate the same instances. We get inherent contradictions whenever two annotators disagree with each other. I\\'ve worked on problems where $86.6 \\\\%$ correct was the highest possible score. This lowers expectations. A good bit of life advice is to expect little from your fellow man, and realize you will have to make do with a lot less than that.\\n\\n%---- Page End Break Here ---- Page : 211\\n\\n\\\\subsection*{7.3.2 Baseline Models for value prediction\\\\index{for value prediction}}\\nIn value prediction problems, we are given a collection of feature-value pairs $\\\\left(f_{i}, v_{i}\\\\right)$ to use to train a function $F$ such that $F\\\\left(v_{i}\\\\right)=v_{i}$. Baseline models for value prediction problems follow from similar techniques to what were proposed for classification, like:\\n\\n\\\\begin{itemize}\\n  \\\\item mean\\\\index{mean} or median\\\\index{median}: Just ignore the features, so you can always output the consensus value of the target. This proves to be quite an informative baseline, because if you can\\'t substantially beat always guessing the mean, either you have the wrong features or are working on a hopeless task.\\n  \\\\item linear regression\\\\index{linear regression}: We will thoroughly cover linear regression in Section 9.1. But for now, it suffices to understand that this powerful but simple-touse technique builds the best possible linear function for value prediction problems. This baseline enables you to better judge the performance of non-linear models. If they do not perform substantially better than the linear classifier, they are probably not worth the effort.\\n  \\\\item Value of the previous point in time: time series\\\\index{time series} forecasting is a common task, where we are charged with predicting the value $f\\\\left(t_{n}, x\\\\right)$ at time $t_{n}$ given feature set $x$ and the observed values $f^{\\\\prime}\\\\left(t_{i}\\\\right)$ for $1 \\\\leq i<n$. But today\\'s weather is a good guess for whether it will rain tomorrow. Similarly, the value of the previous observed value $f^{\\\\prime}\\\\left(t_{n-1}\\\\right)$ is a reasonable forecast for time $f\\\\left(t_{n}\\\\right)$. It is often surprisingly difficult to beat this baseline in practice.\\n\\\\end{itemize}\\n\\nBaseline models must be fair: they should be simple but not stupid. You want to present a target that you hope or expect to beat, but not a sitting duck. You should feel relieved when you beat your baseline, but not boastful or smirking.\\n\\n\\\\subsection*{7.4 Evaluating Models}\\nCongratulations! You have built a predictive model for classification or value prediction. Now, how good is it?\\n\\nThis innocent-looking question does not have a simple answer. We will detail the key technical issues in the sections below. But the informal sniff test is perhaps the most important criteria for evaluating a model. Do you really believe that it is doing a good job on your training and testing instances?\\n\\nThe formal evaluations that will be detailed below reduce the performance of a model down to a few summary statistics, aggregated over many instances. But many sins in a model can be hidden when you only interact with these aggregate scores. You have no way of knowing whether there are bugs in your implementation or data normalization, resulting in poorer performance than it should have. Perhaps you intermingled your training and test data, yielding much better scores on your testbed than you deserve.\\n\\n%---- Page End Break Here ---- Page : 212\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{cc|cc}\\n &  & \\\\multicolumn{3}{|c}{Predicted Class} \\\\\\\\\\n &  & Yes & No &  \\\\\\\\\\n\\\\hline\\nActual & Yes & true positives\\\\index{true positives} (TP) & false negatives\\\\index{false negatives} (FN) &  \\\\\\\\\\nClass & No & false positives\\\\index{false positives} (FP) & true negatives\\\\index{true negatives} (TN) &  \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 7.2: The confusion matrix\\\\index{confusion matrix} for binary classifiers, defining different classes of correct and erroneous predictions.\\n\\nTo really know what is happening, you need to do a sniff test. My personal sniff test involves looking carefully at a few example instances where the model got it right, and a few where it got it wrong. The goal is to make sure that I understand why the model got the results that it did. Ideally these will be records whose \"names\" you understand, instances where you have some intuition about what the right answers should be as a result of exploratory data analys\\\\index{positive class}is or familiarity with the domain.\\n\\nTake-Home Lesson: Too many data scientists only care about the evaluation statistics of their models. But good scientists have an understanding of whether the errors they are making are defensible, serious, or irrelevant.\\n\\nAnother issue is your degree of surprise at the evaluated accuracy of the model. Is it performing better or worse than you expected? How accurate do you think you would be at the given task, if you had to use human judgment.\\n\\nA related question is establishing a sense of how valuable it would be if the model performed just a little better. An NLP task that classifies words correctly with $95 \\\\%$ accuracy makes a mistake roughly once every two to three sentences. Is this good enough? The better its current performance is, the harder it will be to make further improvements.\\n\\nBut the best way to assess models involves out-of-sample predictions, results on data that you never saw (or even better, did not exist) when you built the model. Good performance on the data that you trained models on is very suspect, because models can easily be overfit. Out of sample predictions are the key to being honest, provided you have enough data and time to test them. This is why I had my Quant Shop students build models to make predictions of future events, and then forced them to watch and see whether they were right or not.\\n\\n\\\\subsection*{7.4.1 Evaluating Classifiers}\\nEvaluating a classifier means measuring how accurately our predicted labels match the gold standard labels in the evaluation set. For the common case of two distinct labels or classes (binary classification), we typically call the smaller and more interesting of the two classes as positive and the larger/other class as negative. In a spam classification problem, the spam would typically be positive and the ham (non-spam) would be negative. This labeling aims to ensure that\\\\\\n%---- Page End Break Here ---- Page : 213\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-230}\\n\\nFigure 7.3: What happens if we classify everyone of height $\\\\geq 168$ centimeters as male? The four possible results in the confusion matrix reflect which instances were classified correctly (TP and TN) and which ones were not (FN and FP).\\\\\\\\\\nidentifying the positives is at least as hard as identifying the negatives, although often the test instances are selected so that the classes are of equal cardinality.\\n\\nThere are four possible results of what the classification model could do on any given instance, which defines the confusion matrix or contingency table\\\\index{contingency table} shown in Figure 7.2 .\\n\\n\\\\begin{itemize}\\n  \\\\item True Positives (TP): Here our classifier labels a positive item as positive, resulting in a win for the classifier.\\n  \\\\item True Negatives (TN): Here the classifier correctly determines that a member of the negative class\\\\index{negative class} deserves a negative label. Another win.\\n  \\\\item False Positives (FP): The classifier mistakenly calls a negative item as a positive, resulting in a \"type I\" classification error.\\n  \\\\item False Negatives (FN): The classifier mistakenly declares a positive item as negative, resulting in a \"type II\" classification error.\\n\\\\end{itemize}\\n\\nFigure 7.3 illustrates where these result classes fall in separating two distributions (men and women), where the decision variable is height as measured in centimeters. The classifier under evaluation labels everyone of height $\\\\geq 168$ centimeters as male. The purple regions represent the intersection of both male and female. These tails represent the incorrectly classified elements.\\n\\n\\\\section*{accuracy\\\\index{accuracy}, precision\\\\index{precision}, Recall, and F-Score}\\nThere are several different evaluation statistics which can be computed from the true/false positive/negative counts detailed above. The reason we need so many statistics is that we must defend our classifier against two baseline opponents, the sharp\\\\index{sharp} and the monkey\\\\index{monkey}.\\n\\n%---- Page End Break Here ---- Page : 214\\n\\nThe sharp is the opponent who knows what evaluation system we are using, and picks the baseline model which will do best according to it. The sharp will try to make the evaluation statistic look bad, by achieving a high score with a useless classifier. That might mean declaring all items positive, or perhaps all negative.\\n\\nIn contrast, the monkey randomly guesses on each instance. To interpret our model\\'s performance, it is important to establish by how much it beats both the sharp and the monkey.\\n\\nThe first statistic measures the accuracy of classifier, the ratio of the number of correct predictions over total predictions. Thus:\\n\\n$$\\n\\\\text { accuracy }=\\\\frac{T P+T N}{T P+T N+F N+F P}\\n$$\\n\\nBy multiplying such fractions by 100 , we can get a percentage accuracy score.\\\\\\\\\\nAccuracy is a sensible number which is relatively easy to explain, so it is worth providing in any evaluation environment. How accurate is the monkey, when half of the instances are positive and half negative? The monkey would be expected to achieve an accuracy of $50 \\\\%$ by random guessing. The same accuracy of $50 \\\\%$ would be achieved by the sharp, by always guessing positive, or (equivalently) always guessing negative. The sharp would get a different half of the instances correct in each case.\\n\\nStill, accuracy alone has limitations as an evaluation metric, particularly when the positive class is much smaller than the negative class. Consider the development of a classifier to diagnose whether a patient has cancer, where the positive class has the disease (i.e. tests positive) and the negative class is healthy. The prior distribution is that the vast majority of people are healthy, so\\n\\n$$\\np=\\\\frac{\\\\mid \\\\text { positive } \\\\mid}{\\\\mid \\\\text { positive }|+| \\\\text { negative } \\\\mid} \\\\ll \\\\frac{1}{2}\\n$$\\n\\nThe expected accuracy of a fair-coin monkey would still be 0.5: it should get an average of half of the positives and half the negatives right. But the sharp would declare everyone to be healthy, achieving an accuracy of $1-p$. Suppose that only $5 \\\\%$ of the test takers really had the disease. The sharp could brag about her accuracy of $95 \\\\%$, while simultaneously dooming all members of the diseased class to an early death.\\n\\nThus we need evaluation metrics that are more sensitive to getting the positive class right. Precision measures how often this classifier is correct when it dares to say positive:\\n\\n$$\\n\\\\text { precision }=\\\\frac{T P}{(T P+F P)}\\n$$\\n\\nAchieving high precision is impossible for either a sharp or a monkey, because the fraction of positives $(p=0.05)$ is so low. If the classifier issues too many positive labels, it is doomed to low precision because so many bullets miss their mark, resulting in many false positives. But if the classifier is stingy with positive labels, very few of them are likely to connect with the rare positive\\n\\n%---- Page End Break Here ---- Page : 215\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|cc|}\\n & \\\\multicolumn{2}{|c}{Monkey} \\\\\\\\\\n & predicted & class \\\\\\\\\\n & yes & no \\\\\\\\\\n\\\\hline\\nyes & $(p n) q$ & $(p n)(1-q)$ \\\\\\\\\\nno & $((1-p) n) q$ & $((1-p) n)(1-q)$ \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|cc|}\\n & \\\\multicolumn{2}{|c}{balanced\\\\index{balanced} Classifier} \\\\\\\\\\n & predicted & class \\\\\\\\\\n & yes & no \\\\\\\\\\n\\\\hline\\nyes & $(p n) q$ & $(p n)(1-q)$ \\\\\\\\\\nno & $((1-p) n)(1-q)$ & $((1-p) n) q$ \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 7.4: The expected performance of a monkey classifier on $n$ instances, where $p \\\\cdot n$ are positive and $(1-p) \\\\cdot n$ are negative. The monkey guesses positive with probability $q$ (left). Also, the expected performance of a balanced classifier, which somehow correctly classifies members of each class with probability $q$ (right).\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|ll|ll|lllll}\\n & \\\\multicolumn{3}{|c}{Monkey} & \\\\multicolumn{4}{c}{Sharp} & \\\\multicolumn{4}{c}{Balanced Classifier} \\\\\\\\\\n$q$ & 0.05 & 0.5 & 0.0 & 1.0 & 0.5 & 0.75 & 0.9 & 0.99 & 1.0 &  &  \\\\\\\\\\n\\\\hline\\naccuracy & 0.905 & 0.5 & 0.95 & 0.05 & 0.5 & 0.75 & 0.9 & 0.99 & 1. &  &  \\\\\\\\\\nprecision & 0.05 & 0.05 & - & 0.05 & 0.05 & 0.136 & 0.321 & 0.839 & 1. &  &  \\\\\\\\\\nrecall\\\\index{recall} & 0.05 & 0.5 & 0. & 1. & 0.5 & 0.75 & 0.9 & 0.99 & 1. &  &  \\\\\\\\\\nF score & 0.05 & 0.091 & - & 0.095 & 0.091 & 0.231 & 0.474 & 0.908 & 1. &  &  \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 7.5: Performance of several classifiers, under different performance measures.\\\\\\\\\\ninstances, so the classifier achieves low true positives. These baseline classifiers achieve precision proportional to the positive class probability $p=0.05$, because they are flying blind.\\n\\nIn the cancer diagnosis case, we might be more ready to tolerate false positives (errors where we scare a healthy person with a wrong diagnosis) than false negatives (errors where we kill a sick patient by misdiagnosing their illness). Recall measures how often you prove right on all positive instances:\\n\\n$$\\n\\\\text { recall }=\\\\frac{T P}{(T P+F N)}\\n$$\\n\\nA high recall implies that the classifier has few false negatives. The easiest way to achieve this declares that everyone has cancer, as done by a sharp always answering yes. This classifier has high recall but low precision: $95 \\\\%$ of the test takers will receive an unnecessary scare. There is an inherent trade-off between precision and recall when building classifiers: the braver your predictions are, the less likely they are to be right.\\n\\nBut people are hard-wired to want a single measurement describing the performance of their system. The F-score\\\\index{F-score} (or sometimes F1-score) is such a combination, returning the harmonic mean of precision and recall:\\n\\n$$\\nF=2 \\\\cdot \\\\frac{\\\\text { precision } \\\\cdot \\\\text { recall }}{\\\\text { precision }+ \\\\text { recall }}\\n$$\\n\\nF-score is a very tough measure to beat. The harmonic mean is always less than or equal to the arithmetic mean, and the lower number has a dispropor-\\\\\\n%---- Page End Break Here ---- Page : 216\\n\\\\\\ntionate large effect. Achieving a high F-score requires both high recall and high precision. None of our baseline classifiers manage a decent F-score despite high accuracy and recall values, because their precision is too low.\\n\\nThe F-score and related evaluation metrics were developed to evaluate meaningful classifiers, not monkeys or sharps. To gain insight in how to interpret them, let\\'s consider a class of magically balanced classifiers, which somehow show equal accuracy on both positive and negative instances. This isn\\'t usually the case, but classifiers selected to achieve high F-scores must balance precision and recall statistics, which means they must show decent performance on both positive and negative instances.\\n\\nFigure 7.5 summarizes the performance of both baseline and balanced classifiers on our cancer detection problem, benchmarked on all four of our evaluation metrics. The take away lessons are:\\n\\n\\\\begin{itemize}\\n  \\\\item Accuracy is a misleading statistic when the class sizes are substantially different: A baseline classifier mindlessly answering \"no\" for every instance achieved an accuracy of $95 \\\\%$ on the cancer problem, better even than a balanced classifier that got $94 \\\\%$ right on each class.\\n  \\\\item Recall equals accuracy if and only if the classifiers are balanced: Good things happen when the accuracy for recognizing both classes is the same. This doesn\\'t happen automatically during training, when the class sizes are different. Indeed, this is one reason why it is generally a good practice to have an equal number of positive and negative examples in your training set.\\n  \\\\item High precision is very hard to achieve in unbalanced class sizes: Even a balanced classifier that gets $99 \\\\%$ accuracy on both positive and negative examples cannot achieve a precision above $84 \\\\%$ on the cancer problem. This is because there are twenty times more negative instances than positive ones. The false positives from misclassifying the larger class at a $1 \\\\%$ rate remains substantial against the background of $5 \\\\%$ true positives.\\n  \\\\item F-score does the best job of any single statistic, but all four work together to describe the performance of a classifier: Is the precision of your classifier greater than its recall? Then it is labeling too few instances as positives, and so perhaps you can tune it better. Is the recall higher than the precision? Maybe we can improve the F-score by being less aggressive in calling positives. Is the accuracy far from the recall? Then our classifier isn\\'t very balanced. So check which side is doing worse, and how we might be able to fix it.\\n\\\\end{itemize}\\n\\nA useful trick to increase the precision of a model at the expense of recall is to give it the power to say \"I don\\'t know.\" Classifiers typically do better on easy cases than hard ones, with the difficulty defined by how far the example is from being assigned the alternate label.\\n\\nDefining a notion of confidence that your proposed classification is correct is the key to when you should pass on a question. Only venture a guess when your\\\\\\n%---- Page End Break Here ---- Page : 217\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-234}\\n\\nFigure 7.6: The ROC curve\\\\index{curve} helps us select the best threshold\\\\index{threshold} to use in a classifier, by displaying the trade-off between true positives and false positive at every possible setting. The monkey ROCs the main diagonal here.\\\\\\\\\\nconfidence is above a given threshold. Patients whose test scores are near the boundary would generally prefer a diagnosis of \"borderline result\" to \"you\\'ve got cancer,\" particularly if the classifier is not really confident in its decision.\\n\\nOur precision and recall statistics must be reconsidered to properly accommodate the new indeterminate class. There is no need to change the precision formula: we evaluate only on the instances we call positive. But the denominator for recall must explicitly account for all elements we refused to label. Assuming we are accurate in our confidence measures, precision will increase at the expense of recall.\\n\\n\\\\subsection*{7.4.2 Receiver-Operator Characteristic (ROC) Curves}\\nMany classifiers come with natural knobs that you can tweak to alter the tradeoff between precision and recall. For example, consider systems which compute a numerical score reflecting \"in classness,\" perhaps by assessing how much the given test sample looks like cancer. Certain samples will score more positively than others. But where do we draw the line between positive and negative?\\n\\nIf our \"in classness\" score is accurate, then it should generally be higher for positive items than negative ones. The positive examples will define a different score distribution than the negative instances, as shown in Figure 7.6 (left). It would be great if these distributions were completely disjoint, because then there would be a score threshold $t$ such that all instances with scores $\\\\geq t$ are positive and all $<t$ are negative. This would define a perfect\\\\index{perfect} classifier.\\n\\nBut it is more likely that the two distributions will overlap, to at least some degree, turning the problem of identifying the best threshold into a judgment call based on our relative distaste towards false positives and false negatives.\\n\\nThe Receiver Operating Characteristic (ROC) curve provides a visual representation of our complete space of options in putting together a classifier. Each\\\\\\n%---- Page End Break Here ---- Page : 218\\n\\\\\\npoint on this curve represents a particular classifier threshold, defined by its false positive and false negative rates ${ }^{2}$ These rates are in turn defined by the count of errors divided by the total number of positives in the evaluation data, and perhaps multiplied by one hundred to turn into percentages.\\n\\nConsider what happens as we sweep our threshold from left to right over these distributions. Every time we pass over another example, we either increase the number of true positives (if this example was positive) or false positives (if this example was in fact a negative). At the very left, we achieve true/false positive rates of $0 \\\\%$, since the classifier labeled nothing as positive at that cutoff. Moving as far to the right as possible, all examples will be labeled positively, and hence both rates become $100 \\\\%$. Each threshold in between defines a possible classifier, and the sweep defines a staircase curve in true/false positive rate space taking us from $(0 \\\\%, 0 \\\\%)$ to $(100 \\\\%, 100 \\\\%)$.\\n\\nSuppose the score function was defined by a monkey, i.e. an arbitrary random value for each instance. Then as we sweep our threshold to the right, the label of the next example should be positive or negative with equal probability. Thus we are equally likely to increase our true positive rate as false, and the ROC curve should cruise along the main diagonal.\\n\\nDoing better than the monkey implies an ROC curve that lies above the diagonal. The best possible ROC curve shoots up immediately from ( $0 \\\\%, 0 \\\\%$ ) to ( $0 \\\\%, 100 \\\\%$ ), meaning it encounters all positive instances before any negative ones. It then steps to the right with each negative example, until it finally reaches the upper right corner.\\n\\nThe area under the ROC curve\\\\index{area under the ROC curve} (AUC) is often used as a statistic measuring the quality of scoring function defining the classifier. The best possible ROC curve has an area of $100 \\\\% \\\\times 100 \\\\% \\\\rightarrow 1$, while the monkey\\'s triangle has an area of $1 / 2$. The closer the area is to 1 , the better our classification function is.\\n\\n\\\\subsection*{7.4.3 Evaluating Multiclass Systems}\\nMany classification problems are non-binary, meaning that they must decide among more than two classes. Google News\\\\index{Google News} has separate sections for U.S. and world news, plus business, entertainment, sports, health, science, and technology. Thus the article classifier which governs the behavior of this site must assign each article a label from eight different classes.\\n\\nThe more possible class labels you have, the harder it is to get the classification right. The expected accuracy of a classification monkey with $d$ labels is $1 / d$, so the accuracy drops rapidly with increased class complexity.\\n\\nThis makes properly evaluating multiclass classifiers a challenge, because low success numbers get disheartening. A better statistic is the top-k success rate\\\\index{top-k success rate}, which generalizes accuracy for some specific value of $k \\\\geq 1$. How often was the right label among the top $k$ possibilities?\\n\\nThis measure is good, because it gives us partial credit for getting close to the right answer. How close is good enough is defined by the parameter $k$. For\\n\\n\\\\footnotetext{${ }^{2}$ The strange name for this beast is a legacy of its original application, in tuning the performance of radar systems.\\n\\n%---- Page End Break Here ---- Page : 219\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-236}\\n\\nFigure 7.7: confusion matrix\\\\index{confusion matrix} for a document dating system: the main diagonal reflects accurate classification.\\\\\\\\\\n$k=1$, this reduces to accuracy. For $k=d$, any possible label suffices, and the success rate is $100 \\\\%$ by definition. Typical values are 3,5 , or 10 : high enough that a good classifier should achieve an accuracy above $50 \\\\%$ and be visibly better than the monkey. But not too much better, because an effective evaluation should leave us with substantial room to do better. In fact, it is a good practice to compute the top $k$ rate for all $k$ from 1 to $d$, or at least high enough that the task becomes easy.\\n\\nAn even more powerful evaluation tool is the confusion matrix $C$, a $d \\\\times d$ matrix where $C[x, y]$ reports the number (or fraction) of instances of class $x$ which get labeled as class $y$.\\n\\nHow do we read a confusion matrix, like the one shown in Figure 7.7? It is taken from the evaluation environment we built to test a document dating classifier, which analyzes texts to predict the period of authorship. Such document dating will be the ongoing example on evaluation through the rest of this chapter.\\n\\nThe most important feature is the main diagonal, $C[i, i]$, which counts how many (or what fraction of) items from class $i$ were correctly labeled as class $i$. We hope for a heavy main diagonal in our matrix. Ours in a hard task, and Figure 7.7 shows a strong but not perfect main diagonal. There are several places where documents are more frequently classified in the neighboring period than the correct one.\\n\\nBut the most interesting features of the confusion matrix are the large counts\\\\\\n%---- Page End Break Here ---- Page : 220\\n\\\\\\n$C[i, j]$ that do not lie along the main diagonal. These represent commonly confused classes. In our example, the matrix shows a distressingly high number of documents ( $6 \\\\%$ ) from 1900 classified as 2000, when none are classified as 1800. Such asymmetries suggest directions to improve the classifier.\\n\\nThere are two possible explanations for class confusions. The first is a bug in the classifier, which means that we have to work harder to make it distinguish $i$ from $j$. But the second involves humility, the realization that classes $i$ and $j$ may overlap to such a degree that it is ill-defined what the right answer should be. Maybe writing styles don\\'t really change that much over a twenty-year period?\\n\\nIn the Google News example, the lines between the science and technology categories is very fuzzy. Where should an article about commercial space flights go? Google says science, but I say technology. Frequent confusion might suggest merging the two categories, as they represent a difference without a distinction.\\n\\nSparse rows in the confusion matrix indicate classes poorly represented in the training data, while sparse columns indicate labels which the classifier is reluctant to assign. Either indication is an argument that perhaps we should consider abandoning this label, and merge the two similar categories.\\n\\nThe rows and columns of the confusion matrix provide analogous performance statistics to those of Section 7.4.1 for multiple classes, parameterized by class. precision\\\\index{precision} ${ }_{i}$ is the fraction of all items declared class $i$ that were in fact of class $i$ :\\n\\n$$\\n\\\\text { precision }_{i}=C[i, i] / \\\\sum_{j=1}^{d} C[j, i] .\\n$$\\n\\nrecall\\\\index{recall} $_{i}$ is the fraction of all members of class $i$ that were correctly identified as such:\\n\\n$$\\n\\\\operatorname{recall}_{i}=C[i, i] / \\\\sum_{j=1}^{d} C[i, j] .\\n$$\\n\\n\\\\subsection*{7.4.4 evaluating\\\\index{evaluating} Value Prediction Models}\\nValue prediction problems can be thought of as classification tasks, but over an infinite number of classes. However, there are more direct ways to evaluate regression systems, based on the distance between the predicted and actual values.\\n\\n\\\\section*{Error Statistics}\\nFor numerical values, error is a function of the difference between a forecast $y^{\\\\prime}=$ $f(x)$ and the actual result $y$. Measuring the performance of a value prediction system involves two decisions: (1) fixing the specific individual error function, and (2) selecting the statistic to best represent the full error distribution.\\n\\nThe primary choices for the individual error function include:\\n\\n\\\\begin{itemize}\\n  \\\\item absolute\\\\index{absolute} error: The value $\\\\Delta=y^{\\\\prime}-y$ has the virtue of being simple and symmetric, so the sign can distinguish the case where $y^{\\\\prime\\n%---- Page End Break Here ---- Page : 221\\n}>y$ from\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-238}\\n\\\\end{itemize}\\n\\nFigure 7.8: Error distribution histograms\\\\index{histograms} for random (left) and naive Bayes classifiers predicting the year of authorship for documents (right).\\\\\\\\\\n$y>y^{\\\\prime}$. The problem comes in aggregating these values into a summary statistic. Do offsetting errors like -1 and 1 mean that the system is perfect? Typically the absolute value of the error is taken to obliterate the sign.\\n\\n\\\\begin{itemize}\\n  \\\\item relative\\\\index{relative} error: The absolute magnitude of error is meaningless without a sense of the units involved. An absolute error of 1.2 in a person\\'s predicted height is good if it is measured in millimeters, but terrible if measured in miles.\\n\\\\end{itemize}\\n\\nNormalizing the error by the magnitude of the observation produces a unit-less quantity, which can be sensibly interpreted as a fraction or (multiplied by $100 \\\\%$ ) as a percentage: $\\\\epsilon=\\\\left(y-y^{\\\\prime}\\\\right) / y$. Absolute error weighs instances with larger values of $y$ as more important than smaller ones, a bias corrected when computing relative errors.\\n\\n\\\\begin{itemize}\\n  \\\\item squared\\\\index{squared} error: The value $\\\\Delta^{2}=\\\\left(y^{\\\\prime}-y\\\\right)^{2}$ is always positive, and hence these values can be meaningfully summed. Large errors values contribute disproportionately to the total when squaring: $\\\\Delta^{2}$ for $\\\\Delta=2$ is four times larger than $\\\\Delta^{2}$ for $\\\\Delta=1$. Thus outliers can easily come to dominate the error statistic in a large ensemble.\\n\\\\end{itemize}\\n\\nIt is a very good idea to plot a histogram of the absolute error distribution for any value predictor, as there is much you can learn from it. The distribution should be symmetric, and centered around zero. It should be bell-shaped, meaning small errors are more common than big errors. And extreme outliers should be rare. If any of the conditions are wrong, there is likely a simple way to improve the forecasting procedure. For example, if it is not centered around zero, adding an constant offset to all forecasts will improve the consensus results.\\n\\nFigure 7.8 presents the absolute error distributions from two models for predicting the year of authorship of documents from their word usage distribution. On the left, we see the error distribution for the monkey, randomly guessing a\\\\\\n%---- Page End Break Here ---- Page : 222\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-239}\\n\\nFigure 7.9: Block diagram of a basic model evaluation environment.\\\\\\\\\\nyear from 1800 to 2005. What do we see? The error distribution is broad and bad, as we might have expected, but also asymmetric. Far more documents produced positive errors than negative ones. Why? The test corpus apparently contained more modern documents than older ones, so (year-monkey\\\\_year) is more often positive than negative. Even the monkey can learn something from seeing the distribution.\\n\\nIn contrast, Figure 7.8 (right) presents the error distribution for our naive Bayes classifier for document dating. This looks much better: there is a sharp peak around zero, and much narrower tails. But the longer tail now resides to the left of zero, telling us that we are still calling a distressing number of very old documents modern. We need to examine some of these instances, to figure out why that is the case.\\n\\nWe need a summary statistic reducing such error distributions to a single number, in order to compare the performance of different value prediction models. A commonly-used statistic is mean squared\\\\index{mean squared} error (MSE), which is computed\\n\\n$$\\nM S E\\\\left(Y, Y^{\\\\prime}\\\\right)=\\\\frac{1}{n} \\\\sum_{i=1}^{n}\\\\left(y_{i}^{\\\\prime}-y_{i}\\\\right)^{2}\\n$$\\n\\nBecause it weighs each term quadratically, outliers have a disproportionate effect. Thus median squared error might be a more informative statistic for noisy instances.\\n\\nroot mean squared\\\\index{root mean squared} (RMSD) error is simply the square root of mean squared error:\\n\\n$$\\nR M S D(\\\\Theta)=\\\\sqrt{M S E\\\\left(Y, Y^{\\\\prime}\\\\right)}\\n$$\\n\\nThe advantage of RMSD is that its magnitude is interpretable on the same scale as the original values, just as standard deviation is a more interpretable quantity than variance. But this does not eliminate the problem that outlier elements can substantially skew the total.\\n\\n%---- Page End Break Here ---- Page : 223\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|l|l|l|l|l|l|}\\n\\\\hline\\n & Dataset & Method & MAE & MedAE & Acc \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{0}$ & NYTimes & Random & 73.335463 & 65.0 & 0.004895 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{1}$ & COHA\\\\_Fiction\\\\_100 & Random & 79.865017 & 72.0 & 0.005287 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{2}$ & COHA\\\\_Fiction\\\\_500 & Random & 80.505849 & 74.0 & 0.003825 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{3}$ & COHA\\\\_Fiction\\\\_1000 & Random & 80.604837 & 72.0 & 0.003825 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{4}$ & COHA\\\\_Fiction\\\\_2000 & Random & 79.845332 & 72.0 & 0.005737 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{5}$ & COHA\\\\_News\\\\_100 & Random & 66.539239 & 59.0 & 0.005461 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{6}$ & COHA\\\\_News\\\\_500 & Random & 66.267091 & 59.0 & 0.005461 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{7}$ & COHA\\\\_News\\\\_1000 & Random & 66.077670 & 57.5 & 0.004956 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{8}$ & COHA\\\\_News\\\\_2000 & Random & 66.225526 & 58.0 & 0.005057 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|l|l|l|l|l|l|}\\n\\\\hline\\n & Dataset & Method & MAE & MedAE & Acc \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{0}$ & NYTimes & NB & 21.306301 & 14 & 0.029728 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{1}$ & COHA\\\\_Fiction\\\\_100 & NB & 32.302025 & 22 & 0.041732 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{2}$ & COHA\\\\_Fiction\\\\_500 & NB & 25.428234 & 14 & 0.050056 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{3}$ & COHA\\\\_Fiction\\\\_1000 & NB & 23.493926 & 13 & 0.053656 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{4}$ & COHA\\\\_Fiction\\\\_2000 & NB & 22.493363 & 12 & 0.054781 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{5}$ & COHA\\\\_News\\\\_100 & NB & 19.384001 & 14 & 0.030845 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{6}$ & COHA\\\\_News\\\\_500 & NB & 16.657565 & 12 & 0.034891 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{7}$ & COHA\\\\_News\\\\_1000 & NB & 16.282261 & 12 & 0.035093 \\\\\\\\\\n\\\\hline\\n$\\\\mathbf{8}$ & COHA\\\\_News\\\\_2000 & NB & 16.220065 & 12 & 0.035599 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 7.10: evaluation\\\\index{evaluation} environment results for predicting the year of authorship for documents, comparing the monkey (left) to a naive Bayes classifier (right).\\n\\n\\\\subsection*{7.5 Evaluation environments\\\\index{environments}}\\nA substantial part of any data science project revolves around building a reasonable evaluation environment. In particular, you need a single-command program\\\\index{single-command program} to run your model on the evaluation data, and produce plots/reports on its effectiveness, as shown in Figure 7.9\\n\\nWhy single command? If it is not easy to run, you won\\'t try it often enough. If the results are not easy to read and interpret, you will not glean enough information to make it worth the effort.\\n\\nThe input to an evaluation environment is a set of instances with the associated output results/labels, plus a model under test. The system runs the model on each instance, compares each result against this gold standard, and outputs summary statistics and distribution plots showing the performance it achieved on this test set.\\n\\nA good evaluation system has the following properties:\\n\\n\\\\begin{itemize}\\n  \\\\item It produces error distributions in addition to binary outcomes: how close your prediction was, not just whether it was right or wrong. Recall Figure 7.8 for inspiration.\\n  \\\\item It produces a report with multiple plots about several different input distributions automatically, to read carefully at your leisure.\\n  \\\\item It outputs the relevant summary statistics about performance, so you can quickly gauge quality. Are you doing better or worse than last time?\\n\\\\end{itemize}\\n\\nAs an example, Figure 7.10 presents the output of our evaluation environment for the two document-dating models presented in the previous section. Recall that the task is to predict the year of authorship of a given document from word usage. What is worth noting?\\n\\n%---- Page End Break Here ---- Page : 224\\n\\n\\\\begin{itemize}\\n  \\\\item Test sets broken down by type: Observe the the evaluation environment partitioned the inputs into nine separate subsets, some news and some fiction, and of lengths from 100 to 2000 words. Thus at a glance we could see separately how well we do on each.\\n  \\\\item Logical progressions of difficulty: It is obviously harder to make age determination from shorter documents than longer ones. By separating the harder and smaller cases, we better understand our source of errors. We see a big improvement in naive Bayes as we move from 100 to 500 words, but these gains saturate before 2000 words.\\n  \\\\item Problem appropriate statistics: We did not print out every possible error metric, only mean and median absolute error and accuracy (how often did we get the year exactly right?). These are enough for us to see that news is easier than fiction, that our model is much better than the monkey, and that our chances of identifying the actual year correctly (measured by accuracy) are still too small for us to worry about.\\n\\\\end{itemize}\\n\\nThis evaluation gives us the information we need to see how we are doing, without overwhelming us with numbers that we won\\'t ever really look at.\\n\\n\\\\sub\\\\index{for testing}section*{7.5.1 Data Hygiene for evaluation\\\\index{for evaluation}}\\nAn evaluation is only meaningful when you don\\'t fool yourself. Terrible things happen when people evaluate their models in an undisciplined manner, losing the distinction between training, testing, and evaluation data.\\n\\nUpon taking position of a data set with the intention of building a predictive model, your first operation should be to partition the input into three parts:\\n\\n\\\\begin{itemize}\\n  \\\\item Training data: This is what you are completely free to play with. Use it to study the domain, and set the parameters of your model. Typically about $60 \\\\%$ of the full data set should be devoted to training.\\n  \\\\item Testing data: Comprising about $20 \\\\%$ of the full data set, this is what you use to evaluate how good your model is. Typically, people experiment with multiple machine learning approaches or basic parameter settings, so testing enables you to establish the relative performance of all these different models for the same task.\\\\\\\\\\nTesting a model usually reveals that it isn\\'t performing as well as we would like, thus triggering another cycle of design and refinement. Poor performance on test data relative to how it did on the training data suggests a model which has been overfit.\\n  \\\\item Evaluation data: The final $20 \\\\%$ of the data should be set aside for a rainy day: to confirm the performance of the final model right before it goes into production. This works only if you never opened the evaluation data until it was really needed.\\n\\n%---- Page End Break Here ---- Page : 225\\n\\\\end{itemize}\\n\\nThe reason to enforce these separations should be obvious. Students would do much better on examinations if they were granted access to the answer key in advance, because they would know exactly what to study. But this would not reflect how much they actually had learned. Keeping testing data separate from training enforces that the tests measure something important about what the model understands. And holding out the final evaluation data to use only after the model gets stable ensures that the specifics of the test set have not leaked into the model through repeated testing iterations. The evaluation set serves as out-of-sample data to validate the final model.\\n\\nIn doing the original partitioning, you must be careful not to create undesirable artifacts, or destroy desirable ones. Simply partitioning the file in the order it was given is dangerous, because any structural difference between the populations of the training and testing corpus mean\\\\index{mean}s that the model will not perform as well as it should.\\n\\nBut suppose you were building a model to predict future stock prices. It would be dangerous to randomly select $60 \\\\%$ of the samples over all history as the training data, instead of all the samples over the first $60 \\\\%$ of time. Why? Suppose your model \"learned\" which would be the up and down days in the market from the training data, and then used this insight to make virtual predictions for other stocks on these same days. This model would perform far better in testing than in practice. Proper sampling techniques are quite subtle, and discussed in Section 5.2\\n\\nIt is essential to maintain the veil of ignorance over your evaluation data for as long as possible, because you spoil it as soon as you use it. Jokes are never funny the second time you hear them, after you already know the punchline. If you do wear out the integrity of your testing and evaluation sets, the best solution is to start from fresh, out-of-sample data, but this is not always available. Otherwise, randomly re-partition the full data set into fresh training, testing, and evaluation samples, and retrain all of your models from scratch to reboot the process. But this should be recognized as an unhappy outcome.\\n\\n\\\\subsection*{7.5.2 Amplifying small evaluation set\\\\index{small evaluation set}s}\\nThe idea of rigidly partitioning the input into training, test, and evaluation sets makes sense only on large enough data sets. Suppose you have 100,000 records at your disposal. There isn\\'t going to be a qualitative difference between training on 60,000 records instead of 100,000 , so it is better to facilitate a rigorous evaluation.\\n\\nBut what if you only have a few dozen examples? As of this writing, there have been only 45 U.S. presidents, so any analysis you can do on them represents very small sample statistics. New data points come very slowly, only once every four years or so. Similar issues arise in medical trials, which are very expensive to run, potentially yielding data on well under a hundred patients. Any application where we must pay for human annotation means that we will end up with less data for training\\\\index{for training} than we might like.\\n\\nWhat can you do when you cannot afford to give up a fraction of your data\\\\\\n%---- Page End Break Here ---- Page : 226\\n\\\\\\nfor testing? Cross-validation partitions the data into $k$ equal-sized chunks, then trains $k$ distinct models. Model $i$ is trained on the union of all blocks $x \\\\neq i$, totaling $(k-1) / k$ th of the data, and tested on the held out $i$ th block. The average performance of these $k$ classifiers stands in as the presumed accuracy\\\\index{accuracy} for the full model.\\n\\nThe extreme case here is leave one out cross-validation, where $n$ distinct models are each trained on different sets of $n-1$ examples, to determine whether the classifier was good or not. This maximizes the amount of training data, while still leaving something to evaluate against.\\n\\nA real advantage of cross validation\\\\index{cross validation} is that it yields a standard deviation\\\\index{standard deviation} of performance, not only a mean. Each classifier trained on a particular subset of the data will differ slightly from its peers. Further, the test data for each classifier will differ, resulting in different performance scores. Coupling the mean with the standard deviation and assuming normality gives you a performance distribution, and a better idea of how well to trust the results. This makes cross-validation very much worth doing on large data sets as well, because you can afford to make several partitions and retrain, thus increasing confidence that your model is good.\\n\\nOf the $k$ models resulting from cross validation, which should you pick as your final product? Perhaps you could use the one which performed best on its testing quota. But a better alternative is to retrain on all the data and trust that it will be at least as good as the less lavishly trained models. This is not ideal, but if you can\\'t get enough data then you must do the best with what you\\'ve got.\\n\\nHere are a few other ideas that can help to amplify small data sets for training and evaluation:\\n\\n\\\\begin{itemize}\\n  \\\\item Create negative examples from a prior distribution: Suppose one wanted to build a classifier to identify who would be qualified to be a candidate for president. There are very few real examples of presidential candidates (positive instances), but presumably the elite pool is so small that a random person will almost certainly be unqualified. When positive examples are rare, all others are very likely negative, and can be so labeled to provide training data as necessary.\\n  \\\\item Perturb real examples to create similar but synthetic ones: A useful trick to avoid overfitting creates new training instances by adding random noise to distort labeled examples. We then preserve the original outcome label with the new instance.\\n\\\\end{itemize}\\n\\nFor example, suppose we are trying to train an optical character recognition (OCR) system to recognize the letters of some alphabet in scanned pages. An expensive human was originally given the task of labeling a few hundred images with the characters that were contained in them. We can amplify this to a few million images by adding noise at random, and rotating/translating/dilating the region of interest. A classifier trained on\\\\\\n%---- Page End Break Here ---- Page : 227\\n\\\\\\nthis synthetic data should be far more robust than one restricted to the original annotated data.\\n\\n\\\\begin{itemize}\\n  \\\\item Give partial credit when you can: When you have fewer training/testing examples than you want, you must squeeze as much information from each one as possible.\\\\\\\\\\nSuppose that our classifier outputs a value measuring its confidence in its decision, in addition to the proposed label. This confidence level gives us additional resolution with which to evaluate the classifier, beyond just whether it got the label right. It is a bigger strike against the classifier when it gets a confident prediction wrong, than it is on an instance where it thought the answer was a tossup. On a presidential-sized problem, I would trust a classifier that got 30 right and 15 wrong with accurate confidence values much more than one with 32 right and 13 wrong, but with confidence values all over the map.\\n\\\\end{itemize}\\n\\n\\\\subsection*{7.6 War Story: 100\\\\% Accuracy}\\nThe two businessmen looked a little uncomfortable at the university, out of place with their dark blue suits to our shorts and sneakers. Call them Pablo and Juan. But they needed us to make their vision a reality.\\\\\\\\\\n\"The business world still works on paper,\" Pablo explained. He was the one in the darker suit. \"We have a contract to digitize all of Wall Street\\'s financial documents that are still printed on paper. They will pay us a fortune to get a computer to do the scanning. Right now they hire people to type each document in three times, just to make sure they got it absolutely right.\"\\n\\nIt sounded exciting, and they had the resources to make it happen. But there was one caveat. \"Our system cannot make any errors. It can say \\'I don\\'t know\\' sometimes. But whenever it calls a letter it has to be $100 \\\\%$ correct.\"\\\\\\\\\\n\"No problem.\" I told them. \"Just let me say I don\\'t know $100 \\\\%$ of the time, and I can design a system to meet your specification.\"\\n\\nPablo frowned. \"But that will cost us a fortune. In the system we want to build, images of the I don\\'t knows will go to human operators for them to read. But we can\\'t afford to pay them to read everything.\"\\n\\nMy colleagues and I agreed to take the job, and in time we developed a reasonable OCR system from scratch. But one thing bothered me.\\\\\\\\\\n\"These Wall Street guys who gave you the money are smart, aren\\'t they?,\" I asked Pablo one day.\\\\\\\\\\n\"Smart as a whip,\" he answered.\\\\\\\\\\n\"Then how could they possibly believe it when you said you could build an OCR system that was $100 \\\\%$ accurate.\"\\\\\\\\\\n\"Because they thought that I had done it before,\" he said with a laugh.\\\\\\\\\\nIt seems that Pablo\\'s previous company had built a box that digitized price data from the television monitors of the time. In this case, the letters were exact patterns of bits, all written in exactly the same font and the same size.\\n\\n%---- Page End Break Here ---- Page : 228\\n\\nThe TV signal was digital, too, with no error at all from imaging, imperfectlyformed blobs of printers ink, dark spots, or folds in the paper. It was trivial to test for an exact match between a perfect pattern of bits (the image) and another perfect pattern of bits (the character in the device\\'s font), since there is no source of uncertainty. But this problem had nothing to do with OCR, even if both involved reading letters.\\n\\nOur reasonable OCR system did what it could with the business documents it was given, but of course we couldn\\'t get to $100 \\\\%$. Eventually, the Wall Street guys took their business back to the Philippines, where they paid three people to type in each document and voted two out of three if there was any disagreement.\\n\\nWe shifted direction and got into the business of reading handwritten survey forms submitted by consumers, lured by the promise of grocery store coupons. This problem was harder, but the stakes not as high: they were paying us a crummy $\\\\$ 0.22$ a form and didn\\'t expect perfection. Our competition was an operation that used prison labor to type in the data. We caught a break when one of those prisoners sent a threatening letter to an address they found on a survey form, which then threw the business to us. But even our extensive automation couldn\\'t read these forms for less than $\\\\$ 0.40$ a pop, so the contract went back to prison after we rolled belly up.\\n\\nThe fundamental lesson here is that no pattern recognition system for any reasonable problem will bat $100 \\\\%$ all the time. The only way never to be wrong is to never make a prediction. Careful evaluation is necessary to measure how well your system is working and where \\\\index{Feynman, Richard}it is making mistakes, in order to make the system better.\\n\\n\\\\subsection*{7.7 simulation\\\\index{simulation} Models}\\nThere is an important class of first-principle models which are not primarily data driven, yet which prove very valuable for understanding widely diverse phenomena. simulations\\\\index{simulations} are models that attempt to replicate real-world systems and processes, so we can observe and analyze their behavior.\\n\\nSimulations are important for demonstrating the validity of our understanding of a system. A simple simulation which captures much of the behavioral complexity of a system must explain how it works, by Occam\\'s razor. Famous physicist Richard Feynman said, \"What I cannot create, I do not understand.\" What you cannot simulate, and get some level of accuracy in the observed results, you do not understand.\\n\\nMonte Carlo simulations use random numbers to synthesize alternate realities. Replicating an event millions of times under slightly perturbed conditions permits us to generate a probability distribution on the set of outcomes. This was the idea behind permutation tests for statistical significance. We also saw (in Section5.5.2) that random coin flips could stand in for when a batter got hits or made out, so we could simulate an arbitrary number of careers and observe what happened over the course of them.\\n\\nThe key to building an effective Monte Carlo simulation is designing an\\\\\\n%---- Page End Break Here ---- Page : 229\\n\\\\\\nappropriate discrete event model. A new random number is used by the model to replicate each decision or event outcome. You may have to decide whether to go left or go right in a transportation model, so flip a coin. A health or insurance model may have to decide whether a particular patient will have a heart attack today, so flip an appropriately weighted coin. The price of a stock in a financial model can either go up or down at each tick, which again can be a coin flip. A basketball player will hit or miss a shot, with a likelihood that depends upon their shooting skill and the quality of their defender.\\n\\nThe accuracy of such a simulation rests on the probabilities that you assign to heads and tails. This governs how often each outcome occurs. Obviously, you are not restricted to using a fair coin, meaning 50/50. Instead, the probabilities need to reflect assumptions of the likelihood of the event given the state of the model. These parameters are often set using statistical analysis\\\\index{statistical analysis}, by observing the distribution of events as they occurred in the data. Part of the value of Monte Carlo simulations is that they let us play with alternate realities, by changing certain parameters and seeing what happens.\\n\\nA critical aspect of effective simulation is evaluation. Programming errors and modeling inadequacies are common enough that no simulation can be accepted on faith. The key is to hold back one or more classes of observations of the system from direct incorporation in your model. This provides behavior that is out of sample, so we can compare the distribution of results from the simulation with these observations. If they don\\'t jibe, your simulation is just jive. Don\\'t let it go live.\\n\\n\\\\subsection*{7.8 War Story: Calculated Bets}\\nWhere there is gambling there is money, and where there is money there will be models. During our family trips to Florida as a kid, I developed a passion for the sport of jai-alai. And, as I learned how to build mathematical models as a grown-up, I grew obsessed with developing a profitable betting system for the sport.\\n\\nJai-alai is a sport of Basque origin where opposing players or teams alternate hurling a ball against the wall and catching it, until one of them finally misses and loses the point. The throwing and catching is done with an enlarged basket or cesta, the ball or pelota is made of goat skin and hard rubber, and the wall is of granite or concrete; ingredients which lead to fast and exciting action captured in Figure 7.11 In the United States, jai-alai is most associated with the state of Florida, which permits gambling on the results of matches.\\n\\nWhat makes jai-alai of particular interest is its unique and very peculiar scoring system. Each jai-alai match involves eight players, named 1 through 8 to reflect their playing order. Each match starts with players 1 and 2 playing, and the rest of the players waiting patiently in line. All players start the game with zero points each. Each point in the match involves two players; one who will win and one who will lose. The loser will go to the end of the line, while the winner will add to his point total and await the next point, until he has\\\\\\n%---- Page End Break Here ---- Page : 230\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-247}\\n\\nFigure 7.11: Jai-alai is a fast, exciting ball game like handball, but you can bet on it.\\\\\\\\\\naccumulated enough points to claim the match.\\\\\\\\\\nIt was obvious to me, even as a kid, that this scoring system would not be equally fair to all the different players. Starting early in the queue gave you more chances to play, and even a kludge they added to double the value of points later in the match couldn\\'t perfectly fix it. But understanding the strength of these biases could give me an edge in betting.\\n\\nMy quest to build a betting system for jai-alai, started by simulating this very peculiar scoring system. A jai-alai match consists of a sequence of discrete events, described by the following flow structure:\\n\\nInitialize the current players to 1 and 2 .\\\\\\\\\\nInitialize the queue of players to $\\\\{3,4,5,6,7,8\\\\}$.\\\\\\\\\\nInitialize the point total for each player to zero.\\\\\\\\\\nSo long as the current winner has less than 7 points:\\\\\\\\\\nPick a random number to decide who wins the next point.\\\\\\\\\\nAdd one (or if beyond the seventh point, two) to the total of the simulated point winner.\\\\\\\\\\nPut the simulated point loser at the end of the queue.\\\\\\\\\\nGet the next player off the front of the queue.\\\\\\\\\\nEnd So long as.\\\\\\\\\\nIdentify the current point winner as the winner of the match.\\n\\nThe only step here which needs more elaboration is that of simulating a point between two players. If the purpose of our simulation is to see how biases in the scoring system affect the outcome of the match, it makes the most sense to consider the case in which all players are equally skillful. To give every player a $50 / 50$ chance of winning each point he is involved in, we can flip a simulated coin to determine who wins and who loses.\\n\\nI implemented the jai-alai simulation in my favorite programming language, and ran it on $1,000,000$ jai-alai games. The simulation produced a table of statistics, telling me how often each betting outcome paid off, assuming that all the players were equally skillful. Figure 7.12 reports the number of simulated\\n\\n%---- Page End Break Here ---- Page : 231\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|rr|rr|}\\n\\\\hline\\n & \\\\multicolumn{2}{|c}{Simulated} & \\\\multicolumn{2}{c|}{Observed} \\\\\\\\\\nPosition & Wins & \\\\% Wins & Wins & \\\\% Wins \\\\\\\\\\n\\\\hline\\n1 & 162675 & $16.27 \\\\%$ & 1750 & $14.1 \\\\%$ \\\\\\\\\\n2 & 162963 & $16.30 \\\\%$ & 1813 & $14.6 \\\\%$ \\\\\\\\\\n3 & 139128 & $13.91 \\\\%$ & 1592 & $12.8 \\\\%$ \\\\\\\\\\n4 & 124455 & $12.45 \\\\%$ & 1425 & $11.5 \\\\%$ \\\\\\\\\\n5 & 101992 & $10.20 \\\\%$ & 1487 & $12.0 \\\\%$ \\\\\\\\\\n6 & 102703 & $10.27 \\\\%$ & 1541 & $12.4 \\\\%$ \\\\\\\\\\n7 & 88559 & $8.86 \\\\%$ & 1370 & $11.1 \\\\%$ \\\\\\\\\\n8 & 117525 & $11.75 \\\\%$ & 1405 & $11.3 \\\\%$ \\\\\\\\\\n\\\\hline\\n & $1,000,000$ & $100.00 \\\\%$ & 12,383 & $100.0 \\\\%$ \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 7.12: Win biases observed in the jai-alai simulations match well with results observed in actual matches.\\\\\\\\\\nwins for each of the eight starting positions. What insights can we draw from this table?\\n\\n\\\\begin{itemize}\\n  \\\\item Positions 1 and 2 have a substantial advantage over the rest of the field. Either of the initial players are almost twice as likely to come first, second, or third than the poor shlub in position 7 .\\n  \\\\item Positions 1 and 2 win at essentially the same frequency. This is as it should be, since both players start the game on the court instead of in the queue. The fact that players 1 and 2 have very similar statistics increases our confidence in the correctness of the simulation.\\n  \\\\item Positions 1 and 2 do not have identical statistics because we simulated \"only\" one million games. If you flip a coin a million times, it almost certainly won\\'t come up exactly half heads and half tails. However, the ratio of heads to tails should keep getting closer to 50/50 the more coins we flip.\\n\\\\end{itemize}\\n\\nThe simulated gap between players 1 and 2 tells us something about the limitations on the accuracy of our simulation. We shouldn\\'t trust any conclusions which depends upon such small differences in the observed values.\\n\\nTo validate the accuracy of the simulation, we compared our results to statistics on the actual outcomes of over 12,000 jai-alai matches, also in Figure 7.12 The results basically agree with the simulation, subject to the limits of the small sample size. Post positions 1 and 2 won most often in real matches, and position 7 least often.\\n\\nNow we knew the probability that each possible betting opportunity in jaialai paid off. Were we now ready to start making money? Unfortunately not.\\n\\n%---- Page End Break Here ---- Page : 232\\n\\nEven though we have established that post position is a major factor in determining the outcome of jai-alai matches, perhaps the dominant one, we still had several hurdles to overcome before we could bet responsibly:\\n\\n\\\\begin{itemize}\\n  \\\\item The impact of player skills: Obviously, a good player is more likely to win than a bad one, regardless of their post positions. It is clear that a better model for predicting the outcome of jai-alai matches would factor relative skills into the queuing model.\\n  \\\\item The sophistication of the betting public: Many people had noticed the impact of post-position bias before I did. Indeed, data analysis revealed the jai-alai betting public had largely factored the effect of post position in the odds. Fortunately for us, however, largely did not mean completely.\\n  \\\\item The house cut - Frontons keep about $20 \\\\%$ of the betting pool as the house percentage, and thus we had to do much better then the average bettor just to break even.\\n\\\\end{itemize}\\n\\nMy simulation provided information on which outcomes were most likely. It did not by itself identify which are the best bets. A good bet depends both upon the likelihood of the event occurring and the payoff when it occurs. Payoffs are decided by the rest of the betting public. To find the best bets to make, we had to work a lot harder:\\n\\n\\\\begin{itemize}\\n  \\\\item We had to analyze past match data to determine who were the better players. Once we knew who was better, we could bias the simulated coin tosses in their favor, to make our simulation more accurate for each individual match.\\n  \\\\item We had to analyze payoff data to build a model of other bettor\\'s preferences. In jai-alai, you are betting against the public, so you need to be able to model their thinking in order to predict the payoffs for a particular bet.\\n  \\\\item We had to model the impact of the house\\'s cut on the betting pool. Certain bets, which otherwise might have been profitable, go into the red when you factor in these costs.\\n\\\\end{itemize}\\n\\nThe bottom line is that we did it, with $544 \\\\%$ returns on our initial stake. The full story of our gambling system is reported in my book Calculated Bets [Ski01. Check it out: I bet you will like it. It is fun reading about successful models, but even more fun to build them.\\n\\n\\\\subsection*{7.9 Chapter Notes}\\nSilver [Sil12] is an excellent introduction to the complexities of models and forecasting in a variety of domains. Textbooks on mathematical modeling issues include Bender [Ben12] and Giordano [GFH13].\\n\\n%---- Page End Break Here ---- Page : 233\\n\\nThe Google Flu Trends project is an excellent case study in both the power and limitation of big data analysis. See Ginsberg et al. [GMP ${ }^{+} 09$ for the original description, and Lazer et al. [LKKV14] for a fascinating post-mortem on how it all went wrong.\\n\\nTechnical aspects of the OCR system presented in Section 7.6 is reported in Sazaklis et. al. SAMS97. The work on year of authorship detection (and associated evaluation environment example) is from my students Vivek Kulkarni, Parth Dandiwala, and Yingtao Tian KTDS17.\\n\\n\\\\subsection*{7.10 exercises\\\\index{exercises}}\\n\\\\section*{Properties of Models}\\n7-1. [3] Quantum physics is much more complicated than Newtonian physics. Which model passes the Occam\\'s Razor test, and why?\\\\\\\\[0pt]\\n7-2. [5] Identify a set of models of interest. For each of these, decide which properties these models have:\\\\\\\\\\n(a) Are they discrete or continuous?\\\\\\\\\\n(b) Are they linear or non-linear?\\\\\\\\\\n(c) Are they blackbox or descriptive?\\\\\\\\\\n(d) Are they general or ad hoc?\\\\\\\\\\n(e) Are they data driven or first principle?\\n\\n7-3. [3] Give examples of first-principle and data-driven models used in practice.\\\\\\\\[0pt]\\n7-4. [5] For one or more of the following The Quant Shop challenges, discuss whether principled or data-driven models seem to be the more promising approach:\\n\\n\\\\begin{itemize}\\n  \\\\item Miss Universe.\\n  \\\\item Movie gross.\\n  \\\\item Baby weight.\\n  \\\\item Art auction price.\\n  \\\\item Snow on Christmas.\\n  \\\\item Super Bowl/college champion.\\n  \\\\item Ghoul pool.\\n  \\\\item Future gold/oil price.\\n\\\\end{itemize}\\n\\n7-5. [5] For one or more of the following The Quant Shop challenges, partition the full problem into subproblems that can be independently modeled:\\n\\n\\\\begin{itemize}\\n  \\\\item Miss Universe.\\n  \\\\item Movie gross.\\n  \\\\item Baby weight.\\n  \\\\item Art auction price.\\n  \\\\item Snow on Christmas.\\n  \\\\item Super Bowl/college champion.\\n  \\\\item Ghoul pool.\\n  \\\\item Future gold/oil price.\\n\\\\end{itemize}\\n\\n\\\\section*{Evaluation Environments}\\n7-6. [3] Suppose you build a classifier that answers yes on every possible input. What precision and recall will this classifier achieve?\\\\\\\\[0pt]\\n7-7. [3] Explain what precision and recall are. How do they relate to the ROC curve?\\\\\\\\[0pt]\\n7-8. [5] Is it better to have too many false positives, or too many false negatives? Explain.\\\\\\\\[0pt]\\n7-9. [5] Explain what overfitting is, and how you would control for it.\\\\\\\\[0pt]\\n7-10. [5] Suppose $f \\\\leq 1 / 2$ is the fraction of positive elements in a classification. What is the probability $p$ that the monkey should guess positive, as a function of $f$, in order to maximize the specific evaluation metric below? Report both $p$ and the expected evaluation score the monkey achieves.\\\\\\\\\\n(a) Accuracy.\\\\\\\\\\n(b) Precision.\\\\\\\\\\n(c) Recall.\\\\\\\\\\n(d) F-score.\\n\\n7-11. [5] What is cross-validation? How might we pick the right value of $k$ for $k$-fold cross validation?\\\\\\\\[0pt]\\n7-12. [8] How might we know whether we have collected enough data to train a model?\\\\\\\\[0pt]\\n7-13. [5] Explain why we have training, test, and validation data sets and how they are used effectively?\\\\\\\\[0pt]\\n7-14. [5] Suppose we want to train a binary classifier where one class is very rare. Give an example of such a problem. How should we train this model? What metrics should we use to measure performance?\\n\\n7-15. [5] Propose baseline models for one or more of the following The Quant Shop challenges:\\n\\n\\\\begin{itemize}\\n  \\\\item Miss Universe.\\n  \\\\item Movie gross.\\n  \\\\item Baby weight.\\n  \\\\item Art auction price.\\n  \\\\item Snow on Christmas.\\n  \\\\item Super Bowl/college champion.\\n  \\\\item Ghoul pool.\\n  \\\\item Future gold/oil price.\\n\\\\end{itemize}\\n\\n\\\\section*{Implementation Projects}\\n7-16. [5] Build a model to forecast the outcomes of one of the following types of betable events, and rigorously analyze it through back testing:\\\\\\\\\\n(a) Sports like football, basketball, and horse racing.\\\\\\\\\\n(b) Pooled bets involving multiple events, like soccer pools or the NCAA basketball tournament.\\\\\\\\\\n(c) Games of chance like particular lotteries, fantasy sports, and poker.\\\\\\\\\\n(d) Election forecasts for local and congressional elections.\\\\\\\\\\n(e) Stock or commodity price prediction/trading.\\n\\nRigorous testing will probably confirm that your models are not strong enough for profitable wagering, and this is $100 \\\\%$ ok. Be honest: make sure that you are using fresh enough prices/odds to reflect betting opportunities which would still be available at the time you place your simulated bet. To convince me that your model is in fact genuinely profitable, send me a cut of the money and then I will believe you.\\\\\\\\[0pt]\\n7-17. [5] Build a general model evaluation system in your favorite programming language, and set it up with the right data to assess models for a particular problem. Your environment should report performance statistics, error distributions and/or confusion matrices as appropriate.\\n\\n\\\\section*{Interview Questions}\\n7-18. [3] Estimate prior probabilities for the following events:\\\\\\\\\\n(a) The sun will come up tomorrow.\\\\\\\\\\n(b) A major war involving your country will start over the next year.\\\\\\\\\\n(c) A newborn kid will live to be 100 years old.\\\\\\\\\\n(d) Today you will meet the person whom you will marry.\\\\\\\\\\n(e) The Chicago Cubs will win the World Series this year.\\n\\n7-19. [5] What do we mean when we talk about the bias-variance trade-off?\\\\\\\\\\n$7-20$. [5] A test has a true positive rate of $100 \\\\%$ and false positive rate of $5 \\\\%$. In this population 1 out of 1000 people have the condition the test identifies. Given a positive test, what is the probability this person actually has the condition?\\\\\\\\[0pt]\\n7-21. [5] Which is better: having good data or good models? And how do you define good?\\\\\\\\[0pt]\\n7-22. [3] What do you think about the idea of injecting noise into your data set to test the sensitivity of your models?\\\\\\\\[0pt]\\n7-23. [5] How would you define and measure the predictive power of a metric?\\n\\n\\\\section*{Kaggle Challenges}\\n$7-24$. Will a particular grant application be funded?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/unimelb}{https://www.kaggle.com/c/unimelb}\\\\\\\\\\n$7-25$. Who will win the NCAA basketball tournament? \\\\href{https://www.kaggle.com/c/march-machine-learning-mania-2016}{https://www.kaggle.com/c/march-machine-learning-mania-2016}\\\\\\\\\\n$7-26$. Predict the annual sales in a given restaurant. \\\\href{https://www.kaggle.com/c/restaurant-revenue-prediction}\\n%---- Page End Break Here ---- Page : 236\\n{https://www.kaggle.com/c/restaurant-revenue-prediction}\\n\\n\\\\section*{Chapter 8}\\n\\\\section*{Linear Algebra}\\nWe often hear that mathematics consists mainly of \"proving theorems.\" Is a writer\\'s job mainly that of \"writing sentences?\"\\n\\n\\\\begin{itemize}\\n  \\\\item Gian-Carlo Rota\\n\\\\end{itemize}\\n\\nThe data part of your data science project involves reducing all of the relevant information you can find into one or more data matrices, ideally as large as possible. The rows of each matrix represent items or examples, while the columns represent distinct features or attributes.\\n\\nLinear algebra is the mathematics of matrices: the properties of arrangements of numbers and the operations that act on them. This makes it the language of data science. Many machine learning algorithms are best understood through linear algebra. Indeed algorithms for problems like linear regression can be reduced to a single formula, multiplying the right chain of matrix products to yield the desired results. Such algorithms can simultaneously be both simple and intimidating, trivial to implement and yet hard to make efficient and robust.\\n\\nYou presumably took a course in linear algebra at some point, but perhaps have forgotten much of it. Here I will review most of what you need to know: the basic operations on matrices, why they are useful, and how to build an intuition for what they do.\\n\\n\\\\subsection*{8.1 The Power of Linear Algebra}\\nWhy is linear algebra so powerful? It regulates how matrices work, and matrices are everywhere. Matrix representations of important objects include:\\n\\n\\\\begin{itemize}\\n  \\\\item Data: The most generally useful representation of numerical data sets are as $n \\\\times m$ matrices. The $n$ rows represent objects, items, or instances, while the $m$ columns each represent distinct features or dimensions.\\n  \\\\item geometric point sets\\\\index{geometric point sets}: An $n \\\\times m$ matrix can represent a cloud of points in space. The $n$ rows each represent a geometric point, while the $m$ columns define the dimensions. Certain matrix operations have distinct geometric interpretations, enabling us to generalize the two-dimensional geometry we can actually visualize into higher-dimensional spaces.\\n  \\\\item Systems of equations: A linear equation\\\\index{linear equation} is defined by the sum of variables weighted by constant coefficients, like:\\n\\\\end{itemize}\\n\\n$$\\ny=c_{0}+c_{1} x_{1}+c_{2} x_{2}+\\\\ldots c_{m-1} x_{m-1}\\n$$\\n\\nA system of $n$ linear equations can be represented as an $n \\\\times m$ matrix, where each row represents an equation, and each of the $m$ columns is associated with the coefficients of a particular variable (or the constant \"variable\" 1 in the case of $c_{0}$ ). Often it is necessary to represent the $y$ value for each equation as well. This is typically done using a separate $n \\\\times 1$ array or vector of solution values.\\n\\n\\\\begin{itemize}\\n  \\\\item graphs\\\\index{graphs} and networks\\\\index{networks}: Graphs are made up of vertices and edges, where edges are defined as ordered pairs of vertices, like $(i, j)$. A graph with $n$ vertices and $m$ edges can be represented as an $n \\\\times n$ matrix $M$, where $M[i, j]$ denotes the number (or weight) of edges from vertex $i$ to vertex $j$. There are surprising connections between combinatorial properties and linear algebra, such as the relationship between paths in graphs and matrix multiplication, and how vertex clusters relate to the eigenvalues/vectors of appropriate matrices.\\n  \\\\item rearrangement operations\\\\index{rearrangement operations}: Matrices can do things. Carefully designed matrices can perform geometric operations on point sets, like translation, rotation, and scaling. Multiplying a data matrix by an appropriate permutation matrix will reorder its rows and columns. Movements can be defined by vectors, the $n \\\\times 1$ matrices powerful enough to encode operations like translation and permutation.\\n\\\\end{itemize}\\n\\nThe ubiquity of matrices means that a substantial infrastructure of tools has been developed to manipulate them. In particular, the high-performance linear algebra libraries for your favorite programming language mean that you should never implement any basic algorithm by yourself. The best library implementations optimize dirty things like numerical precision, cache-misses, and the use of multiple cores, right down to the assembly-language level. Our job is to formulate the problem using linear algebra, and leave the algorithmics to these libraries.\\n\\n\\\\subsection*{8.1.1 Interpreting Linear Algebraic Formulae}\\nConcise formulas written as products of matrices can provide the power to do amazing things, including linear regression, matrix compression, and geometric\\\\\\n%---- Page End Break Here ---- Page : 238\\n\\\\\\ntransformations. Algebraic substitution coupled with a rich set of identities yields elegant, mechanical ways to manipulate such formulas.\\n\\nHowever, I find it very difficult to interpret such strings of operations in ways that I really understand. For example, take the \"algorithm\" behind least squares linear regression, which is:\\n\\n$$\\nc=\\\\left(A^{T} A\\\\right)^{-1} A^{T} b\\n$$\\n\\nwhere the $n \\\\times m$ system is $A x=b$ and $w$ is the vector of coefficients of the best fitting line.\\n\\nOne reason why I find linear algebra challenging is the nomenclature. There are many different terms and concepts which must be grokked to really follow what is going on. But a bigger problem is that most of the proofs are, for good reason, algebraic. To my taste, algebraic proofs generally do not carry intuition about why things work the way they do. Algebraic proofs are easier to verify step-by-step in a mechanical way, rather than by understanding the ideas behind the argument.\\n\\nI will present only one formal proof in this text. And by design both the theorem and the proof are incorrect.\\n\\nTheorem 1. $2=1$.\\\\\\\\\\nProof.\\n\\n$$\\n\\\\begin{aligned}\\na & =b \\\\\\\\\\na^{2} & =a b \\\\\\\\\\na^{2}-b^{2} & =a b-b^{2} \\\\\\\\\\n(a+b)(a-b) & =b(a-b) \\\\\\\\\\na+b & =b \\\\\\\\\\n2 b & =b \\\\\\\\\\n2 & =1\\n\\\\end{aligned}\\n$$\\n\\nIf you have never seen such a proof before, you might find it convincing, even though I trust you understand on a conceptual level that $2 \\\\neq 1$. Each line follows from the one before it, through direct algebraic substitution. The problem, as it turns out, comes when canceling $(a-b)$, because we are in fact dividing by zero.\\n\\nWhat are the lessons from this proof? Proofs are about ideas, not just algebraic manipulation. No idea means no proof. To understand linear algebra, your goal should be to first validate the simplest interesting case (typically two dimensions) in order to build intuition, and then try to imagine how it might generalize to higher dimensions. There are always special cases to watch for, like division by zero. In linear algebra, these cases include dimensional mismatches\\\\\\n%---- Page End Break Here ---- Page : 239\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-256}\\n\\nFigure 8.1: Points can be reduced to vectors\\\\index{vectors} on the unit\\\\index{unit} sphere, plus magnitudes.\\\\\\\\\\nand singular (meaning non-invertible) matrices. The theory of linear algebra works except when it doesn\\'t work, and it is better to think in terms of the common cases rather than the pathological ones.\\n\\n\\\\subsection*{8.1.2 geometry\\\\index{geometry} and Vectors}\\nThere is a useful interpretation of \"vectors,\" meaning $1 \\\\times d$ matrices, as vectors in the geometric sense, meaning directed rays from the origin through a given point in $d$ dimensions.\\n\\nNormalizing each such vector $v$ to be of unit length (by dividing each coordinate by the distance from $v$ to the origin) puts it on a $d$-dimensional sphere, as shown in Figure 8.1 a circle for points in the plane, a real sphere for $d=3$, and some unvisualizable hypersphere for $d \\\\geq 4$.\\n\\nThis normalization proves a useful thing to do. The distances between points become angles between vectors, for the purposes of comparison. Two nearby points will define a small angle between them through the origin: small distances imply small angles. Ignoring magnitudes is a form of scaling, making all points directly comparable.\\n\\nThe dot product\\\\index{dot product} is a useful operation reducing vectors to scalar quantities. The dot product of two length- $n$ vectors $A$ and $B$ is defined:\\n\\n$$\\nA \\\\cdot B=\\\\sum_{i=1}^{n} A_{i} B_{i}\\n$$\\n\\nWe can use the dot product operation to compute the angle $\\\\theta=\\\\angle A O B$ between vectors $A$ and $B$, where $O$ is the origin:\\n\\n$$\\n\\\\cos (\\\\Theta)=\\\\frac{A \\\\cdot B}{\\\\|A\\\\|\\\\|B\\\\|}\\n$$\\n\\nLet\\'s try to parse this formula. The $\\\\|V\\\\|$ symbol means \"the length of $V$.\" For unit vectors, this is, by definition, equal to 1 . In general, it is the quantity by which we must divide $V$ by to make it a unit vector.\\\\\\n%---- Page End Break Here ---- Page : 240\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-257}\\n\\nFigure 8.2: The dot product of two vectors defines the cosine of the angle between them.\\n\\nBut what is the connection between dot product and angle? Consider the simplest case of an angle defined between two rays, $A$ at zero degrees and $B=(x, y)$. Thus the unit ray is $A=(1,0)$. In this case, the dot product is $1 \\\\cdot x+0 \\\\cdot y=x$, which is exactly what $\\\\cos (\\\\theta)$ should be if $B$ is a unit vector. We can take it on faith that this generalizes for general $B$, and to higher dimensions.\\n\\nSo a smaller angle means closer points on the sphere. But there is another connection between things we know. Recall the special cases of the cosine function, here given in radians:\\n\\n$$\\n\\\\cos (0)=1, \\\\quad \\\\cos (\\\\pi / 2)=0, \\\\quad \\\\cos (\\\\pi)=-1\\n$$\\n\\nThe values of the cosine function range from $[-1,1]$, exactly the same range as that of the correlation coefficient. Further, the interpretation is the same: two identical vectors are perfectly correlated, while antipodal points are perfectly negatively correlated. Orthogonal points/vectors (the case of $\\\\Theta=\\\\pi / 2$ ) have as little to do with each other as possible.\\n\\nThe cosine function is exactly the correlation of two mean-zero variables. For unit vectors, $\\\\|A\\\\|=\\\\|B\\\\|=1$, so the angle between $A$ and $B$ is completely defined by the dot product.\\n\\nTake-Home Lesson: The dot product of two vectors measures similarity in exactly the same way as the Pearson correlation coefficient.\\n\\n\\\\subsection*{8.2 visualizing\\\\index{visualizing} Matrix Operations}\\nI assume that you have had some previous exposure to the basic matrix operations of transposition, multiplication\\\\index{multiplication}, and inversion. This section is intended as a refresher, rather than an introduction.\\\\\\n%---- Page End Break Here ---- Page : 241\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-258}\\n\\nFigure 8.3: Matrix image examples: Lincoln (left) and his memorial (right). The center image is a linear combination of left and right, for $\\\\alpha=0.5$.\\n\\nBut to provide better intuition, I will represent matrices as images rather than numbers, so we can see what happens when we operate on them. Figure 8.3 shows our primary matrix images: President Abraham Lincoln (left) and the building which serves as his memorial (right). The former is a human face, while the latter contains particularly strong rows and columns.\\n\\nBe aware that we will be quietly rescaling the matrix between each operation, so the absolute color does not matter. The interesting patterns come in the differences between light and dark, meaning the smallest and biggest numbers in the current matrix. Also, note that the origin element of the matrix $M[1,1]$ represents the upper left corner of the image.\\n\\n\\\\subsection*{8.2.1 Matrix addition\\\\index{addition}}\\nMatrix addition is a simple operation: for matrices $A$ and $B$, each of dimensions $n \\\\times m, C=A+B$ implies that:\\n\\n$$\\nC_{i j}=A_{i j}+B_{i j}, \\\\text { for all } 1 \\\\leq i \\\\leq n \\\\text { and } 1 \\\\leq j \\\\leq m\\n$\\\\index{Lincoln, Abraham}$\\n\\nScalar multiplication provides a way to change the weight of every element in a matrix simultaneously, perhaps to normalize them. For any matrix $A$ and number $c, A^{\\\\prime}=c \\\\cdot A$ implies that\\n\\n$$\\nA_{i j}^{\\\\prime}=c A_{i j}, \\\\text { for all } 1 \\\\leq i \\\\leq n \\\\text { and } 1 \\\\leq j \\\\leq m\\n$$\\n\\nCombining matrix addition with scalar multiplication gives us the power to perform linear combinations of\\\\index{linear combinations of} matrices. The formula $\\\\alpha \\\\cdot A+(1-\\\\alpha) \\\\cdot B$ enables us to fade smoothly between $A$ (for $\\\\alpha=1$ ) and $B$ (for $\\\\alpha=0$ ), as shown in Figure 8.3. This provides a way to morph the images from $A$ to $B$.\\n\\nThe transpose of\\\\index{transpose of} a matrix $M$ interchanges rows and columns, turning an $a \\\\times b$ matrix into a $b \\\\times a$ matrix $M^{T}$, where\\n\\n$$\\nM_{i j}^{T}=M_{j i} \\\\text { for all } 1 \\\\leq i \\\\leq n \\\\text { and } 1 \\\\leq j \\\\leq m\\n$$\\n\\nThe transpose of a square matrix is a square matrix, so $M$ and $M^{T}$ can safely be added or multiplied together. More generally, the transpose is an operation that is used to orient a matrix so it can be added to or multiplied by its target.\\\\\\n%---- Page End Break Here ---- Page : 242\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-259}\\n\\nFigure 8.4: Lincoln (left) and its transposition (right). The sum of a matrix and its transposition is symmetric along its main diagonal (right).\\n\\nThe transpose of a matrix sort of \"rotates\" it by 180 degrees, so $\\\\left(A^{T}\\\\right)^{T}=A$. In the case of square matrices, adding a matrix to its transpose is symmetric, as shown in Figure 8.4 (right). The reason is clear: $C=A+A^{T}$ implies that\\n\\n$$\\nC_{i j}=A_{i j}+A_{j i}=C_{j i} .\\n$$\\n\\n\\\\subsection*{8.2.2 matrix multiplication\\\\index{matrix multiplication}}\\nMatrix multiplication is an aggregate version of the vector dot or inner product\\\\index{inner product}. Recall that for two $n$-element vectors, $X$ and $Y$, the dot product $X \\\\cdot Y$ is defined:\\n\\n$$\\nX \\\\cdot Y=\\\\sum_{i=1}^{n} X_{i} Y_{i}\\n$$\\n\\nDot products measure how \"in sync\" the two vectors are. We have already seen the dot product when computing the cosine distance and correlation coefficient. It is an operation that reduces a pair of vectors to a single number.\\n\\nThe matrix product $X Y^{T}$ of these two vectors produces a $1 \\\\times 1$ matrix containing the dot product $X \\\\cdot Y$. For general matrices, the product $C=A B$ is defined by:\\n\\n$$\\nC_{i j}=\\\\sum_{i=1}^{k} A_{i k} \\\\cdot B_{k j}\\n$$\\n\\nFor this to work, $A$ and $B$ must share the same inner dimensions, implying that if $A$ is $n \\\\times k$ then $B$ must have dimensions $k \\\\times m$. Each element of the $n \\\\times m$ product matrix $C$ is a dot product of the $i$ th row of $A$ with the $j$ th column of $B$.\\n\\nThe most important properties of matrix multiplication are:\\n\\n\\\\begin{itemize}\\n  \\\\item It does not commute: commutativity\\\\index{commutativity} is the notation that order doesn\\'t matter, that $x \\\\cdot y=y \\\\cdot x$. Although we take commutativity for granted when multiplying integers, order does matter in matrix multiplication. For any pair of non-square matrices $A$ and $B$, at most one of either $A B$ or $B \\n%---- Page End Break Here ---- Page : 243\\nA$\\\\\\\\\\nhas compatible dimensions. But even square matrix multiplication does not commute, as shown by the products below:\\n\\\\end{itemize}\\n\\n$$\\n\\\\left[\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right] \\\\cdot\\\\left[\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n2 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right] \\\\neq\\\\left[\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right] \\\\cdot\\\\left[\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n1 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nand the covariance\\\\index{covariance} matrices of Figure 8.5\\n\\n\\\\begin{itemize}\\n  \\\\item Matrix multiplication is associative: associativity\\\\index{associativity} grants us the right to parenthesize as we wish, performing operations in the relative order that we choose. In computing the product $A B C$, we have a choice of two options: $(A B) C$ or $A(B C)$. Longer chains of matrices permit even more freedom, with the number of possible parenthesizations growing exponentially in the length of the chain. All of these will return the same answer, as demonstrated here:\\n\\\\end{itemize}\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\left(\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n4 & 4\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 2\\n\\\\end{array}\\\\right]\\\\right)\\\\left[\\\\begin{array}{ll}\\n3 & 2 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n1 & 4 \\\\\\\\\\n3 & 8\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}\\n3 & 2 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}\\n7 & 2 \\\\\\\\\\n17 & 6\\n\\\\end{array}\\\\right] \\\\\\\\\\n& {\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 4\\n\\\\end{array}\\\\right]\\\\left(\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 2\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}\\n3 & 2 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]\\\\right)=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 4\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}\\n3 & 2 \\\\\\\\\\n2 & 0\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}\\n7 & 2 \\\\\\\\\\n17 & 6\\n\\\\end{array}\\\\right]}\\n\\\\end{aligned}\\n$$\\n\\nThere are two primary reasons why associativity matters to us. In an algebraic sense, it enables us to identify neighboring pairs of matrices in a chain and replace them according to an identity, if we have one. But the other issue is computational. The size of intermediate matrix products can easily blow up in the middle. Suppose we seek to calculate $A B C D$, where $A$ is $1 \\\\times n, B$ and $C$ are $n \\\\times n$, and $D$ is $n \\\\times 1$. The product $(A B)(C D)$ costs only $2 n^{2}+n$ operations, assuming the conventional nested-loop matrix multiplication algorithm. In contrast, $(A(B C)) D$ weighs in at $n^{3}+n^{2}+n$ operations.\\n\\nThe nested-loop matrix multiplication algorithm you were taught in high school is trivially easy to program, and indeed appears on page 398 But don\\'t program it. Much faster and more numerically stable algorithms exist in the highly optimized linear algebra libraries associated with your favorite programming language. Formulating your algorithms as matrix products on large arrays, instead of using ad hoc logic is counter-intuitive to most computer scientists. But this strategy can produce very big performance wins in practice.\\n\\n\\\\subsection*{8.2.3 applications\\\\index{applications} of Matrix Multiplication}\\nOn the face of it, matrix multiplication is an ungainly operation. When I was first exposed to linear algebra, I couldn\\'t understand why we couldn\\'t just multiply the numbers on a pairwise basis, like matrix addition, and be done with it.\\n\\nThe reason we care about matrix multiplication is that there are many things we can do with it. We will review these applications here.\\\\\\n%---- Page End Break Here ---- Page : 244\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-261}\\n\\nFigure 8.5: The Lincoln memorial $M$ (left) and its covariance matrices. The big block in the middle of $M \\\\cdot M^{T}$ (center) results from the similarity of all rows from the middle stripe of $M$. The tight grid pattern of $M^{T} \\\\cdot M$ (right) reflects the regular pattern of the columns on the memorial building.\\n\\n\\\\section*{Covariance Matrices}\\nMultiplying a matrix $A$ by its transpose $A^{T}$ is a very common operation. Why? For one thing, we can multiply it: if $A$ is an $n \\\\times d$ matrix, then $A^{T}$ is a $d \\\\times$ $n$ matrix. Thus it is always compatible to multiply $A A^{T}$. They are equally compatible to multiply the other way, i.e. $A^{T} A$.\\n\\nBoth of these products have important interpretations. Suppose $A$ is an $n \\\\times d$ feature matrix, consisting of $n$ rows representing items or points, and $d$ columns representing the observed features of these items. Then:\\n\\n\\\\begin{itemize}\\n  \\\\item $C=A \\\\cdot A^{T}$ is an $n \\\\times n$ matrix of dot products, measuring the \"in syncness\" among the points. In particular $C_{i j}$ is a measure of how similar item $i$ is to item $j$.\\n  \\\\item $D=A^{T} \\\\cdot A$ is a $d \\\\times d$ matrix of dot products, measuring the \"in sync-ness\" among columns or features. Now $D_{i j}$ represents the similarity between feature $i$ and feature $j$.\\n\\\\end{itemize}\\n\\nThese beasts are common enough to earn their own name, covariance matrices. This term comes up often in conversations among data scientists, so get comfortable with it. The covariance formula we gave when computing the correlation coefficient was\\n\\n$$\\n\\\\operatorname{Cov}(X, Y)=\\\\sum_{i=1}^{n}\\\\left(X_{i}-\\\\bar{X}\\\\right)\\\\left(Y_{i}-\\\\bar{Y}\\\\right)\\n$$\\n\\nso, strictly speaking, our beasts are covariance matrices only if the rows or columns of $A$ have mean zero. But regardless, the magnitudes of the matrix product captures the degree to which the values of particular row or column pairs move together.\\n\\nFigure 8.5 presents the covariance matrices of the Lincoln memorial. Darker spots define rows and columns in the image with the greatest similarity. Try to understand where the visible structures in these covariance matrices come from.\\n\\n%---- Page End Break Here ---- Page : 245\\n\\nFigure 8.5 (center) presents $M \\\\cdot M^{T}$, the covariance matrix of the rows. The big dark box in the middle represents the large dot products resulting from any two rows cutting across all the memorial\\'s white columns. These bands of light and dark are strongly correlated, and the intensely dark regions contribute to a large dot product. The light rows corresponding to the sky, pediment, and stairs are equally correlated and coherent, but lack the dark regions to make their dot products large enough.\\n\\nThe right image presents $M^{T} \\\\cdot M$, which is the covariance matrix of the columns. All the pairs of matrix columns strongly correlate with each other, either positively or negatively, but the matrix columns through the white building columns have low weight and hence a small dot product. Together, they define a checkerboard of alternating dark and light stripes.\\n\\n\\\\section*{Matrix Multiplication and paths\\\\index{paths}}\\nSquare matrices can be multiplied by themselves without transposition. Indeed, $A^{2}=A \\\\times A$ is called the square of matrix $A$. More generally $A^{k}$ is called the $k$ th power of the matrix.\\n\\nThe powers of matrix $A$ have a very natural interpretation, when $A$ represents the adjacency\\\\index{adjacency} matrix of a graph or network. In an adjacency matrix, $A[i, j]=1$ when $(i, j)$ is an edge in the network. Otherwise, when $i$ and $j$ are not direct neighbors, $A[i, j]=0$.\\n\\nFor such $0 / 1$ matrices, the product $A^{2}$ yields the number of paths of length two in $A$. In particular:\\n\\n$$\\nA^{2}[i, j]=\\\\sum_{k=1}^{n} A[i, k] \\\\cdot A[k, j] .\\n$$\\n\\nThere is exactly one path of length two from $i$ to $j$ for every intermediate vertex $k$ such that $(i, k)$ and $(k, j)$ are both edges in the graph. The sum of these path counts is computed by the dot product above.\\n\\nBut computing powers of matrices makes sense even for more general matrices. It simulates the effects of diffusion, spreading out the weight of each element among related elements. Such things happen in Google\\'s famous PageRank algorithm, and other iterative processes such as contagion spreading.\\n\\n\\\\section*{Matrix Multiplication and permutation\\\\index{permutation}s}\\nMatrix multiplication is often used just to rearrange the order of the elements in a particular matrix. Recall that high-performance matrix multiplication routines are blindingly fast, enough so they can often perform such operations faster than ad hoc programming logic. They also provide a way to describe such operations in the notation of algebraic formulae, thus preserving compactness and readability.\\n\\nThe most famous rearrangement matrix does nothing at all. The identity matrix is an $n \\\\times n$ matrix consisting of all zeros, except for the ones all along\\\\\\n%---- Page End Break Here ---- Page : 246\\n\\\\\\n$P=\\\\left(\\\\begin{array}{llll}0 & 0 & 1 & 0 \\\\\\\\ 1 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 1 \\\\\\\\ 0 & 1 & 0 & 0\\\\end{array}\\\\right) \\\\quad M=\\\\left(\\\\begin{array}{llll}11 & 12 & 13 & 14 \\\\\\\\ 21 & 22 & 23 & 24 \\\\\\\\ 31 & 32 & 33 & 34 \\\\\\\\ 41 & 42 & 43 & 44\\\\end{array}\\\\right) \\\\quad P M=\\\\left(\\\\begin{array}{llll}31 & 32 & 33 & 34 \\\\\\\\ 11 & 12 & 13 & 14 \\\\\\\\ 41 & 42 & 43 & 44 \\\\\\\\ 21 & 22 & 23 & 24\\\\end{array}\\\\right)$\\n\\nFigure 8.6: Multiplying a matrix by a permutation matrix rearranges its rows and columns.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-263}\\n\\nFigure 8.7: Multiplying the Lincoln matrix $M$ by the reverse permutation matrix $r$ (center). The product $r \\\\cdot M$ flips Lincoln upside down (left), while $M \\\\cdot r$ parts his hair on the other side of his head (right).\\\\\\\\\\nthe main diagonal. For $n=4$,\\n\\n$$\\nI=\\\\left[\\\\begin{array}{llll}\\n1 & 0 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 & 0 \\\\\\\\\\n0 & 0 & 1 & 0 \\\\\\\\\\n0 & 0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nConvince yourself that $A I=I A=A$, meaning that multiplication by the identity matrix commutes.\\n\\nNote that each row and column of $I$ contains exactly one non-zero element. Matrices with this property are called permutation matrices, because the nonzero element in position $(i, j)$ can be interpreted as meaning that element $i$ is in position $j$ of a permutation. For example, the permutation $(2,4,3,1)$ defines the permutation matrix:\\n\\n$$\\nP_{(2431)}=\\\\left[\\\\begin{array}{llll}\\n0 & 0 & 0 & 1 \\\\\\\\\\n1 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 1 & 0 \\\\\\\\\\n0 & 1 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nObserve that the identity matrix corresponds to the permutation $(1,2, \\\\ldots, n)$.\\\\\\\\\\nThe key point here is that we can multiply $A$ by the appropriate permutation matrix to rearrange the rows and columns, however we wish. Figure 8.7 shows what happens when we multiply our image by a \"reverse\" permutation matrix\\\\\\n%---- Page End Break Here ---- Page : 247\\n\\\\\\n$r$, where the ones lie along the minor diagonal. Because matrix multiplication is not generally commutative, we get different results for $A \\\\cdot r$ and $r \\\\cdot A$. Convince yourself why.\\n\\n\\\\section*{rotating\\\\index{rotating} Points in Space}\\nMultiplying something by the right matrix can have magical properties. We have seen how a set of $n$ points in the plane (i.e. two dimensions) can be represented by an ( $n \\\\times 2$ )-dimensional matrix $S$. Multiplying such points by the right matrix can yield natural geometric transformations.\\n\\nThe rotation\\\\index{rotation} matrix $R_{\\\\theta}$ performs the transformation of rotating points about the origin through an angle of $\\\\theta$. In two dimensions, $R_{\\\\theta}$ is defined as\\n\\n$$\\nR_{\\\\theta}=\\\\left[\\\\begin{array}{cc}\\n\\\\cos (\\\\theta) & -\\\\sin (\\\\theta) \\\\\\\\\\n\\\\sin (\\\\theta) & \\\\cos (\\\\theta)\\n\\\\end{array}\\\\right]\\n$$\\n\\nIn particular, after the appropriate multiplication/rotation, point $(x, y)$ goes to\\n\\n$$\\n\\\\left[\\\\begin{array}{l}\\nx^{\\\\prime} \\\\\\\\\\ny^{\\\\prime}\\n\\\\end{array}\\\\right]=R_{\\\\theta}\\\\left[\\\\begin{array}{l}\\nx \\\\\\\\\\ny\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\nx \\\\cos (\\\\theta)-y \\\\sin (\\\\theta) \\\\\\\\\\nx \\\\sin (\\\\theta)+y \\\\cos (\\\\theta)\\n\\\\end{array}\\\\right]\\n$$\\n\\nFor $\\\\theta=180^{\\\\circ}=\\\\pi$ radians, $\\\\cos (\\\\theta)=-1$ and $\\\\sin (\\\\theta)=0$, so this reduces to $(-x,-y)$, doing the right thing by putting the point in the opposing quadrant.\\n\\nFor our ( $n \\\\times 2$ )-dimensional point matrix $S$, we can use the transpose function to orient the matrix appropriately. Check to confirm that\\n\\n$$\\nS^{\\\\prime}=\\\\left(R_{\\\\theta} S^{T}\\\\right)^{T}\\n$$\\n\\ndoes exactly what we want to do.\\\\\\\\\\nNatural generalizations of $R_{\\\\theta}$ exist to rotate points in arbitrary dimensions. Further, arbitrary sequences of successive transformations can be realized by multiplying chains of rotation, dilation, and reflection matrices, yielding a compact description of complex manipulations.\\n\\n\\\\subsection*{8.2.4 Identity Matrices and inversion\\\\index{inversion}}\\nIdentity operations play a big role in algebraic structures. For numerical addition, zero is the identity element, since $0+x=x+0=x$. The same role is played by one for multiplication, since $1 \\\\cdot x=x \\\\cdot 1=x$.\\n\\nIn matrix multiplication, the identity element is the identity matrix, with all ones down the main diagonal. Multiplication by the identity matrix commutes, so $I A=A I=A$.\\n\\nThe inverse operation is about taking an element $x$ down to its identity element. For numerical addition, the inverse of $x$ is $(-x)$, because $x+(-x)=0$. The inverse operation for multiplication is called division. We can invert a number by multiplying it by its reciprocal, since $x \\\\cdot(1 / x)=1$.\\n\\nPeople do not generally talk about dividing matrices. However, they very frequently go about inverting them. We say $A^{-1}$ is the multiplicative inverse\\\\\\n%---- Page End Break Here ---- Page : 248\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-265}\\n\\nFigure 8.8: The inverse of Lincoln does not look much like the man (left) but $M \\\\cdot M^{-1}$ produces the identity matrix, modulo small non-zero terms due to numerical precision issues.\\\\\\\\\\nof matrix $A$ if $A \\\\cdot A^{-1}=I$, where $I$ is the identity matrix. Inversion is an important special case of division, since $A \\\\cdot A^{-1}=I$ implies $A^{-1}=I / A$. They are in fact equivalent operations, because $A / B=A \\\\cdot B^{-1}$.\\n\\nFigure 8.8 (left) shows the inverse of our Lincoln picture, which looks pretty much like random noise. But multiplying it by the image yields the thin main diagonal of the identity matrix, albeit superimposed on a background of numerical error. Floating point computations are inherently imprecise, and algorithms like inversion which perform repeated additions and multiplications often accumulate error in the process.\\n\\nHow can we compute the inverse of a matrix? A closed form exists for finding the inverse $A^{-1}$ of a $2 \\\\times 2$ matrix $A$, namely:\\n\\n$$\\nA^{-1}\\\\index{multiplicative inverse of}=\\\\left[\\\\begin{array}{ll}\\na & b \\\\\\\\\\nc & d\\n\\\\end{array}\\\\right]^{-1}=\\\\frac{1}{a d-b c}\\\\left[\\\\begin{array}{cc}\\nd & -b \\\\\\\\\\n-c & a\\n\\\\end{array}\\\\right]\\n$$\\n\\nMore generally, there is an approach to inverting matrices by solving a linear system using Gaussian elimination\\\\index{elimination}.\\n\\nObserve that this closed form for inversion divides by zero whenever the products of the diagonals are equal, i.e. $a d=b c$. This tells us that such matrices are not invertible or singular\\\\index{singular}, meaning no inverse exists. Just as we cannot divide numbers by zero, we cannot invert singular matrices.\\n\\nThe matrices we can invert are called non-singular\\\\index{non-singular}, and life is better when our matrices have this property. The test of whether a matrix is invertible is whether its determinant\\\\index{determinant} is not zero. For $2 \\\\times 2$ matrices, the determinant is the difference between the product of its diagonals, exactly the denominator in the inversion formula.\\n\\nFurther, the determinant is only defined for square matrices, so only square matricies are invertible. The cost of computing this determinant is $O\\\\left(n^{3}\\\\right)$, so it is expensive on large matricies, indeed as expensive as trying to invert the matrix itself using Gaussian elimination.\\n\\n%---- Page End Break Here ---- Page : 249\\n\\n$$\\n\\\\begin{aligned}\\n{[A \\\\mid I] } & =\\\\left[\\\\begin{array}{ccc|ccc}\\n6 & 4 & 1 & 1 & 0 & 0 \\\\\\\\\\n10 & 7 & 2 & 0 & 1 & 0 \\\\\\\\\\n5 & 3 & 1 & 0 & 0 & 1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{lll|lll}\\n1 & 1 & 0 & 1 & 0 & -1 \\\\\\\\\\n0 & 1 & 0 & 0 & 1 & -2 \\\\\\\\\\n5 & 3 & 1 & 0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\\\\\\\n& =\\\\left[\\\\begin{array}{ccc|ccc}\\n1 & 0 & 0 & 1 & -1 & 1 \\\\\\\\\\n0 & 1 & 0 & 0 & 1 & -2 \\\\\\\\\\n5 & 3 & 1 & 0 & 0 & 1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ccc|ccc}\\n1 & 0 & 0 & 1 & -1 & 1 \\\\\\\\\\n0 & 1 & 0 & 0 & 1 & -2 \\\\\\\\\\n0 & 0 & 1 & -5 & 2 & 2\\n\\\\end{array}\\\\right] \\\\\\\\\\n\\\\rightarrow A^{-1} & =\\\\left[\\\\begin{array}{ccc|}\\n1 & -1 & 1 \\\\\\\\\\n0 & 1 & -2 \\\\\\\\\\n-5 & 2 & 2\\n\\\\end{array}\\\\right]\\n\\\\end{aligned}\\n$$\\n\\nFigure 8.9: The inverse of a matrix can be computed by Gaussian elimination.\\n\\n\\\\subsection*{8.2.5 Matrix Inversion and linear systems\\\\index{linear systems}}\\nLinear equations are defined by the sum of variables weighted by constant coefficients:\\n\\n$$\\ny=c_{0}+c_{1} x_{1}+c_{2} x_{2}+\\\\ldots c_{m-1} x_{m-1}\\n$$\\n\\nThus the coefficients defining a system of $n$ linear equations can be represented as an $n \\\\times m$ matrix $C$. Here each row represents an equation, and each of the $m$ columns the coefficients of a distinct variable.\\n\\nWe can neatly evaluate all $n$ of these equations on a particular $m \\\\times 1$ input vector $X$, by multiplying $C \\\\cdot X$. The result will be an $n \\\\times 1$ vector, reporting the value $f_{i}(X)$ for each of the $n$ linear equations, $1 \\\\leq i \\\\leq n$. The special case here is the additive term $c_{0}$. For proper interpretation, the associated column in $X$ should contain all ones.\\n\\nIf we generalize $X$ to be an $m \\\\times p$ matrix containing $p$ distinct points, our product $C \\\\cdot x$ results in an $n \\\\times p$ matrix, evaluating every point against every equation in a single matrix multiplication.\\n\\nBut the primary operation on systems of $n$ equations is to solve them, meaning to identify the $X$ vector necessary to yield a target $Y$ value for each equation. Give the $n \\\\times 1$ vector of solution values $Y$ and coefficient matrix $C$, we seek $X$ such that $C \\\\cdot X=Y$.\\n\\nMatrix inversion can be used to solve linear systems. Multiplying both sides of $C X=Y$ by the inverse of C yields:\\n\\n$$\\n\\\\left(C^{-1} C\\\\right) X=C^{-1} Y \\\\longrightarrow X=C^{-1} Y\\n$$\\n\\nThus the system of equations can be solved by inverting $C$ and then multiplying $C^{-1}$ by $Y$.\\n\\nGaussian elimination is another approach to solving linear systems, which I trust you have seen before. Recall that it solves the equations by performing row addition/subtraction operations to simplify the equation matrix $C$ until it reduces to the identity matrix. This makes it trivial to read off the values of the\\\\\\n%---- Page End Break Here ---- Page : 250\\n\\\\\\nvariables, since every equation has been reduced to the form $X_{i}=Y_{i}^{\\\\prime}$, where $Y^{\\\\prime}$ is the result of applying these same row operations to the original target vector $Y$.\\n\\nComputing the matrix inverse can be done in the same fashion, as shown in Figure 8.9. We perform row operations to simplify the coefficient matrix to the identity matrix $I$ in order to create the inverse. I think of this as the algorithm of Dorian Gray: the coefficient matrix $C$ beautifies to the identity matrix, while the target $I$ ages into the inverse $\\\\frac{\\\\square}{\\\\square}$\\n\\nTherefore we can use matrix inversion to solve linear systems, and linear system solvers to invert matrices. Thus the two problems are in some sense equivalent. Computing the inverse makes it cheap to evaluate multiple $Y$ vectors for a given system $C$, by reducing it to a single matrix multiplication. But this can be done even more efficiently with LU-decomposition, discussed in Section 8.3.2. Gaussian elimination proves more numerically stable than inversion, and is generally the method of choice when solving linear systems.\\n\\n\\\\subsection*{8.2.6 Matrix rank\\\\index{rank}}\\nA system of equations is properly determined\\\\index{determined} when there are $n$ linearly independent equations and $n$ unknowns. For example, the linear system\\n\\n$$\\n\\\\begin{aligned}\\n& 2 x_{1}+1 x_{2}=5 \\\\\\\\\\n& 3 x_{1}-2 x_{2}=4\\n\\\\end{aligned}\\n$$\\n\\nis properly determined. The only solution is the point ( $x_{1}=2, x_{2}=1$ ).\\\\\\\\\\nIn contrast, systems of equations are underdetermined if there are rows (equations) that can be expressed as linear combinations of other rows. The linear system\\n\\n$$\\n\\\\begin{aligned}\\n& 2 x_{1}+1 x_{2}=5 \\\\\\\\\\n& 4 x_{1}+2 x_{2}=10\\n\\\\end{aligned}\\n$$\\n\\nis underdetermined, because the second row is twice that of the first row. It should be clear that there is not enough information to solve an undetermined system of linear equations.\\n\\nThe rank of a matrix measures the number of linearly independent rows. An $n \\\\times n$ matrix should be rank $n$ for all operations to be properly defined on it.\\n\\nThe rank of the matrix can be computed by running Gaussian elimination. If it is underdetermined, then certain variables will disappear in the course of row-reduction operations. There is also a connection between underdetermined systems and singular matrices: recall that they were identified by having a determinant of zero. That is why the difference in the cross product here ( 2 . $2-4 \\\\cdot 1)$ equals zero.\\n\\n\\\\footnotetext{${ }^{1}$ In Oscar Wilde\\'s novel The Picture of Dorian Gray, the protagonist remains beautiful, while his picture ages horribly over the years.\\n\\n%---- Page End Break Here ---- Page : 251\\n}Feature matrices are often of lower rank than we might desire. Files of examples tend to contain duplicate entries, which would result in two rows of the matrix being identical. It is also quite possible for multiple columns to be equivalent: imagine each record as containing the height measured in both feet and meters, for example.\\n\\nThese things certainly happen, and are bad when they do. Certain algorithms on our Lincoln memorial image failed numerically. It turned out that our $512 \\\\times 512$ image had a rank of only 508 , so not all rows were linearly independent. To make it a full-rank matrix, you can add a small amount of random noise to each element, which will increase the rank without serious image distortion. This kludge might get your data to pass through an algorithm without a warning message, but it is indicative of numerical trouble to come.\\n\\nLinear systems can be \"almost\" of lower rank, which results in a greater danger of precision loss due to numerical issues. This is formally captured by a matrix invariant called the condition number, which in the case of a linear system measures how sensitive the value of $X$ is to small changes of $Y$ in $Y=A X$.\\n\\nBe aware of the vagaries of numerical computation when evaluating your results. For example, it is a good practice to compute $A X$ for any purported solution \\\\index{reasons for factoring}$X$, and see how well $A X$ really compares to $Y$. In theory the difference will be zero, but in practice you may be surprised how rough the calculation really is.\\n\\n\\\\subsection*{8.3 factoring\\\\index{factoring} Matrices}\\nFactoring matrix $A$ into matrices $B$ and $C$ represents a particular aspect of division. We have seen that any non-singular matrix $M$ has an inverse $M^{-1}$, so the identity matrix $I$ can be factored as $I=M M^{-1}$. This proves that some matrices (like $I$ ) can be factored, and further that they might have many distinct factorizations. In this case, every possible non-singular $M$ defines a different factorization.\\n\\nMatrix factorization is an important abstraction in data science, leading to concise feature representations and ideas like topic modeling. It plays an important part in solving linear systems, through special factorizations like LUdecomposition.\\n\\nUnfortunately, finding such factorizations is problematic. Factoring integers is a hard problem, although that complexity goes away when you are allowed floating point numbers. Factoring matrices proves harder: for a particular matrix, exact factorization may not be possible, particularly if we seek the factorization $M=X Y$ where $X$ and $Y$ have prescribed dimensions.\\n\\n\\\\subsection*{8.3.1 Why Factor Feature Matrices?}\\nMany important machine learning algorithms can be viewed in terms of factoring a matrix. Suppose we are given an $n \\\\times m$ feature matrix $A$ where, as per the\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-269}\\n\\nFigure 8.10: Factoring a feature matrix $A \\\\approx B C$ yields $B$ as a more concise representation of the items and $C$ as a more concise representations of the features $A^{T}$.\\\\\\\\\\nusual convention, rows represent items/examples and columns represent features of the examples.\\n\\nNow suppose that we can factor matrix $A$, meaning express it as the product $A \\\\approx B \\\\cdot C$, where $B$ is an $n \\\\times k$ matrix and $C$ a $k \\\\times m$ matrix. Presuming that $k<\\\\min (n, m)$, as shown in Figure 8.10 this is a good thing for several reasons:\\n\\n\\\\begin{itemize}\\n  \\\\item Together, B and C provide a compressed representation of matrix A: Feature matrices are generally large, ungainly things to work with. Factorization provides a way to encode all the information of the large matrix into two smaller matrices, which together will be smaller than the original.\\n  \\\\item $B$ serves as a smaller feature matrix on the items, replacing $A$ : The factor matrix $B$ has $n$ rows, just like the original matrix $A$. However, it has substantially fewer columns, since $k<m$. This means that \"most\" of the information in $A$ is now encoded in $B$. Fewer columns mean a smaller matrix, and less parameters to fit in any model built using these new features. These more abstract features may also be of interest to other applications, as concise descriptions of the rows of the data set.\\n  \\\\item $C^{T}$ serves as a small feature matrix on the features, replacing $A^{T}$ : Transposing the feature matrix turns columns/features into rows/items. The factor matrix $C^{T}$ has $m$ rows and $k$ columns of properties representing them. In many cases, the $m$ original \"features\" are worth modeling in their own right.\\n\\\\end{itemize}\\n\\nConsider a representative example from text analysis\\\\index{text analysis}. Perhaps we want to represent $n$ documents, each a tweet or other social message post, in terms of the vocabulary it uses. Each of our $m$ features will correspond to a distinct vocabulary word, and $A[i, j]$ will record how often vocabulary word $w_{j}$ (say, cat) appeared in message number $i$. The working vocabulary in English is large with a long tail, so perhaps we can restrict it to the $m=50,000$ most frequently used words. Most messages will be short, with no more than a few hundred words. Thus our feature matrix $A$ will be very sparse, riddled with a huge number of zeros.\\n\\nNow suppose that we can factor $A=B C$, where the inner dimension $k$ is relatively small. Say $k=100$. Now each post will be represented by a row of\\\\\\n%---- Page End Break Here ---- Page : 253\\n\\\\\\n$B$ containing only a hundred numbers, instead of the full 50,000 . This makes it much easier to compare the texts for similarity in a meaningful way. These $k$ dimensions can be thought of as analogous to the \"topics\" in the documents, so all the posts about sports should light up a different set of topics than those about relationships.\\n\\nThe matrix $C^{T}$ can now be thought of as containing a feature vector for each of the vocabulary words. This is interesting. We would expect words that apply in similar contexts to have similar topic vectors. Color words like yellow and red are likely to look fairly similar in topic space, while baseball and sex should have quite distant relationships.\\n\\nNote that this word-topic matrix is potentially useful in any problem seeking to use language as features. The connection with social message posts is largely gone, so it would be applicable to other domains like books and news. Indeed, such compressed word embeddings prove a very powerful tool in natural language processing (NLP), as will be discussed in Section 11.6.3\\n\\n\\\\subsection*{8.3.2 LU Decomposition and Determinants}\\n$L U$ decomposition is a particular matrix factorization which factors a square matrix $A$ into lower and upper triangular matrices $L$ and $U$, such that $A=L \\\\cdot U$.\\n\\nA matrix is triangular if it contains all zero terms either above or below the main diagonal. The lower triangular matrix $L$ has all non-zero terms below the main diagonal. The other factor, $U$, is the upper triangular matrix. Since the main diagonal of $L$ consists of all ones, we can pack the entire decomposition into the same space as the original $n \\\\times n$ matrix.\\n\\nThe primary value of LU decomposition is that it proves useful in solving linear systems $A X=Y$, particularly when solving multiple problems with the same $A$ but different $Y$. The matrix $L$ is what results from clearing out all of the values above the main diagonal, via Gaussian elimination. Once in this triangular form, the remaining equations can be directly simplified. The matrix $U$ reflects what row operations have occurred in the course of building $L$. Simplifying $U$ and applying $L$ to $Y$ requires less work than solving $A$ from scratch.\\n\\nThe other importance of LU decomposition is in yielding an algorithm to compute the determinant of a matrix. The determinant of $A$ is the product of the main diagonal elements of $U$. As we have seen, a determinant of zero means the matrix is not of full rank.\\n\\nFigure 8.11 illustrates the LU decomposition of the Lincoln memorial. There is a distinct texture visible to the two triangular matrices. This particular LU decomposition function (in Mathematica) took advantage of the fact that the equations in a system can be permuted with no loss of information. The same does not hold true for images, but we see accurate reconstructions of the white columns of the memorial, albeit out of position.\\\\\\n%---- Page End Break Here ---- Page : 254\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-271}\\n\\nFigure 8.11: The LU decomposition of the Lincoln memorial (left), with the product $L \\\\cdot U$ (center). The rows of the LU matrix were permuted during calculation, but when properly ordered fully reconstructed the image (right).\\n\\n\\\\subsection*{8.4 eigenvalues\\\\index{eigenvalues} and eigenvectors\\\\index{eigenvectors}}\\nMultiplying a vector $U$ by a square matrix $A$ can have the same effect as multiplying it by a scalar $l$. Consider this pair of examples. Indeed, check them out by hand:\\n\\n$$\\n\\\\begin{aligned}\\n& {\\\\left[\\\\begin{array}{cc}\\n-5 & 2 \\\\\\\\\\n2 & -2\\n\\\\end{array}\\\\right] \\\\cdot\\\\left[\\\\begin{array}{c}\\n2 \\\\\\\\\\n-1\\n\\\\end{array}\\\\right] }=-6\\\\left[\\\\begin{array}{c}\\n2 \\\\\\\\\\n-1\\n\\\\end{array}\\\\right] \\\\\\\\\\n& {\\\\left[\\\\begin{array}{cc}\\n-5 & 2 \\\\\\\\\\n2 & -2\\n\\\\end{array}\\\\right] \\\\cdot\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n2\\n\\\\end{array}\\\\right]=-1\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n2\\n\\\\end{array}\\\\right] }\\n\\\\end{aligned}\\n$$\\n\\nBoth of these equalities feature products with the same $2 \\\\times 1$ vector $U$ on the left as on the right. On one side $U$ is multiplied by a matrix $A$, and on the other by a scalar $\\\\lambda$. In cases like this, when $A U=\\\\lambda U$, we say that $\\\\lambda$ is an eigenvalue of matrix $A$, and $U$ is its associated eigenvector.\\n\\nSuch eigenvector-eigenvalue pairs are a curious thing. That the scalar $\\\\lambda$ can do the same thing to $U$ as the entire matrix $A$ tells us that they must be special. Together, the eigenvector $U$ and eigenvalue $\\\\lambda$ must encode a lot of information about $A$.\\n\\nFurther, there are generally multiple such eigenvector-eigenvalue pairs for any matrix. Note that the second example above works on the same matrix $A$, but yields a different $U$ and $\\\\lambda$.\\n\\n\\\\subsection*{8.4.1 properties of\\\\index{properties of} Eigenvalues}\\nThe theory of eigenvalues gets us deeper into the thicket of linear algebra than I am prepared to do in this text. Generally speaking, however, we can summarize the properties that will prove important to us:\\n\\n\\\\begin{itemize}\\n  \\\\item Each eigenvalue has an associated eigenvector. They always come in pairs.\\n  \\\\item There are, in general, $n$ eigenvector-eigenvalue pairs for every full rank $n \\\\times n$ matrix.\\n  \\\\item Every pair of eigenvectors of a symmetric matrix are mutually orthogonal, the same way that the $x$ and $y$-axes in the plane are orthogonal. Two vectors are orthogonal if their dot product is zero. Observe that $(0,1)$. $(1,0)=0$, as does $(2,-1) \\\\cdot(1,2)=0$ from the previous example.\\n  \\\\item The upshot from this is that eigenvectors can play the role of dimensions or bases in some $n$-dimensional space. This opens up many geometric interpretations of matrices. In particular, any matrix can be encoded where each eigenvalue represents the magnitude of its associated eigenvector.\\n\\\\end{itemize}\\n\\n\\\\subsection*{8.4.2 Computing Eigenvalues}\\nThe $n$ distinct eigenvalues of a rank- $n$ matrix can be found by factoring its characteristic equation\\\\index{characteristic equation}. Start from the defining equality $A U=\\\\lambda U$. Convince yourself that this remains unchanged when we multiply by the identity matrix $I$, so\\n\\n$$\\nA U=\\\\lambda I U \\\\rightarrow(A-\\\\lambda I) U=0\\n$$\\n\\nFor our example matrix, we get\\n\\n$$\\nA-\\\\lambda I=\\\\left[\\\\begin{array}{cc}\\n-5-\\\\lambda & 2 \\\\\\\\\\n2 & -2-\\\\lambda\\n\\\\end{array}\\\\right]\\n$$\\n\\nNote that our equality $(A-\\\\lambda I) U=0$ remains true if we multiply vector $U$ by any scalar value $c$. This implies that there are an infinite number of solutions, and hence the linear system must be underdetermined\\\\index{underdetermined}.\\n\\nIn such a situation, the determinant of the matrix must be zero. With a $2 \\\\times 2$ matrix, the determinant is just the cross product $a d-b c$, so\\n\\n$$\\n(-5-\\\\lambda)(-2-\\\\lambda)-2 \\\\cdot 2=\\\\lambda^{2}+7 \\\\lambda+6=0 .\\n$$\\n\\nSolving for $\\\\lambda$ with the quadratic formula yields $\\\\lambda=-1$ and $\\\\lambda=-6$. More generally, the determinant $|A-\\\\lambda I|$ is a polynomial of degree $n$, and hence the roots of this characteristic equation define the eigenvalues of $A$.\\n\\nThe vector associated with any given eigenvalue can be computed by solving a linear system. As per our example, we know that\\n\\n$$\\n\\\\left[\\\\begin{array}{cc}\\n-5 & 2 \\\\\\\\\\n2 & -2\\n\\\\end{array}\\\\right] \\\\cdot\\\\left[\\\\begin{array}{l}\\nu_{1} \\\\\\\\\\nu_{2}\\n\\\\end{array}\\\\right]=\\\\lambda\\\\left[\\\\begin{array}{l}\\nu_{1} \\\\\\\\\\nu_{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nfor any eigenvalue $\\\\lambda$ and associated eigenvector $U=\\\\left[\\\\begin{array}{l}u_{1} \\\\\\\\ u_{2}\\\\end{array}\\\\right]$. Once we fix the value of $\\\\lambda$, we have a system of $n$ equations and $n$ unknowns, and thus can solve for the values of $U$. For $\\\\lambda=-1$,\\n\\n$$\\n\\\\begin{aligned}\\n& -5 u_{1}+2 u_{2}=-1 u_{1} \\\\longrightarrow-4 u_{1}+2 u_{2}=0 \\\\\\\\\\n& 2 u_{1}+-2 u_{2}=-1 u_{2} \\\\longrightarrow 2 u_{1}+-1 u_{2}=0\\n\\\\end{aligned}\\n$$\\n\\nwhich has the solution $u_{1}=1, u_{2}=2$, yielding the associated eigenvector.\\n\\nFor $\\\\lambda=-6$, we get\\n\\n$$\\n\\\\begin{aligned}\\n& -5 u_{1}+2 u_{2}=-6 u_{1} \\\\longrightarrow 1 u_{1}+2 u_{2}=0 \\\\\\\\\\n& 2 u_{1}+-2 u_{2}=-6 u_{2} \\\\longrightarrow 2 u_{1}+4 u_{2}=0\\n\\\\end{aligned}\\n$$\\n\\nThis system is underdetermined, so $u_{1}=2, u_{2}=-1$ and any constant multiple of this qualifies as an eigenvector. This makes sense: because $U$ is on both sides of the equality $A U=\\\\lambda U$, for any constant $c$, the vector $U^{\\\\prime}=c \\\\cdot U$ equally satisfies the definition.\\n\\nFaster algorithms for eigenvalue/vector computation\\\\index{computation}s are based on a matrix factorization approach called $Q R$ decomposition. Other algorithms try to avoid solving the full linear system. For example, an alternate approach repeatedly uses $U^{\\\\prime}=(A U) / \\\\lambda$ to compute better and better approximations to $U$ until it converges. When conditions are right, this can be much faster than solving the full linear system.\\n\\nThe largest eigenvalues and their associated vectors are, generally speaking, more important that the rest. Why? Because they make a larger contribution to approximating the matrix $A$. Thus high-performance linear algebra systems use special routines for finding the $k$ largest (and smallest) eigenvalues and then iterative methods to reconstruct the vectors for each.\\n\\n\\\\subsection*{8.5 Eigenvalue Decomposition}\\nAny $n \\\\times n$ symmetric matrix $M$ can be decomposed into the sum of its $n$ eigenvector products. We call the $n$ eigenpairs $\\\\left(\\\\lambda_{i}, U_{i}\\\\right)$, for $1 \\\\leq i \\\\leq n$. By convention we sort by size, so $\\\\lambda_{i} \\\\geq \\\\lambda_{i-i}$ for all $i$.\\n\\nSince each eigenvector $U_{i}$ is an $n \\\\times 1$ matrix, multiplying it by its transpose yields an $n \\\\times n$ matrix product, $U_{i} U_{i}{ }^{T}$. This has exactly the same dimensions as the original matrix $M$. We can compute the linear combination of these matrices weighted by its corresponding eigenvalue. In fact, this reconstructs the original matrix, since:\\n\\n$$\\nM=\\\\sum_{i=1}^{n} \\\\lambda_{i} U_{i} U_{i}^{T}\\n$$\\n\\nThis result holds only for symmetric matrices, so we cannot use it to encode our image. But covariance matrices are always symmetric, and they encode the basic features of each row and column of the matrix.\\n\\nThus the covariance matrix can be represented by its eigenvalue decomposition. This takes slightly more space than the initial matrix: $n$ eigenvectors of length $n$, plus $n$ eigenvalues vs. the $n(n+1) / 2$ elements in the upper triangle of the symmetric matrix plus main diagonal.\\n\\nHowever, by using only the vectors associated with the largest eigenvalues we get a good approximation of the matrix. The smaller dimensions contribute very little to the matrix values, and so they can be excluded with little resulting error. This dimension reduction method is very useful to produce smaller, more effective feature sets.\\\\\\n%---- Page End Break Here ---- Page : 257\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-274}\\n\\nFigure 8.12: The Lincoln memorial\\'s biggest eigenvector suffices to capture much of the detail of its covariance matrix.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-274(1)}\\n\\nFigure 8.13: Error in reconstructing the Lincoln memorial from the one, five, and fifty largest eigenvectors.\\n\\nFigure 8.12 (left) shows the reconstruction of the Lincoln memorial\\'s covariance matrix $M$ from its single largest eigenvector, i.e. $U_{1} \\\\cdot U_{1}^{T}$, along with its associated error matrix $M-U_{1} \\\\cdot U_{1}^{T}$. Even a single eigenvector does a very respectable job at reconstruction, restoring features like the large central block.\\n\\nThe plot in Figure 8.12 (right) shows that the errors occur in patchy regions, because more subtle detail requires additional vectors to encode. Figure 8.13 shows the error plot when using the one, five, and fifty largest eigenvectors. The error regions get smaller as we reconstruct finer detail, and the magnitude of the errors smaller. Realize that even fifty eigenvectors is less than $10 \\\\%$ of the 512 necessary to restore a perfect matrix, but this suffices for a very good approximation.\\n\\n\\\\subsection*{8.5.1 Singular Value Decomposition}\\nEigenvalue decomposition is a very good thing. But it only works on symmetric matrices. Singular value decomposition is a more general matrix factorization approach, that similarly reduces a matrix to the sum of other matrices defined by vectors.\\n\\nThe singular value decomposition of an $n \\\\times m$ real matrix $M$ factors it into three matrices $U, D$, and $V$, with dimensions $n \\\\times n, n \\\\times m$, and $m \\\\times m$ respec-\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-275}\\n\\nFigure 8.14: Singular value matrices in the decomposition of Lincoln, for 50 singular values.\\\\\\\\\\ntively. This factorization is of the form ${ }^{2}$\\n\\n$$\\nM=U D V^{T}\\n$$\\n\\nThe center matrix $D$ has the property that it is a diagonal matrix, meaning all non-zero values lie on the main diagonal like the identity matrix $I$.\\n\\nDon\\'t worry about how we find this factorization. Instead, let\\'s concentrate on what it means. The product $U \\\\cdot D$ has the effect of multiplying $U[i, j]$ by $D[j, j]$, because all terms of $D$ are zero except along the main diagonal. Thus $D$ can be interpreted as measuring the relative importance of each column of $U$, or through $D \\\\cdot V^{T}$, the importance of each row of $V^{T}$. These weight values of $D$ are called the singular values of $M$.\\n\\nLet $X$ and $Y$ be vectors, of dimensionality $n \\\\times 1$ and $1 \\\\times m$, respectively. The matrix outer product $P=X \\\\otimes Y$ is the $n \\\\times m$ matrix where $P[j, k]=X[j] Y[k]$. The traditional matrix multiplication $C=A \\\\cdot B$ can be expressed as the sum of these outer products, namely:\\n\\n$$\\nC=A \\\\cdot B=\\\\sum_{k} A_{k} \\\\bigotimes B_{k}^{T}\\n$$\\n\\nwhere $A_{k}$ is the vector defined by the $k$ th column of $A$, and $B_{k}^{T}$ is the vector defined by the $k$ th row of $B$.\\n\\nPutting this together, matrix $M$ can be expressed as the sum of outer products of vectors resulting from the singular value decomposition, namely $(U D)_{k}$ and $\\\\left(V^{T}\\\\right)_{k}$ for $1 \\\\leq k \\\\leq m$. Further, the singular values $D$ define how much contribution each outer product makes to $M$, so it suffices to take only the vectors associated with the largest singular values to get an approximation to $M$.\\n\\n\\\\footnotetext{${ }^{2}$ Should $M$ contain convex numbers, then this generalizes to $M=U D V^{*}$, where $V^{*}$ means the conjugate transpose of $V$.\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-276}\\n\\nFigure 8.15: Lincoln\\'s face reconstructed from 5 (left) and 50 (center) singular values, with the error for $k=50$ (right).\\n\\nFigure 8.14 (left) presents the vectors associated with the first fifty singular values of Lincoln\\'s face. If you look carefully, you can see how the first five to ten vectors are considerably more blocky than subsequent ones, indicating that the early vectors rough out the basic structure of the matrix, with subsequent vectors adding greater detail. Figure 8.14 (right) shows how the mean squared error between the matrix and its reconstruction shrinks as we add additional vectors.\\n\\nThese effects become even more vivid when we look at the reconstructed images themselves. Figure 8.15 (left) shows Lincoln\\'s face with only the five strongest vectors, which is less than $1 \\\\%$ of what is available for perfect reconstruction. But even at this point you could pick him out of a police lineup. Figure 8.15 (center) demonstrates the greater detail when we include fifty vectors. This looks as good as the raw image in print, although the error plot (Figure 8.15 (right)) highlights the missing detail.\\n\\nTake-Home Lesson: Singular value decomposition (SVD) is a powerful technique to reduce the dimensionality of any feature matrix.\\n\\n\\\\subsection*{8.5.2 Principal Components Analysis}\\nPrincipal components analysis (PCA) is a closely related technique for reducing the dimensionality of data sets. Like SVD, we will define vectors to represent the data set. Like SVD, we will order them by successive importance, so we can reconstruct an approximate representation using few components. PCA and SVD are so closely related as to be indistinguishable for our purposes. They do the same thing in the same way, but coming from different directions.\\n\\nThe principal components define the axes of an ellipsoid best fitting the points. The origin of this set of axes is the centroid of the points. PCA starts by identifying the direction to project the points on to so as to explain the maximum amount of variance. This is the line through the centroid that, in some sense, best fits the points, making it analogous to linear regression. We can then project each point onto this line, with this point of intersection defining a particular position on the line relative to the centroid. These projected\\\\\\n%---- Page End Break Here ---- Page : 260\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-277(1)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-277}\\n\\nFigure 8.16: PCA projects the black points onto orthogonal axis, rotated to yield the alternate representation in red (left). The values of each component are given by projecting each point onto the appropriate axis (right).\\\\\\\\\\npositions now define the first dimension (or principal component) of our new representation, as shown in Figure 8.16\\n\\nFor each subsequent component, we seek the line $l_{k}$, which is orthogonal to all previous lines and explains the largest amount of the remaining variance. That each dimension is orthogonal to each other means they act like coordinate axes, establishing the connection to eigenvectors. Each subsequent dimension is progressively less important than the ones before it, because we chose the most promising directions first. Later components contribute only progressively finer detail, and hence we can stop once this is small enough.\\n\\nSuppose that dimensions $x$ and $y$ are virtually identical. We would expect that the regression line will project down to $y=x$ on these two dimensions, so they could then largely be replaced by a single dimension. PCA constructs new dimensions as linear combinations of the original ones, collapsing those which are highly correlated into a lower-dimensional space. Statistical factor analysis is a technique which identifies the most important orthogonal dimensions (as measured by correlation) that explain the bulk of the variance.\\n\\nRelatively few components suffice to capture the basic structure of the point set. The residual that remains is likely to be noise, and is often better off removed from the data. After dimension reduction via PCA (or SVD), we should end up with cleaner data, not merely a smaller number of dimensions.\\n\\nTake-Home Lesson: PCA and SVD are essentially two different approaches to computing the same thing. They should serve equally well as low-dimensional approximations of a feature matrix.\\n\\n%---- Page End Break Here ---- Page : 261\\n\\n\\\\subsection*{8.6 War Story: The Human Factors}\\nI first came to be amazed by the power of dimension reduction methods like PCA and SVD in the course of our analysis of historical figures for our book Who\\'s Bigger. Recall (from the war story of Section 4.7) how we analyzed the structure and content of Wikipedia, ultimately extracting a half-dozen features like PageRank and article length for each of the $800,000+$ articles about people in the English edition. This reduced each of these people to a six-dimensional feature vector, which we would analyze to judge their relative significance.\\n\\nBut things proved not as straightforward as we thought. Wildly different people were ranked highest by each particular variable. It wasn\\'t clear how to interpret them.\\\\\\\\\\n\"There is so much variance and random noise in our features,\" my co-author Charles observed. \"Let\\'s identify the major factors underlying these observed variables that really show what is going on.\"\\n\\nCharles\\' solution was factor analysis, which is a variant of PCA, which is in turn a variant of SVD. All of these techniques compress feature matrices into a smaller set of variables or factors, with the goal that these factors explain most of the variance in the full feature matrix. We expected factor analysis would extract a single underlying factor defining individual significance. But instead, our input variables yielded two independent factors explaining the data. Both explained roughly equal proportions of the variance ( $31 \\\\%$ and $28 \\\\%$ ), meaning that these latent variables were approximately of equal importance. But the cool thing is what these factors showed.\\n\\nFactors (or singular vectors, or principle components) are just linear combinations of the original input features. They don\\'t come with names attached to them, so usually you would just describe them as Factor 1 and Factor 2. But our two factors were so distinctive that Charles gave them the names gravitas and celebrity, and you can see why in Figure 8.17 .\\n\\nOur gravitas factor largely comes from (or \"loads on,\" in statistical parlance) the two forms of PageRank. Gravitas seems to accurately capture notions of achievement-based recognition. In contrast, the celebrity factor loads more strongly on page hits, revisions, and article length. The celebrity factor better captures the popular (some might say vulgar) notions of reputation. The wattage of singers, actors, and other entertainers are better measured by celebrity then gravitas.\\n\\nTo get a feel for the distinction between gravitas and celebrity, compare our highest ranked figures for each factor, in Figure 8.17. The high gravitas figures on the left are clearly old-fashioned heavyweights, people of stature and accomplishment. They are philosophers, kings, and statesmen. Those names listed in Figure 8.17 (right) are such complete celebrities that the top four walk this earth with only one name. They are professional wrestlers, actors, and singers. It is quite telling that the only two figures here showing any gravitas on our celebrity-gravitas meter are Britney Spears (1981- ) [566] and Michael Jackson (1958-2009) [136], both among the Platonic ideals of modern celebrity.\\n\\nI find it amazing that these unsupervised methods were able to tease apart\\n\\n%---- Page End Break Here ---- Page : 262\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|c|c|c|c|c|c|}\\n\\\\hline\\n\\\\multicolumn{4}{|l|}{Highest Gravitas Ranking} & \\\\multicolumn{4}{|c|}{Highest Celebrity Ranking} \\\\\\\\\\n\\\\hline\\nPerson & Grav. & Sig & Celeb/Grav & Person & Celeb. & Sig & Celeb/Grav \\\\\\\\\\n\\\\hline\\nNapoleon & 8 & 2 & c $\\\\square$ G & The Undertaker & 2 & 2172 & c $\\\\square$ G \\\\\\\\\\n\\\\hline\\nCarl Linnaeus & 13 & 31 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & Vijay & 8 & 4456 & $C \\\\square \\\\mathrm{G}$ \\\\\\\\\\n\\\\hline\\nPlato & 23 & 25 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & Edge & 10 & 2603 & $\\\\mathrm{C} \\\\square \\\\mathrm{\\\\square}$ \\\\\\\\\\n\\\\hline\\nAristotle & 27 & 8 & $C \\\\square \\\\mathrm{G}$ & Kane & 13 & 2229 & $C \\\\square \\\\mathrm{G}$ \\\\\\\\\\n\\\\hline\\nF. D. Roosevelt & 30 & 43 & $C \\\\square \\\\mathrm{G}$ & John Cena & 16 & 2277 & C $\\\\square$ G \\\\\\\\\\n\\\\hline\\nPlutarch & 32 & 258 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & BeyoncÃ© Knowles & 19 & 1519 & $C \\\\square \\\\mathrm{G}$ \\\\\\\\\\n\\\\hline\\nCharles II & 33 & 78 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & Triple H & 26 & 1596 & C $\\\\square$ G \\\\\\\\\\n\\\\hline\\nElizabeth II & 35 & 132 & C $\\\\square$ G & Rey Mysterio & 36 & 2740 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ \\\\\\\\\\n\\\\hline\\nQueen Victoria & 38 & 16 & $C \\\\square \\\\mathrm{G}$ & Britney Spears & 37 & 689 & C $\\\\square: \\\\square$ \\\\\\\\\\n\\\\hline\\nWilliam Shakespeare & 42 & 4 & $\\\\mathrm{C} \\\\square \\\\mathrm{\\\\square}$ & Ann Coulter & 45 & 3376 & C $\\\\square$ G \\\\\\\\\\n\\\\hline\\nPliny the Elder & 43 & 212 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & Jesse McCartney & 48 & 4236 & $C \\\\square \\\\mathrm{G}$ \\\\\\\\\\n\\\\hline\\nTacitus & 52 & 300 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & Roger Federer & 57 & 743 & C $\\\\square: \\\\square$ \\\\\\\\\\n\\\\hline\\nHerodotus & 58 & 123 & C $\\\\square$ G & Ashley Tisdale & 60 & 4445 & C $\\\\square$ G \\\\\\\\\\n\\\\hline\\nCharles V & 61 & 84 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & Michael Jackson & 75 & 180 & C $\\\\square$ : G \\\\\\\\\\n\\\\hline\\nGeorge V & 64 & 235 & $\\\\mathrm{C} \\\\square \\\\mathrm{G}$ & Dwayne Johnson & 78 & 1446 & C $\\\\square$ G \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 8.17: The gravitas and celebrity factors do an excellent job partitioning two types of famous people.\\\\\\\\\\ntwo distinct types of fame, without any labeled training examples or even a preconception of what they were looking for. The factors/vectors/components simply reflected what was there in the data to be found.\\n\\nThis celebrity-gravitas continuum serves as an instructive example of the power of dimension reduction methods. All the factors/vectors/components all must, by definition, be orthogonal to each other. This means that they each measure different things, in a way that two correlated input variables do not. It pays to do some exploratory data analysis on your main components to try to figure out what they really mean, in the context of your application. The factors are yours to name as you wish, just like a cat or a dog, so choose names you will be happy to live with.\\n\\n\\\\subsection*{8.7 Chapter Notes}\\nThere are many popular textbooks providing introductions to linear algebra, including LLM15, Str11, Tuc88. Klein Kle13 presents an interesting introduction to linear algebra for computer science, with an emphasis on programming and applications like coding theory and computer graphics.\\n\\n\\\\subsection*{8.8 exercises\\\\index{exercises}}\\n\\\\section*{Basic Linear Algebra}\\n8-1. [3] Give a pair of square matrices $A$ and $B$ such that:\\\\\\\\\\n(a) $A B=B A$ (it commutes).\\\\\\\\\\n(b) $A B \\\\neq B A$ (does not commute).\\n\\nIn general, matrix multiplication is not commutative.\\\\\\\\[0pt]\\n8-2. [3] Prove that matrix addition is associative, i.e. that $(A+B)+C=A+(B+C)$ for compatible matrices $A, B$ and $C$.\\\\\\\\[0pt]\\n8-3. [5] Prove that matrix multiplication is associative, i.e. that $(A B) C=A(B C)$ for compatible matrices $A, B$ and $C$.\\\\\\\\[0pt]\\n8-4. [3] Prove that $A B=B A$, if $A$ and $B$ are diagonal matrices of the same order.\\\\\\\\[0pt]\\n8-5. [5] Prove that if $A C=C A$ and $B C=C B$, then\\n\\n$$\\nC(A B+B A)=(A B+B A) C .\\n$$\\n\\n8-6. [3] Are the matrices $M M^{T}$ and $M^{T} M$ square and symmetric? Explain.\\\\\\\\[0pt]\\n8-7. [5] Prove that $\\\\left(A^{-1}\\\\right)^{-1}=A$.\\\\\\\\[0pt]\\n8-8. [5] Prove that $\\\\left(A^{T}\\\\right)^{-1}=\\\\left(A^{-1}\\\\right)^{T}$ for any non-singular matrix $A$.\\\\\\\\[0pt]\\n8-9. [5] Is the LU factorization of a matrix unique? Justify your answer.\\\\\\\\[0pt]\\n8-10. [3] Explain how to solve the matrix equation $A x=b$ ?\\\\\\\\[0pt]\\n8-11. [5] Show that if $M$ is a square matrix which is not invertible, then either $L$ or $U$ in the LU-decomposition $M=L \\\\cdot U$ has a zero in its diagonal.\\n\\n\\\\section*{Eigenvalues and Eigenvectors}\\n8-12. [3] Let $M=\\\\left[\\\\begin{array}{ll}2 & 1 \\\\\\\\ 0 & 2\\\\end{array}\\\\right]$. Find all eigenvalues of $M$. Does $M$ have two linearly independent eigenvectors?\\\\\\\\[0pt]\\n8-13. [3] Prove that the eigenvalues of $A$ and $A^{T}$ are identical.\\\\\\\\[0pt]\\n8-14. [3] Prove that the eigenvalues of a diagonal matrix are equal to the diagonal elements.\\\\\\\\[0pt]\\n8-15. [5] Suppose that matrix $A$ has an eigenvector $v$ with eigenvalue $\\\\lambda$. Show that $v$ is also an eigenvector for $A^{2}$, and find the corresponding eigenvalue. How about for $A^{k}$, for $2 \\\\leq k \\\\leq n$ ?\\\\\\\\[0pt]\\n8-16. [5] Suppose that $A$ is an invertible matrix with eigenvector $v$. Show that $v$ is also an eigenvector for $A^{-1}$.\\\\\\\\[0pt]\\n8-17. [8] Show that the eigenvalues of $M M^{T}$ are the same as that of $M^{T} M$. Are their eigenvectors also the same?\\n\\n\\\\section*{Implementation Projects}\\n8-18. [5] Compare the speed of a library function for matrix multiplication to your own implementation of the nested loops algorithm.\\n\\n\\\\begin{itemize}\\n  \\\\item How much faster is the library on products of random $n \\\\times n$ matricies, as a function of $n$ as $n$ gets large?\\n  \\\\item What about the product of an $n \\\\times m$ and $m \\\\times n$ matrix, where $n \\\\ll m$ ?\\n  \\\\item By how much do you improve the performance of your implementation to calculate $C=A \\\\cdot B$ by first transposing $B$ internally, so all dot products are computed along rows of the matrices to improve cache performance?\\\\\\\\[0pt]\\n8-19. [5] Implement Gaussian elimination for solving systems of equations, $C \\\\cdot X=Y$. Compare your implementation against a popular library routine for:\\\\\\n%---- Page End Break Here ---- Page : 264\\n\\\\\\n(a) Speed: How does the run time compare, for both dense and sparse coefficient matrices?\\\\\\\\\\n(b) Accuracy: What are the size of the numerical residuals $C X-Y$, particularly as the condition number of the matrix increases.\\\\\\\\\\n(c) Stability: Does your program crash on a singular matrix? What about almost singular matrices, created by adding a little random noise to a singular matrix?\\n\\\\end{itemize}\\n\\n\\\\section*{Interview Questions}\\n8-20. [5] Why is vectorization considered a powerful method for optimizing numerical code?\\n\\n8-21. [3] What is singular value decomposition? What is a singular value? And what is a singular vector?\\n\\n8-22. [5] Explain the difference between \"long\" and \"wide\" format data. When might each arise in practice?\\n\\n\\\\section*{Kaggle Challenges}\\n$8-23$. Tell what someone is looking at from analysis of their brain waves.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/decoding-the-human-brain}{https://www.kaggle.com/c/decoding-the-human-brain}\\\\\\\\\\n8 -24. Decide whether a particular student will answer a given question correctly. \\\\href{https://www.kaggle.com/c/WhatDoYouKnow}{https://www.kaggle.com/c/WhatDoYouKnow}\\\\\\\\\\n8-25. Identify mobile phone users from accelerometer data.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/accelerometer-biometric-competition}\\n%---- Page End Break Here ---- Page : 265\\n{https://www.kaggle.com/c/accelerometer-biometric-competition}\\n\\n\\\\section*{Chapter 9}\\n\\\\section*{Linear and Logistic Regression}\\nAn unsophisticated forecaster uses statistics as a drunken man uses lamp posts - for support rather than illumination.\\n\\n\\\\begin{itemize}\\n  \\\\item Andrew Lang\\n\\\\end{itemize}\\n\\nlinear regression\\\\index{linear regression} is the most representative \"machine learning\" method to build models for value prediction and classification from training data. It offers a study in contrasts:\\n\\n\\\\begin{itemize}\\n  \\\\item Linear regression has a beautiful theoretical foundation yet, in practice, this algebraic formulation is generally discarded in favor of faster, more heuristic optimization.\\n  \\\\item Linear regression models are, by definition, linear. This provides an opportunity to witness the limitations of such models, as well as develop clever techniques to generalize to other forms.\\n  \\\\item Linear regression simultaneously encourages model building with hundreds of variables, and regularization techniques to ensure that most of them will get ignored.\\n\\\\end{itemize}\\n\\nLinear regression is a bread-and-butter modeling technique that should serve as your baseline approach to building data-driven models. These models are typically easy to build, straightforward to interpret, and often do quite well in practice. With enough skill and toil, more advanced machine learning techniques might yield better performance, but the possible payoff is often not worth the effort. Build your linear regression models first, then decide whether it is worth working harder to achieve better results.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-283}\\n\\nFigure 9.1: Linear regression produces the line which best fits a set of points.\\n\\n\\\\subsection*{9.1 Linear Regression}\\nGiven a collection of $n$ points, linear regression seeks to find \\\\index{Lang, Andrew}the line which best approximates or fits the points, as shown in Figure 9.1 There are many reasons why we might want to do this. One class of goals involves simplification and compression: we can replace a large set of noisy data points in the $x y$-plane by a tidy line that describes them, as shown in Figure 9.1 This regression line is useful for visualization, by showing the underlying trend in the data and highlighting the location and magnitude of outliers.\\n\\nHowever, we will be most interested in regression as a method for value forecasting. We can envision each observed point $p=(x, y)$ to be the result of a function $y=f(x)$, where $x$ represents the feature variables and $y$ the independent target variable. Given a collection of $n$ such points $\\\\left\\\\{p_{1}, p_{2}, \\\\ldots, p_{n}\\\\right\\\\}$, we seek the $f(x)$ which best explains these points. This function $f(x)$ interpolates or models the points, providing a way to estimate the value $y^{\\\\prime}$ associated with any possible $x^{\\\\prime}$, namely that $y^{\\\\prime}=f\\\\left(x^{\\\\prime}\\\\right)$.\\n\\n\\\\subsection*{9.1.1 Linear Regression and Duality}\\nThere is a connection between regression and solving linear equations, which is interesting to explore. When solving linear systems, we seek the single point that lies on $n$ given lines. In regression, we are instead given $n$ points, and we seek the line that lies on \"all\" points. There are two differences here: (a) the interchange of points for lines and (b) finding the best fit under constraints verses a totally constrained problem (\"all\" vs. all).\\n\\nThe distinction between points and lines proves trivial, because they both are really the same thing. In two-dimensional space, both points $(s, t)$ and lines $y=m x+b$ are defined by two parameters: $\\\\{s, t\\\\}$ and $\\\\{m, b\\\\}$, respectively. Further, by an appropriate duality transformation, these lines are equivalent to\\\\\\n%---- Page End Break Here ---- Page : 268\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-284}\\n\\nFigure 9.2: Points are equivalent to lines under a duality transform. The point $(4,8)$ in red (left) maps to the red line $y=4 x-8$ on right. Both sets of three collinear points on the left correspond to three lines passing through the same point on right.\\\\\\\\\\npoints in another space. In particular, consider the transform that\\n\\n$$\\n(s, t) \\\\longleftrightarrow y=s x-t\\n$$\\n\\nNow any set of points that lie on a single line get mapped to a set of lines which intersect a singular point - so finding a line that hits all of a set of points is algorithmically the same thing as finding a point that hits all of a set of lines.\\n\\nFigure 9.2 shows an example. The point of intersection in Figure 9.2 (left) is $p=(4,8)$, and it corresponds to the red line $y=4 x-8$ on the right. This red point $p$ is defined by the intersection of black and blue lines. In the dual space, these lines turn into black and blue points lying on the red line. Three collinear points on left (red with either two black or two blue) map to three lines passing through a common point on the right: one red and two of the same color. This duality transformation reverses the roles of points and lines in a way that everything makes sense.\\n\\nThe big difference in defining linear regression is that we seek a line that comes as close as possible to hitting all the points. We must be careful about measuring error in the proper way in order to make this work.\\n\\n\\\\subsection*{9.1.2 Error in Linear Regression}\\nThe residual error of a fitted line $f(x)$ is the difference between the predicted and actual values. As shown in Figure 9.3 for a particular feature vector $x_{i}$ and corresponding target value $y_{i}$, the residual error $r_{i}$ is defined:\\n\\n$$\\nr_{i}=y_{i}-f\\\\left(x_{i}\\\\right)\\n$$\\n\\nThis is what we will care about, but note that it is not the only way that error might have been defined. The closest distance to the line is in fact defined\\\\\\n%---- Page End Break Here ---- Page : 269\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-285}\\n\\nFigure 9.3: The residual error in least squares is the projection of $y_{i}-f(X)$ down to $X$, not the shortest distance between the line and the point.\\\\\\\\\\nby the perpendicular-bisector through the target point. But we are seeking to forecast the value of $y_{i}$ from $x_{i}$, so the residual is the right notion of error for our purposes.\\n\\nleast squares regression\\\\index{least squares regression} minimizes the sum of the squares of the residuals of all points. This metric has been chosen because (1) squaring the residual ignores the signs of the errors, so positive and negative residuals do not offset each other, and (2) it leads to a surprisingly nice closed form for finding the coefficients of the best-fitting line.\\n\\n\\\\subsection*{9.1.3 Finding the Optimal Fit}\\nLinear regression seeks the line $y=f(x)$ which minimizes the sum of the squared errors over all the training points, i.e. the coefficient vector $w$ that minimizes\\n\\n$$\\n\\\\sum_{i=1}^{n}\\\\left(y_{i}-f\\\\left(x_{i}\\\\right)\\\\right)^{2}, \\\\text { where } f(x)=w_{0}+\\\\sum_{i=1}^{m-1} w_{i} x_{i}\\n$$\\n\\nSuppose we are trying to fit a set of $n$ points, each of which is $m$ dimensional. The first $m-1$ dimensions of each point is the feature vector $\\\\left(x_{1}, \\\\ldots, x_{m-1}\\\\right)$, with the last value $y=x_{m}$ serving as the target or dependent variable.\\n\\nWe can encode these $n$ feature vectors as an $n \\\\times(m-1)$ matrix\\\\index{matrix}. We can make it an $n \\\\times m$ matrix $A$ by prepending a column of ones to the matrix. This column can be thought of as a \"constant\" feature, one that when multiplied by the appropriate coefficient becomes the $y$-intercept of the fitted line. Further, the $n$ target values can be nicely represented in an $n \\\\times 1$ vector $b$.\\n\\nThe optimal regression line $f(x)$ we seek is defined by an $m \\\\times 1$ vector of coefficients $w=\\\\left\\\\{w_{0}, w_{1}, \\\\ldots, w_{m-1}\\\\right\\\\}$. Evaluating this function on these points is exactly the product $A \\\\cdot w$, creating an $n \\\\times 1$ vector of target value predictions. Thus $(b-A \\\\cdot w)$ is the vector of residual values.\\n\\nHow can we find the coefficients of the best fitting line? The vector $w$ is given by:\\n\\n$$\\nw=\\\\left(A^{T} A\\\\right)^{-1} A^{T} b\\n$$\\n\\nFirst, let\\'s grok this before we try to understand it. The dimensions of the term on the right are\\n\\n$$\\n((m \\\\times n)(n \\\\times m))(m \\\\times n)(n \\\\times 1) \\\\rightarrow(m \\\\times 1) .\\n$$\\n\\nwhich exactly matches the dimensions of the target vector $w$, so that is good. Further, $\\\\left(A^{T} A\\\\right)$ defines the covariance\\\\index{covariance} matrix on the columns/features of the data matrix, and inverting it is akin to solving\\\\index{solving} a system of equations. The term $A^{T} b$ computes the dot products of the data values and the target values for each of the $m$ features, providing a measure of how correlated each feature is with the target results. We don\\'t understand why this works yet, but it should be clear that this equation is made up of meaningful components.\\n\\nTake-Home Lesson: That the least squares regression line is defined by $w=\\\\left(A^{T} A\\\\right)^{-1} A^{T} b$ means that solving regression problems reduces to inverting and multiplying matrices. This formula works fine for small matrices, but the gradient descent algorithm (see Section 9.4) will prove more efficient in practice.\\n\\nConsider the case of a single variable $x$, where we seek the best-fitting line of the form $y=w_{0}+w_{1} x$. The slope of this line is given by\\n\\n$$\\nw_{1}=\\\\sum_{i=1}^{n} \\\\frac{\\\\left(x_{i}-\\\\bar{x}\\\\right)\\\\left(y_{i}-\\\\bar{y}\\\\right)}{\\\\sum_{i=1}^{n}\\\\left(x_{i}-\\\\bar{x}\\\\right)^{2}}=r_{x y} \\\\frac{\\\\sigma_{x}}{\\\\sigma_{y}}\\n$$\\n\\nwith $w_{0}=\\\\bar{y}-w_{1} \\\\bar{x}$, because of the nice observation that the best-fitting line passes through $(\\\\bar{x}, \\\\bar{y})$.\\n\\nThe connection with the correlation coefficient $\\\\left(r_{x y}\\\\right)$ here is clear. If $x$ were uncorrelated with $y\\\\left(r_{x y}=0\\\\right)$, then $w_{1}$ should indeed be zero. Even if they were perfectly correlated ( $r_{x y}=1$ ), we must scale $x$ to bring it into the right size range of $y$. This is the role of $\\\\sigma_{y} / \\\\sigma_{x}$.\\n\\nNow, where does the linear regression formula come from? It should be clear that in the best-fitting line, we cannot change any of the coefficients $w$ and hope to make a better fit. This means that the error vector $(b-A w)$ has to be orthogonal with the vector associated with each variable $x_{i}$, or else there would be a way to change the coefficient to fit it better.\\n\\nOrthogonal vectors have dot products of zero. Since the $i$ th column of $A^{T}$ has a zero dot product with the error vector, $\\\\left(A^{T}\\\\right)(b-A w)=\\\\overline{0}$, where $\\\\overline{0}$ is a vector of all zeros. Straightforward algebra then yields\\n\\n$$\\nw=\\\\left(A^{T} A\\\\right)^{-1} A^{T} b .\\n$$\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-287}\\n\\\\end{center}\\n\\nFigure 9.4: Removing outlier points (left) can result in much more meaningful fits (right).\\n\\n\\\\subsection*{9.2 Better Regression Models}\\nGiven a matrix $A$ of $n$ points, each of $m-1$ dimensions, and an $n \\\\times 1$ target array $b$, we can invert and multiply the appropriate matrices to get the desired coefficient matrix $w$. This defines a regression model. Done!\\n\\nHowever, there are several steps one can take which can lead to better regression models. Some of these involve manipulating the input data to increase the likelihood of an accurate model, but others require more conceptual issues of what our model should look like.\\n\\n\\\\subsection*{9.2.1 Removing Outliers}\\nLinear regression seeks the line $y=f(x)$ which minimizes the sum of the squared errors over all training points, i.e. the coefficient vector $w$ that minimizes\\n\\n$$\\n\\\\sum_{i=1}^{n}\\\\left(y_{i}-f\\\\left(x_{i}\\\\right)\\\\right)^{2}, \\\\text { where } f(x)=w_{0}+\\\\sum_{i=1}^{m-1} w_{i} x_{i}\\n$$\\n\\nBecause of the quadratic weight of the residuals, outlying points can greatly affect the fit. A point at a distance 10 from its prediction has 100 times the impact on training error than a point only 1 unit from the fitted line. One might argue that this is appropriate, but it should be clear that outlier points have a big impact in the shape of the best-fitting line. This creates a problem when the outlier points reflect noise rather than signal, because the regression line goes out of its way to accommodate the bad data instead of fitting the good.\\n\\nWe first encountered this problem back in Figure 6.3 with the Anscombe quartet, a collection of four small data sets with identical summary statistics and regression lines. Two of these point sets achieved their magic because of solitary outlier points. Remove the outliers, and the fit now goes through the heart of the data.\\n\\n%---- Page End Break Here ---- Page : 272\\n\\nFigure 9.4 shows the best-fitting regression line with (left) and without (right) an outlier point in the lower right. The fit on the right is much better: with an $r^{2}$ of 0.917 without the outlier, compared to 0.548 with the outlier.\\n\\nTherefore identifying outlying points and removing them in a principled way can yield a more robust fit. The simplest approach is to fit the entire set of points, and then use the magnitude of the residual $r_{i}=\\\\left(y_{i}-f\\\\left(x_{i}\\\\right)\\\\right)^{2}$ to decide whether point $p_{i}$ is an outlier. It is important to convince yourself that these points really represent errors before deleting them, however. Otherwise you will be left with an impressively linear fit that works well only on the examples you didn\\'t delete.\\n\\n\\\\subsection*{9.2.2 Fitting Non-Linear Functions}\\nLinear relationships are easier to understand than non-linear ones, and grossly appropriate as a default assumption in the absence of better data. Many phenomena are linear in nature, with the dependent variable growing roughly proportionally with the input variables:\\n\\n\\\\begin{itemize}\\n  \\\\item Income grows roughly linearly with the amount of time worked.\\n  \\\\item The price of a home grows roughly linearly with the size of the living area it contains.\\n  \\\\item People\\'s weight increases roughly linearly with the amount of food eaten.\\n\\\\end{itemize}\\n\\nLinear regression does great when it tries to fit data that in fact has an underlying linear relationship. But, generally speaking, no interesting function is perfectly linear. Indeed, there is an old statistician\\'s rule that states if you want a function to be linear, measure it at only two points.\\n\\nWe could greatly increase the repertoire of shapes we can model if we move beyond linear functions. Linear regression fits lines, not high-order curves. But we can fit quadratics by adding an extra variable with the value $x^{2}$ to our data matrix, in addition to $x$. The model $y=w_{0}+w_{1} x+w_{2} x^{2}$ is quadratic, but note that it is a linear function of its non-linear input values. We can fit arbitrarilycomplex functions by adding the right higher-order variables to our data matrix, and forming linear combinations of them. We can fit arbitrary polynomials and exponentials/logarithms by explicitly including the right component variables in our data matrix, such as $\\\\sqrt{x}, \\\\lg (x), x^{3}$, and $1 / x$.\\n\\nExtra features can also be used to capture non-linear interactions between pairs of input variables. The area of a rectangle $A$ is computed length $\\\\times$ width, meaning one cannot get an accurate approximation of $A$ as a linear combination of length and width. But, once we add an area feature to our data matrix, this non-linear interaction can be captured with a linear model.\\n\\nHowever, explicit inclusion of all possible non-linear terms quickly becomes intractable. Adding all powers $x^{i}$ for $1 \\\\leq i \\\\leq k$ will blow up the data matrix by a factor of $k$. Including all product pairs among $n$ variables is even worse, making the matrix $n(n+1) / 2$ times larger. One must be judicious about which nonlinear terms to consider for a role in the model. Indeed, one of the advantages\\\\\\n%---- Page End Break Here ---- Page : 273\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-289}\\n\\nFigure 9.5: Higher-order models (red) can lead to better fits than linear models (green).\\\\\\\\\\nof more powerful learning methods, like support vector machines, will be that they can incorporate non-linear terms without explicit enumeration.\\n\\n\\\\subsection*{9.2.3 Feature and target scaling\\\\index{target scaling}}\\nIn principle, linear regression can find the best linear model fitting any data set. But we should do whatever we can to help it find the right model. This generally involves preprocessing the data to optimize for expressibility, interpretability, and numerical stability. The issue here is that features which vary over wide numerical ranges require coefficients over similarly wide ranges to bring them together.\\n\\nSuppose we wanted to build a model to predict the gross national product of countries in dollars, as a function of their population size $x_{1}$ and literacy rate $x_{2}$. Both factors seem like reasonable components of such a model. Indeed, both factors may well contribute equally to the amount of economic activity. But they operate on entirely different scales: national populations vary from tens of thousands to over a billion people, while the fraction of people who can read is, by definition, between zero and one. One might imagine the resulting fitted model as looking somewhat like this:\\n\\n$$\\nG D P=\\\\$ 10,000 x_{1}+\\\\$ 10,000,000,000,000 x_{2}\\n$$\\n\\nThis is very bad, for several reasons:\\n\\n\\\\begin{itemize}\\n  \\\\item Unreadable coefficients: Quick, what is the coefficient of $x_{2}$ in the above equation? It is hard for us to deal with the magnitude of such numbers (it is 10 trillion), and hard for us to tell which variable makes a more important contribution to the result given their ranges. Is it $x_{1}$ or $x_{2}$ ?\\n  \\\\item Numerical imprecision: Numerical optimization algorithms have trouble when values range over many orders of magnitude. It isn\\'t just the fact\\\\\\n%---- Page End Break Here ---- Page : 274\\n\\\\\\nthat floating point numbers are represented by a finite number of bits. More important is that many machine learning algorithms get parameterized by constants that must hold simultaneously for all variables. For example, using fixed step sizes in gradient descent search (to be discussed in Section 9.4 .2 might cause it to wildly overshoot in certain directions while undershooting in others.\\n  \\\\item Inappropriate formulations: The model given above to predict GDP is silly on the face of it. Suppose I decide to form my own country, which would have exactly one person in it, who could read. Do we really think that Skienaland should have a GDP of $\\\\$ 10,000,000,010,000$ ?\\\\\\\\\\nA better model might be something like:\\n\\\\end{itemize}\\n\\n$$\\nG D P=\\\\$ 20,000 x_{1} x_{2}\\n$$\\n\\nwhich can be interpreted as each of the $x_{1}$ people creating wealth at a rate modulated by their literacy. This generally requires seeding the data matrix with the appropriate product terms. But there is a chance that with proper (logarithmic) target scaling, this model might fall directly out from linear regression.\\n\\nWe will now consider three different forms of scaling, which address these different types of problems.\\n\\n\\\\section*{feature scaling\\\\index{feature scaling}: z-scores\\\\index{z-scores}}\\nWe have previously discussed Z-scores, which scale the values of each feature individually, so that the mean is zero and the ranges are comparable. Let $\\\\mu$ be the mean value of the given feature, and $\\\\sigma$ the standard deviation. Then the Z-score of $x$ is $Z(x)=(x-\\\\mu) / \\\\sigma$.\\n\\nUsing Z-scores in regression addresses the question of interpretability. Since all features will have similar means and variances, the magnitude of the coefficients will determine the relative importance of these factors towards the forecast. Indeed, in proper conditions, these coefficients will reflect the correlation coefficient of each variable with the target. Further, that these variables now range over the same magnitude simplifies the work for the optimization algorithm.\\n\\n\\\\section*{sublinear\\\\index{sublinear} Feature Scaling}\\nConsider a linear model for predicting the number of years of education $y$ that a child will receive as a function of household income. Education levels can vary between 0 and $12+4+5=19$ years, since we consider up to the possible completion of a Ph.D. A family\\'s income level $x$ can vary between 0 and Bill Gates. But observe that no model of the form\\n\\n$$\\ny=w_{1} x+w_{0}\\n$$\\n\\ncan possibly give sensible answers for both my kids and Bill Gates\\' kids. The real impact of income on education level is presumably at the lower end: children below the poverty line may not, on average, go beyond high school, while upper-middle-class kids generally go to college. But there is no way to capture this in a linearly-weighted variable without dooming the Gates children to hundreds or thousands of years at school.\\n\\nAn enormous gap between the largest/smallest and median values means that no coefficient can use the feature without blowup on big values. Income level is power law distributed, and the Z-scores of such power law variables can\\'t help, because they are just linear transformations. The key is to replace/augment such features $x$ with sublinear functions like $\\\\log (x)$ and $\\\\sqrt{x}$. Z-scores of these transformed variables will prove much more meaningful to build models from.\\n\\n\\\\section*{Sublinear Target Scaling}\\nSmall-scale variables need small-scale targets, in order to be realized using smallscale coefficients. Trying to predict GDP from Z-scored variables will require enormously large coefficients. How else could you get to trillions of dollars from a linear combination of variables ranging from -3 to +3 ?\\n\\nPerhaps scaling the target value from dollars to billions of dollars here would be helpful, but there is a deeper problem. When your features are normally distributed, you can only do a good job regressing to a similarly distributed target. Statistics like GDP are likely power law distributed: there are many small poor countries compared to very few large rich ones. Any linear combination of normally-distributed variables cannot effectively realize a power law-distributed target.\\n\\nThe solution here is that trying to predict the $\\\\operatorname{logarithm}\\\\left(\\\\log _{c}(y)\\\\right)$ of a power law target $y$ is usually better than predicting $y$ itself. Of course, the value $c^{f(x)}$ can then be used to estimate $y$, but the potential now exists to make meaningful predictions over the full range of values. Hitting a power law function\\\\index{power law function} with a logarithm generally produces a better behaved, more normal distribution.\\n\\nIt also enables us to implicitly realize a broader range of functions. Suppose the \"right\" function to predict gross domestic product was in fact\\n\\n$$\\nG D P=\\\\$ 20,000 x_{1} x_{2} .\\n$$\\n\\nThis could never be realized by linear regression without interaction variables. But observe that\\n\\n$$\\n\\\\log (G D P)=\\\\log \\\\left(\\\\$ 20,000 x_{1} x_{2}\\\\right)=\\\\log (\\\\$ 20,000)+\\\\log \\\\left(x_{1}\\\\right)+\\\\log \\\\left(x_{2}\\\\right)\\n$$\\n\\nThus the logarithms of arbitrary interaction products could be realized, provided the feature matrix contained the logs of the original input variables as well.\\n\\n%---- Page End Break Here ---- Page : 276\\n\\n\\\\subsection*{9.2.4 Dealing with Highly-Correlated Features}\\nA final pitfall we will discuss is the problem of highly-correlated features. It is great to have features that are highly correlated with the target: these enable us to build highly-predictive models. However, having multiple features which are highly correlated with each other can be asking for trouble.\\n\\nSuppose you have two perfectly-correlated features in your data matrix, say the subject\\'s height in feet $\\\\left(x_{1}\\\\right)$ as well as their height in meters $\\\\left(x_{2}\\\\right)$. Since 1 meter equals 3.28084 feet, these two variables are perfectly correlated. But having both of these variables can\\'t really help our model, because adding a perfectly correlated feature provides no additional information to make predictions. If such duplicate features really had value for us, it would imply that we could build increasingly accurate models simply by making additional copies of columns from any data matrix!\\n\\nBut correlated features are harmful to models, not just neutral. Suppose our dependent variable is a function of height. Note that equally good models can be built dependent only on $x_{1}$, or only on $x_{2}$, or on any arbitrary linear combination of $x_{1}$ and $x_{2}$. Which is the right model to report as the answer?\\n\\nThis is confusing, but even worse things can happen. The rows in the covariance matrix will be mutually dependent, so computing $w=\\\\left(A^{T} A\\\\right)^{-1} A^{T} b$ now requires inverting a singular matrix! Numerical methods for computing the regression are liable to fail.\\n\\nThe solution here is to identify feature pairs which correlate excessively strongly, by computing the appropriate covariance matrix. If they are lurking, you can eliminate either variable with little loss of power. Better is to eliminate these correlations entirely, by combining the features. This is one of the problems solved by dimension reduction, using techniques like singular value decomposition that we discussed in Section 8.5.1\\n\\n\\\\subsection*{9.3 War Story: Taxi Deriver}\\nI am proud of many things in my life, but perhaps most so of being a New Yorker. I live in the most exciting city on earth, the true center of the universe. Astronomers, at least the good ones, will tell you that each new year starts when the ball drops in Times Square, and then radiates out from New York at the speed of light to the rest of the world.\\n\\nNew York cab drivers are respected around the world for their savvy and street smarts. It is customary to tip the driver for each ride, but there is no established tradition of how much that should be. In New York restaurants, the \"right\" amount to tip the waiter is to double the tax, but I am unaware of any such heuristic for taxi tipping. My algorithm is to round up to the nearest dollar and then toss in a couple of bucks depending upon how fast he got me there. But I have always felt unsure. Am I a cheapskate? Or maybe a sucker?\\n\\nThe taxi data set discussed in Section 1.2 .4 promised to hold the answer. It contained over 80 million records, with fields for date, time, pickup and\\\\\\n%---- Page End Break Here ---- Page : 277\\n\\\\\\ndrop off locations, distance traveled, fare, and of course tip. Do people pay disproportionately for longer or shorter trips? Late at night or on weekends? Do others reward fast drivers like I do? It should all be there in the data.\\n\\nMy student, Oleksii Starov, rose to the challenge. We added appropriate features to the data set to capture some of these notions. To explicitly capture conditions like late night and weekends, we set up binary indicator variables, where 1 would denote that the trip was late night and 0 at some other time of day. The coefficients of our final regression equation was:\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|r}\\nvariable & LR coefficient \\\\\\\\\\n\\\\hline\\n(intercept) & 0.08370835 \\\\\\\\\\nduration & 0.00000035 \\\\\\\\\\ndistance & 0.00000004 \\\\\\\\\\nfare & 0.17503086 \\\\\\\\\\ntolls & 0.06267343 \\\\\\\\\\nsurcharge & 0.01924337 \\\\\\\\\\nweekends & -0.02823731 \\\\\\\\\\nbusiness day & 0.06977724 \\\\\\\\\\nrush hour & 0.01281997 \\\\\\\\\\nlate night & 0.04967453 \\\\\\\\\\n\\\\# of passengers & -0.00657358 \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nThe results here can be explained simply. Only one variable really matters: the fare on the meter. This model tips $17.5 \\\\%$ of the total fare, with very minor adjustments for other things. A single parameter model tipping $18.3 \\\\%$ of each fare proved almost as accurate as the ten-factor model.\\n\\nThere were very strong correlations between the fare and both distance traveled (0.95) and trip duration (0.88), but both of these factors are part of the formula by which fares are calculated. These correlations with fare are so strong that neither variable can contribute much additional information. To our disappointment, we couldn\\'t really tease out an influence of time of day or anything else, because these correlations were so weak.\\n\\nA deeper look at the data revealed that every single tip in the database was charged to a credit card, as opposed to being paid by cash. Entering the tip amount into the meter after each cash transaction is tedious and time consuming, particularly when you are hustling to get as many fares as possible on each 12 hour shift. Further, real New York cabbies are savvy and street-smart enough not to want to pay taxes on tips no one else knows about.\\n\\nI always pay my fares with cash, but the people who pay by credit card are confronted with a menu offering them the choice of what tip to leave. The data clearly showed most of them mindlessly hitting the middle button, instead of modulating their choice to reflect the quality of service.\\n\\nUsing 80 million fare records to fit a simple linear regression on ten variables is obscene overkill. Better use of this data would be to construct hundreds or even thousands of different models, each designed for a particular class of trips. Perhaps we could build a separate model for trips between each pair of city zip\\\\\\n%---- Page End Break Here ---- Page : 278\\n\\\\\\ncodes. Indeed, recall our map of such tipping behavior, presented back in Figure 1.7\\n\\nIt took several minutes for the solver to find the best fit on such a large data set, but that it finished at all meant some algorithm faster and more robust than matrix inversion had to be involved. These algorithms view regression as a parameter fitting problem, as we will discuss in the next section.\\n\\n\\\\subsection*{9.4 Regression as Parameter Fitting}\\nThe closed form formula for linear regression, $w=\\\\left(A^{T} A\\\\right)^{-1} A^{T} b$, is concise and elegant. However, it has some issues which make it suboptimal for computation in practice. Matrix inversion is slow for large systems, and prone to numerical instability. Further, the formulation is brittle: the linear algebra magic here is hard to extend to more general optimization problems.\\n\\nBut there is an alternate way to formulate and solve linear regression problems, which proves better in practice. This approach leads \\\\index{parameter ï¬tting}to faster algorithms, more robust numerics, and can be readily adapted to other learning algorithms. It models linear regression as a parameter fitting problem, and deploys search algorithms to find the best values that it can for these parameters.\\n\\nFor linear regression, we seek the line that best fits the points, over all possible sets of coefficients. Specifically, we seek the line $y=f(x)$ which minimizes the sum of the squared errors over all training points, i.e. the coefficient vector $w$ that minimizes\\n\\n$$\\n\\\\sum_{i=1}^{n}\\\\left(y_{i}-f\\\\left(x_{i}\\\\right)\\\\right)^{2}, \\\\text { where } f(x)=w_{0}+\\\\sum_{i=1}^{m-1} w_{i} x_{i}\\n$$\\n\\nFor concreteness, let us start with the case where we are trying to model $y$ as a linear function of a single variable or feature $x$, so $y=f(x)$ means $y=w_{0}+w_{1} x$. To define our regression line, we seek the parameter pair $\\\\left(w_{0}, w_{1}\\\\right)$ which minimizes error or cost or loss, namely the sum of squares deviation between the point values and the line.\\n\\nEvery possible pair of values for $\\\\left(w_{0}, w_{1}\\\\right)$ will define some line, but we really want the values that minimize the error or loss function\\\\index{loss function} $J\\\\left(w_{0}, w_{1}\\\\right)$, where\\n\\n$$\\n\\\\begin{aligned}\\nJ\\\\left(w_{0}, w_{1}\\\\right) & =\\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(y_{i}-f\\\\left(x_{i}\\\\right)\\\\right)^{2} \\\\\\\\\\n& =\\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(y_{i}-\\\\left(w_{0}+w_{1} x_{i}\\\\right)\\\\right)^{2}\\n\\\\end{aligned}\\n$$\\n\\nThe sum of squares errors should be clear, but where does the $1 /(2 n)$ come from? The $1 / n$ turns this into an average error per row, and the $1 / 2$ is a common convention for technical reasons. But be clear that the $1 /(2 n)$ in no way effects the results of the optimization. This multiplier will be the same for each $\\\\left(w_{0}, w_{1}\\\\right)$ pair, and so has no say in which parameters get chosen.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-295}\\n\\nFigure 9.6: The best possible regression line $y=w_{1} x$ (left) can be found by identifying the $w_{1}$ that minimizes the error of the fit, defined by the minima of a convex\\\\index{convex} function.\\n\\nSo how can we find the right values for $w_{0}$ and $w_{1}$ ? We might try a bunch of random value pairs, and keep the one which scores best, i.e. with minimum loss $J\\\\left(w_{0}, w_{1}\\\\right)$. But it seems very unlikely to stumble on the best or even a decent solution. To search more systematically, we will have to take advantage of a special property lurking within the loss function.\\n\\n\\\\subsection*{9.4.1 Convex Parameter Spaces}\\nThe upshot of the above discussion is that the loss function $J\\\\left(w_{0}, w_{1}\\\\right)$ defines a surface in $\\\\left(w_{0}, w_{1}\\\\right)$-space, with our interest being in the point in this space with smallest $z$ value, where $z=J\\\\left(w_{0}, w_{1}\\\\right)$.\\n\\nLet\\'s start by making it even simpler, forcing our regression line to pass through the origin by setting $w_{0}=0$. This leaves us only one free parameter to find, namely the slope of the line $w_{1}$. Certain slopes will do a wildly better job of fitting the points shown in Figure 9.6 (left) than others, with the line $y=x$ clearly being the desired fit.\\n\\nFigure 9.6 (right) shows how the fitting error (loss) varies with $w_{1}$. The interesting thing is that the error function is shaped sort of like a parabola. It hits a single minimum value at the bottom of the curve. The $x$-value of this minimum point defines the best slope $w_{1}$ for the regression line, which happens to be $w_{1}=1$.\\n\\nAny convex surface has exactly one local minima. Further, for any convex search space it is quite easy to find this minima: just keep walking in a downward direction until you hit it. From every point on the surface, we can take a small step to a nearby point on the surface. Some directions will take us up to a\\\\\\n%---- Page End Break Here ---- Page : 280\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-296}\\n\\nFigure 9.7: Linear regression defines a convex parameter space, where each point represents a possible line, and the minimum point defines the best fitting line.\\\\\\\\\\nhigher value, but others will take us down. Provided that we can identify which step will take us lower, we will move closer to the minima. And there always is such direction, except when we are standing on the minimal point itself!\\n\\nFigure 9.7 shows the surface we get for the full regression problem in $\\\\left(w_{0}, w_{1}\\\\right)$ space. The loss function $J\\\\left(w_{0}, w_{1}\\\\right)$ looks like a bowl with a single smallest $z$ value, which defines the optimal values for the two parameters of the line. The great thing is that this loss function $J\\\\left(w_{0}, w_{1}\\\\right)$ is again convex, and indeed it remains convex for any linear regression problem in any number of dimensions.\\n\\nHow can we tell whether a given function is convex? Remember back to when you took calculus in one variable, $x$. You learned how to take the derivative $f^{\\\\prime}(x)$ of a function $f(x)$, which corresponds to the value of the slope of the surface of $f(x)$ at every point. Whenever this derivative was zero, it meant that you had hit some point of interest, be it a local maxima or a minima. Recall the second derivative $f^{\\\\prime \\\\prime}(x)$, which was the derivative function of the derivative $f^{\\\\prime}(x)$. Depending upon the sign of this second derivative $f^{\\\\prime \\\\prime}(x)$, you could identify whether you hit a maxima or minima.\\n\\nBottom line: the analysis of such derivatives can tell us which functions are and are not convex. We will not delve deeper here. But once it has been established that our loss function is convex, we know that we can trust a procedure like gradient descent search, which gets us to the global optima by walking downward.\\n\\n\\\\subsection*{9.4.2 Gradient Descent Search}\\nWe can find the minima of a convex function simply by starting at an arbitrary point, and repeatedly walking in a downward direction. There is only one point\\\\\\n%---- Page End Break Here ---- Page : 281\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-297}\\n\\nFigure 9.8: The tangent line\\\\index{tangent line} approximates the derivative at a point.\\\\\\\\\\nwhere there is no way down: the global minima itself. And it is this point that defines the parameters of the best fitting regression line.\\n\\nBut how can we find a direction that leads us down the hill? Again, let\\'s consider the single variable case first, so we seek the slope $w_{1}$ of the best-fitting line where $w_{0}=0$. Suppose our current slope candidate is $x_{0}$. In this restrictive one-dimensional setting, we can only move to the left or to the right. Try a small step in each direction, i.e. the values $x_{0}-\\\\epsilon$ and $x_{0}+\\\\epsilon$. If the value of $J\\\\left(0, x_{0}-\\\\epsilon\\\\right)<J\\\\left(0, x_{0}\\\\right)$, then we should move to the left to go down. If $J\\\\left(0, x_{0}+\\\\epsilon\\\\right)<J\\\\left(0, x_{0}\\\\right)$, then we should move to the right. If neither is true, it means that we have no place to go to reduce $J$, so we must have found the minima.\\n\\nThe direction down at $f\\\\left(x_{0}\\\\right)$ is defined by the slope of the tangent line at this point. A positive slope means that the minima must lie on the left, while a negative slope puts it on the right. The magnitude of this slope describes the steepness of this drop: by how much will $J\\\\left(0, x_{0}-\\\\epsilon\\\\right)$ differ from $J\\\\left(0, x_{0}\\\\right)$ ?\\n\\nThis slope can be approximated by finding the unique line which passes through points $\\\\left(x_{0}, J\\\\left(0, x_{0}\\\\right)\\\\right)$ and $\\\\left(x_{0}, J\\\\left(0, x_{0}-\\\\epsilon\\\\right)\\\\right)$, as shown in Figure 9.8. This is exactly what is done in computing the derivative, which at each point specifies the tangent to the curve.\\n\\nAs we move beyond one dimension, we gain the freedom to move in a greater range of directions. Diagonal moves let us cut across multiple dimensions at once. But in principle, we can get the same effect by taking multiple steps, along each distinct dimension in an axis-oriented direction. Think of the Manhattan street grid, where we can get anywhere we want by moving in a combination of north-south and east-west steps. Finding these directions requires computing the partial\\\\index{partial} derivative of the objective function along each dimension, namely:\\n\\n%---- Page End Break Here ---- Page : 282\\n\\n\\\\section*{Gradient descent search in two dimensions}\\nRepeat until convergence \\\\{\\n\\n$$\\n\\\\begin{aligned}\\n& w_{0}^{t+1}:=w_{0}^{t}-\\\\alpha \\\\frac{\\\\partial}{\\\\partial w_{0}} J\\\\left(w_{0}^{t}, w_{1}^{t}\\\\right) \\\\\\\\\\n& w_{1}^{t+1}:=w_{1}^{t}-\\\\alpha \\\\frac{\\\\partial}{\\\\partial w_{1}} J\\\\left(w_{0}^{t}, w_{1}^{t}\\\\right)\\n\\\\end{aligned}\\n$$\\n\\n\\\\}\\n\\nFigure 9.9: Pseudocode for regression by gradient descent search. The variable $t$ denotes the iteration number of the computation.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\partial}{\\\\partial w_{j}} & =\\\\frac{2}{\\\\partial w_{j}} \\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(f\\\\left(x_{i}\\\\right)-b_{i}\\\\right)^{2} \\\\\\\\\\n& =\\\\frac{2}{\\\\partial w_{j}} \\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(w_{0}+\\\\left(w_{1} x_{i}\\\\right)-b_{i}\\\\right)^{2}\\n\\\\end{aligned}\\n$$\\n\\nBut zig-zagging along dimensions seems slow and clumsy. Like Superman, we want to leap buildings in a single bound. The magnitude of the partial derivatives defines the steepness in each direction, and the resulting vector (say three steps west for every one step north) defines the fastest way down from this point.\\n\\n\\\\subsection*{9.4.3 What is the Right learning rate\\\\index{learning rate}?}\\nThe derivative of the loss function points us in the right direction to walk towards the minima, which specifies the parameters to solve our regression problem. But it doesn\\'t tell us how far to walk. The value of this direction decreases with distance. It is indeed true that the fastest way to drive to Miami from New York is to head south, but at some point you will need more detailed instructions.\\n\\nGradient descent search operates in rounds: find the best direction, take a step, and then repeat until we hit the target. The size of our step is called the learning rate, and it defines the speed with which we find the minima. Taking tiny baby steps and repeatedly consulting the map (i.e. partial derivatives) will indeed get us there, but only very slowly.\\n\\nHowever, bigger isn\\'t always better. If the learning rate is too high, we might jump past the minima, as shown in Figure 9.10 (right). This might mean slow\\\\\\n%---- Page End Break Here ---- Page : 283\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-299(1)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-299}\\n\\nFigure 9.10: The effect of learning rate/step size. Taking too small a step size requires many iterations to converge, while too large a step size causes us to overshoot the minima.\\\\\\\\\\nprogress towards the hole as we bounce past it on each step, or even negative progress as we end up at a value of $J(w)$ higher than where we were before.\\n\\nIn principle, we want a large learning rate at the beginning of our search, but one which decreases as we get closer to our goal. We need to monitor the value of our loss function over the course of the optimization. If progress becomes too slow, we can increase the step size by a multiplicative factor (say 3) or give up: accepting the current parameter values for our fitting line as good enough. But if the value of $J(w)$ increases, it means that we have overshot our goal. Thus our step size was too large, so we should decrease the learning rate by a multiplicative factor: say by $1 / 3$.\\n\\nThe details of this are messy, heuristic, and ad hoc. But fortunately library functions for gradient descent search have built-in algorithms for adjusting the learning rate. Presumably these algorithms have been highly tuned, and should generally do what they are supposed to do.\\n\\nBut the shape of the surface makes a big difference as to how successfully gradient descent search finds the global\\\\index{global} minimum. If our bowl-shaped surface was relatively flat, like a plate, the truly lowest point might be obscured by a cloud of noise and numerical error. Even if we do eventually find the minima, it might take us a very long time to get there.\\n\\nHowever, even worse things happen when our loss function is not convex, meaning there can be many local\\\\index{local} minima, as in Figure 9.11 Now this can\\'t be the case for linear regression, but does happen for many other interesting machine learning problems we will encounter.\\n\\nLocal optimization can easily get stuck in local minima for non-convex functions. Suppose we want to reach the top of a ski slope from our lodge in the valley. If we start by walking up to the second floor of the lodge, we will get trapped forever, unless there is some mechanism for taking steps backwards to free us from the local optima. This is the value of search heuristics like simulated annealing, which provides a way out of small local optima to keep us advancing towards the global goal.\\\\\\n%---- Page End Break Here ---- Page : 284\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-300}\\n\\nFigure 9.11: Gradient descent search finds local minima for non-convex surfaces, but does not guarantee a globally optimum solution.\\n\\nTake-Home Lesson: Gradient descent search remains useful in practice for nonconvex optimization, although it no longer guarantees an optimal solution. Instead, we should start repeatedly from different initialization points, and use the best local minima we find to define our solution.\\n\\n\\\\subsection*{9.4.4 stochastic gradient descent\\\\index{stochastic gradient descent}}\\nThe algebraic definition of our loss function hides something very expensive going on:\\n\\n$$\\n\\\\frac{\\\\partial}{\\\\partial w_{j}}=\\\\frac{2}{\\\\partial w_{j}} \\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(f\\\\left(x_{i}\\\\right)-b_{i}\\\\right)^{2}=\\\\frac{2}{\\\\partial w_{j}} \\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(w_{0}+\\\\left(w_{1} x_{i}\\\\right)-b_{i}\\\\right)^{2}\\n$$\\n\\nIt\\'s that summation. To compute the best direction and rate of change for each dimension $j$, we must cycle through all $n$ of our training points. Evaluating each partial derivative takes time linear in the number of examples, for each step! For linear regression on our lavish taxicab data set, this means 80 million squared-difference computations just to identify the absolute best direction to advance one step towards the goal.\\n\\nThis is madness. Instead, we can try an approximation that uses only a small number of examples to estimate the derivative, and hopes that the resulting direction indeed points down. On average it should, since every point will eventually get to vote on direction.\\n\\nStochastic gradient descent is an optimization approach based on sampling a small batch of training points, ideally at random, and using them to estimate the derivative at our current position. The smaller the batch size we use, the faster the evaluation is, although we should be more skeptical that the estimated direction is correct. Optimizing the learning rate and the batch size for gradient\\\\\\n%---- Page End Break Here ---- Page : 285\\n\\\\\\ndescent leads to very fast optimization for convex functions, with the details blessedly concealed by a call to a library function.\\n\\nIt can be expensive to make random choices at every step of the search. Better is to randomize the order of the training examples once, to avoid systematic artifacts in how they are presented, and then build our batches by simply marching down the list. This way we can insure that all $n$ of our training instances eventually do contribute to the search, ideally several times as we repeatedly sweep through all examples over the course of optimization.\\n\\n\\\\subsection*{9.5 simplifying\\\\index{simplifying} Models through regularization\\\\index{regularization}}\\nLinear regression is happy to determine the best possible linear fit to any collection of $n$ data points, each specified by $m-1$ independent variables and a given target value. But the \"best\" fit may not be what we really want.\\n\\nThe problem is this. Most of the $m-1$ possible features may be uncorrelated with the target, and thus have no real predictive power. Typically, these will show as variables with small coefficients. However, the regression algorithm will use these values to nudge the line so as to reduce least square error on the given training examples. Using noise (the uncorrelated variables) to fit noise (the residual left from a simple model on the genuinely correlated variables) is asking for trouble.\\n\\nRepresentative here is our experience with the taxi tipping model\\\\index{tipping model}, as detailed in the war story. The full regression model using ten variables had a mean squared error of 1.5448 . The single-variable regression model operating only on fare did slightly worse, with an error of 1.5487 . But this difference is just noise. The single variable model is obviously better, by Occam\\'s or anybody else\\'\\\\index{Occamâs razor}s razor.\\n\\nOther problems arise when using unconstrained regression. We have seen how strongly correlated features introduce ambiguity into the model. If features $A$ and $B$ are perfectly correlated, using both yields the same accuracy as using either one, resulting in more complicated and less interpretable models.\\n\\nProviding a rich set of features to regression is good, but remember that \"the simplest explanation is best.\" The simplest explanation relies on the smallest number of variables that do a good job of modeling the data. Ideally our regression would select the most important variables and fit them, but the objective function we have discussed only tries to minimize sum of squares error. We need to change our objective function, through the magic of regularization.\\n\\n\\\\subsection*{9.5.1 ridge\\\\index{ridge} Regression}\\nRegularization is the trick of adding secondary terms to the objective function to favor models that keep coefficients small. Suppose we generalize our loss function with a second set of terms that are a function of the coefficients, not\\\\\\n%---- Page End Break Here ---- Page : 286\\n\\\\\\nthe training data:\\n\\n$$\\nJ(w)=\\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(y_{i}-f\\\\left(x_{i}\\\\right)\\\\right)^{2}+\\\\lambda \\\\sum_{j=1}^{m} w_{j}^{2}\\n$$\\n\\nIn this formulation, we pay a penalty proportional to the sum of squares of the coefficients used in the model. By squaring the coefficients, we ignore sign and focus on magnitude. The constant $\\\\lambda$ modulates the relative strength of the regularization constraints. The higher $\\\\lambda$ is, the harder the optimization will work to reduce coefficient size, at the expense of increased residuals. It eventually becomes more worthwhile to set the coefficient of an uncorrelated variable to zero, rather than use it to overfit the training set.\\n\\nPenalizing the sum of squared coefficients, as in the loss function above, is called ridge regression or Tikhonov regularization\\\\index{Tikhonov regularization}. Assuming that the dependent variables have all been properly norm\\\\index{norm}alized to mean zero, their coefficient magnitude is a measure of their value to the objective function.\\n\\nHow can we optimize the parameters for ridge regression? A natural extension to the least squares formulation does the job. Let $\\\\Gamma$ be our $n \\\\times n$ \"coefficient weight penalty\" matrix. For simplicity, let $\\\\Gamma=I$, the identity matrix. The sum-of-squares loss function we seek to minimize then becomes\\n\\n$$\\n\\\\|A w-b\\\\|^{2}+\\\\|\\\\lambda \\\\Gamma w\\\\|^{2}\\n$$\\n\\nThe notation $\\\\|v\\\\|$ denotes the norm of $v$, a distance function on a vector or matrix. The norm of $\\\\|\\\\Gamma w\\\\|^{2}$ is exactly the sum of squares of the coefficients when $\\\\Gamma=I$. Seen this way, the closed form to optimize for $w$ is believable as\\n\\n$$\\nw=\\\\left(A^{T} A+\\\\lambda \\\\Gamma^{T} \\\\Gamma\\\\right)^{-1} A^{T} b\\n$$\\n\\nThus the normal form equation can be generalized to deal with regularization. But, alternately, we can compute the partial derivatives of this loss function and use gradient descent search to do the job faster on large matrices. In any case, library functions for ridge regression and its cousin LASSO\\\\index{LASSO} regression will be readily available to use on your problem.\\n\\n\\\\subsection*{9.5.2 LASSO Regression}\\nRidge regression optimizes to select small coefficients. Because of sum-of-squares cost function, it particularly punishes the largest coefficients. This makes it great to avoid models of the form $y=w_{0}+w_{1} x_{1}$, where $w_{0}$ is a large positive number and $w_{1}$ an offsetting large negative number.\\n\\nAlthough ridge regression is effective at reducing the magnitude of the coefficients, this criteria does not really push them to zero and totally eliminate the variable from the model. An alternate choice here is to try to minimize the sum of the absolute values of the coefficients, which is just as happy to drive down the smallest coefficients as the big ones.\\n\\n%---- Page End Break Here ---- Page : 287\\n\\nLASSO regression (for \"Least Absolute Shrinkage and Selection Operator\") meets this criteria: minimizing the $L_{1}$ metric on the coefficients instead of the $L_{2}$ metric of ridge regression. With LASSO, we specify an explicit constraint $t$ as to what the sum of the coefficients can be, and the optimization minimizes the sum of squares error under this constraint:\\n\\n$$\\nJ(w, t)=\\\\frac{1}{2 n} \\\\sum_{i=1}^{n}\\\\left(y_{i}-f\\\\left(x_{i}\\\\right)\\\\right)^{2} \\\\text { subject to } \\\\sum_{j=1}^{m}\\\\left|w_{j}\\\\right| \\\\leq t .\\n$$\\n\\nSpecifying a smaller value of $t$ tightens the LASSO, further constraining the magnitudes of the coefficients $w$.\\n\\nAs an example of how LASSO zeros out small coefficients, observe what it did to the taxi tipping model for a particular value of $t$ :\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|rr}\\nvariable & LR coefficient & LASSO \\\\\\\\\\n\\\\hline\\n(intercept) & 0.08370835 & 0.079601141 \\\\\\\\\\nduration & 0.00000035 & 0.00000035 \\\\\\\\\\ndistance & 0.00000004 & 0.00000004 \\\\\\\\\\nfare & 0.17503086 & 0.17804921 \\\\\\\\\\ntolls & 0.06267343 & 0.00000000 \\\\\\\\\\nsurcharge & 0.01924337 & 0.00000000 \\\\\\\\\\nweekends & -0.02823731 & 0.00000000 \\\\\\\\\\nbusiness day & 0.06977724 & 0.00000000 \\\\\\\\\\nrush hour & 0.01281997 & 0.00000000 \\\\\\\\\\nlate night & 0.04967453 & 0.00000000 \\\\\\\\\\n\\\\# of passengers & -0.00657358 & 0.00000000 \\\\\\\\\\n\\\\end{tabular}\\n\\\\en\\\\index{ï¬t and complexity}d{center}\\n\\nAs you can see, LASSO zeroed out most of the coefficients, resulting in a simpler and more robust model, which fits the data almost as well as the unconstrained linear regression.\\n\\nBut why does LASSO actively drive coefficients to zero? It has to do with the shape of the circle of the $L_{1}$ metric. As we will see in Figure 10.2 the shape of the $L_{1}$ circle (the collection of points equidistant from the origin) is not round, but has vertices and lower-dimensional features like edges and faces. Constraining our coefficients $w$ to lie on the surface of a radius- $t L_{1}$ circle means it is likely to hit one of these lower-dimensional features, meaning the unused dimensions get zero coefficients.\\n\\nWhich works better, LASSO or ridge regression? The answer is that it depends. Both methods should be supported in your favorite optimization library, so try each of them and see what happens.\\n\\n\\\\subsection*{9.5.3 Trade-Offs between Fit and Complexity}\\nHow do we set the right value for our regularization parameter, be it $\\\\lambda$ or $t$ ? Using a small-enough $\\\\lambda$ or a large-enough $t$ provides little penalty against selecting the coefficients to minimize training error. By contrast, using a very large $\\\\lambda$ or very small $t$ ensures small coefficients, even at the cost of substantial\\\\\\\\\\nmodeling error. Tuning these parameters enables us to seek the sweet spot between over and under-fitting.\\n\\nBy optimizing these models over a large range of values for the appropriate regularization parameter $t$, we get a graph of the evaluation error as a function of $t$. A good fit to the training data with few/small parameters is more robust than a slightly better fit with many parameters.\\n\\nManaging this trade-off is largely a question of taste. However, several metrics have been developed to help with model selection. Most prominent are the Akaike Information Criteria (AIC) and the Baysian information criteria\\\\index{Baysian information criteria} (BIC). We will not delve deeper than their names, so it is fair for you to think of these metrics as voodoo at this point. But your optimization/evaluation system may well output them for the fitted models they produce, providing a way to compare models with different numbers of parameters.\\n\\nEven though LASSO/ridge regression punishes coefficients based on magnitude, they do not explicitly set them to zero if you want exactly $k$ parameters. You must be the one to remove useless variables from your model. Automatic feature-selection methods might decide to zero-out small coefficients, but explicitly constructing models from all possible subsets of features is generally computationally infeasible.\\n\\nThe features to be removed first should be those with (a) small coefficients, (b) low correlation with the objective function, (c) high correlation with another feature in the model, and (d) no obvious justifiable relationship with the target. For example, a famous study once showed a strong correlation between the U.S. gross national product and the annual volume of butter production in Bangladesh. The sage modeler can reject this variable as ridiculous, in ways that automated methods cannot.\\n\\n\\\\subsection*{9.6 Classification and logistic\\\\index{logistic} Regression}\\nWe are often faced with the challenge of assigning items the right label according to a predefined set of classes:\\n\\n\\\\begin{itemize}\\n  \\\\item Is the vehicle in the image a car or a truck? Is a given tissue sample indicative of cancer, or is it benign?\\n  \\\\item Is a particular piece of email spam, or personalized content of interest to the user?\\n  \\\\item Social media analysis seeks to identify properties of people from associated data. Is a given person male or female? Will they tend to vote Democrat or Republican?\\n\\\\end{itemize}\\n\\nClassification is the problem of predicting the right label for a given input record. The task differs from regression in that labels are discrete entities, not continuous function values. Trying to pick the right answer from two possibilities might seem easier than forecasting open-ended quantities, but it is also a lot easier to get dinged for being wrong.\\\\\\n%---- Page End Break Here ---- Page : 289\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-305}\\n\\nFigure 9.12: The optimal regression line cuts through the classes, even though a perfect separator line $(x=0)$ exists.\\n\\nIn this section, approaches to building classification systems using linear regression will be developed, but this is just the beginning. Classification is a bread-and-butter problem in data science, and we will see several other approaches over the next two chapters.\\n\\n\\\\subsection*{9.6.1 Regression for Classification}\\nWe can apply linear regression to classification problems by converting the class names of training examples to numbers. For now, let\\'s restrict our attention to two class problems, or binary classification. We will generalize this to multi-class problems in Section 9.7.2.\\n\\nNumbering these classes as $0 / 1$ works fine for binary classifiers. By convention, the \"positive\" class gets 0 and the \"negative\" one 1 :\\n\\n\\\\begin{itemize}\\n  \\\\item male $=0$ / female $=1$\\n  \\\\item democrat=0 / republican=1\\n  \\\\item spam $=1 /$ non-spam $=0$\\n  \\\\item cancer $=1$ / benign=0\\n\\\\end{itemize}\\n\\nThe negative/ 1 class generally denotes the rarer or more special case. There is no value judgment intended here by positive/negative: indeed, when the classes are of equal size the choice is made arbitrarily.\\n\\nWe might consider training a regression line $f(x)$ for our feature vector $x$ where the target values are these $0 / 1$ labels, as shown in Figure 9.12 There is some logic here. Instances similar to positive training examples should get lower scores than those closer to negative instances. We can threshold the value returned by $f(x)$ to interpret it as a label: $f(x) \\\\leq 0.5$ means that $x$ is positive. When $f(x)>0.5$ we instead assign the negative label.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-306}\\n\\nFigure 9.13: A separating line partitions two classes in feature space (left). However, non-linear separators are better at fitting certain training sets (right).\\n\\nBut there are problems with this formulation. Suppose we add a number of \"very negative\" examples to the training data. The regression line will tilt towards these examples, putting the correct classification of more marginal examples at risk. This is unfortunate, because we would have already properly classified these very negative points, anyway. We really want the line to cut between the classes and serve as a border, instead of through these classes as a scorer.\\n\\n\\\\subsection*{9.6.2 decision boundaries\\\\index{decision boundaries}}\\nThe right way to think about classification is as carving feature space into regions, so that all the points within any given region are destined to be assigned the same label. Regions are defined by their boundaries, so we want regression to find separating lines instead of a fit.\\n\\nFigure 9.13 (left) shows how training examples for binary classification can be viewed as colored points in feature space. Our hopes for accurate classification rest on regional coherence among the points. This means that nearby points tend to have similar labels, and that boundaries between regions tend to be sharp instead of fuzzy.\\n\\nIdeally, our two classes will be well-separated in feature space, so a line can easily partition them. But more generally, there will be outliers. We need to judge our classifier by the \"purity\" of the resulting separation, penalizing the misclassification of points which lie on the wrong side of the line.\\n\\nAny set of points can be perfectly partitioned, if we design a complicatedenough boundary that swerves in and out to capture all instances with a given label. See Figure 9.14 Such complicated separators usually reflect overfitting the training set. Linear separators offer the virtue of simplicity and robustness and, as we will see, can be effectively constructed using logistic regression.\\n\\nMore generally, we may be interested in non-linear but low-complexity decision boundaries, if they better separate the class boundaries. The ideal sep-\\\\\\n%---- Page End Break Here ---- Page : 291\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-307(1)}\\n\\nFigure 9.14: Linear classifiers cannot always separate two classes (left). However, perfect separation achieved using complex boundaries usually reflects overfitting more than insight (right).\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-307}\\n\\nFigure 9.15: The logit function maps a score to a probability.\\\\\\\\\\narating curve in Figure 9.13 (right) is not a line, but a circle. However, it can be found as a linear function of quadratic features like $x_{1}^{2}$ and $x_{1} x_{2}$. We can use logistic regression to find non-linear boundaries if the data matrix is seeded with non-linear features, as discussed in Section 9.2.2\\n\\n\\\\subsection*{9.6.3 Logistic Regression}\\nRecall the logit function $f(x)$, which we introduced back in Section 4.4.1.\\n\\n$$\\nf(x)=\\\\frac{1}{1+e^{-c x}}\\n$$\\n\\nThis function takes as input a real value $-\\\\infty \\\\leq x \\\\leq \\\\infty$, and produces a value ranging over $[0,1]$, i.e. a probability. Figure 9.15 plots the logit function $f(x)$, which is a sigmoidal curve: flat at both sides but a steep rise in the middle.\\n\\n%---- Page End Break Here ---- Page : 292\\n\\nThe shape of the logit function makes it particularly suited to the interpretation of classification boundaries. In particular, let $x$ be a score that reflects the distance that a particular point $p$ lies above/below or left/right of a line $l$ separating two classes. We want $f(x)$ to measure the probability that $p$ deserves a negative label.\\n\\nThe logit function maps scores into probabilities using only one parameter. The important cases are those at the midpoint and endpoints. Logit says that $f(0)=1 / 2$, meaning that the label of a point on the boundary is essentially a coin toss between the two possibilities. This is as it should be. More unambiguous decisions can be made the greater our distance from this boundary, so $f(\\\\infty)=1$ and $f(-\\\\infty)=0$.\\n\\nOur confidence as a function of distance is modulated by the scaling constant\\\\index{scaling constant} c. A value of $c$ near zero makes for a very gradual transition from positive to negative. In contrast, we can turn the logit into a staircase by assigning a large enough value to $c$, meaning that small distances from the boundary translate into large increases in confidence of classification.\\n\\nWe need three things to use the logit function effectively for classification:\\n\\n\\\\begin{itemize}\\n  \\\\item Extending $f(x)$ beyond a single variable, to a full ( $m-1$ )-dimensional input vector $x$.\\n  \\\\item The threshold value $t$ setting the midpoint of our score distribution (here zero).\\n  \\\\item The value of the scaling constant $c$ regulating the steepness of the transition.\\n\\\\end{itemize}\\n\\nWe can achieve all three by fitting a linear function $h(x, w)$ to the data, where\\n\\n$$\\nh(x, w)=w_{0}+\\\\sum_{i=1}^{m-1} w_{i} \\\\cdot x_{i}\\n$$\\n\\nwhich can then be plugged into the logistic function to yield the classifier:\\n\\n$$\\nf(x)=\\\\frac{1}{1+e^{-h(x, w)}}\\n$$\\n\\nNote that the coefficients of $h(x, w)$ are rich enough to encode the threshold $(t=$ $w_{0}$ ) and steepness ( $c$ is essentially the average of $w_{1}$ through $w_{n-1}$ ) parameters.\\n\\nThe only remaining question is how to fit the coefficient vector $w$ to the training data. Recall that we are given a zero/one class label $y_{i}$ for each input vector $x_{i}$, where $1 \\\\leq i \\\\leq n$. We need a penalty function\\\\index{penalty function} that ascribes appropriate costs to returning $f\\\\left(x_{i}\\\\right)$ as the probability that the class $y_{i}$ is positive, i.e. $y_{i}=1$.\\n\\nLet us first consider the case where $y_{i}$ really is 1 . Ideally $f\\\\left(x_{i}\\\\right)=1$ in this case, so we want to penalize it for being smaller than 1 . Indeed, we want to punish it aggressively when $f\\\\left(y_{i}\\\\right) \\\\rightarrow 0$, because that means that the classifier is stating that element $i$ has little chance of being in class- 1 , when that actually is the case.\\\\\\n%---- Page End Break Here ---- Page : 293\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-309}\\n\\nFigure 9.16: Cost penalties for positive (blue) and negative (right) elements. The penalty is zero if the correct label is assigned with probability 1 , but increasing as a function of misplaced confidence.\\n\\nThe logarithmic function $\\\\operatorname{cost}\\\\left(x_{i}, 1\\\\right)=-\\\\log \\\\left(f\\\\left(x_{i}\\\\right)\\\\right)$ turns out to be a good penalty function when $y_{i}=1$. Recall the definition of the logarithm (or inverse exponential function) from Section 2.4 namely that\\n\\n$$\\ny=\\\\log _{b} x \\\\rightarrow b^{y}=x .\\n$$\\n\\nAs shown in Figure 9.16 $\\\\log (1)=0$ for any reasonable base, so zero penalty is charged when $f\\\\left(x_{i}\\\\right)=1$, which is as it should be for correctly identifying $y_{i}=1$. Since $b^{\\\\log _{b} x}=x, \\\\log (x) \\\\rightarrow-\\\\infty$ as $x \\\\rightarrow 0$. This makes $\\\\operatorname{cost}\\\\left(x_{i}, 1\\\\right)=-\\\\log \\\\left(f\\\\left(x_{i}\\\\right)\\\\right)$ an increasingly severe penalty the more we misclassify $y_{i}$.\\n\\nNow consider the case where $y_{i}=0$. We want to punish the classifier for high values of $f\\\\left(x_{i}\\\\right)$, i.e. more as $f\\\\left(x_{i}\\\\right) \\\\rightarrow 1$. A little reflection should convince you that the right penalty is now $\\\\operatorname{cost}\\\\left(x_{i}, 0\\\\right)=-\\\\log \\\\left(1-f\\\\left(x_{i}\\\\right)\\\\right)$.\\n\\nTo tie these together, note what happens when we multiply $\\\\operatorname{cost}\\\\left(x_{i}, 1\\\\right)$ times $y_{i}$. There are only two possible values, namely $y_{i}=0$ or $y_{i}=1$. This has the desired effect, because the penalty is zeroed out in the case where it does not apply. Similarly, multiplying by $\\\\left(1-y_{i}\\\\right)$ has the opposite effect: zeroing out the penalty when $y_{i}=1$, and applying it when $y_{i}=0$. Multiplying the costs by the appropriate indicator variables enables us to define the loss function\\\\index{loss function} for logistic regression as an algebraic formula:\\n\\n$$\\n\\\\begin{aligned}\\nJ(w) & =\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\operatorname{cost}\\\\left(f\\\\left(x_{i}, w\\\\right), y_{i}\\\\right) \\\\\\\\\\n& =-\\\\frac{1}{n}\\\\left[\\\\sum_{i=1}^{n} y_{i} \\\\log f\\\\left(x_{i}, w\\\\right)+\\\\left(1-y_{i}\\\\right) \\\\log \\\\left(1-f\\\\left(x_{i}, w\\\\right)\\\\right)\\\\right]\\n\\\\end{aligned}\\n$$\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-310}\\n\\\\end{center}\\n\\nFigure 9.17: The logistic regression classifier best separating men and women in weight-height space. The red region contains 229 women and only 63 men, while the blue region contains 223 men to 65 women.\\n\\nThe wonderful thing about this loss function is that it is convex, meaning that we can find the parameters $w$ which best fit the training examples using gradient descent. Thus we can use logistic regression to find the best linear separator between two classes, providing a natural approach to binary classification.\\n\\n\\\\subsection*{9.7 issues\\\\index{issues} in Logistic Classification}\\nThere are several nuances to building effective classifiers, issues which are relevant both to logistic regression and the other machine learning methods that we will explore over the next two chapters. These include managing unbalanced class sizes, multi-class classification, and constructing true probability distributions from independent classifiers.\\n\\n\\\\subsection*{9.7.1 balanced training classes\\\\index{balanced training classes}}\\nConsider the following classification problem, which is of great interest to law enforcement agencies in any country. Given the data you have on a particular person $p$, decide whether $p$ is a terrorist or is no particular threat.\\n\\nThe quality of the data available to you will ultimately determine the accuracy of your classifier, but regardless, there is something about this problem that makes it very hard. It is the fact that there are not enough terrorists available in the general population.\\n\\n%---- Page End Break Here ---- Page : 295\\n\\nIn the United States, we have been blessed with general peace and security. It would not surprise me if there were only 300 or so genuine terrorists in the entire country. In a country of 300 million people, this means that only one out of every million people are active terrorists.\\n\\nThere are two major consequences of this imbalance. The first is that any meaningful classifier is doomed to have a lot of false positives. Even if our classifier proved correct an unheard of $99.999 \\\\%$ of the time, it would classify 3,000 innocent people as terrorists, ten times the number of bad guys we will catch. Similar issues were discussed in Section 7.4.1 concerning precision and recall.\\n\\nBut the second consequence of this imbalance is that there cannot be many examples of actual terrorists to train on. We might have tens of thousands of innocent people to serve as positive/class-0 examples, but only a few dozen known terrorists to be negative/class-1 training instances.\\n\\nConsider what the logistic classifier is going to do in such an instance. Even misclassifying all of the terrorists as clean cannot contribute too much to the loss function, compared with the cost of how we treat the bigger class. It is more likely to draw a separating line to clear everybody than go hunting for terrorists. The moral here is that it is generally best to use equal numbers of positive and negative examples.\\n\\nBut one class may be hard to find examples for. So what are our options to produce a better classifier?\\n\\n\\\\begin{itemize}\\n  \\\\item Force balanced classes by discarding members of the bigger class: This is the simplest way to realize balanced training classes. It is perfectly justified if you have enough rare-class elements to build a respectable classifier. By discarding the excess instances we don\\'t need, we create a harder problem that does not favor the majority class.\\n  \\\\item Replicate elements of the smaller class, ideally with perturbation: A simple way to get more training examples is to clone the terrorists, inserting perfect replicas of them into the training set under different names. These repeated examples do look like terrorists, after all, and adding enough of them will make the classes balanced.\\\\\\\\\\nThis formulation is brittle, however. These identical data records might create numerical instabilities, and certainly have a tendency towards overfitting, since moving one extra real terrorist to the right side of the boundary moves all her clones as well. It might be better to add a certain amount of random noise to each cloned example, consistent with variance in the general population. This makes the classifier work harder to find them, and thus minimizes overfitting.\\n  \\\\item Weigh the rare training examples more heavily than instances of the bigger class: The loss function for parameter optimization contains a separate term for the error of each training instance. Adding a coefficient to ascribe more weight to the most important instances leaves a convex optimization problem, so it can still be optimized by stochastic gradient descent.\\\\\\n%---- Page End Break Here ---- Page : 296\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-312}\\n\\\\end{itemize}\\n\\nFigure 9.18: multi-class\\\\index{multi-class} classification problems are a generalization of binary classification.\\n\\nThe problem with all three of these solutions is that we bias the classifier, by changing the underlying probability distribution. It is important for a classifier to know that terrorists are extremely rare in the general population, perhaps by specifying a Baysian prior distribution.\\n\\nOf course the best solution would be to round up more training examples from the rarer class, but that isn\\'t always possible. These three techniques are about the best we can muster as an alternative.\\n\\n\\\\subsection*{9.7.2 Multi-Class Classification}\\nOften classification tasks involve picking from more than two distinct labels. Consider the problem of identifying the genre of a given movie. Logical possibilities include drama, comedy, animation, action, documentary, and musical.\\n\\nA natural but misguided approach to represent $k$-distinct classes would add class numbers beyond $0 / 1$. In a hair-color classification problem, perhaps we could assign blond $=0$, brown $=1$, red $=2$, black $=4$, and so on until we exhaust human variation. Then we could perform a linear regression to predict class number.\\n\\nBut this is generally a bad idea. ordinal\\\\index{ordinal} scales are defined by either increasing or decreasing values. Unless the ordering of your classes reflects an ordinal scale, the class numbering will be a meaningless target to regress against.\\n\\nConsider the hair-color numbering above. Should red hair lie between brown and black (as currently defined), or between blond and brown? Is grey hair a lighter shade of blond, say class -1 , or is it an incomparable condition due principally to aging? Presumably the features that contribute to grey hair (age and the number of teen-age children) are completely orthogonal to those of blond hair (hair salon exposure and Northern European ancestry). If so, there is no way a linear regression system fitting hair color as a continuous variable would be destined to separate these colors out from darker hair.\\n\\nCertain sets of classes are properly defined by ordinal scales. For example, consider classes formed when people grade themselves on survey questions like \"Skiena\\'s class is too much work\" or \"How many stars do you give this movie?\"\\\\\\n%---- Page End Break Here ---- Page : 297\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-313}\\n\\nFigure 9.19: Voting among multiple one-vs.-rest classifiers is generally the best way to do multi-class classification.\\n\\nCompletely Agree $\\\\leftrightarrow$ Mostly Agree $\\\\leftrightarrow$ Neutral $\\\\leftrightarrow$ Mostly Disagree $\\\\leftrightarrow$ Completely Disagree\\n\\n$$\\n\\\\text { Four stars } \\\\leftrightarrow \\\\text { Three stars } \\\\leftrightarrow \\\\text { Two stars } \\\\leftrightarrow \\\\text { One stars } \\\\leftrightarrow \\\\text { Zero stars }\\n$$\\n\\nClasses defined by such Likert\\\\index{Likert} scales are ordinal, and hence such class numbers are a perfectly reasonable thing to regress against. In particular, mistakenly assigning an element to an adjacent class is much less of a problem than assigning it to the wrong end of the scale.\\n\\nBut generally speaking, class labels are not ordinal. A better idea for multiclass discrimination involves building many one-vs.-all\\\\index{one-vs.-all} classifiers, as shown in Figure 9.19 For each of the possible classes $C_{i}$, where $1 \\\\leq i \\\\leq c$, we train a logistic classifier to distinguish elements of $C_{i}$ against the union of elements from all other classes combined. To identify the label associated with a new element $x$, we test it against all $c$ of these classifiers, and return the label $i$ which has the highest associated probability.\\n\\nThis approach should seem straightforward and reasonable, but note that the classification problem gets harder the more classes you have. Consider the monkey. By flipping a coin, a monkey should be able to correctly label $50 \\\\%$ of the examples in any binary classification problem. But now assume there are a hundred classes. The monkey will only guess correctly $1 \\\\%$ of the time. The task is now very hard, and even an excellent classifier will have a difficult time producing good results on it.\\n\\n\\\\subsection*{9.7.3 Hierarchical Classification}\\nWhen your problem contains a large number of classes, it pays to group them into a tree\\\\index{tree} or hierarchy\\\\index{hierarchy} so as to improve both accuracy and efficiency. Suppose\\\\\\n%---- Page End Break Here ---- Page : 298\\n\\\\\\nwe built a binary tree, where each individual category is represented by a leaf node. Each internal node represents a classifier for distinguishing between the left descendants and the right descendants.\\n\\nTo use this hierarchy to classify a new item $x$, we start at the root. Running the root classifier on $x$ will specify it as belonging to either the left or right subtree. Moving down one level, we compare $x$ with the new node\\'s classifier and keep recurring until we hit a leaf, which defines the label assigned to $x$. The time it takes is proportional to the height of the tree, ideally logarithmic in the number of classes $c$, instead of being linear in $c$ if we explicitly compare against every class. Classifiers based on this approach are called decision trees, and will be discussed further in Section 11.2\\n\\nIdeally this hierarchy can be built from domain knowledge, ensuring that categories representing similar classes are grouped together. This has two benefits. First, it makes it more likely that misclassifications will still produce labels from similar classes. Second, it means that intermediate nodes can define higher-order concepts, which can be more accurately recognized. Suppose that the one hundred categories in a image classification problem included \"car,\" \"truck,\" \"boat,\" and \"bicycle\". When all of these categories are descendants of an intermediate node called \"vehicle,\" we can interpret the path to this node as a lower-resolution, higher-accuracy classifier.\\n\\nThere is another, independent danger with classification that becomes more acute as the number of classes grows. Members of certain classes (think \"college students\") are much more plentiful than others, like \"rock stars.\" The relative disparity between the size of the largest and\\\\i\\\\index{decision tree classiï¬ers}ndex{Bayesâ theorem} smallest classes typically grows along with number of classes.\\n\\nFor this example, let\\'s agree that \"rock stars\" tend to be sullen, grungylooking males, providing useful features for any classifier. However, only a small fraction of sullen, grungy-looking males are rock stars, because there are extremely few people who have succeeded in this demanding profession. Classification systems which do not have a proper sense of the prior distribution on labels are doomed having many false positives, by assigning rare labels much too frequently.\\n\\nThis is the heart of Baysian analysis: updating our current (prior) understanding of the probability distribution in the face of new evidence. Here, the evidence is the result is from a classifier. If we incorporate a sound prior distribution into our reasoning, we can ensure that items require particularly strong evidence to be assigned to rare classes.\\n\\n\\\\subsection*{9.7.4 partition function\\\\index{partition function}s and multinomial regression\\\\index{multinomial regression}}\\nRecall that our preferred means of multi-class classification involved training independent single-class vs. all logistic classifiers $F_{i}(x)$, where $1 \\\\leq i \\\\leq c$ and $c$ is the number of distinct labels. One minor issue remains. The probabilities we get from logistic regression aren\\'t really probabilities. Turning them into real probabilities requires the idea of a partition function.\\n\\n%---- Page End Break Here ---- Page : 299\\n\\nFor any particular item $x$, summing up the \"probabilities\" over all possible labels for $x$ should yield $T=1$, where\\n\\n$$\\nT=\\\\sum_{i=1}^{c} F_{i}(x)\\n$$\\n\\nBut should doesn\\'t mean is. All of these classifiers were trained independently, and hence there is nothing forcing them to sum to $T=1$.\\n\\nA solution is to divide all of these probabilities by the appropriate constant, namely $F^{\\\\prime}(x)=F(x) / T$. This may sound like a kludge, because it is. But this is essentially what physicists do when they talk about partition functions, which serve as denominators turning something proportional to probabilities into real probabilities.\\n\\nMultinomial regression is a more principled method of training independent single-class vs. all classifiers, so that the probabilities work out right. This involves using the correct partition function for log odds ratios, which are computed with exponentials of the resulting values. More than this I will not say, but it is reasonable to look for a multinomial regression function in your favorite machine learning library and see how it does when faced with a multi-class regression problem.\\n\\nA related notion to the partition function arises in Baysian analysis. We are often faced with a challenge of identifying the most likely item label, say $A$, as a function of evidence $E$. Recall that Bayes\\' theorem states that\\n\\n$$\\nP(A \\\\mid E)=\\\\frac{P(E \\\\mid A) P(A)}{P(E)}\\n$$\\n\\nComputing this as a real probability requires knowing the denominator $P(E)$, which can be a murky thing to compute. But comparing $P(A \\\\mid E)$ to $P(B \\\\mid E)$ in order to determine whether label $A$ is more likely than label $B$ does not require knowing $P(E)$, since it is the same in both expressions. Like a physicist, we can waive it away, mumbling about the \"partition function.\"\\n\\n\\\\subsection*{9.8 Chapter Notes}\\nLinear and logistic regression are standard topics in statistics and optimization. Textbooks on linear/logistic regression and its applications include (JWHT13, Wei05.\\n\\nThe treatment of the gradient descent approach to solving regression here was inspired by Andrew Ng , as presented in his Coursera machine learning course. I strongly recommend his video lectures to those interested in a more thorough treatment of the subject.\\n\\nThe discovery that butter production in Bangladesh accurately forecasted the S\\\\&P 500 stock index is due to Leinweber Lei07. Unfortunately, like most spurious correlations it broke down immediately after its discovery, and no longer has predictive power.\\n\\n%---- Page End Break Here ---- Page : 300\\n\\n\\\\subsection*{9.9 exercises\\\\index{exercises}}\\n\\\\section*{Linear Regression}\\n$9-1$. [3] Construct an example on $n \\\\geq 6$ points where the optimal regression line is $y=x$, even though none of the input points lie directly on this line.\\\\\\\\[0pt]\\n9-2. [3] Suppose we fit a regression line to predict the shelf life of an apple based on its weight. For a particular apple, we predict the shelf life to be 4.6 days. The apples residual is -0.6 days. Did we over or under estimate the shelf-life of the apple? Explain your reasoning.\\\\\\\\[0pt]\\n9-3. [3] Suppose we want to find the best-fitting function $y=f(x)$ where $y=w^{2} x+$ $w x$. How can we use linear regression to find the best value of $w$ ?\\\\\\\\[0pt]\\n9-4. [3] Suppose we have the opportunity to pick between using the best fitting model of the form $y=f(x)$ where $y=w^{2} x$ or $y=w x$, for constant coefficient $w$. Which of these is more general, or are they identical?\\\\\\\\[0pt]\\n9-5. [5] Explain what a long-tailed distribution is, and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?\\n\\n9-6. [5] Using a linear algebra library/package, implement the closed form regression solver $w=\\\\left(A^{T} A\\\\right)^{-1} A^{T} b$. How well does it perform, relative to an existing solver?\\\\\\\\[0pt]\\n9-7. [3] Establish the effect that different values for the constant $c$ of the logit function have on the probability of classification being $0.01,1,2$, and 10 units from the boundary.\\n\\n\\\\section*{Experiments with Linear Regression}\\n9-8. [5] Experiment with the effects of fitting non-linear functions with linear regression. For a given $(x, y)$ data set, construct the best fitting line where the set of variables are $\\\\left\\\\{1, x, \\\\ldots, x^{k}\\\\right\\\\}$, for a range of different $k$. Does the model get better or worse over the course of this process, both in terms of fitting error and general robustness?\\\\\\\\[0pt]\\n9-9. [5] Experiment with the effects of feature scaling in linear regression. For a given data set with at least two features (dimensions), multiply all the values of one feature by $10^{k}$, for $-10 \\\\leq k \\\\leq 10$. Does this operation cause a loss of numerical accuracy in fitting?\\\\\\\\[0pt]\\n9-10. [5] Experiment with the effects of highly correlated features in linear regression. For a given $(x, y)$ data set, replicate the value of $x$ with small but increasing amounts of random noise. What is returned when the new column is perfectly correlated with the original? What happens with increasing amounts of random noise?\\\\\\\\[0pt]\\n9-11. [5] Experiment with the effects of outliers on linear regression. For a given $(x, y)$ data set, construct the best fitting line. Repeatedly delete the point with the largest residual, and refit. Is the sequence of predicted slopes relatively stable for much of this process?\\\\\\\\[0pt]\\n9-12. [5] Experiment with the effects of regularization on linear/logistic regression. For a given multi-dimensional data set, construct the best fitting line with (a)\\\\\\n%---- Page End Break Here ---- Page : 301\\n\\\\\\nno regularization, (b) ridge regression, and (c) LASSO regression; the latter two with a range of constraint values. How does the accuracy of the model change as we reduce the size and number of parameters?\\n\\n\\\\section*{Implementation Projects}\\n9-13. [5] Use linear/logistic regression to build a model for one of the following The Quant Shop challenges:\\\\\\\\\\n(a) Miss Universe.\\\\\\\\\\n(b) Movie gross.\\\\\\\\\\n(c) Baby weight.\\\\\\\\\\n(d) Art auction price.\\\\\\\\\\n(e) White Christmas.\\\\\\\\\\n(f) Football champions.\\\\\\\\\\n(g) Ghoul pool.\\\\\\\\\\n(h) Gold/oil prices.\\n\\n9-14. [5] This story about predicting the results of the NCAA college basketball tournament is instructive:\\\\\\\\\\n\\\\href{http://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy}{http://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy}. html.\\\\\\\\\\nImplement such a logistic regression classifier, and extend it to other sports like football.\\n\\n\\\\section*{Interview Questions}\\n9-15. [8] Suppose we are training a model using stochastic gradient descent. How do we know if we are converging to a solution?\\\\\\\\[0pt]\\n9-16. [5] Do gradient descent methods always converge to the same point?\\\\\\\\[0pt]\\n9-17. [5] What assumptions are required for linear regression? What if some of these assumptions are violated?\\\\\\\\[0pt]\\n9-18. [5] How do we train a logistic regression model? How do we interpret its coefficients?\\n\\n\\\\section*{Kaggle Challenges}\\n9-19. Identify what is being cooked, given the list of ingredients.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/whats-cooking}{https://www.kaggle.com/c/whats-cooking}\\\\\\\\\\n9-20. Which customers are satisfied with their bank?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/santander-customer-satisfaction}{https://www.kaggle.com/c/santander-customer-satisfaction}\\\\\\\\\\n9-21. What does a worker need access to in order to do their job?\\n\\n\\\\section*{Chapter 10}\\n\\\\section*{Distance and network methods\\\\index{network methods}}\\n\\\\begin{abstract}\\nWhen a measure becomes a target, it ceases to be a measure.\\n\\\\end{abstract}\\n\\n\\\\begin{itemize}\\n  \\\\item Charles Goodhart (Goodhart\\'s Law)\\n\\\\end{itemize}\\n\\nAn $n \\\\times d$ data matrix, consisting of $n$ examples/rows each defined by $d$ features/columns, naturally defines a set of $n$ points in a $d$-dimensional geometric space. Interpreting examples as points in space provides a powerful way to think about them - like the stars in the heavens. Which stars are the closest to our sun, i.e. our nearest neighbors? Galaxies are natural groupings of stars identified by clustering the data. Which stars share the Milky Way with our sun?\\n\\nThere is a close connection between collections of points in space and vertices in networks. Often we build networks from geometric point sets, by connecting close pairs of points by edges. Conversely, we can build point sets from networks, by embedding the vertices in space, so that pairs of connected vertices are located n\\\\index{distance methods}ear each\\\\index{Good\\\\index{Goodhart, Charles}hartâs law} other in the embedding.\\n\\nSeveral of the important problems on geometric data readily generalize to network data, including nearest neighbor classification and clustering. Thus we treat both topics together in this chapter, to better exploit the synergies between them.\\n\\n\\\\subsection*{10.1 measuring\\\\index{measuring} Distances}\\nThe most basic issue in the geometry of points $p$ and $q$ in $d$ dimensions is how best to measure the distance between them. It might not be obvious that there is any issue here to speak of, since the traditional Euclidean metric\\\\index{Euclidean metric} is obviously\\\\\\\\\\nhow you measure distances. The Euclidean matrix defines\\n\\n$$\\nd(p, q)=\\\\sqrt{\\\\sum_{i=1}^{d}\\\\left|p_{i}-q_{i}\\\\right|^{2}}\\n$$\\n\\nBut there are other reasonable notions of distance to consider. Indeed, what is a distance metric? How does it differ from an arbitrary scoring function?\\n\\n\\\\subsection*{10.1.1 Distance Metrics}\\nDistance measures most obviously differ from similarity scores, like the correlation coefficient, in their direction of growth. Distance measures get smaller as items become more similar, while the converse is true of similarity functions.\\n\\nThere are certain useful mathematical properties we assume of any reasonable distance measure. We say a distance measure is a metric if it satisfies the following properties:\\n\\n\\\\begin{itemize}\\n  \\\\item Positivity: $\\\\quad d(x, y) \\\\geq 0$ for all $x$ and $y$.\\n  \\\\item Identity: $d(x, y)=0$ if and only if $x=y$.\\n  \\\\item Symmetry: $d(x, y)=d(y, x)$ for all $x$ and $y$.\\n  \\\\item Triangle inequality: $d(x, y) \\\\leq d(x, z)+d(z, y)$ for all $x, y$, and $z$.\\n\\\\end{itemize}\\n\\nThese properties are important for reasoning about data. Indeed, many algorithms work correctly only when the distance function is a metric.\\n\\nThe Euclidean distance is a metric, which is why these conditions seem so natural to us. However, other equally-natural similarity measures are not distance metrics:\\n\\n\\\\begin{itemize}\\n  \\\\item Correlation coefficient: Fails positivity because it ranges from -1 to 1 . Also fails identity, as the correlation of a sequence with itself is 1 .\\n  \\\\item Cosine similarity/dot product: Similar to correlation coefficient, it fails positivity and identity for the same reason.\\n  \\\\item Travel times in a directed network: In a world with one-way streets, the distance from $x$ to $y$ is not necessarily the same as the distance from $y$ to $x$.\\n  \\\\item Cheapest airfare: This often violates the triangle inequality, because the cheapest way to fly from $x$ to $y$ might well involve taking a detour through $z$, due to bizarre airline pricing strategies.\\n\\\\end{itemize}\\n\\nBy contrast, it is not immediately obvious that certain well-known distance functions are metrics, such as edit distance used in string matching. Instead of making assumptions, prove or disprove each of the four basic properties, to be sure you understand what you are working with.\\\\\\n%---- Page End Break Here ---- Page : 304\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-320}\\n\\nFigure 10.1: Many different paths across a grid have equal Manhattan $\\\\left(L_{1}\\\\right)$ distance.\\n\\n\\\\subsection*{10.1.2 The $L_{k}$ Distance Metric}\\nThe Euclidean distance is just a special case of a more general family of distance functions, known as the $L_{k}$ distance metric or norm:\\n\\n$$\\nd_{k}(p, q)=\\\\sqrt[k]{\\\\sum_{i=1}^{d}\\\\left|p_{i}-q_{i}\\\\right|^{k}}=\\\\left(\\\\sum_{i=1}^{d}\\\\left|p_{i}-q_{i}\\\\right|^{k}\\\\right)^{1 / k}\\n$$\\n\\nThe parameter $k$ provides a way to trade off between the largest and the total dimensional differences. The value for $k$ can be any number between 1 and $\\\\infty$, with particularly popular values including:\\n\\n\\\\begin{itemize}\\n  \\\\item Manhattan distance ( $k=1$ ): If we ignore exceptions like Broadway, all streets in Manhattan run east-west and all avenues north-south, thus defining a regular grid. The distance between two locations is then the sum of this north-south difference and the east-west difference, since tall buildings prevent any chance of shortcuts.\\\\\\\\\\nSimilarly, the $L_{1}$ or Manhattan distance is the total sum of the deviations between the dimensions. Everything is linear, so a difference of 1 in each of two dimensions is the same as a difference of 2 in only one dimension. Because we cannot take advantage of diagonal short-cuts, there are typically many possible shortest paths between two points, as shown in Figure 10.1\\n  \\\\item Euclidean distance $(k=2)$ : This is the most popular distance metric, offering more weight to the largest dimensional deviation without overwhelming the lesser dimensions.\\n  \\\\item Maximum component $(k=\\\\infty)$ : As the value of $k$ increases, smaller dimensional differences fade into irrelevance. If $a>b$, then $a^{k} \\\\gg b^{k}$. Taking the $k$ th root of $a^{k}+b^{k}$ approaches $a$ as $b^{k} / a^{k} \\\\rightarrow 0$.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-321(1)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-321}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-321(3)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-321(2)}\\n\\\\end{itemize}\\n\\nFigure 10.2: The shape of circles defining equal distances changes with $k$.\\n\\nConsider the distance of points $p_{1}=(2,0)$ and $p_{2}=(2,1.99)$ from the origin:\\n\\n\\\\begin{itemize}\\n  \\\\item For $k=1$, the distances are 2 and 3.99, respectively.\\n  \\\\item For $k=2$, they are 2 and 2.82136 .\\n  \\\\item For $k=1000$, they are 2 and 2.00001 .\\n  \\\\item For $k=\\\\infty$, they are 2 and 2 .\\n\\\\end{itemize}\\n\\nThe $L_{\\\\infty}$ metric returns the largest single dimensional difference as the distance.\\n\\nWe are comfortable with Euclidean distance because we live in a Euclidean world. We believe in the truth of the Pythagorean theorem\\\\index{Pythagorean theorem}, that the sides of a right triangle obey the relationship that $a^{2}+b^{2}=c^{2}$. In the world of $L_{k}$ distances, the Pythagorean theorem would be $a^{k}+b^{k}=c^{k}$.\\n\\nWe are similarly comfortable with the notion that circles are round. Recall that a circle is defined as the collection of points which are at a distance $r$ from an origin point $p$. Change the definition of distance, and you change the shape of a circle.\\n\\nThe shape of an $L_{k}$ \"circle\" governs which points are equal neighbors about a center point $p$. Figure 10.2 illustrates how the shape evolves with $k$. Under Manhattan distance $(k=1)$, the circle looks like a diamond. For $k=2$, it is the round object we are familiar with. For $k=\\\\infty$, this circle stretches out to an axis-oriented box.\\n\\nThere is smooth transition from the diamond to the box as we vary $1 \\\\leq k \\\\leq$ $\\\\infty$. Selecting the value of $k$ is equivalent to choosing which circle best fits our domain model. The distinctions here become particularly important in higher dimensional spaces: do we care about deviations in all dimensions, or primarily the biggest ones?\\n\\nTake-Home Lesson: Selecting the right value of $k$ can have a significant effect on the meaningfulness of your distance function, particularly in high-dimensional spaces.\\n\\n%---- Page End Break Here ---- Page : 306\\n\\nTaking the $k$ th root of the sum of $k$ th-power terms is necessary for the resulting \"distance\" values to satisfy the metric property. However, in many applications we will be only using the distances for comparison: testing whether $d(x, p) \\\\leq d(x, q)$ as opposed to using the values in formulas or isolation.\\n\\nBecause we take the absolute value of each dimensional distance before raising it to the $k$ th power, the summation within the distance function always yields a positive value. The $k$ th root/power function is monotonic, meaning that for $x, y, k \\\\geq 0$\\n\\n$$\\n(x>y) \\\\rightarrow\\\\left(x^{k}>y^{k}\\\\right)\\n$$\\n\\nThus the order of distance comparison is unchanged if we do not take the $k$ th root of the summation. Avoiding the $k$ th root calculation saves time, which can prove non-trivial when many distance computations are performed, as in nearest neighbor search.\\n\\n\\\\subsection*{10.1.3 Working in Higher Dimensions}\\nI personally have no geometric sense about higher-dimensional spaces, anything where $d>3$. Usually, the best we can do is to think about higher-dimensional geometries through linear algebra: the equations which govern our understanding of two/three-dimensional geometries readily generalize for arbitrary $d$, and that is just the way things work.\\n\\nWe can develop some intuition about working with a higher-dimensional data set through projection methods, which reduce the dimensionality to levels we can understand. It is often helpful to visualize the two-dimensional projections of the data by ignoring the other $d-2$ dimensions entirely, and instead study dot plots of dimensional pairs. Through dimension reduction methods like principle component analysis (see Section 8.5.2), we can combine highly correlated features to produce a cleaner representation. Of course, some details are lost in the process: whether it is noise or nuance depends upon your interpretation.\\n\\nIt should be clear that as we increase the number of dimensions in our data set, we are implicitly saying that each dimension is a less important part of the whole. In measuring the distance between two points in feature space, understand that large $d$ means that there are more ways for points to be close (or far) from each other: we can imagine them being almost identical along all dimensions but one.\\n\\nThis makes the choice of distance metric most important in high-dimensional data spaces. Of course, we can always stick with $L_{2}$ distance, which is a safe and standard choice. But if we want to reward points for being close on many dimensions, we prefer a metric leaning more towards $L_{1}$. If instead things are similar when there are no single fields of gross dissimilarity, we perhaps should be interested in something closer to $L_{\\\\infty}$.\\n\\nOne way to think about this is whether we are more concerned about random added noise to our features, or exceptional events leading to large artifacts. $L_{1}$ is undesirable in the former case, because the metric will add up the noise from all dimensions in the distance. But artifacts make $L_{\\\\infty}$ suspect, because a\\\\\\\\\\nsubstantial error in any single column will come to dominate the entire distance calculation.\\n\\nTake-Home Lesson: Use your freedom to select the best distance metric. Evaluate how well different functions work to tease out the similarity of items in your data set.\\n\\n\\\\subsection*{10.1.4 dimensional egalitarianism\\\\index{dimensional egalitarianism}}\\nThe $L_{k}$ distance metrics all implicitly weigh each dimension equally. It doesn\\'t have to be this way. Sometimes we come to a problem with a domain-specific understanding that certain features are more important for similarity than others. We can encode this information using a coefficient $c_{i}$ to specify a different weight to each dimension:\\n\\n$$\\nd_{k}(p, q)=\\\\sqrt[k]{\\\\sum_{i=1}^{d} c_{i}\\\\left|p_{i}-q_{i}\\\\right|^{k}}=\\\\left(\\\\sum_{i=1}^{d} c_{i}\\\\left|p_{i}-q_{i}\\\\right|^{k}\\\\right)^{1 / k}\\n$$\\n\\nWe can view the traditional $L_{k}$ distance as a special case of this more general formula, where $c_{i}=1$ for $1 \\\\leq i \\\\leq d$. This dimension-weighted distance still satisfies the metric properties.\\n\\nIf you have ground-truth data about the desired distance between certain pairs of points, then you can use linear regression to fit the coefficients $c_{i}$ to best match your training set. But, generally speaking, dimension-weighted distance is often not a great idea. Unless you have a genuine reason to know that certain dimensions are more important than others, you are simply encoding your biases into the distance formula.\\n\\nBut much more serious biases creep in if you do not normalize your variables before computing distances. Suppose we have a choice of reporting a distance in either meters or kilometers. The contribution of a 30 meter difference in the distance function will either be $30^{2}=900$ or $0.03=0.0009$, literally a million-fold difference in weight.\\n\\nThe correct approach is to normalize the values of each dimension by Zscores before computing your distance. Replace each value $x_{i}$ by its Z -score $z=\\\\left(x-\\\\mu_{i}\\\\right) / \\\\sigma_{i}$, where $\\\\mu_{i}$ is the mean value of dimension $i$ and $\\\\sigma_{i}$ its standard deviation. Now the expected value of $x_{i}$ is zero for all dimensions, and the spread is tightly controlled if they were normally distributed to start with. More stringent efforts must be taken if a particular dimension is, say, power law distributed. Review Section 4.3 on normalization for relevant techniques, like first hitting it with a logarithm before computing the Z-score\\\\index{Z-score}.\\n\\nTake-Home Lesson: The most common use of dimension-weighted distance metrics is as a kludge to mask the fact that you didn\\'t properly normalize your data. Don\\'t fall into this trap. Replace the original values by Z-scores before computing distances, to ensure that all dimensions contribute equally to the result.\\n\\n%---- Page End Break Here ---- Page : 308\\n\\n\\\\subsection*{10.1.5 Points vs. Vectors}\\nVectors and points are both defined by arrays of numbers, but they are conceptually different beasts for representing items in feature space. Vectors decouple direction from magnitude, and so can be thought of as defining points on the surface of a unit sphere.\\n\\nTo see why this is important, consider the problem of identifying the nearest documents from word-topic counts. Suppose we have partitioned the vocabulary of English into $n$ different subsets based on topics, so each vocabulary word sits in exactly one of the topics. We can represent each article $A$ as a bag of words, as a point $p$ in $n$-dimensional space where $p_{i}$ equals the number of words appearing in article $A$ that come from topic $i$.\\n\\nIf we want a long article on football to be close to a short article on football, the magnitude of this vector cannot matter, only its direction. Without normalization for length, all the tiny tweet-length documents will bunch up near the origin, instead of clustering semantically in topic space as we desire.\\n\\nNorms are measures of vector magnitude, essentially distance functions involving only one point, because the second is taken to be the origin. Vectors are essentially normalized points, where we divide the value of each dimension of $p$ by its $L_{2}$-norm $L_{2}(p)$, which is the distance between $p$ and the origin $O$ :\\n\\n$$\\nL_{2}(p)=\\\\sqrt{\\\\sum_{i=1}^{n} p_{i}^{2}}\\n$$\\n\\nAfter such normalization, the length of each vector will be 1 , turning it into a point on the unit sphere about the origin.\\n\\nWe have several possible distance metrics to use in comparing pairs of vectors. The first class is defined by the $L_{k}$ metrics, including Euclidean distance. This works because points on the surface of a sphere are still points in space. But we can perhaps more meaningfully consider the distance between two vectors in terms of the angle defined between them. We have seen that the cosine similarity between two points $p$ and $q$ is their dot product divided by their $L_{2}$-norms:\\n\\n$$\\n\\\\cos (p, q)=\\\\frac{p \\\\cdot q}{\\\\|p\\\\|\\\\|q\\\\|}\\n$$\\n\\nFor previously normalized vectors, these norms equal 1, so all that matters is the dot product.\\n\\nThe cosine function here is a similarity function, not a distance measure, because larger values mean higher similarity. Defining a cosine distance as $1-|\\\\cos (p, q)|$ does yield distance measure that satisfies three of the metric properties, all but the triangle inequality. A true distance metric follows from\\\\\\n%---- Page End Break Here ---- Page : 309\\n\\\\\\nangular distance\\\\index{angular distance}, where\\n\\n$$\\nd(p, q)=1-\\\\frac{\\\\arccos (\\\\cos (p, q))}{\\\\pi}\\n$$\\n\\nHere $\\\\arccos ()$ is the inverse cosine function $\\\\cos ^{-1}()$, and $\\\\pi$ is the largest angle range in radians.\\n\\n\\\\subsection*{10.1.6 Distances between Probability Distributions}\\nRecall the Kolmogorov-Smirnov test (Section 5.3.3), which enabled us to determine whether two sets of samples were likely drawn from the same underlying probability distribution.\\n\\nThis suggests that we often need a way to compare a pair of distributions and determine a measure of similarity or distance between them. A typical application comes in measuring how closely one distribution approximates another, providing a way to identify the best of a set of possible models.\\n\\nThe distance measures that have been described for points could, in principle, be applied to measure the similarity of two probability distributions $P$ and $Q$ over a given discrete variable range $R$.\\n\\nSuppose that $R$ can take on any of exactly $d$ possible values, say $R=$ $\\\\left\\\\{r_{1}, \\\\ldots, r_{d}\\\\right\\\\}$. Let $p_{i}\\\\left(q_{i}\\\\right)$ denote the probability that $X=r_{i}$ under distribution $P(Q)$. Since $P$ and $Q$ are both probability distributions, we know that\\n\\n$$\\n\\\\sum_{i=1}^{d} p_{i}=\\\\sum_{i=1}^{d} q_{i}=1\\n$$\\n\\nThe spectrum of $p_{i}$ and $q_{i}$ values for $1 \\\\leq i \\\\leq d$ can be thought of as $d$-dimensional points representing $P$ and $Q$, whose distance could be computed using the Euclidean metric.\\n\\nStill, there are more specialized measures, which do a better job of assessing the similarity of probability distributions. They are based on the informationtheoretic notion of entropy\\\\index{entropy}, which defines a measure of uncertainty for the value of a sample drawn from the distribution. This makes the concept mildly analogous to variance.\\n\\nThe entropy $H(P)$ of a probability distribution $P$ is given by\\n\\n$$\\nH(P)=\\\\sum_{i=1}^{d} p_{i} \\\\log _{2}\\\\left(\\\\frac{1}{p_{i}}\\\\right)=-\\\\sum_{i=1}^{d} p_{i} \\\\log _{2}\\\\left(p_{i}\\\\right) .\\n$$\\n\\nLike distance, entropy is always a non-negative quantity. The two sums above differ only in how they achieve it. Because $p_{i}$ is a probability, it is generally less than 1 , and hence $\\\\log \\\\left(p_{i}\\\\right)$ is generally negative. Thus either taking the reciprocal of the probabilities before taking the log or negating each term suffices to make $H(P) \\\\geq 0$ for all $P$.\\n\\nEntropy is a measure of uncertainty. Consider the distribution where $p_{1}=$ 1 and $p_{i}=0$, for $2 \\\\leq i \\\\leq d$. This is like tossing a totally loaded die, so\\\\\\\\\\ndespite having $d$ sides there is no uncertainty about the outcome. Sure enough, $H(P)=0$, because either $p_{i}$ or $\\\\log _{2}(1)$ zeros out every term in the summation. Now consider the distribution where $q_{i}=1 / d$ for $1 \\\\leq i \\\\leq d$. This represents fair dice roll, the maximally uncertain distribution where $H(Q)=\\\\log _{2}(d)$ bits.\\n\\nThe flip side of uncertainty is information. The entropy $H(P)$ corresponds to how much information you learn after a sample from $P$ is revealed. You learn nothing when someone tells you something you already know.\\n\\nThe standard distance measures on probability distributions are based on entropy and information theory. The Kullback-Leibler (KL) divergence measures the uncertainty gained or information lost when replacing distribution $P$ with Q. Specifically,\\n\\n$$\\nK L(P \\\\| Q)=\\\\sum_{i=1}^{d} p_{i} \\\\log _{2} \\\\frac{p_{i}}{q_{i}}\\n$$\\n\\nSuppose $P=Q$. Then nothing should be gained or lost, and $K L(P, P)=0$ because $\\\\lg (1)=0$. But the worse a replacement $Q$ is for $P$, the larger $K L(P \\\\| Q)$ gets, blowing u\\\\index{Kullback-L\\\\index{nearest neighbor classiï¬cation}eibler divergence}p to $\\\\infty$ when $p_{i}>q_{i}=0$.\\n\\nThe $K L$ divergence resembles a distance measure, but is not a metric, because it is not symmetric $(K L(P \\\\| Q) \\\\neq K L(Q \\\\| P))$ and does not satisfies the triangle inequality. However, it forms the basis of the Jensen-Shannon divergence $J S(P, Q)$ :\\n\\n$$\\nJ S(P, Q)=\\\\frac{1}{2} K L(P \\\\| M)+\\\\frac{1}{2} K L(Q \\\\| M)\\n$$\\n\\nwhere the distribution $M$ is the average of $P$ and $Q$, i.e. $m_{i}=\\\\left(p_{i}+q_{i}\\\\right) / 2$.\\\\\\\\\\n$J S(P, Q)$ is clearly symmetric while preserving the other properties of KL divergence. Further $\\\\sqrt{J S(P, Q)}$ magically satisfies the triangle inequality, turning it into a true metric. This is the right function to use for measuring the distance between probability distributions.\\n\\n\\\\subsection*{10.2 Nearest Neighbor Classification}\\nDistance functions grant us the ability to identify which points are closest to a given target. This provides great power, and is the engine behind nearest neighbor classification. Given a set of labeled training examples, we seek the training example which is most similar to an unlabeled point $p$, and then take the class label for $p$ from its nearest labeled neighbor.\\n\\nThe idea here is simple. We use the nearest labeled neighbor to a given query point $q$ as its representative. If we are dealing with a classification problem, we will assign $q$ the same label as it nearest neighbor(s). If we are dealing with a regression problem, assign $q$ the mean/median value of its nearest neighbor(s). These forecasts are readily defensible assuming (1) the feature space coherently captures the properties of the elements in question, and (2) the distance function meaningfully recognizes similar rows/points when they are encountered.\\n\\nThe Bible exhorts us to love thy neighbor. There are three big advantages\\\\index{advantages} to nearest neighbor methods for classification:\\\\\\n%---- Page End Break Here ---- Page : 311\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-327}\\n\\nFigure 10.3: The decision boundary of nearest-neighbor classifiers can be nonlinear.\\n\\n\\\\begin{itemize}\\n  \\\\item Simplicity: Nearest neighbor methods are not rocket science; there is no math here more intimidating than a distance metric. This is important, because it means we can know exactly what is going on and avoid being the victim of bugs or misconceptions.\\n  \\\\item Interpretability: Studying the nearest-neighbors of a given query point $q$ explains exactly why the classifier made the decision it did. If you disagree with this outcome, you can systematically debug things. Were the neighboring points incorrectly labeled? Did your distance function fail to pick out the items which were the logical peer group for $q$ ?\\n  \\\\item Non-linearity: Nearest neighbor classifiers have decision boundaries which are piecewise-linear, but can crinkle arbitrarily following the training example herd, as shown in Figure 10.3 From calculus we know that piecewiselinear functions approach smooth curves once the pieces get small enough. Thus nearest neighbor classifiers enable us to realize very complicated decision boundaries, indeed surfaces so complex that they have no concise representation.\\n\\\\end{itemize}\\n\\nThere are several aspects to building effective nearest neighbor classifiers, including technical issues related to robustness and efficiency. But foremost is learning to appreciate the power of analogy. We discuss these issues in the sections below.\\n\\n\\\\subsection*{10.2.1 Seeking Good Analogies}\\nCertain intellectual disciplines rest on the power of analogies. Lawyers don\\'t reason from laws directly as much as they rely on precedents: the results of\\\\\\n%---- Page End Break Here ---- Page : 312\\n\\\\\\npreviously decided cases by respected jurists. The right decision for the current case (I win or I lose) is a function of which prior cases can be demonstrated to be most fundamentally similar to the matter at hand.\\n\\nSimilarly, much of medical practice rests on experience. The old country doctor thinks back to her previous patients to recall cases with similar symptoms to yours that managed to survive, and then gives you the same stuff she gave them. My current physician (Dr. Learner) is now in his eighties, but I trust him ahead of all those young whipper-snappers relying only on the latest stuff taught in medical school.\\n\\nGetting the greatest benefits from nearest neighbor methods involves learning to respect analogical reasoning. What is the right way to predict the price of a house? We can describe each property in terms of features like the area of the lot and the number of bedrooms, and assign each a dollar weight to be added together via linear regression. Or we can look for \"comps,\" seeking comparable properties in similar neighborhoods, and forecast a price similar to what we see. The second approach is analogical reasoning.\\n\\nI encourage you to get hold of a data set where you have domain knowledge and interest, and do some experiments with finding nearest neighbors. One resource that always inspires me is \\\\href{http://www.baseball-reference.com}{http://www.baseball-reference.com}, which reports the ten nearest neighbors for each player, based on their statistics to date. I find these analogies amazingly evocative: the identified players often fill similar roles and styles which should not be explicitly captured by the statistics. Yet somehow they are.\\n\\nTry t\\\\index{k-nearest neighbors}o do this with another domain you care about: books, movies, music, or whatever. Come to feel the power of nearest neighbor methods, and analogies.\\n\\nTake-Home Lesson: Identifying the ten nearest neighbors to points you know about provides an excellent way to understand the strengths and limitations of a given data set. Visualizing such analogies should be your first step in dealing with any high-dimensional data set.\\n\\n\\\\subsection*{10.2.2 $k$-Nearest Neighbors}\\nTo classify a given query point $q$, nearest neighbor methods return the label of $q^{\\\\prime}$, the closest labeled point to $q$. This is a reasonable hypothesis, assuming that similarity in feature space implies similarity in label space. However, this classification is based on exactly one training example, which should give us pause.\\n\\nMore robust classification or interpolation follows from voting over multiple close neighbors. Suppose we find the $k$ points closest to our query, where $k$ is typically some value ranging from 3 to 50 depending upon the size of $n$. The arrangement of the labeled points coupled with the choice of $k$ carves the feature space into regions, with all the points in a particular given region assigned the same label.\\n\\nConsider Figure 10.4 which attempts to build a gender classifier from data\\\\\\n%---- Page End Break Here ---- Page : 313\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-329}\\n\\nFigure 10.4: The effect of $k$ on the decision boundary for gender classification using $k$-NN. Compare $k=3$ (left) and $k=10$ (right) with $k=1$ in Figure 10.3 .\\\\\\\\\\non height and weight. Generally speaking, women are shorter and lighter than men, but there are many exceptions, particularly near the decision boundary. As shown in Figure 10.4 increasing $k$ tends to produce larger regions with smoother boundaries, representing more robust decisions. However, the larger we make $k$, the more generic our decisions are. Choosing $k=n$ is simply another name for the majority classifier, where we assign each point the most common label regardless of its individual features.\\n\\nThe right way to set $k$ is to assign a fraction of labeled training examples as an evaluation set, and then experiment with different values of the parameter $k$ to see where the best performance is achieved. These evaluation values can then be thrown back into the training/target set, once $k$ has been selected.\\n\\nFor a binary\\\\index{binary} classification problem, we want $k$ to be an odd number, so the decision never comes out to be a tie. Generally speaking, the difference between the number of positive and negative votes can be interpreted as a measure of our confidence in the decision.\\n\\nThere are potential asymmetries concerning geometric nearest neighbors. Every point has a nearest neighbor, but for outlier points these nearest neighbors may not be particular close. These outlier points in fact can have an outsized role in classification, defining the nearest neighbor to a huge volume of feature space. However, if you picked your training examples properly this should be largely uninhabited territory, a region in feature space where points rarely occur.\\n\\nThe idea of nearest neighbor classification can be generalized to function interpolation, by averaging the values of the $k$ nearest points. This is presumably done by real-estate websites like \\\\href{http://www.zillow.com}{www.zillow.com} to predict housing prices from nearest neighbors. Such averaging schemes can be generalized by non-uniform\\\\\\n%---- Page End Break Here ---- Page : 314\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-330}\\n\\nFigure 10.5: Data structures for nearest neighbor search include Voronoi diagrams\\\\index{Voronoi diagrams} (left) and $k d$-trees (right).\\\\\\\\\\nweights, valuing points differently according to distance rank or magnitude. Similar ideas work for all classification methods.\\n\\n\\\\subsection*{10.2.3 Finding Nearest Neighbors}\\nPerhaps the biggest limitation of nearest neighbor classification methods is their runtime cost. Comparing a query point $q$ in $d$ dimensions against $n$ such training points is most obviously done by performing $n$ explicit distance comparisons, at a cost of $O(n d)$. With thousands or even millions of training points available, this search can introduce a notable lag into any classification system.\\n\\nOne approach to speeding up the search involves the use of geometric data structures. Popular choices include:\\n\\n\\\\begin{item\\\\index{ï¬nding}ize}\\n  \\\\item Voronoi diagrams: For a set of target points, we would like to partition the space around them into cells such that each cell contains exactly one target point. Further, we want each cell\\'s target point to be the nearest target neighbor for all locations in the cell. Such a partition is called a Voronoi diagram, and is illustrated in Figure 10.5 (left).\\\\\\\\\\nThe boundaries of Voronoi diagrams are defined by the perpendicular bisectors between pairs of points $(a, b)$. Each bisector cuts the space in half: one half containing $a$ and the other containing $b$, such that all points on $a$ \\'s half are closer to $a$ than $b$, and visa versa.\\\\\\\\\\nVoronoi diagrams are a wonderful tool for thinking about data, and have many nice properties. Efficient algorithms for building them and searching them exist, particularly in two dimensions. However, these procedures rapidly become more complex as the dimensionality increases, making them generally impractical beyond two or three dimensions.\\n  \\\\item grid indexes\\\\index{grid indexes}: We can carve up space into $d$-dimensional boxes, by dividing the range of each dimension into $r$ intervals or buckets. For example,\\\\\\n%---- Page End Break Here ---- Page : 315\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-331}\\n\\\\end{itemize}\\n\\nFigure 10.6: A grid index data structure provides fast access to nearest neighbors when the points are uniformly distributed, but can be inefficient when points in certain regions are densely clustered.\\\\\\\\\\nconsider a two-dimensional space where each axis was a probability, thus ranging from 0 to 1 . This range can be divided into $r$ equal-sized intervals, such that the $i$ th interval ranges between $[(i-1) / r, i / r]$.\\\\\\\\\\nThese intervals define a regular grid over the space, so we can associate each of the training points with the grid cell where it belongs. Search now becomes the problem of identifying the right grid cell for point $q$ through array lookup or binary search, and then comparing $q$ against all the points in this cell to identify the nearest neighbor.\\n\\nSuch grid indexes can be effective, but there are potential problems. First, the training points might not be uniformly distributed, and many cells might be empties, as in Figure 10.6 Establishing a non-uniform grid might lead to a more balanced arrangement, but makes it harder to quickly find the cell containing $q$. But there is also no guarantee that the nearest neighbor of $q$ actually lives within the same cell as $q$, particularly if $q$ lies very close to the cell\\'s boundary. This means we must search neighboring cells as well, to ensure we find the absolute nearest neighbor.\\n\\n\\\\begin{itemize}\\n  \\\\item kd-trees\\\\index{kd-trees}: There are a large class of tree-based data structures which partition space using a hierarchy of divisions that facilitates search. Starting from an arbitrary dimension as the root, each node in the $k d$-tree defines median line/plane that splits the points equally according to that dimension. The construction recurs on each side using a different dimension, and so on until the region defined by a node contains just one training point.\\n\\n%---- Page End Break Here ---- Page : 316\\n\\\\end{itemize}\\n\\nThis construction hierarchy is ideally suited to support search. Starting at the root, we test whether the query point $q$ is to the left or right of the median line/plane. This identifies which side $q$ lies on, and hence which side of the tree to recur on. The search time is $\\\\log n$, since we split the point set in half with each step down the tree.\\\\\\\\\\nThere are a variety of such space-partition search tree structures available, with one or more likely implemented in your favorite programming language\\'s function library. Some offer faster search times on problems like nearest neighbor, with perhaps a trade-off of accuracy for speed.\\n\\nAlthough these techniques can indeed speed nearest neighbor search in modest numbers of dimensions (say $2 \\\\leq d \\\\leq 10$ ), they get less effective as the dimensionality increases. The reason is that the number of ways that two points can be close to each other increases rapidly with the dimensionality, making it harder to cut away regions which have no chance of containing the nearest neighbor to $q$. Deterministic nearest neighbor search eventually reduces to linear search, for high-enough dimensionality data.\\n\\n\\\\subsection*{10.2.4 locality sensitive hashing\\\\index{locality sensitive hashing}}\\nTo achieve faster running times, we must abandon the idea of finding the exact nearest neighbor, and settle for a good guess. We want to batch up nearby points into buckets by similarity, and quickly find the most appropriate bucket $B$ for our query point $q$. By only computing the distance between $q$ and the points in the bucket, we save search time when $|B| \\\\ll n$.\\n\\nThis was the basic idea behind the grid index, described in the previous section, but the search structures become unwieldy and unbalanced in practice. A better approach is based on hashing.\\n\\nLocality sensitive hashing (LSH) is defined by a hash function $h(p)$ that takes a point or vector as input and produces a number or code as output such that it is likely that $h(a)=h(b)$ if $a$ and $b$ are close to each other, and $h(a) \\\\neq h(b)$ if they are far apart.\\n\\nSuch locality sensitive hash functions readily serve the same role as the grid index, without the fuss. We can simply maintain a table of points bucketed by this one-dimensional hash value, and then look up potential matches for query point $q$ by searching for $h(q)$.\\n\\nHow can we build such locality sensitive hash functions? The idea is easiest to understand at first when restricting to vectors instead of points. Recall that sets of $d$-dimensional vectors can be thought of as points on the surface of a sphere, meaning a circle when $d=2$.\\n\\nLet us consider an arbitrary line $l_{1}$ through the origin of this circle, which cuts the circle in half, as in Figure 10.7. Indeed, we can randomly select $l_{1}$ by simply picking a random angle $0 \\\\leq \\\\theta_{1}<2 \\\\pi$. This angle defines the slope of a line passing through the origin $O$, and together $\\\\theta_{1}$ and $O$ completely specify $l_{1}$. If randomly chosen, $l_{1}$ should grossly partition the vectors, putting about half of them on the left and the remainder on the right.\\\\\\n%---- Page End Break Here ---- Page : 317\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-333}\\n\\nFigure 10.7: Nearby points on the circle generally lie on the same side of random lines through the origin. Locality-sensitive hash codes for each point can be composed as a sequence of sidedness tests (left or right) for any specific sequence of lines.\\n\\nNow add a second random divider $l_{2}$, which should share the same properties. This then partitions all the vectors among four regions, $\\\\{L L, L R, R L, R R\\\\}$, defined by their status relative to these dividers $l_{1}$ and $l_{2}$.\\n\\nThe nearest neighbor of any vector $v$ should lie in the same region as $v$, unless we got unlucky and either $l_{1}$ or $l_{2}$ separated them. But the probability $p\\\\left(v_{1}, v_{2}\\\\right)$ that both $v_{1}$ and $v_{2}$ are on the same side of $l$ depends upon the angle between $v_{1}$ and $v_{2}$. Specifically $p\\\\left(v_{1}, v_{2}\\\\right)=1-\\\\theta\\\\left(v_{1}, v_{2}\\\\right) / \\\\pi$.\\n\\nThus we can compute the exact probability that near neighbors are preserved for $n$ points and $m$ random planes. The pattern of $L$ and $R$ over these $m$ planes defines an $m$-bit locality-sensitive hash code $h(v)$ for any vector $v$. As we move beyond the two planes of our example to longer codes, the expected number of points in each bucket drops to $n / 2^{m}$, albeit with an increased risk that one of the $m$ planes separates a vector from its true nearest neighbor.\\n\\nNote that this approach can easily be generalized beyond two dimensions. Let the hyperplane be defined by its normal vector $r$, which is perpendicular in direction to the plane. The sign of $s=v \\\\cdot r$ determines which side a query vector $v$ lies on. Recall that the dot product of two orthogonal vectors is 0 , so $s=0$ if $v$ lies exactly on the separating plane. Further, $s$ is positive if $v$ is above this plane, and negative if $v$ is below it. Thus the $i$ th hyperplane contributes exactly one bit to the hash code, where $h_{i}(q)=0$ iff $v \\\\cdot r_{i} \\\\leq 0$.\\n\\nSuch functions can be generalized beyond vectors to arbitrary point sets. Further, their precision can be improved by building multiple sets of code words for each item, involving different sets of random hyperplanes. So long as $q$ shares at least one codeword with its true nearest neighbor, we will eventually\\\\\\n%---- Page End Break Here ---- Page : 318\\n\\\\\\nencounter a bucket containing both of these points.\\\\\\\\\\nNote that LSH has exactly the opposite goal from traditional hash functions used for cryptographic applications or to manage hash tables. Traditional hash functions seek to ensure that pairs of similar items result in wildly different hash values, so we can recognize changes and utilize the full range of the table. In contrast, LSH wants similar items to recieve the exact same hash code, so we can recognize similarity by collision. With LSH, nearest neighbors belong in the same bucket.\\n\\nLocality sensitive hashing has other applications in data science, beyond nearest neighbor search. Perhaps the most important is constructing compressed feature representations from complicated objects, say video or music streams. LSH codes constructed from intervals of these streams define numerical values potentially suitable as features for pattern matching or model building.\\n\\n\\\\subsection*{10.3 graphs\\\\index{graphs}, networks\\\\index{networks}, and distances\\\\index{distances}}\\nA graph $G=(V, E)$ is defined on a set of vertices\\\\index{vertices} $V$, and contains a set of edges\\\\index{edges} $E$ of ordered or unordered pairs of vertices from $V$. In modeling a road network, the vertices may represent the cities or junctions, certain pairs of which are directly connected by roads/edges. In analyzing human interactions, the vertices typically represent people, with edges connecting pairs of related souls.\\n\\nMany other modern data sets are naturally modeled in terms of graphs or networks:\\n\\n\\\\begin{itemize}\\n  \\\\item The Worldwide Web ( $W W W$ ): Here there is a vertex\\\\index{vertex} in the graph for each webpage, with a directed edge $(x, y)$ if webpage $x$ contains a hyperlink to webpage $y$.\\n  \\\\item Product/customer networks: These arise in any company that has many customers and types of products: be it Amazon, Netflix, or even the corner grocery store. There are two types of vertices: one set for customers and another for products. Edge $(x, y)$ denotes a product $y$ purchased by customer $x$.\\n  \\\\item Genetic networks: Here the vertices represent the different genes/proteins in a particular organisms. Think of this as a parts list for the beast. Edge ( $x, y$ ) denotes that there are interactions between parts $x$ and $y$. Perhaps gene $x$ regulates gene $y$, or proteins $x$ and $y$ bind together to make a larger complex. Such interaction networks encode considerable information about how the underlying system works.\\n\\\\end{itemize}\\n\\nGraphs and point sets are closely related objects. Both are composed of discrete entities (points or vertices) representing items in a set. Both of them encode important notions of distance and relationships, either near-far or connectedindependent. Point sets can be meaningfully represented by graphs, and graphs by point sets.\\\\\\n%---- Page End Break Here ---- Page : 319\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-335}\\n\\nFigure 10.8: The pairwise distances between a set of points in space (left) define a complete weighted\\\\index{weighted} graph (center). Thresholding by a distance cutoff removes all long edges, leaving a sparse graph that captures the structure of the points (right).\\n\\n\\\\subsection*{10.3.1 Weighted graphs\\\\index{graphs} and induced\\\\index{induced} Networks}\\nThe edges in graphs capture binary relations\\\\index{binary relations}, where each edge $(x, y)$ represents that there is a relationship between $x$ and $y$. The existence of this relationship is sometimes all there is to know about it, as in the connection between webpages or the fact of someone having purchased a particular product.\\n\\nBut there is often an inherent measure of the strength or closeness of the relationship. Certainly we see it in road networks: each road segment has a length or travel time, which is essential to know for finding the best route to drive between two points. We say that a graph is weighted if every edge has a numerical value associated with it.\\n\\nAnd this weight is often (but not always) naturally interpreted as a distance. Indeed one can interpret a data set of $n$ points in space as a complete weighted graph on $n$ vertices, where the weight of edge $(x, y)$ is the geometric distance between points $x$ and $y$ in space. For many applications, this graph encodes all the relevant information about the points.\\n\\nGraphs are most naturally represented by $n \\\\times n$ adjacency\\\\index{adjacency} matrices. Define a non-edge symbol $x$. Matrix $M$ represents graph $G=(V, E)$ when $M[i, j] \\\\neq x$ if and only if vertices $i, j \\\\in V$ are connected by an edge $(i, j) \\\\in E$. For unweighted networks, typically the edge symbol is 1 while $x=0$. For distance weighted graphs, the weight of edge $(i, j)$ is the cost of travel between them, so setting $x=\\\\infty$ denotes the lack of any direct connection between $i$ and $j$.\\n\\nThis matrix representation for networks has considerable power, because we can introduce all our tools from linear algebra to work with them. Unfortunately, it comes with a cost, because it can be hopelessly expensive to store $n \\\\times n$ matrices once networks get beyond a few hundred vertices. There are more efficient ways to store large sparse graphs, with many vertices but relatively few pairs connected by edges. I will not discuss the details of graph algorithms here, but refer you with confidence to my book The Algorithm Design Manual Ski08 for you to learn more.\\n\\nPictures of graphs/networks are often made by assigning each vertex a point in the plane, and drawing lines between these vertex-points to represent edges. Such node-ink diagrams are immensely valuable to visualize the structure of\\\\\\n%---- Page End Break Here ---- Page : 320\\n\\\\\\nthe networks you are working with. They can be algorithmically constructed using force-directed\\\\index{directed} layout, where edges act like springs to bring adjacent pairs of vertices close together, and non-adjacent vertices repel each other.\\n\\nSuch drawings establish the connection between graph structures and point positions. An embedding\\\\index{embedding} is a point representation of the vertices of a graph that captures some aspect of its structure. Performing a feature compression like eigenvalue or singular value decomposition (see Section 8.5) on the the adjacency matrix of a graph produces a lower-dimensional representation that serves as a point representation of each vertex. Other approaches to graph embeddings include DeepWalk, to be discussed in Section 11.6.3.\\n\\nTake-Home Lesson: Point sets can be meaningfully represented by graphs/distance matrices, and graphs/distance matrices meaningfully represented by point sets (embeddings).\\n\\nGeometric graphs defined by the distances between points are representative of a class of graphs I will call induced networks, where the edges are defined in a mechanical way from some external data source. This is a common source of networks in data science, so it is important to keep an eye out for ways that your data set might be turned into a graph.\\n\\nDistance or similarity functions are commonly used to construct networks on sets of items. Typically we are interested in edges connecting each vertex to its $k$ closest/most similar vertices. We get a sparse\\\\index{sparse} graph by keeping $k$ modest, say $k \\\\approx 10$, meaning that it can be easily worked with even for large values of $n$.\\n\\nBut there are other types of induced networks. Typical would be to connect vertices $x$ and $y$ whenever they have a meaningful attribute in common. For example, we can construct an induced social network on people from their resumes, linking any two people who worked at the same company or attended the same school in a similar period. Such networks tend to have a blocky structure, where there are large subsets of vertices forming fully connected cliques. After all, if $x$ graduated from the same college as $y$, and $y$ graduated from the same college as $z$, then this implies that $(x, z)$ must also be an edge in the graph.\\n\\n\\\\subsection*{10.3.2 Talking About Graphs}\\nThere is a vocabulary about graphs that is important to know for working with them. Talking the talk is an important part of walking the walk. Several fundamental properties of graphs impact what they represent, and how we can use them. Thus the first step in any graph problem is determining the flavors of the graphs you are dealing with:\\n\\n\\\\begin{itemize}\\n  \\\\item undirected\\\\index{undirected} vs. Directed: A graph $G=(V, E)$ is undirected if edge $(x, y) \\\\in$ $E$ implies that $(y, x)$ is also in $E$. If not, we say that the graph is directed. road network\\\\index{road network}s between cities are typically undirected, since any large road has lanes going in both directions. Street networks within cities are almost\\\\\\n%---- Page End Break Here ---- Page : 321\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-337}\\n\\\\end{itemize}\\n\\nFigure 10.9: Important properties/flavors of graphs\\\\\\\\\\nalways directed, because there are at least a few one-way streets lurking somewhere. Webpage graphs are typically directed, because the link from page $x$ to page $y$ need not be reciprocated.\\n\\n\\\\begin{itemize}\\n  \\\\item weighted\\\\index{weighted} vs. unweighted\\\\index{unweighted}: As discussed in Section 10.3.1, each edge (or vertex) in a weighted graph\\\\index{weighted graph} $G$ is assigned a numerical value, or weight. The edges of a road network graph might be weighted with their length, drive-time, or speed limit, depending upon the application. In unweighted graph\\\\index{unweighted graph}s, there is no cost distinction between various edges and vertices.\\\\\\\\\\nDistance graphs are inherently weighted, while social/web networks are generally unweighted. The differe\\\\index{program ï¬ow gra\\\\index{simple graph}ph}nce determines whether the feature vectors associated with vertices are $0 / 1$ or numerical values of importance, which may have to be normalized.\\n  \\\\item simple\\\\index{simple} vs. non-simple\\\\index{non-simple}: Certain types of edges complicate the task of working with graphs. A self-loop\\\\index{self-loop} is an edge $(x, x)$, involving only one vertex. An edge $(x, y)$ is a multiedge\\\\index{multiedge} if it occurs more than once in the graph.\\\\\\\\\\nBoth of these structures require special care in preprocessing for feature generation. Hence any graph that avoids them is called simple. We often seek to remove both self-loops and multiedges at the beginning of analysis.\\n  \\\\item Sparse vs. dense\\\\index{dense}: Graphs are sparse when only a small fraction of the total possible vertex pairs ( $\\\\binom{n}{2}$ for a simple, undirected graph\\\\index{undirected graph}\\\\index{directed graph} on $n$ vertices) actually have edges defined between them. Graphs where a large fraction of the vertex pairs define edges are called dense. There is no official boundary between what is called sparse and what is called dense,\\\\\\n%---- Page End Break Here ---- Page : 322\\n\\\\\\nbut typically dense graphs have a quadratic number of edges, while sparse graphs are linear in size.\\\\\\\\\\nSparse graphs are usually sparse for application-specific reasons. Road networks must be sparse graphs because of road junctions. The most ghastly intersection I\\'ve ever heard of was the endpoint of only nine different roads. $k$-nearest neighbor graphs have vertex degrees of exactly $k$. Sparse graphs make possible much more space efficient representations than adjacency matrices, allowing the representation of much larger networks.\\n  \\\\item embedded\\\\index{embedded} vs. topological\\\\index{topological} - A graph is embedded if the vertices and edges are assigned geometric positions. Thus, any drawing of a graph is an embedding, which may or may not have algorithmic significance.\\n\\\\end{itemize}\\n\\nOccasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\\n\\n\\\\begin{itemize}\\n  \\\\item labeled\\\\index{labeled} vs. unlabeled\\\\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In unlabeled graphs\\\\index{unlabeled graphs}\\\\index{labeled graphs} no such distinctions are made.\\n\\\\end{itemize}\\n\\nGraphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\\n\\n\\\\subsection*{10.3.3 graph theory\\\\index{graph theory}}\\nGraph theory is an important area of mathematics which deals with the fundamental properties of networks and how\\\\index{topological graph} to compute them. Most computer science students get exposed to graph theory through the\\\\index{embedded graph}ir courses in discrete structures or algorithms.\\n\\nThe classical algorithms for finding shortest paths\\\\index{shortest paths}, connected components\\\\index{connected components}, spanning trees, cuts, matchings\\\\index{matchings} and topological sorting\\\\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\\n\\nI take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\\n\\n%---- Page End Break Here ---- Page : 323\\n\\n\\\\begin{itemize}\\n  \\\\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\\n  \\\\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\\n  \\\\item minimum spanning tree\\\\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\\n  \\\\item edge cuts\\\\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\\\in c$ and $y \\\\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\\n  \\\\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\\n  \\\\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\\n\\n%---- Page End Break Here ---- Page : 324\\n\\\\end{itemize}\\n\\n\\\\subsection*{10.4 PageRank\\\\index{PageRank}}\\nIt is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\\n\\nThe degree of vertex\\\\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google\\'s search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\\n\\nPageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\\n\\n$$\\nP R_{j}(v)=\\\\sum_{(u, v) \\\\in E} \\\\frac{P R_{j-1}(u)}{\\\\text { out }-\\\\operatorname{degree}(u)}\\n$$\\n\\nThis is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\\\left(v_{i}\\\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\\\leq i \\\\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex\\'s in-degree, but much more interesting things happen with directed graphs.\\n\\nIn essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\\n\\nThere are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the damping factor\\\\index{damping factor}. Then\\n\\n$$\\nP R_{j}(v)=\\\\sum_{(u, v) \\\\in E} \\\\frac{(1-p)}{n}+p \\\\frac{P R_{j-1}(u)}{\\\\text { out-degree }(u)}\\n$$\\n\\nwhere $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\\n\\n%---- Page End Break Here ---- Page : 325\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|cl|}\\n\\\\hline\\n\\\\multicolumn{2}{|c|}{PageRank PR1 (all pages)} \\\\\\\\\\n\\\\hline\\n1 & Napoleon\\\\index{Napoleon} \\\\\\\\\\n2 & George W. Bush \\\\\\\\\\n3 & Carl Linnaeus \\\\\\\\\\n4 & Jesus\\\\index{Jesus} \\\\\\\\\\n5 & Barack Obama \\\\\\\\\\n6 & Aristotle\\\\index{Aristotle}\\\\index{Aristotle} \\\\\\\\\\n7 & William Shakespeare \\\\\\\\\\n8 & Elizabeth II\\\\index{Elizabeth II} \\\\\\\\\\n9 & Adolf Hitler \\\\\\\\\\n10 & Bill Clinton \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|cl|}\\n\\\\hline\\n\\\\multicolumn{2}{|c|}{PageRank PR2 (only people)} \\\\\\\\\\n\\\\hline\\n1 & George W. Bush \\\\\\\\\\n2 & Bill Clinton \\\\\\\\\\n3 & William Shakespeare \\\\\\\\\\n4 & Ronald Reagan \\\\\\\\\\n5 & Adolf Hitler \\\\\\\\\\n6 & Barack Obama \\\\\\\\\\n7 & Napoleon \\\\\\\\\\n8 & Richard Nixon \\\\\\\\\\n9 & Franklin D. Roosevelt \\\\\\\\\\n10 & Elizabeth II \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nTable 10.1: Historical individuals with the highest PageRank in the 2010 English Wikipedia\\\\index{Wikipedia}, drawn over the full Wikipedia graph (left) and when restricted to links to other people (right).\\\\\\\\\\nsingle super-vertex, we ensure that random walks cannot get trapped in some small corner of the network. Self-loops and parallel edges (multiedges) can be deleted to avoid biase\\\\index{Bu\\\\ind\\\\in\\\\index{Nixo\\\\index{Obama, Barack}n, \\\\in\\\\index{Reag\\\\index{Roosevel\\\\index{Shakespeare, William}t, Franklin D.}an, Ronald}dex{Obama, Barack]}Richard}dex{Linnaeus, Carl}ex{Hitler, Adolf}sh,\\\\index{Cli\\\\index{Clinton, Bill]}nton, Bill} George W.}s from repetition.\\n\\nThere is also a linear algebraic interpretation of PageRank. Let $M$ be a matrix of vertex-vertex transition probabilities, so $M_{i j}$ is the probability that our next step from $i$ will be to $j$. Clearly $M_{i j}=1 /$ out-degree $(i)$ if there is a directed edge from $i$ to $j$, and zero if otherwise. The $j$ th round estimate for the PageRank vector $P R_{j}$ can be computed as\\n\\n$$\\nP R_{j}=M \\\\cdot P R_{j-1}\\n$$\\n\\nAfter this estimate converges, $P R=M \\\\cdot P R$, or $\\\\lambda U=M U$ where $\\\\lambda=$ 1 and $U$ represents the PageRank vector. This is the defining equation for eigenvalues, so the $n \\\\times 1$ vector of PageRank values turns out to be the principle eigenvector of the transition probability matrix defined by the links. Thus iterative methods for computing eigenvectors and fast matrix multiplication leads to efficient PageRank computations.\\n\\nHow well does PageRank work at smoking out the most central vertices? To provide some intuition, we ran PageRank on the link network from the English edition of Wikipedia, focusing on the pages associated with people. Table 10.1 (left) lists the ten historical figures with the highest PageRank.\\n\\nThese high-PageRank figures are all readily recognized as very significant people. The least familiar among them is probably Carl Linnaeus (1707-1778) [46], biology\\'s \"father of taxonomy\" whose Linnaean system (Genus species; e.g., Homo sapiens) is used to classify all life on earth. He was a great scientist, but why is he so very highly regarded by PageRank? The Wikipedia pages of all the plant and animal species he first classified link back to him, so thousands of life forms contribute prominent paths to his page.\\n\\nThe Linnaeus example points out a possible weakness of PageRank: do we really want plants and other inanimate objects voting on who the most promi-\\\\\\n%---- Page End Break Here ---- Page : 326\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-342}\\n\\nFigure 10.10: PageRank graphs for Barack Obama, over all Wikipedia pages (left) and when restricted only to people (right)\\\\\\\\[0pt]\\nnent people are? Figure 10.10 (left) shows a subset of the full PageRank graph for Barack Obama [91]. Note, for example, that although the links associated with Obama seem very reasonable, we can still reach Obama in only two clicks from the Wikipedia page for Dinosaur\\\\index{Obama, Barack [91]}s. Should extinct beasts contribute to the President\\'s centrality?\\n\\nAdding and deleting sets of edges from a given network gives rise to different networks, some of which better reveal underlying significance using PageRank. Suppose we compute PageRank (denoted PR2) using only the Wikipedia edges linking people. This computation would ignore any contribution from places, organizations, and lower organisms. Figure 10.10 (right) shows a sample of the PageRank graph for Barack Obama [91] when we restrict it to only people.\\n\\nPageRank on this graph favors a slightly different cohort of people, shown in Table 10.1 (right). Jesus, Linnaeus and Aristotle (384-322 b.c.) [8] are now gone, replaced by three recent U.S. presidents - who clearly have direct connections from many important people. So which version of PageRank is better, PR1 or PR2? Both seem to capture reasonable notions of centrality with a substantial but not overwhelming correlation (0.68), so both make sense as potential features in a data set.\\n\\n\\\\subsection*{10.5 clustering\\\\index{clustering}}\\nClustering is the problem of grouping points by similarity. Often items come from a small number of logical \"sources\" or \"explanations\", and clustering is a good way to reveal these origins. Consider what would happen if an alien species were to come across height and weight data for a large number of humans. They would presumably figure out that there seem to be two clusters representing distinct populations, one consistently bigger than the other. If the aliens were really on the ball, they might call these populations \\'men\" and \"women\". Indeed, the two height-weight clusters in Figure 10.11 are both highly\\\\\\n%---- Page End Break Here ---- Page : 327\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-343}\\n\\nFigure 10.11: Clustering people in weight-height space, using 2-means clustering. The left cluster contains 240 women and 112 men, while the right cluster contains 174 men to 54 women. Compare this to the logistic regression classifier trained on this same data set, in Figure 9.17 .\\\\\\\\\\nconcentrated in one particular gender.\\\\\\\\\\nPatterns on a two-dimensional dot plot are generally fairly easy to see, but we often deal with higher-dimensional data that humans cannot effectively visualize. Now we need algorithms to find these patterns for us. Clustering is perhaps the first thing to do with any interesting data set. Applications include:\\n\\n\\\\begin{itemize}\\n  \\\\item hypothesis development\\\\index{hypothesis development}: Learning that there appear to be (say) four distinct populations represented in your data set should spark the question as to why they are there. If these clusters are compact and well-separated enough, there has to be a reason and it is your business to find it. Once you have assigned each element a cluster label, you can study multiple representatives of the same cluster to figure out what they have in common, or look at pairs of items from different clusters and identify why they are different.\\n  \\\\item modeling\\\\index{modeling} over smaller subsets of data: Data sets often contain a very large number of rows $(n)$ relative to the number of feature columns $(m)$ : think the taxi cab data of 80 million trips with ten recorded fields per trip. Clustering provides a logical way to partition a large single set of records in a (say) a hundred distinct subsets each ordered by similarity. Each of these clusters still contains more than enough records to fit a forecasting model on, and the resulting model may be more accurate on this restricted class of items then a general model trained over all items. Making a forecast now involves identifying the appropriate cluster your\\\\\\n%---- Page End Break Here ---- Page : 328\\n\\\\\\nquery item $q$ belongs to, via a nearest neighbor search, and then using the appropriate model for that cluster to make the call on $q$.\\n  \\\\item data reduction\\\\index{data reduction}: Dealing with millions or billions of records can be overwhelming, for processing or visualization. Consider the computational cost of identifying the nearest neighbor to a given query point, or trying to understand a dot plot with a million points. One technique is to cluster the points by similarity, and then appoint the centroid of each cluster to represent the entire cluster. Such nearest neighbor models can be quite robust because you are reporting the consensus label of the cluster, and it comes with a natural measure of confidence: the accuracy of this consensus over the full cluster.\\n  \\\\item outlier detection\\\\index{outlier detection}: Certain items resulting from any data collection procedure will be unlike all the others. Perhaps they reflect data entry errors or bad measurements. Perhaps they signal lies or other misconduct. Or maybe they result from the unexpected mixture of populations, a few strange apples potentially spoiling the entire basket.\\n\\\\end{itemize}\\n\\nOutlier detection is the problem of ridding a data set of discordant items, so the remainder better reflects the desired population. Clustering is a useful first step to find outliers. The cluster elements furthest from their assigned cluster center don\\'t really fit well there, but also don\\'t fit better anywhere else. This makes them candidates to be outliers. Since invaders from another population would tend to cluster themselves together, we may well cast suspicions on small clusters whose centers lie unusually far from all the other cluster centers.\\n\\nClustering is an inherently ill-defined problem, since proper clusters depend upon context and the eye of the beholder. Look at Figure 10.12 How many different clusters do you see there? Some see three, other see nine, and others vote for pretty much any number in between.\\n\\nHow many clusters you see depends somewhat upon how many clusters you want to see. People can be clustered into two groups, the lumpers\\\\index{lumpers} and the splitters\\\\index{splitters}, depending upon their inclination to make fine distinctions. Splitters look at dogs, and see poodles, terriers, and cocker spaniels. Lumpers look at dogs, and see mammals. Splitters draw more exciting conclusions, while lumpers are less likely to overfit their data. Which mindset is most appropriate depends upon your task.\\n\\nMany different clustering algorithms have been developed, and we will review the most prominent methods ( $k$-means, agglomerative clustering, and spectral clustering) in the sections below. But it is easy to get too caught up in the differences between methods. If your data exhibits strong-enough clusters, any method is going to find something like it. But when an algorithm returns clusters with very poor coherence, usually your data set is more to blame than the algorithm itself.\\\\\\n%---- Page End Break Here ---- Page : 329\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-345}\\n\\nFigure 10.12: How many clusters do you see here?\\n\\nTake-Home Lesson: Make sure you are using a distance metric which accurately reflects the similarities that you are looking to find. The specific choice of clustering algorithm usually proves much less important than the similarity/distance measure which underlies it.\\n\\n\\\\subsection*{10.5.1 $k$-means Clustering}\\nWe have been somewhat lax in defining exactly what a clustering algorithm should return as an \\\\index{k-means}answer. One possibility is to label each point with the name of the cluster that it is in. If there are $k$ clusters, these labels can be the integers 1 through $k$, where labeling point $p$ with $i$ means it is in the $i$ th cluster. An equivalent output representation might be $k$ separate lists of points, where list $i$ represents all the points in the $i$ th cluster.\\n\\nBut a more abstract notion reports the center point of each cluster. Typically we think of natural clusters as compact, Gaussian-like regions, where there is an ideal center defining the location where the points \"should\" be. Given the set of these centers, clustering the points becomes easy: simply assign each point $p$ to the center point $C_{i}$ closest to it. The $i$ th cluster consists of all points whose nearest center is $C_{i}$.\\\\\\\\\\n$k$-means clustering is a fast, simple-to-understand, and generally effective approach to clustering. It starts by making a guess as to where the cluster centers might be, evaluates the quality of these centers, and then refines them to make better center estimates.\\n\\n%---- Page End Break Here ---- Page : 330\\n\\n\\\\section*{$K$-means clustering}\\nSelect $k$ points as initial cluster centers $C_{1}, \\\\ldots, C_{k}$.\\\\\\\\\\nRepeat until convergence \\\\{\\\\\\\\\\nFor $1 \\\\leq i \\\\leq n$, map point $p_{i}$ to its nearest cluster center $C_{j}$\\\\\\\\\\nCompute centroid $C_{j}^{\\\\prime}$ of the points nearest $C_{j}$, for $1 \\\\leq j \\\\leq k$\\\\\\\\\\nFor all $1 \\\\leq j \\\\leq k$, set $C_{j}=C_{j}^{\\\\prime}$\\\\\\\\\\n\\\\}\\n\\nFigure 10.13: Pseudocode for the $k$-means clustering algorithm.\\n\\nThe algorithm starts by assuming that there will be exactly $k$ clusters in the data, and then proceeds to pick initial centers for each cluster. Perhaps this means randomly selecting $k$ points from the set of $n$ points $S$ and calling them centers, or selecting $k$ random points from the bounding box of $S$. Now test each of the $n$ points against all $k$ of the centers, and assign each point in $S$ to its nearest current center. We can now compute a better estimate of the center of each cluster, as the centroid of the points assigned to it. Repeat until the cluster assignments are sufficiently stable, presumably when they have not changed since the previous generation. Figure 10.13 provides pseudocode of this $k$-means procedure.\\n\\nFigure 10.14 presents an animation of $k$-means in action. The initial guesses for the cluster centers are truly bad, and the initial assignments of points to centers splits the real clusters instead of respecting them. But the situation rapidly improves, with the centroids\\\\index{centroids} drifting into positions that separates the points in the desired way. Note that the $k$-means procedure does not necessarily terminate with the best possible set of $k$ centers, only at a locally-optimal solution that provides a logical stopping point. It is a good idea to repeat the entire procedure several times with different random initializations and accept the best clustering found over all. The mean squared\\\\index{mean squared} error is the sum of squares of the distance between each point $P_{i}$ and its center $C_{j}$, divided by the number of points $n$. The better of two clusterings can be identified as having lower mean squared error, or some other reasonable error statistic.\\n\\n\\\\section*{Centers or Centroids?}\\nThere are at least two possible criteria for computing a new estimate for the center point as a function of the set $S^{\\\\prime}$ of points assigned to it. The centroid $C$ of a point set is computed by taking the average value of each dimension. For\\\\\\n%---- Page End Break Here ---- Page : 331\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-347}\\n\\nFigure 10.14: The iterations of $k$-means (for $k=3$ ) as it converges on a stable and accurate clustering. Fully seven iterations are needed, because of the unfortunate placement of the three initial cluster centers near the logical center.\\\\\\\\\\nthe $d$ th dimension,\\n\\n$$\\nC_{d}=\\\\frac{1}{\\\\left|S^{\\\\prime}\\\\right|} \\\\sum_{p \\\\in S^{\\\\prime}} p[d]\\n$$\\n\\nThe centroid serves as the center of mass\\\\index{center of mass} of $S^{\\\\prime}$, the place where the vectors defined through this point sum to zero. This balance criteria defines a natural and unique center for any $S^{\\\\prime}$. Speed of computation is another nice thing about using the centroid. For $n d$-dimensional points in $S^{\\\\prime}$, this takes $O(n d)$ time, meaning linear in the input size of the points.\\n\\nFor numerical data points, using the centroid over an appropriate $L_{k}$ metric (like Euclidean distance) should work just fine. However, centroids are not well defined when clustering data records with non-numeri\\\\index{k-mediods algorithm}cal attributes, like categorical data. What is the centroid of 7 blonds, 2 red-heads, and 6 gray-haired people? We have discussed how to construct meaningful distance functions over categorical records. The problem here is not so much measuring similarity, as constructing a representative center.\\n\\nThere is a natural solution, sometimes called the $k$-mediods algorithm. Suppose instead of the centroid we define the centermost point $C$ in $S^{\\\\prime}$ to be the cluster representative. This is the point which minimizes the sum of distances\\\\\\n%---- Page End Break Here ---- Page : 332\\n\\\\\\nto all other points in the cluster:\\n\\n$$\\nC=\\\\underset{c \\\\in S^{\\\\prime}}{\\\\arg \\\\min } \\\\sum_{i=1}^{n} d\\\\left(c, p_{i}\\\\right)\\n$$\\n\\nAn advantage of using a centerpoint to define the cluster is that it gives the cluster a potential name and identity, assuming the input points correspond to items with identifiable names.\\n\\nUsing the centermost input example as center means we can run k-means so long as we have a meaningful distance function. Further, we don\\'t lose very much precision by picking the centermost point instead of the centroid. Indeed, the sum of distances through the centermost point is at most twice that of the centroid, on numerical examples where the centroid can be computed. The big win of the centroid is that it can be computed faster than the centermost vertex, by a factor of $n$.\\n\\nUsing center vertices to represent clusters permits one to extend $k$-means naturally to graphs and networks. For weighted graphs, it is natural to employ a shortest path algorithm to construct a matrix $D$ such that $D[i, j]$ is the length of the shortest path in the graph from vertex $i$ to vertex $j$. Once $D$ is constructed, $k$-means can proceed by reading the distances off this matrix, instead of calling a distance function. For unweighted graphs, a linear time algorithm like breadthfirst search can be efficiently used to compute graph distances on demand.\\n\\n\\\\section*{How Many Clusters?}\\nInherent to the interpretation of $k$-means clustering is the idea of a mixture model\\\\index{mixture model}. Instead of all our observed data coming from a single source, we presume that our data is coming from $k$ different populations or sources. Each source is generating points to be like its center, but with some degree of variation or error. The question of how many clusters a data set has is fundamental: how many different populations were drawn upon when selecting the sample?\\n\\nThe first step in the $k$-means algorithm is to initialize $k$, the number of\\\\index{number of} clusters in the given data set. Sometimes we have a preconception of how many clusters we want to see: perhaps two or three for balance or visualization, or maybe 100 or 1000 as a proxy for \"many\" when partitioning a large input file into smaller sets for separate modeling.\\n\\nBut generally speaking this is a problem, because the \"right\" number of clusters is usually unknown. Indeed, the primary reason to cluster in the first place is our limited understanding of the structure of the data set.\\n\\nThe easiest way to find the right $k$ is to try them all, and then pick the best one. Starting from $k=2$ to as high as you feel you have time for, perform $k$-means and evaluate the resulting clustering according to the mean squared error (MSE) of the points from their centers. Plotting this yields an error curve, as shown in Figure 10.16. The error curve for random centers is also provided.\\n\\nBoth error curves show the MSE of points from their centers decreasing as we allow more and more cluster centers. But the wrong interpretation would\\\\\\n%---- Page End Break Here ---- Page : 333\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-349(1)}\\n\\nFigure 10.15: Running $k$-means for $k=1$ to $k=9$. The \"right\" clustering is found for $k=3$, but the algorithm is unable to properly distinguish between nested circular clusters and long thin clusters for large $k$.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-349}\\n\\nFigure 10.16: The error curve for $k$-means clustering on the point set of Figure 10.12 , showing a bend in the elbow reflecting the three major clusters in the data. The error curve for random cluster centers is shown for comparison.\\\\\\n%---- Page End Break Here ---- Page : 334\\n\\\\\\nbe to suggest we need $k$ as large as possible, because the MSE should decrease when allowing more centers. Indeed, inserting a new center at a random position $r$ into a previous $k$-means solution can only decrease the mean squared error, by happening to land closer to a few of the input points than their previous center. This carves out a new cluster around $r$, but presumably an even better clustering would have been found running by $k$-means from scratch on $(k+1)$ centers.\\n\\nWhat we seek from the error curve in Figure 10.16 is the value $k$ where the rate of decline decreases, because we have exceeded number of true sources, and so each additional center is acting like a random point in the previous discussion. The error curve should look something like an arm in typing position: it slopes down rapidly from shoulder to elbow, and then slower from the elbow to the wrist. We want $k$ to be located exactly at the elbow. This point might be easier to identify when compared to a similar MSE error plot for random centers, since the relative rate of error reduction for random centers should be analogous to what we see past the elbow. The slow downward drift is telling us the extra clusters are not doing anything special for us.\\n\\nEach new cluster center adds $d$ parameters to the model, where $d$ is the dimensionality of the point set. Occam\\'s razor tells us that the simplest model is best, which is the philosophical basis for using the bend in the elbow to select $k$. There are formal criterias of merit which incorporate both the number of parameters and the prediction error to evaluate models, such as the Akaike information criterion\\\\index{Akaike information criterion}\\\\index{Akaike information criterion} (AIC). However, in practice you should feel confident in making a reasonable choice for $k$ based on the shape of the error curve.\\n\\n\\\\section*{expectation maximization\\\\index{expectation maximization}}\\nThe $k$-means algorithm is the most prominent example of a class of learning algorithms based on expectation maximization (EM). The details require more formal statistics than I am prepared to delve into here, but the principle can be observed in the two logical steps of the $k$-means algorithm: (a) assigning points to the estimated cluster center that is closest to them, and (b) using these point assignments to improve the estimate of the cluster center. The assignment operation is the expectation or E-step\\\\index{E-step} of the algorithm, while the centroid computation is the parameter maximization or M-step\\\\index{M-step}.\\n\\nThe names \"expectation\" and \"maximization\" have no particular resonance to me in terms of the $k$-means algorithm. However, the general form of an iterative parameter-fitting algorithm which improves parameters in rounds based on the errors of the previous models does seem a sensible thing to do. For example, perhaps we might have partially labeled classification data, where there are relatively few training examples confidently assigned to the correct class. We can build classifiers based on these training examples, and use them to assign the unlabeled points to candidate classes. This presumably defines larger training sets, so we should be able to fit a better model for each class. Now reassigning the points and iterating again should converge on a better model.\\\\\\n%---- Page End Break Here ---- Page : 335\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-351}\\n\\nFigure 10.17: agglomerative\\\\index{agglomerative} clustering of gene expression data.\\n\\n\\\\subsection*{10.5.2 Agglomerative Clustering}\\nMany sources of data are generated from a process defined by an underlying hierarchy or taxonomy. Often this is the result of an evolutionary process: In the Beginning there was one thing, which repeatedly bifurcated to create a rich universe of items. All animals and plant species are the result of an evolutionary process, and so are human languages and cultural/ethic groupings. To a lesser but still real extent, so are products like movies and books. This book can be described as a \"Data Science Textbook\", which is an emerging sub-genre split off from \"Computer Science Textbook\", which logically goes back to \"Engineering Textbook\", to \"Textbook\", to \"Non-Fiction\" eventually back to the original source: perhaps \"Book\".\\n\\nIdeally, in the course of clustering items we will reconstruct these evolutionary histories. This goal is explicit in agglomerative clustering, a collection of bottom-up methods that repeatedly merge the two nearest clusters into a bigger super-cluster, defining a rooted tree\\\\index{tree} whose leaves are the individual items and whose root defines the universe.\\n\\nFigure 10.17 illustrates agglomerative clustering applied to gene expression data. Here each column represents a particular gene, and each row the results of an experiment measuring how active each gene was in a particular condition. As an analogy, say each of the columns represented different people, and one particular row assessed their spirits right after an election. Fans of the winning party would be more excited than usual (green), while voters for the losing team would be depressed (red). Most of the rest of the world wouldn\\'t care (black). As it is with people, so it is with genes: different things turn them on and off, and analyzing gene expression data can reveal what makes them tick.\\n\\nSo how do we read Figure 10.17? By inspection it is clear that there are blocks of columns which all behave similarly, getting turned on and turned off in similar conditions. The discovery of these blocks are reflected in the tree above the matrix: regions of great similarity are associated with small branchings. Each node of the tree represents the merging of two clusters. The height of the node is proportional to the distance\\\\index{distance} between the two clusters being merged. The taller the edge, the more dodgy the notion that these clusters should be merged. The columns of the matrix have been permuted to reflect this tree\\\\\\n%---- Page End Break Here ---- Page : 336\\n\\\\\\norganization, enabling us to visualize hundreds of genes quantified in fourteen dimensions (with each row defining a distinct dimension).\\n\\nbiological\\\\index{biological} clusterings are often associated with such dendograms or phylogenic trees, because they are the result of an evolutionary process. Indeed, the clusters of similar gene expression behavior seen here are the results of the organism evolving a new function, that changes the response of certain genes to a particular condition.\\n\\n\\\\section*{Using Agglomerative Trees}\\nAgglomerative clustering returns a tree on top of the groupings of items. After cutting the longest edges in this tree, what remains are the disjoint groups of items produced by clustering algorithms like $k$-means. But this tree is a marvelous thing, with powers well beyond the item partitioning:\\n\\n\\\\begin{itemize}\\n  \\\\item organization of\\\\index{organization of} clusters and subclusters: Each internal node in the tree defines a particular cluster, comprised of all the leaf-node elements below it. But the tree describes hierarchy among these clusters, from the most refined/specific clusters near the leaves to the most general clusters near the root. Ideally, nodes of a tree define nameable concepts: natural groupings that a domain expert could explain if asked. These various levels of granularity are important, because they define structural concepts we might not have noticed prior to doing clustering.\\n  \\\\item visualization of\\\\index{visualization of} the clustering process: A drawing of this agglomeration tree tells us a lot about the clustering process, particularly if the drawing reflects the cost of each merging step. Ideally there will be very long edges near the root of the tree, showing that the highest-level clusters are well separated and belong in distinct groupings. We can tell if the groupings are balanced, or whether the high level groupings are of substantially different sizes. Long chains of merging small clusters into a big cluster is generally a bad sign, although the choice of merging criteria (to be discussed below) can bias the shape of the tree. Outliers show up nicely on a phylogenic tree, as singleton elements or small clusters that connect near the root through long edges.\\n  \\\\item Natural measure of cluster distance: An interesting property of any tree $T$ is that there is exactly one path in $T$ between any two nodes $x$ and $y$. Each internal vertex in an agglomerative clustering tree has a weight associated with it, the cost of merging together the two subtrees below it. We can compute a \"cluster distance\" between any two leaves by the sum of the merger costs on the path between them. If the tree is good, this can be more meaningful than the Euclidean distance between the records associated with $x$ and $y$.\\n  \\\\item Efficient classification of new items: One important application for clustering is classification. Suppose have agglomeratively clustered the prod-\\\\\\n%---- Page End Break Here ---- Page : 337\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-353}\\n\\\\end{itemize}\\n\\nFigure 10.18: Four distance measures for identifying the nearest pair of clusters.\\\\\\\\\\nucts in a store, to build a taxonomy of clusters. Now a new part comes along. What category should it be classified under?\\n\\nFor $k$-means, each of the $c$ clusters are categorized by their centroid, so classifying a new item $q$ reduces to computing the distance between $q$ and all $c$ centroids to identify the nearest cluster. A hierarchical tree provides a potentially faster method. Suppose we have precomputed the centroids of all the leaves on the left and right subtrees beneath each node. Identifying the right position in the hierarchy for a new item $q$ starts by comparing $q$ to the centroids of the root\\'s left and right subtrees. The nearest of the two centroids to $q$ defines the appropriate side of the tree, so we resume the search there one level down. This search takes time proportional to the height of the tree, instead of the number of leaves. This is typically an improvement from $n$ to $\\\\log n$, which is much better.\\n\\nUnderstand that binary merging trees can be drawn in many different ways that reflect exactly the same structure, because there is no inherent notion of which is the left child and which is the right child. This means that there are $2^{n-1}$ distinct permutations of the $n$ leaves possible, by flipping the direction of any subset of the $n-1$ internal nodes in the tree. Realize this when trying to read such a taxonomy: two items which look far away in left-right order might well have been neighbors had this flipping been done in a different way. And the rightmost node of the left subtree might be next to the leftmost node in the right subtree, even though they are really quite far apart in the taxonomy.\\n\\n\\\\section*{Building agglomerative cluster trees\\\\index{agglomerative cluster trees}}\\nThe basic agglomerative clustering algorithm is simple enough to be described in two sentences. Initially, each item is assigned to its own cluster. Merge the two closest clusters into one by putting a root over them, and repeat until only one cluster remains.\\n\\nAll that remains is to specify how to compute the distance between clusters. When the clusters contain single items, the answer is easy: use your favorite distance metric like $L_{2}$. But there are several reasonable answers for the distance\\\\\\n%---- Page End Break Here ---- Page : 338\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-354}\\n\\nFigure 10.19: single link\\\\index{single link}age clustering is equivalent to finding the minimum spanning tree\\\\index{minimum spanning tree} of a network.\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-354(1)}\\n\\nFigure 10.20: Kruskal\\'s algorithm for minimum spanning tree is indeed singlelinkage agglomerative clustering, as shown by the cluster tree on right.\\\\\\\\\\nbetween two non-trivial clusters, which lead to different trees on the same input, and can have a profound impact on the shape of the resulting clusters. The leading candidates, illustrated in Figure 10.18 are:\\n\\n\\\\begin{itemize}\\n  \\\\item nearest neighbor\\\\index{nearest neighbor} (single link): Here the distance between clusters $C_{1}$ and $C_{2}$ is defined by the closest pair of points spanning them:\\n\\\\end{itemize}\\n\\n$$\\nd\\\\left(C_{1}, C_{2}\\\\right)=\\\\min _{x \\\\in C_{1}, y \\\\in C_{2}}\\\\|x-y\\\\|\\n$$\\n\\nUsing this metric is called single link\\\\index{Kruskalâs algorithm} clustering, because the decision to merge is based solely on the single closest link between the clusters.\\\\\\\\\\nThe minimum spanning tree of a graph $G$ is tree drawn from the edges of $G$ connecting all vertices at lowest total cost. Agglomerative clustering with the single link criteria is essentially the same as Kruskal\\'s algorithm, which creates the minimum spanning tree (MST) of a graph by repeatedly adding the lowest weight edge remaining which does not create a cycle in the emerging tree.\\n\\n%---- Page End Break Here ---- Page : 339\\n\\nThe connection between the MST (with $n$ nodes and $n-1$ edges) and the cluster tree (with $n$ leaves, $n-1$ internal nodes, and $2 n-2$ edges) is somewhat subtle: the order of insertion edges in the MST from smallest to largest describes the order of merging in the cluster tree, as shown in Figure 10.20\\\\\\\\\\nThe Platonic ideal of clusters are as compact circular regions, which generally radiate out from centroids, as in $k$-means clustering. By contrast, single-link clustering tends to create relatively long, skinny clusters, because the merging decision is based only on the nearness of boundary points. Single link clustering is fast, but tends to be error prone, as outlier points can easily suck two well-defined clusters together.\\n\\n\\\\begin{itemize}\\n  \\\\item average link\\\\index{average link}: Here we compute distance between all pairs of clusterspanning points, and average them for a more robust merging criteria than single-link:\\n\\\\end{itemize}\\n\\n$$\\nd\\\\left(C_{1}, C_{2}\\\\right)=\\\\frac{1}{\\\\left|C_{1}\\\\right|\\\\left|C_{2}\\\\right|} \\\\sum_{x \\\\in C_{1}} \\\\sum_{y \\\\in C_{2}}\\\\|x-y\\\\|\\n$$\\n\\nThis will tend to avoid the skinny clusters of single-link, but at a greater computational cost. The straightforward implementation of average link clustering is $O\\\\left(n^{3}\\\\right)$, because each of the $n$ merges will potentially require touching $O\\\\left(n^{2}\\\\right)$ edges to recompute the nearest remaining cluster. This is $n$ times slower than single link clustering, which can be implemented in $O\\\\left(n^{2}\\\\right)$ time.\\n\\n\\\\begin{itemize}\\n  \\\\item nearest centroid\\\\index{nearest centroid}: Here we maintain the centroid of each cluster, and merge the cluster-pair with the closest centroids. This has two main advantages. First, it tends to produce clusters similar to average link, because outlier points in a cluster get overwhelmed as the cluster size (number of points) increases. Second, it is much faster to compare the centroids of the two clusters than test all $\\\\left|C_{1}\\\\right|\\\\left|C_{2}\\\\right|$ point-pairs in the simplest implementation. Of course, centroids can only be computed for records with all numerical values, but the algorithm can be adapted to use the centermost point in each cluster (medioid) as a representative in the general case.\\n  \\\\item furthest link\\\\index{furthest link}: Here the cost of merging two clusters is the farthest pair of points between them:\\n\\\\end{itemize}\\n\\n$$\\nd\\\\left(C_{1}, C_{2}\\\\right)=\\\\max _{x \\\\in C_{1}, y \\\\in C_{2}}\\\\|x-y\\\\|\\n$$\\n\\nThis sounds like madness, but this is the criteria which works hardest to keep clusters round, by penalizing mergers with distant outlier elements.\\n\\nWhich of these is best? As always in this business, it depends. For very large data sets, we are most concerned with using the fastest algorithms, which are typically single linkage or nearest centroid with appropriate data structures. For small to modest-sized data sets, we are most concerned with quality, making more robust methods attractive.\\n\\n%---- Page End Break Here ---- Page : 340\\n\\n\\\\subsection*{10.5.3 Comparing Clusterings}\\nIt is a common practice to try several clustering algorithms on the same data set, and use the one which looks best for our purposes. The clusterings produced by two different algorithms should be fairly similar if both algorithms are doing reasonable things, but it is often of interest to measure exactly how similar they are. This means we need to define a similarity or distance measure on clusterings.\\n\\nEvery cluster is defined by a subset of items, be they points or records. The Jaccard similarity\\\\index{Jaccard similarity} $J\\\\left(s_{1}, s_{2}\\\\right)$ of sets $s_{1}$ and $s_{2}$ is defined as the ratio of their intersection and union:\\n\\n$$\\nJ\\\\left(s_{1}, s_{2}\\\\right)=\\\\frac{\\\\left|s_{1} \\\\cap s_{2}\\\\right|}{\\\\left|s_{1} \\\\cup s_{2}\\\\right|}\\n$$\\n\\nBecause the intersection of two sets is always no bigger than the union of their elements, $0 \\\\leq J\\\\left(s_{1}, s_{2}\\\\right) \\\\leq 1$. Jaccard similarity is a generally useful measure to know about, for example, in comparing the similarity of the $k$ nearest neighbors of a point under two different distance metrics, or how often the top elements by one criteria match the top elements by a different metric.\\n\\nThis similarity measure can be turned into a proper distance metric $d\\\\left(s_{1}, s_{2}\\\\right)$ called the Jaccard distance\\\\index{Jaccard distance}, where\\n\\n$$\\nd\\\\left(s_{1}, s_{2}\\\\right)=1-J\\\\left(s_{1}, s_{2}\\\\right)\\n$$\\n\\nThis distance function only takes on values between 0 and 1 , but satisfies all of the properties of a metric, including the triangle inequality.\\n\\nEach clustering is described by a partition of the universal set, and may have many parts. The Rand index\\\\index{Rand index} is a natural measure of similarity between two clusterings $c_{1}$ and $c_{2}$. If the clusterings are compatible, then any pair of items in the same subset of $c_{1}$ should be in the same subset of $c_{2}$, and any pairs in different clusters of $c_{1}$ should be separated in $c_{2}$. The Rand index counts the number of such consistent pairs of items, and divides it by the total number of pairs $\\\\binom{n}{2}$ to create a ratio from 0 to 1 , where 1 denotes identical clusterings.\\n\\n\\\\subsection*{10.5.4 similarity graphs\\\\index{similarity graphs} and cut-based\\\\index{cut-based} Clustering}\\nRecall our initial discussion of clustering, where I asked asked how many clusters you saw in the point set repeated in Figure 10.21 . To come up with the reasonable answer of nine clusters, your internal clustering algorithm had to manage tricks like classifying a ring around a central blob as two distinct clusters, and avoid merging two lines that move suspiciously close to each other. $k$-means doesn\\'t have a chance of doing this, as shown in Figure 10.21 (left), because it always seeks circular clusters and is happy to split long stringy clusters. Of the agglomerative clustering procedures, only single-link with exactly the right threshold might have a chance to do the right thing, but it is easily fooled into merging two clusters by a single close point pair.\\n\\nClusters are not always round. Recognizing those that are not requires a high-enough density of points that are sufficiently contiguous that we are not\\\\\\n%---- Page End Break Here ---- Page : 341\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-357}\\n\\nFigure 10.21: The results of $k$-means (left) and cut\\\\index{cut}-based spectral clustering\\\\index{spectral clustering} (right) on our 9 -cluster example. Spectral clustering correctly finds connected clusters here that $k$-means cannot.\\\\\\\\\\ntempted to cut a cluster in two. We seek clusters that are connected in an appropriate similarity graph.\\n\\nAn $n \\\\times n$ similarity matrix\\\\index{similarity matrix} $S$ scores how much alike each pair of elements $p_{i}$ and $p_{j}$ are. Similarity is essentially the inverse of distance: when $p_{i}$ is close to $p_{j}$, then the item associated with $p_{i}$ must be similar to that of $p_{j}$. It is natural to measure similarity on a scale from 0 to 1 , where 0 represents completely different and 1 means identical. This can be realized by making $S[i, j]$ an inverse exponential function of distance, regulated by a parameter $\\\\beta$ :\\n\\n$$\\nS[i, j]=e^{-\\\\beta\\\\left\\\\|p_{i}-p_{j}\\\\right\\\\|}\\n$$\\n\\nThis works because $e^{0}=1$ and $e^{-x}=1 / e^{x} \\\\rightarrow 0$ as $x \\\\rightarrow \\\\infty$.\\\\\\\\\\nA similarity graph has a weighted edge $(i, j)$ between each pair of vertices $i$ and $j$ reflecting the similar of $p_{i}$ and $p_{j}$. This is exactly the similarity matrix described above. However, we can make this graph sparse by setting all small terms $(S[i, j] \\\\leq t$ for some threshold $t)$ to zero. This greatly reduces the number of edges in the graph. We can even turn it into an unweighted graph by setting the weight to 1 for all $S[i, j]>t$.\\n\\n\\\\section*{cuts\\\\index{cuts} in Graphs}\\nReal clusters in similarity graphs have the appearance of being dense regions which are only loosely connected to the rest of graph. A cluster $C$ has a weight which is a function of the edges within the cluster:\\n\\n$$\\nW(C)=\\\\sum_{x \\\\in C} \\\\sum_{y \\\\in C} S[i, j]\\n$$\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-358}\\n\\\\end{center}\\n\\nFigure 10.22: Low weight cuts in similarity graphs identify natural clusters.\\n\\nThe edges connecting $C$ to the rest of the graph define a cut, meaning the set of edges who have one vertex in $C$ and the other in the rest of the graph $(V-C)$. The weight of this cut $W^{\\\\prime}(C)$ is defined:\\n\\n$$\\nW^{\\\\prime}(C)=\\\\sum_{x \\\\in C} \\\\sum_{y \\\\in V-C} S[i, j]\\n$$\\n\\nIdeally clusters will have a high weight $W(C)$ but a small cut $W^{\\\\prime}(C)$, as shown in Figure 10.22 The conductance\\\\index{conductance} of cluster $C$ is the ratio of cut weight over internal weight $\\\\left(W^{\\\\prime}(C) / W(C)\\\\right)$, with better clusters having lower conductance.\\n\\nFinding low conductance clusters is a challenge. Help comes, surprisingly, from linear algebra. The similarity matrix $S$ is a symmetric matrix, meaning that it has an eigenvalue decomposition as discussed in Section 8.5 We saw that the leading eigenvector results in a blocky approximation to $S$, with the contribution of additional eigenvectors gradually improving the approximation. Dropping the smallest eigenvectors removes either details or noise, depending upon the interpretation.\\n\\nNote that the ideal similarity matrix is a blocky matrix, because within each cluster we expect a dense connection of highly-similar pairs, with little cross talk to vertices of other clusters. This suggests using the eigenvectors of $S$ to define robust features to cluster the vertices on. Performing $k$-means clu\\\\index{k-means clustering}stering on this transformed feature space will recover good clusters.\\n\\nThis approach is called spectral clustering. We construct an appropriately normalized similarity matrix called the Laplacian\\\\index{Laplacian}, where $L=D-S$ and $D$ is the degree-weighted identity matrix, so $D[i, i]=\\\\sum_{j} S[i, j]$. The $k$ most important eigenvectors of $L$ define an $n \\\\times k$ feature matrix. Curiously, the most valuable eigenvectors for clustering here turn out to have the smallest non-zero eigenvalues, due to special properties of the Laplacian matrix. Performing $k$ means clustering in this feature space generates highly connected clusters.\\n\\n%---- Page End Break Here ---- Page : 343\\n\\nTake-Home Lesson: What is the right clustering algorithm to use for your data? There are many possibilities to consider, but your most important decisions are:\\n\\n\\\\begin{itemize}\\n  \\\\item What is the right distance function to use?\\n  \\\\item What are you doing to properly normalize your variables?\\n  \\\\item Do your clusters look sensible to you when appropriately visualized? Understand that clustering is never perfect, because the algorithm can\\'t read your mind. But is it good enough?\\n\\\\end{itemize}\\n\\n\\\\subsection*{10.6 War Story: Cluster Bombing}\\nMy host at the research labs of a major media/tech company during my sabbatical was Amanda Stent, the leader of their natural language processing (NLP) group. She is exceptionally efficient, excessively polite, and generally imperturbable. But with enough provocation she can get her dander up, and I heard the exasperation in her voice when she muttered \"Product people!\"\\n\\nPart of her mission at the lab was to interface with company product groups which needed expertise in language technologies. The offenders here were with the news product, responsible for showing users recent articles of interest to them. The article clustering module was an important part of this effort, because it grouped together all articles written about the same story/event. Users did not want to read ten different articles about the same baseball game or Internet meme. Showing users repeated stories from a single article cluster proved highly annoying, and chased them away from our site.\\n\\nBut article clustering only helps when the clusters themselves are accurate.\\\\\\\\\\n\"This is the third time they have come to me complaining about the clustering. They never give me specific examples of what is wrong, just complaints that the clusters are not good enough. They keep sending me links to postings they find on Stack Overflow about new clustering algorithms, and ask if we should be using these instead.\"\\n\\nI agreed to talk to them for her.\\\\\\\\\\nFirst, I made sure that the product people understood that clustering is an ill-defined problem, and that no matter what algorithm they used, there were going to be occasional mistakes that they would have to live with. This didn\\'t mean that there wasn\\'t any room for improvement over their current clustering algorithm, but that they would have to temper any dreams of perfection.\\n\\nSecond, I told them that we could not hope to fix the problem until we were given clear examples of what exactly was going wrong. I asked them for twenty examples of article pairs which were co-clustered by the algorithm, but should not have been. And another twenty examples of article pairs which naturally belonged in the same cluster, yet this similarity was not recognized by the algorithm.\\n\\n%---- Page End Break Here ---- Page : 344\\n\\nThis had the desired effect. They readily agreed that my requests were sensible, and necessary to diagnose the problem. They told me they would get right on top of it. But this required work from their end, and everyone is busy with too many things. So I never heard back from them again, and was left to spend the rest of my sabbatical in a productive peace.\\n\\nMonths later, Amanda told me she had again spoken with the product people. Someone had discovered that their clustering module was only using the words from the headlines as features, and ignoring the entire contents of the actual article. There was nothing wrong with the algorithm per-se, only with the features, and it worked much better soon as it was given a richer feature set.\\n\\nWhat are the morals of this tale? A man has got to know his limitations, and so does a clustering algorithm. Go to Google News right now, and carefully study the article clusters. If you have a discerning eye you will find several small errors, and maybe something really embarrassing. But the more amazing thing is how well this works in the big picture, that you can produce an informative, non-redundant news feed algorithmically from thousands of different sources. Effective clustering is never perfect, but can be immensely valuable.\\n\\nThe second moral is that feature engineering and distance functions matter in clustering much more than the specific algorithmic approach. Those product people dreamed of a high-powered algorithm which would solve all their problems, yet were only clustering on the headlines. Headlines are designed to attract attention, not explain the story. The best newspaper headlines in history, such as \"Headless Body Found in Topless Bar\" and \"Ford to City, Drop Dead\" would be impossible to link to more sober ledes associated with the same stories.\\n\\n\\\\subsection*{10.7 Chapter Notes}\\nDistance computations are a basis of the field of computational geometry, the study of algorithms and data structures for manipulating point sets. Excellent introductions to computational geometry include [O\\'R01 dBvKOS00.\\n\\nSamet Sam06 is the best reference on kd-trees and other spatial data structures for nearest neighbor search. All major (and many minor) variants are developed in substantial detail. A shorter survey Sam05 is also available. Indyk Ind04 ably surveys recent results in approximate nearest neighbor search in high dimensions, based on random projection methods.\\n\\nGraph theory is the study of the abstract properties of graphs, with West Wes00 serving as an excellent introduction. Networks represent empirical connections between real-world entities for encoding information about them. Easley and Kleinberg [EK10] discuss the foundations of a science of networks in society.\\n\\nClustering, also known as cluster analysis, is a classical topic in statistics and computer science. Representative treatments include Everitt et al. [ELLS11] and James et al. JWHT13.\\n\\n%---- Page End Break Here ---- Page : 345\\n\\n\\\\subsection*{10.8 exercises\\\\index{exercises}}\\n\\\\section*{Distance Metrics}\\n10-1. [3] Prove that Euclidean distance is in fact a metric.\\\\\\\\[0pt]\\n10-2. [5] Prove that $L_{p}$ distance is a metric, for all $p \\\\geq 1$.\\\\\\\\[0pt]\\n10-3. [5] Prove that dimension-weighted $L_{p}$ distance is a metric, for all $p \\\\geq 1$.\\\\\\\\[0pt]\\n10-4. [3] Experiment with data to convince yourself that (a) cosine distance is not a true distance metric, and that (b) angular distance is a distance metric.\\\\\\\\[0pt]\\n10-5. [5] Prove that edit distance on text strings defines a metric.\\\\\\\\[0pt]\\n10-6. [8] Show that the expected distance between two points chosen uniformly and independently from a line of length 1 is $1 / 3$. Establish convincing upper and lower bounds on this expected distance for partial credit.\\n\\n\\\\section*{Nearest Neighbor Classification}\\n10-7. [3] What is the maximum number of nearest neighbors that a given point $p$ can have in two dimensions, assuming the possibility of ties?\\\\\\\\[0pt]\\n10-8. [5] Following up on the previous question, what is the maximum number of different points that can have a given point $p$ as its nearest neighbor, again in two dimensions?\\n\\n10-9. [3] Construct a two-class point set on $n \\\\geq 10$ points in two dimensions, where every point would be misclassified according to its nearest neighbor.\\\\\\\\[0pt]\\n10-10. [5] Repeat the previous question, but where we now classify each point according to its three nearest neighbors $(k=3)$.\\\\\\\\[0pt]\\n10-11. [5] Suppose a two-class, $k=1$ nearest-neighbor classifier is trained with at least three positive points and at least three negative points.\\\\\\\\\\n(a) Might it possible this classifier could label all new examples as positive?\\\\\\\\\\n(b) What if $k=3$ ?\\n\\n\\\\section*{Networks}\\n10-12. [3] Give explanations for what the nodes with the largest in-degree and outdegree might be in the following graphs:\\\\\\\\\\n(a) The telephone graph, where edge $(x, y)$ means $x$ calls $y$.\\\\\\\\\\n(b) The Twitter graph, where edge $(x, y)$ means $x$ follows $y$.\\n\\n10-13. [3] Power law distributions on vertex degree in networks usually result from preferential attachment, a mechanism by which new edges are more likely to connect to nodes of high degree. For each of the following graphs, suggest what their vertex degree distribution is, and if they are power law distributed describe what the preferential attachment mechanism might be.\\\\\\\\\\n(a) Social networks like Facebook or Instagram.\\\\\\\\\\n(b) Sites on the World Wide Web (WWW).\\\\\\\\\\n(c) Road networks connecting cities.\\\\\\n%---- Page End Break Here ---- Page : 346\\n\\\\\\n(d) Product/customer networks like Amazon or Netflix.\\n\\n10-14. [5] For each of the following graph-theoretic properties, give an example of a real-world network that satisfies the property, and a second network which does not.\\\\\\\\\\n(a) Directed vs. undirected.\\\\\\\\\\n(b) Weighted vs. unweighted.\\\\\\\\\\n(c) Simple vs. non-simple.\\\\\\\\\\n(d) Sparse vs. dense.\\\\\\\\\\n(e) Embedded vs. topological.\\\\\\\\\\n(f) Labeled vs. unlabeled.\\n\\n10-15. [3] Prove that in any simple graph, there are always an even number of vertices with odd vertex degree.\\\\\\\\[0pt]\\n10-16. [3] Implement a simple version of the PageRank algorithm, and test it on your favorite network. Which vertices get highlighted as most central?\\n\\n\\\\section*{Clustering}\\n10-17. [5] For a data set with points at positions $(4,10),(7,10)(4,8),(6,8),(3,4)$, $(2,2),(5,2),(9,3),(12,3),(11,4),(10,5)$, and $(12,6)$, show the clustering that results from\\\\\\\\\\n(a) Single-linkage clustering\\\\\\\\\\n(b) Average-linkage clustering\\\\\\\\\\n(c) Furthest-neighbor (complete linkage) clustering.\\n\\n10-18. [3] For each of the following The Quant Shop prediction challenges, propose available data that might make it feasible to employ nearest-neighbor/analogical methods to the task:\\\\\\\\\\n(a) Miss Universe.\\\\\\\\\\n(b) Movie gross.\\\\\\\\\\n(c) Baby weight.\\\\\\\\\\n(d) Art auction price.\\\\\\\\\\n(e) White Christmas.\\\\\\\\\\n(f) Football champions.\\\\\\\\\\n(g) Ghoul pool.\\\\\\\\\\n(h) Gold/oil prices.\\n\\n10-19. [3] Perform $k$-means clustering manually on the following points, for $k=2$ :\\n\\n$$\\nS=\\\\{(1,4),(1,3),(0,4),(5,1),(6,2),(4,0)\\\\}\\n$$\\n\\nPlot the points and the final clusters.\\n\\n10-20. [5] Implement two versions of a simple $k$-means algorithm: one of which uses numerical centroids as centers, the other of which restricts centers to be input points from the data set. Then experiment. Which algorithm converges faster on average? Which algorithm produces clusterings with lower absolute and mean-squared error, and by how much?\\\\\\\\[0pt]\\n10-21. [5] Suppose $s_{1}$ and $s_{2}$ are randomly selected subsets from a universal set with $n$ items. What is the expected value of the Jaccard similarity $J\\\\left(s_{1}, s_{2}\\\\right)$ ?\\\\\\\\[0pt]\\n10-22. [5] Identify a data set on entities where you have some sense of natural clusters which should emerge, be it on people, universities, companies, or movies. Cluster it by one or more algorithms, perhaps $k$-means and agglomerative clustering. Then evaluate the resulting clusters based on your knowledge of the domain. Did they do a good job? What things did it get wrong? Can you explain why the algorithm did not reconstruct what was in your head?\\\\\\\\[0pt]\\n10-23. [5] Assume that we are trying to cluster $n=10$ points in one dimension, where point $p_{i}$ has a position of $x=i$. What is the agglomerative clustering tree for these points under\\\\\\\\\\n(a) Single-link clustering\\\\\\\\\\n(b) Average-link clustering\\\\\\\\\\n(c) Complete-link/furthest-neighbor clustering\\n\\n10-24. [5] Assume that we are trying to cluster $n=10$ points in one dimension, where point $p_{i}$ has a position of $x=2^{i}$. What is the agglomerative clustering tree for these points under\\\\\\\\\\n(a) Single-link clustering\\\\\\\\\\n(b) Average-link clustering\\\\\\\\\\n(c) Complete-link/furthest-neighbor clustering\\n\\n\\\\section*{Implementation Projects}\\n$10-25$. [5] Do experiments studying the impact of merging criteria (single-link, centroid, average-link, furthest link) on the properties of the resulting cluster tree. Which leads to the tallest trees? The most balanced? How do their running times compare? Which method produces results most consistent with $k$-means clustering?\\\\\\\\[0pt]\\n10-26. [5] Experiment with the performance of different algorithms/data structures for finding the nearest neighbor of a query point $q$ among $n$ points in $d$ dimensions. What is the maximum $d$ for which each method remains viable? How much faster are heuristic methods based on LSH than methods that guarantee the exact nearest neighbor, at what loss of accuracy?\\n\\n\\\\section*{Interview Questions}\\n10-27. [5] What is curse of dimensionality? How does it affect distance and similarity measures?\\\\\\\\[0pt]\\n10-28. [5] What is clustering? Describe an example algorithm that performs clustering. How can we know whether it produced decent clusters on our data set?\\\\\\\\[0pt]\\n10-29. [5] How might we be able to estimate the right number of clusters to use with a given data set?\\n\\n%---- Page End Break Here ---- Page : 348\\n\\n10-30. [5] What is the difference between unsupervised and supervised learning?\\\\\\\\[0pt]\\n10-31. [5] How can you deal with correlated features in your data set by reducing the dimensionality of the data.\\\\\\\\[0pt]\\n10-32. [5] Explain what a local optimum is. Why is it important in $k$-means clustering?\\n\\n\\\\section*{Kaggle Challenges}\\n10-33. Which people are most influential in a given social network?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/predict-who-is-more-influential-in-a-social-network}{https://www.kaggle.com/c/predict-who-is-more-influential-in-a-social-network}\\n\\n10-34. Who is destined to become friends in an online social network?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/socialNetwork}{https://www.kaggle.com/c/socialNetwork}\\\\\\\\\\n10-35. Predict which product a consumer is most likely to buy.\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/coupon-purchase-prediction}{https://www.kaggle.com/c/coupon-purchase-prediction}\\n\\n\\\\section*{Chapter 11}\\n\\\\section*{Machine Learning}\\n\\\\begin{abstract}\\nAny sufficiently advanced form of cheating is indistinguishable from learning.\\n\\\\end{abstract}\\n\\n\\\\textbackslash author\\\\{\\n\\n\\\\begin{itemize}\\n  \\\\item Jan Schaumann\\\\\\\\\\n\\\\}\\n\\\\end{itemize}\\n\\nFor much of my career, I was highly suspicious of the importance of machine learning. I sat through many talks over the years, with grandiose claims and very meager results. But it is clear that the tide has turned. The most interesting work in computer science today revolves around machine learning, both powerful new algorithms and exciting new applications.\\n\\nThis revolution has occurred for several reasons. First, the volume of data and computing power available crossed a magic threshold where machine learning systems started doing interesting things, even using old approaches. This inspired greater activity in developing methods that scale better, and greater investment in data resources and system development. The culture of open source software deserves to take a bow, because new ideas turn into available tools amazingly quickly. Machine learning today is an exploding field with a great deal of excitement about it.\\n\\nWe have so far discussed two ways of building models based on data, linear regression and nearest neighbor approaches, both in fairly extensive detail. For many applications, this is all you will need to know. If you have enough labeled training data, all methods are likely to produce good results. And if you don\\'t, all methods are likely to fail. The impact of the best machine learning algorithm can make a difference, but generally only at the margins. I feel that the purpose of my book is to get you from crawling to walking, so that more specialized books can teach you how to run.\\n\\nThat said, a slew of interesting and important machine learning algorithms have been developed. We will review these methods here in this chapter, with the goal of understanding the strengths and weaknesses of each, along several relevant dimensions of performance:\\n\\n\\\\begin{itemize}\\n  \\\\item Power and expressibility: Machine learning methods differ in the richness and complexity of the models they support. Linear regression fits linear functions, while nearest neighbor methods define piecewise-linear separation boundaries with enough pieces to approximate arbitrary curves. Greater expressive power provides the possibility of more accurate models, as well as the dangers of overfitting.\\n  \\\\item Interpretability: Powerful methods like deep learning often produce models that are completely impenetrable. They might provide very accurate classification in practice, but no human-readable explanation of why they are making the decisions they do. In contrast, the largest coefficients in a linear regression model identify the most powerful features, and the identities of nearest neighbors enable us to independently determine our confidence in these analogies.\\\\\\\\\\nI personally believe that interpretability is an important property of a model, and am generally happier to take a lesser-performing model I understand over a slightly more accurate one that I don\\'t. This may not be a universally shared opinion, but you have a sense whether you really understand your model and its particular application domain.\\n  \\\\item Ease of use: Certain machine learning methods feature relatively few parameters or decisions, meaning they work right out of the box. Both linear regression and nearest neighbor classification are quite simple in this regard. In contrast, methods like support vector machines (SVMs) provide much greater scope to optimize algorithm performance with the proper settings. My sense is that the available tools for machine learning will continue to get better: easier to use and more powerful. But for now, certain methods allow the user enough rope to hang themselves if they don\\'t know what they are doing.\\n  \\\\item Training speed: Methods differ greatly in how fast they fit the necessary parameters of the model, which determines how much training data you can afford to use in practice. Traditional linear regression methods can be expensive to fit for large models. In contrast, nearest neighbor search requires almost no training time at all, outside that of building the appropriate search data structure.\\n  \\\\item Prediction speed: Methods differ in how fast they make classification decisions on a new query $q$. Linear/logistic regression is fast, just computing a weighted sum of the fields in the input records. In contrast, nearest neighbor search requires explicitly testing $q$ against a substantial amount of the training test. In general there is a trade-off with training speed: you can pay me now or pay me later.\\n\\\\end{itemize}\\n\\nFigure 11.1 presents my subjective ratings of roughly where the approaches discussed in this chapter fit along these performance dimensions. These ratings are not the voice of G-d, and reasonable people can have different opinions.\\n\\n%---- Page End Break Here ---- Page : 352\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{l|ccccc}\\nMethod & \\\\begin{tabular}{c}\\nPower of \\\\\\\\\\nExpression \\\\\\\\\\n\\\\end{tabular} & \\\\begin{tabular}{c}\\nEase of \\\\\\\\\\nInterpretation \\\\\\\\\\n\\\\end{tabular} & \\\\begin{tabular}{c}\\nEase of \\\\\\\\\\nUse \\\\\\\\\\n\\\\end{tabular} & \\\\begin{tabular}{c}\\nTraining \\\\\\\\\\nSpeed \\\\\\\\\\n\\\\end{tabular} & \\\\begin{tabular}{c}\\nPrediction \\\\\\\\\\nSpeed \\\\\\\\\\n\\\\end{tabular} \\\\\\\\\\n\\\\hline\\nLinear Regression & 5 & 9 & 9 & 9 & 9 \\\\\\\\\\nNearest Neighbor & 5 & 9 & 8 & 10 & 2 \\\\\\\\\\nnaive Bayes\\\\index{naive Bayes} & 4 & 8 & 7 & 9 & 8 \\\\\\\\\\nDecision Trees & 8 & 8 & 7 & 7 & 9 \\\\\\\\\\nSupport Vector Machines & 8 & 6 & 6 & 7 & 7 \\\\\\\\\\nBoosting & 9 & 6 & 6 & 6 & 6 \\\\\\\\\\nGraphical Models & 9 & 8 & 3 & 4 & 4 \\\\\\\\\\nDeep Learning & 10 & 3 & 4 & 3 & 7 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 11.1: Subjective rankings of machine learning approaches along five dimensions, on a 1 to 10 scale with higher being better.\\n\\nHopefully they survey the landscape of machine learning algorithms in a useful manner. Certainly no single machine learning method dominates all the others. This observation is formalized in the appropriately named no free lunch theorem, which proves there does not exist a single machine learning algorithm better than all the others on all problems.\\n\\nThat said, it is still possible to rank methods according to priority of use for practitioners. My ordering of methods in this book (and Figure 11.1) starts with the ones that are easy to use/tune, but have lower discriminative power than the most advanced methods. Generally speaking, I encourage you to start with the easy methods and work your way down the list if the potential improvements in accuracy really justify it.\\n\\nIt is easy to misuse the material I will present in this chapter, because there is a natural temptation to try all possible machine learning algorithms and pick whichever model gives the highest reported accuracy or $F 1$ score. Done naively through a single library call, which makes this easy, you are likely to discover that all models do about the same on your training data. Further, any performance differences between them that you do find are more likely attributable to variance than insight. Experiments like this is what statistical significance testing was invented for.\\n\\nThe most important factor that will determine the quality of your models is the quality of your features. We talked a lot about data cleaning in Chapter 33, which concerns the proper preparation of your data matrix. We delve more deeply into feature engineering in Section 11.5.4 before discussing deep learning methods that strive to engineer their own features.\\n\\nOne final comment. Data scientists tend to have a favorite machine learning approach, which they advocate for in a similar manner to their favorite programming language or sports team. A large part of this is experience, meaning that because they are most familiar with a particular implementation it works best in their hands. But part of it is magical thinking, the fact that they noticed one library slightly outperforming others on a few examples and inappropriately generalized.\\n\\nDon\\'t fall into this trap. Select methods which best fit the needs of your application based on the criteria above, and gain enough experience with their various knobs and levers to optimize performance.\\n\\n\\\\subsection*{11.1 Naive Bayes}\\nRecall that two events $A$ and $B$ are independent if $p(A$ and $B)=p(A) \\\\cdot p(B)$. If $A$ is the event that \"my favorite sports team wins today\" and $B$ is \"the stock market goes up today,\" then presumably $A$ and $B$ are independent. But this is not true in general. Consider the case if $A$ is the event that \"I get an A in Data Science this semester\" and $B$ is \"I get an A in a different course this semester.\" There are dependencies between these events: renewed enthusiasms for either study or drinking will affect course performance in a correlated manner. In the general case,\\n\\n$$\\np(A \\\\text { and } B)=p(A) \\\\cdot p(B \\\\mid A)=p(A)+P(B)-p(A \\\\text { or } B) .\\n$$\\n\\nIf everything was independent, the world of probability\\\\index{probability} would be a much simpler place. The naive Bayes classification algorithm crosses its fingers and assumes independence\\\\index{independence}, to avoid the need to compute these messy conditional probabilities.\\n\\n\\\\subsection*{11.1.1 formulation\\\\index{formulation}}\\nSuppose we wish to classify the vector $X=\\\\left(x_{1}, \\\\ldots x_{n}\\\\right)$ into one of $m$ classes $C_{1}, \\\\ldots, C_{m}$. We seek to compute the probability of each possible class given $X$, so we can assign $X$ the label of the class with highest probability. By Bayes theorem,\\n\\n$$\\np\\\\left(C_{i} \\\\mid X\\\\right)=\\\\frac{p\\\\left(C_{i}\\\\right) \\\\cdot p\\\\left(X \\\\mid C_{i}\\\\right)}{p(X)}\\n$$\\n\\nLet\\'s parse this equation. The term $p\\\\left(C_{i}\\\\right)$ is the prior probability\\\\index{prior probability}, the probability of the class label without any specific evidence. I know that you the reader are more likely to have black hair than red hair, because more people in the world have black hair than red hair $\\\\downarrow$\\n\\nThe denominator $P(X)$ gives the probability of seeing the given input vector $X$ over all possible input vectors. Establishing the exact value of $P(X)$ seems somewhat dicey, but mercifully is usually unnecessary. Observe that this denominator is the same for all classes. We only seek to establish a class label for $X$, so the value of $p(X)$ has no effect on our decision. Selecting the class with highest probability means\\n\\n$$\\nC(X)=\\\\underset{i=1, \\\\ldots, m}{\\\\arg \\\\max } \\\\frac{p\\\\left(C_{i}\\\\right) \\\\cdot p\\\\left(X \\\\mid C_{i}\\\\right)}{p(X)}=\\\\underset{i=1, \\\\ldots, m}{\\\\arg \\\\max } p\\\\left(C_{i}\\\\right) \\\\cdot p\\\\left(X \\\\mid C_{i}\\\\right) .\\n$$\\n\\n\\\\footnotetext{${ }^{1}$ Wikipedia claims that only $1-2 \\\\%$ of the world\\'s population are redheads.\\n}\\\\begin{center}\\n\\\\begin{tabular}{|c|c|c|c|c|c|c|c|}\\n\\\\hline\\n\\\\multirow[b]{2}{*}{Day} & \\\\multirow[b]{2}{*}{Outlook} & \\\\multirow[b]{2}{*}{Temp} & \\\\multirow[b]{2}{*}{Humidity} & \\\\multirow[b]{2}{*}{Beach?} & P (X|Class) & \\\\multicolumn{2}{|l|}{Probability in Class} \\\\\\\\\\n\\\\hline\\n &  &  &  &  & Outlook & Beach & No Beach \\\\\\\\\\n\\\\hline\\n1 & Sunny & High & High & Yes & Sunny & 3/4 & 1/6 \\\\\\\\\\n\\\\hline\\n2 & Sunny & High & Normal & Yes & Rain & 0/4 & 3/6 \\\\\\\\\\n\\\\hline\\n3 & Sunny & Low & Normal & No & Cloudy & 1/4 & 2/6 \\\\\\\\\\n\\\\hline\\n4 & Sunny & Mild & High & Yes & Temperature & Beach & No Beach \\\\\\\\\\n\\\\hline\\n5 & Rain & Mild & Normal & No & High & 3/4 & 2/6 \\\\\\\\\\n\\\\hline\\n6 & Rain & High & High & No & Mild & 1/4 & 2/6 \\\\\\\\\\n\\\\hline\\n7 & Rain & Low & Normal & No & Low & 0/4 & 2/6 \\\\\\\\\\n\\\\hline\\n8 & Cloudy & High & High & No & Humidity & Beach & No Beach \\\\\\\\\\n\\\\hline\\n9 & Cloudy & High & Normal & Yes & High & 2/4 & 2/6 \\\\\\\\\\n\\\\hline\\n10 & Cloudy & Mild & Normal & No & Normal & 2/4 & 4/6 \\\\\\\\\\n\\\\hline\\n &  &  &  &  & P(Beach Day) & 4/10 & 6/10 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 11.2: Probabilities to support a naive Bayes calculation on whether today is a good day to go to the beach: tabulated events (left) with marginal probability distributions (right).\\n\\nThe remaining term $p\\\\left(X \\\\mid C_{i}\\\\right)$ is the probability of seeing input vector $X$ given that we know the class of the item is $C_{i}$. This also seems somewhat dicey. What is the probability someone weighs 150 lbs and is 5 foot 8 inches tall, given that they are male? It should be clear that $p\\\\left(X \\\\mid C_{i}\\\\right)$ will generally be very small: there is a huge space of possible input vectors consistent with the class, only one of which corresponds to the given item.\\n\\nBut now suppose we lived where everything was independent, i.e. the probability of event $A$ and event $B$ was always $p(A) \\\\cdot p(B)$. Then\\n\\n$$\\np\\\\left(X \\\\mid C_{i}\\\\right)=\\\\prod_{j=1}^{n} p\\\\left(x_{j} \\\\mid C_{i}\\\\right)\\n$$\\n\\nNow anyone who really believes in a world of independent probabilities is quite naive, hence the name naive Bayes. But such an assumption really does make the computations much easier. Putting this together:\\n\\n$$\\nC(X)=\\\\underset{i=1, \\\\ldots, m}{\\\\arg \\\\max } p\\\\left(C_{i}\\\\right) \\\\cdot p\\\\left(X \\\\mid C_{i}\\\\right)=\\\\underset{i=1, \\\\ldots, m}{\\\\arg \\\\max } p\\\\left(C_{i}\\\\right) \\\\prod_{j=1}^{n} p\\\\left(x_{j} \\\\mid C_{i}\\\\right) .\\n$$\\n\\nFinally, we should hit the product with a $\\\\log$ to turn it to a sum, for better numerical stability. The logs of probabilities will be negative numbers, but less likely events are more negative than common ones. Thus the complete naive Bayes algorithm is given by the following formula:\\n\\n$$\\nC(X)=\\\\underset{i=1, \\\\ldots, m}{\\\\arg \\\\max }\\\\left(\\\\log \\\\left(p\\\\left(C_{i}\\\\right)\\\\right)+\\\\sum_{j=1}^{n} \\\\log \\\\left(p\\\\left(x_{j} \\\\mid C_{i}\\\\right)\\\\right)\\\\right)\\n$$\\n\\nHow do we calculate the $p\\\\left(x_{j} \\\\mid C_{i}\\\\right)$, the probability of observation $x_{j}$ given class label $i$ ? This is easy from the training data, particularly if $x_{j}$ is a categorical variable, like \"has red hair.\" We can simply select all class $i$ instances\\\\\\\\\\nin the training set, and compute the fraction of them which have property $x_{j}$. This fraction defines a reasonable estimate of $p\\\\left(x_{j} \\\\mid C_{i}\\\\right)$. A bit more imagination is needed when $x_{j}$ is a numerical variable, like \"age $=18$ \" or \"the word dog occurred six times in the given document,\" but in principle is computed by how often this value is observed in the training set.\\n\\nFigure 11.2 illustrates the naive Bayes procedure. On the left, it presents a table of ten observations of weather conditions, and whether each observation proved to be a day to go to the beach, or instead stay home. This table has been broken down on the right, to produce conditional probabilities of the weather condition given the activity. From these probabilities, we can use Bayes theorem to compute:\\n\\n$$\\n\\\\begin{aligned}\\n& P(\\\\text { Beach } \\\\mid \\\\text { (Sunny,Mild,High })) \\\\\\\\\\n& \\\\quad=(P(\\\\text { Sunny } \\\\mid \\\\text { Beach }) \\\\times P(\\\\text { Mild } \\\\mid \\\\text { Beach }) \\\\times P(\\\\text { High } \\\\mid \\\\text { Beach }) \\\\times P(\\\\text { Beach }) \\\\\\\\\\n& \\\\quad=(3 / 4) \\\\times(1 / 4) \\\\times(2 / 4) \\\\times(4 / 10)=0.0375\\n\\\\end{aligned}\\n$$\\n\\n$$\\n\\\\begin{aligned}\\n& P(\\\\text { No Beach } \\\\mid(\\\\text { Sunny,Mild,High })) \\\\\\\\\\n& \\\\quad=(P(\\\\text { Sunny } \\\\mid \\\\text { No }) \\\\times P(\\\\text { Mild } \\\\mid \\\\text { No }) \\\\times P(\\\\text { High } \\\\mid \\\\text { No })) \\\\times P(\\\\text { No }) \\\\\\\\\\n& \\\\\\\\\\n& \\\\quad=(1 / 6) \\\\times(2 / 6) \\\\times(2 / 6) \\\\times(6 / 10)=0.0111\\n\\\\end{aligned}\\n$$\\n\\nSince $0.0375>0.0111$, naive Bayes is telling us to hit the beach. Note that it is irrelevant that this particular combination of (Sunny,Mild,High) appeared in the training data. We are basing our decision on the aggregate probabilities, not a single row as in nearest neighbor classification.\\n\\n\\\\subsection*{11.1.2 Dealing with Zero Counts (Discounting)}\\nThere is a subtle but important feature preparation issue particularly associated with the naive Bayes algorithm. Observed counts do not accurately capture the frequency of rare events, for which there is typically a long tail.\\n\\nThe issue was first raised by the mathematician Laplace, who asked: What is the probability the sun will rise tomorrow? It may be close to one, but it ain\\'t exactly 1.0. Although the sun has risen like clockwork each morning for the 36.5 million mornings or so since man started noticing such things, it will not do so forever. The time will come where the earth or sun explodes, and so there is a small but non-zero chance that tonight\\'s the night.\\n\\nThere can always be events which have not yet been seen in any finite data set. You might well have records on a hundred people, none of whom happen to have red hair. Concluding that the probability of red hair is $0 / 100=0$ is potentially disastrous when we are asked to classify someone with red hair, since the probability of them being in each and every class will be zero. Even worse would be if there was exactly one redhead in the entire training set, say labeled\\\\\\n%---- Page End Break Here ---- Page : 356\\n\\\\\\nwith class $C_{2}$. Our naive Bayes classifier would decide that every future redhead just had to be in class $C_{2}$, regardless of other evidence.\\n\\nDiscounting is a statistical technique to adjust counts for yet-unseen events, by explicitly leaving probability mass available for them. The simplest and most popular technique is add-one discounting, where we add one to the frequency all outcomes, including unseen. For example, suppose we were drawing balls from an urn. After seeing five reds and three greens, what is the probability we will see a new color on the next draw? If we employ add-one discounting,\\n\\n$$\\n\\\\begin{gathered}\\nP(\\\\text { red })=(5+1) /((5+1)+(3+1)+(0+1))=6 / 11, \\\\text { and } \\\\\\\\\\nP(\\\\text { green })=(3+1) /((5+1)+(3+1)+(0+1))=4 / 11,\\n\\\\end{gathered}\\n$$\\n\\nleaving the new color a probability mass of\\n\\n$$\\nP(\\\\text { new-color })=1 /((5+1)+(3+1)+(0+1))=1 / 11\\n$$\\n\\nFor small numbers of samples or large numbers of known classes, the discounting causes a non-trivial damping of the probabilities. Our estimate for the probability of seeing a red ball changes from $5 / 8=0.625$ to $6 / 11=0.545$ when we employ add-one discounting. But this is a safer and more honest estimate, and the differences will disappear into nothingness after we have seen enough samples.\\n\\nYou should be aware that other discounting methods have been developed, and adding one might not be the best possible estimator in all situations. That said, not discounting counts is asking for trouble, and no one will be fired for using the add-one method.\\n\\nDiscounting becomes particularly important in natural language processing, where the traditional bag of words representation models a document as a word frequency count vector over the language\\'s entire vocabulary, say 100,000 words. Because word usage frequency is governed by a power law (Zipf\\'s law), words in the tail are quite rare. Have you ever seen the English word defenestrate before? 2 Even worse, documents of less than book length are too short to contain 100,000 words, so we are doomed to see zeros wherever we look. Addone discounting turns these count vectors into sensible probability vectors, with non-zero probabilities of seeing rare and so far unencountered words.\\n\\n\\\\subsection*{11.2 Decision Tree Classifiers}\\nA decision tree is a binary branching structure used to classify an arbitrary input vector $X$. Each node in the tree contains a simple feature comparison against some field $x_{i} \\\\in X$, like \"is $x_{i} \\\\geq 23.7$ ?\" The result of each such comparison is either true or false, determining whether we should proceed along to the left or right child of the given node. These structures are sometimes called classification and regression trees (CART) because they can be applied to a broader class of problems.\\n\\n\\\\footnotetext{${ }^{2}$ It means to throw someone out the window.\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-372}\\n\\nFigure 11.3: Simple decision tree for predicting mortality on the Titanic\\\\index{Titanic}.\\n\\nThe decision tree partitions training examples into groups of relatively uniform class composition, so the decision then becomes easy. Figure 11.3 presents an example of a decision tree, designed to predict your chances of surviving the shipwreck of the Titanic. Each row/instance travels a unique root-to-leaf path to classification. The root test here reflects the naval tradition of women and children first: $73 \\\\%$ of the women survived, so this feature alone is enough to make a prediction for women. The second level of the tree reflects children first: any male 10 years or older is deemed out of luck. Even the younger ones must pass one final hurdle: they generally made it to a lifeboat only if they had brothers and sisters to lobby for them.\\n\\nWhat is the accuracy of this model on the training data? It depends upon what faction of the examples end on each leaf, and how pure these leaf samples are. For the example of Figure 11.3 augmented with coverage percentage and survival fraction (purity) at each node, the classification accuracy $A$ of this tree is:\\n\\n$$\\nA=(0.35)(73 \\\\%)+(0.61)(83 \\\\%)+(0.02)(95 \\\\%)+(0.02)(89 \\\\%)=78.86 \\\\%\\n$$\\n\\nAn accuracy of $78.86 \\\\%$ is not bad for such a simple decision procedure. We could have driven it up to $100 \\\\%$ by completing the tree so each of the 1317 passengers had a leaf to themselves, labeling that node with their ultimate fate. Perhaps 23 -year-old second-class males were more likely to survive than either 22- or 24 -year-old males, an observation the tree could leverage for higher training accuracy. But such a complicated tree would be wildly overfit, finding structure that isn\\'t meaningfully there. The tree in Figure 11.3 is interpretable, robust, and reasonably accurate. Beyond that, it is every man for himself.\\n\\nAdvantages of decision trees include:\\n\\n\\\\begin{itemize}\\n  \\\\item non-linearity\\\\index{non-linearity}: Each leaf represents a chunk of the decision space, but reached through a potentially complicated path. This chain of logic permits decision trees to represent highly complicated decision boundaries.\\n \\n%---- Page End Break Here ---- Page : 358\\n \\\\item Support for categorical variables: Decision trees make natural use of categorical variables, like \"if hair color $=$ red,\" in addition to numerical data. Categorical variables fit less comfortably into most other machine learning methods.\\n  \\\\item Interpretability: Decision trees are explainable; you can read them and understand what their reasoning is. Thus decision tree algorithms can tell you something about your data set that you might not have seen before. Also, interpretability lets you vet whether you trust the decisions it will make: is it making decisions for the right reasons?\\n  \\\\item Robustness: The number of possible decision trees grows exponentially in the number of features and possible tests, which means that we can build as many as we wish. Constructing many random decision trees (CART) and taking the result of each as a vote for the given label increases robustness, and permits us to assess the confidence of our classification.\\n  \\\\item Application to regression: The subset of items which follow a similar path down a decision tree are likely similar in properties other than just label. For each such subset, we can use linear regression to build a special prediction model for the numerical values of such leaf items. This will presumably perform better than a more general model trained over all instances.\\n\\\\end{itemize}\\n\\nThe biggest disadvantage of decision trees is a certain lack of elegance. Learning methods like logistic regression and support vector machines use math. Advanced probability theory, linear algebra, higher-dimensional geometry. You know, math.\\n\\nBy contrast, decision trees are a hacker\\'s game. There are many cool knobs to twist in the training procedure, and relatively little theory to help you twist them in the right way.\\n\\nBut the fact of the matter is that decision tree models work very well in practice. Gradient boosted decision trees (GBDTs) are currently the most frequently used machine learning method to win Kaggle competitions. We will work through this in stages. First decision trees, then boosting in the subsequent section.\\n\\n\\\\subsection*{11.2.1 Constructing Decision Trees}\\nDecision trees are built in a top-down manner. We start from a given collection of training instances, each with $n$ features and labeled with one of $m$ classes $C_{1}, \\\\ldots, C_{m}$. Each node in the decision tree contains a binary predicate, a logic condition derived from a given feature.\\n\\nFeatures with a discrete set of values $v_{i}$ can easily be turned into binary predicates through equality testing: \"is feature $x_{i}=v_{i j}$ ?\" Thus there are $\\\\left|v_{i}\\\\right|$ distinct predicates associated with $x_{i}$. Numerical features can be turned into binary predicates with the addition of a threshold $t$ : \"is feature $x_{i} \\\\geq t$ ?\"\\n\\nThe set of potentially interesting thresholds $t$ are defined by the gaps between the observed values that $x_{i}$ takes on in the training set. If the complete set of observations of $x_{i}$ are ( $10,11,11,14,20$ ), the meaningful possible values for $t \\\\in(10,11,14)$ or perhaps $t \\\\in(10.5,11.5,17)$. Both threshold sets produce the same partitions of the observations, but using the midpoints of each gap seems sounder when generalizing to future values unseen in training.\\n\\nWe need a way to evaluate each predicate for how well it will contribute to partitioning the set $S$ of training examples reachable from this node. An ideal predicate $p$ would be a pure partition\\\\index{pure partition} of $S$, so that the class labels are disjoint. In this dream all members of $S$ from each class $C_{i}$ will appear exclusively on one side of the tree, however, such purity is not usually possible. We also want predicates that produce balanced splits of $S$, meaning that the left subtree contains roughly as many elements from $S$ as the right subtree. Balanced splits make faster progress in classification, and also are potentially more robust. Setting the threshold $t$ to the minimum value of $x_{i}$ picks off a lone element from $S$, produces a perfectly pure but maximally imbalanced split.\\n\\nThus our selection criteria should reward both balance and purity, to maximize what we learn from the test. One way to measure the purity of an item subset $S$ is as the converse of disorder, or entropy. Let $f_{i}$ denote the fraction of $S$ which is of class $C_{i}$. Then the information theoretic entropy\\\\index{information theoretic entropy} of $S, H(S)$, can be computed:\\n\\n$$\\nH(S)=-\\\\sum_{i=1}^{m} f_{i} \\\\log _{2} f_{i}\\n$$\\n\\nThe negative sign here exists to make the entire quantity positive, since the logarithm of a proper fraction is always negative.\\n\\nLet\\'s parse this formula. The purest possible contribution occurs when all elements belong to a single class, meaning $f_{j}=1$ for some class $j$. The contribution of class $j$ to $H(S)$ is $1 \\\\log _{2}(1)=0$, identical to that of all other classes: $0 \\\\cdot \\\\log _{2}(0)=0$. The most disordered version is when all $m$ classes are represented equally, meaning $f_{i}=1 / m$. Then $H(S)=\\\\log _{2}(m)$ by the above definition. The smaller the entropy, the better the node is for classification.\\n\\nThe value of a potential split applied to a tree node is how much it reduces the entropy of the system. Suppose a Boolean predicate $p$ partitions $S$ into two disjoint subsets, so $S=S_{1} \\\\cup S_{2}$. Then the information gain\\\\index{information gain} of $p$ is defined\\n\\n$$\\nI G_{p}(S)=H(S)-\\\\sum_{j=1}^{2} \\\\frac{\\\\left|S_{i}\\\\right|}{|S|} H\\\\left(S_{i}\\\\right)\\n$$\\n\\nWe seek the predicate $p^{\\\\prime}$ which maximizes this information gain, as the best splitter for $S$. This criteria implicitly prefers balanced splits since both sides of the tree are evaluated.\\n\\nAlternate measures of purity have been defined and are used in practice. The Gini impurity\\\\index{Gini impurity} is based on another quantity $\\\\left(f_{i}\\\\left(1-f_{i}\\\\right)\\\\right)$, which is zero in\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-375}\\n\\nFigure 11.4: The exclusive OR function cannot be fit by linear classifiers. On the left, we present four natural clusters in $x-y$ space. This demonstrates the complete inability of logistic regression to find a meaningful separator, even though a small decision tree easily does the job (right).\\\\\\\\\\nboth cases of pure splits, $f_{i}=0$ or $f_{i}=1$ :\\n\\n$$\\nI_{G}(f)=\\\\sum_{i=1}^{m} f_{i}\\\\left(1-f_{i}\\\\right)=\\\\sum_{i=1}^{m}\\\\left(f_{i}-f_{i}^{2}\\\\right)=\\\\sum_{i=1}^{m} f_{i}-\\\\sum_{i=1}^{m} f_{i}^{2}=1-\\\\sum_{i=1}^{m} f_{i}^{2}\\n$$\\n\\nPredicate selection criteria to optimize Gini impurity can be similarly defined.\\\\\\\\\\nWe need a stopping condition to complete the heuristic. When is a node pure enough to call it a leaf? By setting a threshold $\\\\epsilon$ on information gain, we stop dividing when the reward of another test is less than $\\\\epsilon$.\\n\\nAn alternate strategy is to build out the full tree until all leaves are completely pure, and then prune it back by eliminating nodes which contribute the least information gain. It is fairly common that a large universe may have no good splitters near the root, but better ones emerge as the set of live items gets smaller. This approach has the benefit of not giving up too early in the process.\\n\\n\\\\subsection*{11.2.2 Realizing Exclusive Or}\\nSome decision boundary shapes can be hard or even impossible to fit using a particular machine learning approach. Most notoriously, linear classifiers cannot be used to fit certain simple non-linear functions like eXclusive OR (XOR). The logic function $A \\\\oplus B$ is defined as\\n\\n$$\\nA \\\\oplus B=(A \\\\text { or } \\\\bar{B}) \\\\text { or }(\\\\bar{A} \\\\text { or } B) .\\n$$\\n\\nFor points $(x, y)$ in two dimensions, we can define predicates such that $A$ means \"is $x \\\\geq 0$ ?\" and $B$ means the \"is $y \\\\geq 0$ ?\". Then there are two distinct regions where $A \\\\oplus B$ true, opposing quadrants in this $x y$-plane shown in Figure\\\\\\\\\\n11.4 (left). The need to carve up two regions with one line explains why XOR is impossible for linear classifiers.\\n\\nDecision trees are powerful enough to recognize XOR. Indeed, the two-level tree in Figure 11.4 (right) does the job. After the root tests whether $A$ is true or false, the second level tests for $B$ are already conditioned on $A$, so each of the four leaves can be associated with a distinct quadrant, allowing for proper classification.\\n\\nAlthough decision trees can recognize XOR, that doesn\\'t mean it is easy to find the tree that does it. What makes XOR hard to deal with is that you can\\'t see yourself making progress toward better classification, even if you pick the correct root node. In the example above, choosing a root node of \"is $x>0$ ?\" causes no apparent enrichment of class purity on either side. The value of this test only becomes apparent if we look ahead another level, since the information gain is zero.\\n\\nGreedy decision tree construction heuristics fail on problems like XOR. This suggests the value of more sophisticated and computationally expensive tree building procedures in difficult cases, which look-ahead like computer chess programs, evaluating the worth of move $p$ not now, but how it looks several moves later.\\n\\n\\\\subsection*{11.2.3 Ensembles of Decision Trees}\\nThere are an enormous number of possible decision trees which can be built on any training set $S$. Further, each of them will classify all training examples perfectly, if we keep refining until all leaves are pure. This suggests building hundreds or even thousands of different trees, and evaluating a query item $q$ against each of them to return a possible label. By letting each tree cast its own independent vote, we gain confidence that the most commonly seen label will be the right label.\\n\\nFor this to avoid group-think, we need the trees to be diverse. Repeatedly using a deterministic construction procedure that finds the best tree is worthless, because they will all be identical. Better would be to randomly select a new splitting dimension at each tree node, and then find the best possible threshold for this variable to define the predicate.\\n\\nBut even with random dimension selection, the resulting trees often are highly correlated. A better approach is bagging, building the best possible trees on relatively small random subsets of items. Done properly, the resulting trees should be relatively independent of each other, providing a diversity of classifiers to work with, facilitating the wisdom of crowds.\\n\\nUsing ensembles of decision trees has another advantage beyond robustness. The degree of consensus among the trees offers a measure of confidence for any classification decision. There is a big difference in the majority label appearing in 501 of 1000 trees vs. 947 of them.\\n\\nThis fraction can be interpreted as a probability, but even better might be to feed this number into logistic regression for a better motivated measure of confidence. Assuming we have a binary classification problem, let $f_{i}$ denote the\\\\\\n%---- Page End Break Here ---- Page : 362\\n\\\\\\nfraction of trees picking class $C_{1}$ on input vector $X_{i}$. Run the entire training set through the decision tree ensemble. Now define a logistic regression problem where $f_{i}$ is input variable and the class of $X_{i}$ the output variable. The resulting logit function will determine an appropriate confidence level, for any observed fraction of agreement.\\n\\n\\\\subsection*{11.3 boosting\\\\index{boosting} and Ensemble Learning}\\nThe idea of aggregating large numbers of noisy \"predictors\" into one stronger classifier applies to algorithms as well as crowds. It is often the case that many different features all weakly correlate with the dependent variable. So what is the best way we can combine them into one stronger classifier?\\n\\n\\\\subsection*{11.3.1 Voting with Classifiers}\\nEnsemble learning is the strategy of combining many different classifiers into one predictive unit. The naive Bayes approach of Section 11.1 has a little of this flavor, because it uses each feature as a separate relatively weak classifier, then multiplies them together. Linear/logistic regression has a similar interpretation, in that it assigns a weight to each feature to maximize the predictive power of the ensemble.\\n\\nBut more generally, ensemble learning revolves on the idea of voting. We saw that decision trees can be more powerful in aggregate, by constructing hundreds or thousands of them over random subsets of examples. The wisdom of crowds comes from the triumph of diversity of thought over the individual with greatest expertise.\\n\\nDemocracy rests on the principle of one man, one vote. Your educated, reasoned judgment of the best course of action counts equally as the vote of that loud-mouthed idiot down the hall. Democracy makes sense in terms of the dynamics of society: shared decisions generally affect the idiot just as much as they do you, so equality dictates that all people deserve an equal say in the matter.\\n\\nBut the same argument does not apply to classifiers. The most natural way to use multiple classifiers gives each a vote, and takes the majority label. But why should each classifier get the same vote?\\n\\nFigure 11.5 captures some of the complexity of assigning weights to classifiers. The example consists of five voters, each classifying five items. All voters are pretty good, each getting $60 \\\\%$ correct, with the exception of $v_{1}$, who batted $80 \\\\%$. The majority option proves no better than the worst individual classifier, however, at $60 \\\\%$. But a perfect classifier results if we drop voters $v_{4}$ and $v_{5}$ and weigh the remainders equally. What makes $v_{2}$ and $v_{3}$ valuable is not their overall accuracy, but their performance on the hardest problems ( $D$ and especially $E)$.\\n\\nThere seem to be three primary ways to assign weights to the classifier/voters. The simplest might be to give more weight to the votes of classifiers who have\\n\\n%---- Page End Break Here ---- Page : 363\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|ccccc|cc}\\nItem/voter & $V_{1}$ & $V_{2}$ & $V_{3}$ & $V_{4}$ & $V_{5}$ & Majority & Best weights \\\\\\\\\\n\\\\hline\\nA & $*$ &  & $*$ & $*$ & $*$ & $*$ & $*$ \\\\\\\\\\nB & $*$ &  & $*$ & $*$ & $*$ & $*$ & $*$ \\\\\\\\\\nC & $*$ & $*$ &  & $*$ & $*$ & $*$ & $*$ \\\\\\\\\\nD & $*$ & $*$ &  &  &  &  & $*$ \\\\\\\\\\nE &  & $*$ & $*$ &  &  &  & $*$ \\\\\\\\\\n\\\\hline\\n\\\\% correct & $80 \\\\%$ & $60 \\\\%$ & $60 \\\\%$ & $60 \\\\%$ & $60 \\\\%$ & $60 \\\\%$ & $100 \\\\%$ \\\\\\\\\\nbest weight & $1 / 3$ & $1 / 3$ & $1 / 3$ & 0 & 0 &  &  \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 11.5: Uniform weighting of votes does not always produce the best possible classifier, even when voters are equally accurate, because some problem instances are harder than others (here, $D$ and $E$ ). The \"*\" denotes that the given voter classified the given item correctly.\\\\\\\\\\nproven accurate in the past, perhaps assigning $v_{i}$ the multiplicative weight $t_{i} / T$, where $t_{i}$ is the number of times $v_{i}$ classified correctly and $T=\\\\sum_{i=1}^{c} t_{i}$. Note that this weighting scheme would do no better than majority rule on the example of Figure 11.5\\n\\nA second approach could be to use linear/logistic regression to find the best possible weights. In a binary classification problem, the two classes would be denoted as 0 and 1 , respectively. The $0-1$ results from each classifier can be used as a feature to predict the actual class value. This formulation would find non-uniform weights that favor classifiers correlated with the correct answers, but do not explicitly seek to maximize the number of correct classifications.\\n\\n\\\\subsection*{11.3.2 Boosting Algorithms}\\nThe third idea is boosting. The key point is to weigh the examples according to how hard they are to get right, and reward classifiers based on the weight of the examples they get right, not just the count.\\n\\nTo set the weights of the classifier, we will adjust the weights of the training examples. Easy training examples will be properly classified by most classifiers: we reward classifiers more for getting the hard cases right.\\n\\nA representative boosting algorithm is AdaBoost\\\\index{AdaBoost}, presented in Figure 11.6. We will not stress the details here, particularly the specifics of the weight adjustments in each round. We presume our classifier will be constructed as the union of non-linear classifiers of the form \"is $\\\\left(v_{i} \\\\geq t_{i}\\\\right)$ ?\", i.e. using thresholded features as classifiers.\\n\\nThe algorithm proceeds in $T$ rounds, for $t=\\\\{0, \\\\ldots, T\\\\}$. Initially all training examples (points) should be of equal weight, so $w_{i, 0}=1 / n$ for all points $x_{1}, \\\\ldots, x_{n}$. We consider all possible feature/threshold classifiers, and identify the $f_{i}(x)$ which minimizes $\\\\epsilon_{t}$, the sum of the weights of the misclassified points. The weight $\\\\alpha_{t}$ of the new classifier depends upon how accurate it is on the\\n\\n\\\\section*{AdaBoost}\\nFor $t$ in $1 \\\\ldots T$ :\\n\\n\\\\begin{itemize}\\n  \\\\item Choose $f_{t}(x)$ :\\n  \\\\item Find weak learner $h_{t}(x)$ that minimizes $\\\\epsilon_{t}$, the weighted sum error for misclassified points $\\\\epsilon_{t}=\\\\sum_{i} w_{i, t}$\\n  \\\\item Choose $\\\\alpha_{t}=\\\\frac{1}{2} \\\\ln \\\\left(\\\\frac{1-\\\\epsilon_{t}}{\\\\epsilon_{t}}\\\\right)$\\n  \\\\item Add to ensemble:\\n  \\\\item $F_{t}(x)=F_{t-1}(x)+\\\\alpha_{t} h_{t}(x)$\\n  \\\\item Update weights:\\n  \\\\item $w_{i, t+1}=\\\\left(w_{i, t}\\\\right) e^{-y_{i} \\\\alpha_{t} h_{t}\\\\left(x_{i}\\\\right)}$ for all i\\n  \\\\item Renormalize $w_{i, t+1}$ such that $\\\\sum_{i} w_{i, t+1}=1$\\n\\\\end{itemize}\\n\\nFigure 11.6: Pseudocode for the AdaBoost algorithm.\\\\\\\\\\ncurrent point set, as measured by\\n\\n$$\\n\\\\alpha_{t}=\\\\frac{1}{2} \\\\ln \\\\left(\\\\frac{1-\\\\epsilon_{t}}{\\\\epsilon_{t}}\\\\right)\\n$$\\n\\nThe point weights are normalized so $\\\\sum_{i=1}^{n} w_{i}=1$, so there must always be a classifier with error $\\\\epsilon_{t} \\\\leq 0.53^{3}$\\n\\nIn the next round, the weights of the misclassified points are boosted to make them more important. Let $h_{t}\\\\left(x_{i}\\\\right)$ be the class ( -1 or 1 ) predicted for $x_{i}$, and $y_{i}$ the correct class or that point. The sign of $h_{t}\\\\left(x_{i}\\\\right) \\\\cdot y$ reflects whether the classes agree (positive) or disagree (negative). We then adjust the weights according to\\n\\n$$\\nw_{i, t+1}^{\\\\prime}=w_{i, t} e^{-y_{i} \\\\alpha_{t} h_{t}\\\\left(x_{i}\\\\right)}\\n$$\\n\\nbefore re-normalizing all of them so they continue to sum to 1 , i.e.\\n\\n$$\\nC=\\\\sum_{i=1}^{n} w_{i, t+1}^{\\\\prime}, \\\\quad \\\\text { and } \\\\quad w_{i, t+1}=w_{i, t+1}^{\\\\prime} / C\\n$$\\n\\nThe example in Figure 11.7 shows a final classifier as the linear sum of three thresholded single-variable classifiers. Think of them as the simplest possible\\n\\n\\\\footnotetext{${ }^{3}$ Consider two classifiers, one which calls class $C_{0}$ if $x_{i} \\\\geq t_{i}$, the other of which calls class $C_{1}$ if $x_{i} \\\\geq t_{i}$. The first classifier is right exactly when the second one is wrong, so one of these two must be at least $50 \\n%---- Page End Break Here ---- Page : 365\\n\\\\%$.\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-380}\\n\\nFigure 11.7: The final classifier is a weighted ensemble that correctly classifies all points, despite errors in each component classifier which are highlighted in red.\\\\\\\\\\ndecision trees, each with exactly one node. The weights assigned by AdaBoost are not uniform, but not so crazily skewed in this particular instance that they behave different than a majority classifier. Observe the non-linear decision boundary, resulting from the discrete nature of thresholded tests/decision trees.\\n\\nBoosting is particularly valuable when applied to decision trees as the elementary classifiers. The popular gradient boosted decision trees (GBDT) approach typically starts with a universe of small trees, with perhaps four to ten nodes each. Such trees each encode a simple-enough logic that they do not overfit the data. The relative weights assigned to each of these trees follows from a training procedure, which tries to fit the errors from the previous rounds (residuals) and increases the weights of the trees that correctly classified the harder examples.\\n\\nBoosting works hard to classify every training instance correctly, meaning it works particularly hard to classify the most difficult instances. There is an adage that \"hard cases make bad law,\" suggesting that difficult-to-decide cases make poor precedents for subsequent analysis. This is an important argument against boosting, because the method would seem prone to overfitting, although it generally performs well in practice.\\n\\nThe danger of overfitting is particularly severe when the training data is not a perfect gold standard. Human class annotations are often subjective and inconsistent, leading boosting to amplify the noise at the expense of the signal. The best boosting algorithms will deal with overfitting though regularization. The goal will be to minimize the number of non-zero coefficients, and avoid large coefficients that place too much faith in any one classifier in the ensemble.\\n\\nTake-Home Lesson: Boosting can take advantage of weak classifiers in an effective way. However, it can behave in particularly pathological ways when a fraction of your training examples are incorrectly annotated.\\n\\n\\\\subsection*{11.4 Support Vector Machines}\\nSupport vector machines (SVMs) are an important way of building non-linear classifiers. They can be viewed as a relative of logistic regression, which sought\\\\\\n%---- Page End Break Here ---- Page : 366\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-381}\\n\\nFigure 11.8: SVMs seek to separate the two classes by the largest margin, creating a channel around the separating line.\\\\\\\\\\nthe line/plane $l$ best separating points with two classes of labels. Logistic regression assigned a query point $q$ its class label depending upon whether $q$ lay above or below this line $l$. Further, it used the logit function to transform the distance from $q$ to $l$ into the probability that $q$ belongs in the identified class.\\n\\nThe optimization consideration in logistic regression involved minimizing the sum of the misclassification probabilities over all the points. By contrast, support vector machines work by seeking maximum margin linear separators between the two classes. Figure 11.8 (left) shows red and blue points separated by a line. This line seeks to maximize the distance $d$ to the nearest training point, the maximum margin of separation between red and blue. This a natural objective in building a decision boundary between two classes, since the larger the margin, the farther any of our training points are from being misclassified. The maximum margin classifier should be the most robust separator between the two classes.\\n\\nThere are several properties that help define the maximum margin separator\\\\index{maximum margin separator} between sets of red and blue points:\\n\\n\\\\begin{itemize}\\n  \\\\item The optimal line must be in the midpoint of the channel, a distance $d$ away from both the nearest red point and the nearest blue point. If this were not so, we could shift the line over until it did bisect this channel, thus enlarging the margin in the process.\\n  \\\\item The actual separating channel is defined by its contact with a small number of the red and blue points, where \"a small number\" means at most twice the number of dimensions of the points, for well-behaved point sets avoiding $d+1$ points lying on any $d$-dimensional face. This is different than with logistic regression, where all the points contribute to fitting the\\\\\\n%---- Page End Break Here ---- Page : 367\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-382}\\n\\\\end{itemize}\\n\\nFigure 11.9: Both logistic regression and SVMs produce separating lines between point sets, but optimized for different criteria.\\\\\\\\\\nbest position of the line. These contact points are the support vectors\\\\index{support vectors} defining the channel.\\n\\n\\\\begin{itemize}\\n  \\\\item Points inside the convex hull\\\\index{convex hull} of either the red or blue points have absolutely no effect on the maximum margin separator, since we need all same-colored points to be on the same side of the boundary. We could delete these interior points or move them around, but the maximum margin separator will not change until one of the points leaves the hull and enters the separating strip.\\n  \\\\item It is not always possible to perfectly separate red from blue by using a straight line. Imagine a blue point sitting somewhere within the convex hull of the red points. There is no way to carve this blue point away from the red using only a line.\\n\\\\end{itemize}\\n\\nLogistic regression and support vector machines\\\\index{machines} both produce separating lines between point sets. These are optimized for different criteria, and hence can be different, as shown in Figure 11.9. Logistic regression seeks the separator which maximizes the total confidence in our classification summed over all the points, while the wide margin separator of SVM does the best it can with the closest points between the sets. Both methods generally produce similar classifiers.\\n\\n%---- Page End Break Here ---- Page : 368\\n\\n\\\\subsection*{11.4.1 Linear SVMs}\\nThese properties define the optimization of linear support vector machines\\\\index{linear support vector machines}. The separating line/plane, like any other line/plane, can be written as\\n\\n$$\\nw \\\\cdot x-b=0\\n$$\\n\\nfor a vector of coefficients $w$ dotted with a vector of input variables $x$. The channel separating the two classes will be defined by two lines parallel to this and equidistant on both sides, namely $w \\\\cdot x-b=1$ and $w \\\\cdot x-b=1$.\\n\\nThe actual geometric separation between the lines depends upon $w$, namely $2 /\\\\|w\\\\|$. For intuition, think about the slope in two dimensions: these lines will be distance 2 apart for horizontal lines but negligibly far apart if they are nearly vertical. This separating channel must be devoid of points, and indeed separate red from blue points. Thus we must add constraints. For every red (class 1) point $x_{i}$, we insist that\\n\\n$$\\nw \\\\cdot x-b \\\\geq 1\\n$$\\n\\nwhile every blue (class -1 ) point $x_{i}$ must satisfy\\n\\n$$\\nw \\\\cdot x-b \\\\leq-1\\n$$\\n\\nIf we let $y_{i} \\\\in[-1,1]$ denote the class of $x_{i}$, then these can be combined to yield the optimization problem\\n\\n$$\\n\\\\max \\\\|w\\\\|, \\\\quad \\\\text { where } y_{i}\\\\left(w \\\\cdot x_{i}-b\\\\right) \\\\geq 1 \\\\text { for all } 1 \\\\leq i \\\\leq n\\n$$\\n\\nThis can be solved using techniques akin to linear programming\\\\index{linear programming}. Note that the channel must be defined by the points making contact with its boundaries. These vectors \"support\" the channel, which is where the provocative name support vector machines comes from. The optimization algorithm of efficient solvers like LibLinear and LibSVM search through the relevant small subsets of support vectors which potentially define separating channels to find the widest one.\\n\\nNote that there are more general optimization criteria for SVMs, which seek the line that defines a wide channel and penalizes (but does not forbid) points that are misclassified. This sort of dual-objective function (make the channel wide while misclassifying few points) can be thought of as a form of regularization, with a constant to trade off between the two objectives. Gradient descent search can be used to solve these general problems.\\n\\n\\\\subsection*{11.4.2 Non-linear SVMs}\\nSVMs define a hyperplane which separates the points from the two classes. Planes are lines in higher dimensions\\\\index{higher dimensions}, readily defined using linear algebra. So how can this linear method produce a non-linear decision boundary?\\n\\nFor a given point set to have a maximum margin separator, the two colors must first be linearly separable. But as we have seen this is not always the case. Consider the pathological case of Figure 11.10 (left), where the cluster of red\\\\\\n%---- Page End Break Here ---- Page : 369\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-384(1)}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-384}\\n\\nFigure 11.10: Projecting points to higher dimensions can make them linearly separable.\\\\\\\\\\npoints is surrounded by a ring-shaped cluster of black points. How might such a thing arise? Suppose we partition travel destinations into day trips or long trips, depending upon whether they are close enough to our given location. The longitude and latitudes of each possible destination will yield data with exactly the same structure as Figure 11.10 (left).\\n\\nThe key idea is that we can project our $d$-dimensional points into a higherdimensional space, where there will be more possibilities to separate them. For $n$ red/blue points along a line in one dimension, there are only $n-1$ potentially interesting ways to separate them, specifically with a cut between the $i$ th and $(i+1)$ st points for $1 \\\\leq i<n$. But this blows up to approximately $\\\\binom{n}{2}$ ways as we move to two dimensions, because there is more freedom to partition as we increase dimensionality. Figure 11.10 (right) demonstrates how lifting points through the transformation $(x, y) \\\\rightarrow\\\\left(x, y, x^{2}+y^{2}\\\\right)$ puts them on a paraboloid, and makes it possible to linearly separate classes which were inseparable in the original space.\\n\\nIf we jack the dimensionality of any two-class point set high enough, there will always be a separating line between the red and black points. Indeed, if we put the $n$ points in $n$ dimensions through a reasonable transform, they will always be linearly separable in a very simple way. For intuition, think about the special case of two points (one red and one blue) in two dimensions: obviously there must be a line separating them. Projecting this separating plane down to the original space results in some form of curved decision boundary, and hence the non-linearity of SVMs depends upon exactly how the input was projected to a higher-dimensional space.\\n\\nOne nice way to turn $n$ points in $d$ dimensions into $n$ points in $n$ dimensions\\\\\\\\\\nmight be to represent each point by its distances to all $n$ input points. In particular, for each point $p_{i}$ we can create a vector $v_{i}$ such that $v_{i j}=\\\\operatorname{dist}(i, j)$, the distance from $p_{i}$ to $p_{j}$. The vector of such distances should serve as a powerful set of features for classifying any new point $q$, since the distances to members of the actual class should be small compared to those of the other class.\\n\\nThis feature space is indeed powerful, and one can readily imagine writing a function to turn the original $n \\\\times d$ feature matrix into a new $n \\\\times n$ feature matrix for classification. The problem here is space, because the number of input points $n$ is usually vastly larger than the dimension $d$ that they sit in. Such a transform would be feasible to construct only for fairly small point sets, say $n \\\\leq 1000$. Further, working with such high-dimensional points should be very expensive, since every single distance evaluation now takes time linear in the number of points $n$, instead of the data dimension $d$. But something amazing happens...\\n\\n\\\\subsection*{11.4.3 kernels\\\\index{kernels}}\\nThe magic of SVMs is that this distance-feature matrix need not actually be computed explicitly. In fact, the optimization inherent in finding the maximum margin separator only performs the dot products of points with other points and vectors. Thus we could imagine performing the distance expansion on the fly, when the associated point is being used in a comparison. Hence there would be no need to precompute the distance matrix: we can expand the points from $d$ to $n$ dimensions as needed, do the distance computation, and then throw the expansions away.\\n\\nThis would work to eliminate the space bottleneck, but we would still pay a heavy price in computation time. The really amazing thing is that there are functions, called kernels, which return what is essentially the distance computation on the larger vector without ever constructing the larger vector. Doing SVMs with kernels gives us the power of finding the best separator over a variety of non-linear functions without much additional cost. The mathematics moves beyond the scope of what I\\'d like to cover here, but:\\n\\nTake-Home Lesson: Kernel functions are what gives SVMs their power to separate project $d$-dimensional points to $n$ dimensions, so they can be separated without spending more that $d$ steps on the computation.\\n\\nSupport vector machines require experience to use effectively. There are many different kernel functions available, beyond the distance kernel I presented here. Each have advantages on certain data sets, so there is a need to futz with the options of tools like LibSVM to get the best performance. They work best on medium-sized data sets, with thousands but not millions of points.\\n\\n%---- Page End Break Here ---- Page : 371\\n\\n\\\\subsection*{11.5 Degrees of Supervision}\\nThere is a natural distinction between machine learning approaches based on the degree and nature of the supervision employed in amassing training and evaluation data. Like any taxonomy, there is some fuzziness around the margins, making it an unsatisfying exercise to try to label exactly what a given system is and is not doing. However, like any good taxonomy it gives you a frame to guide your thinking, and suggests approaches that might lead to better results.\\n\\nThe methods discussed so far in this chapter assume that we are given training data with class labels or target variables, leaving our task as one to train classifier or regression systems. But getting to the point of having labeled data is usually the hard part. Machine learning algorithms generally perform better the more data you can give them, but annotation is often difficult and expensive. Modulating the degree of supervision provides a way to raise the volume so your classifier can hear what is going on.\\n\\n\\\\subsection*{11.5.1 Supervised Learning}\\nSupervised learning is the bread-and-butter paradigm for classification and regression problems. We are given vectors of features $x_{i}$, each with an associated class label or target value $y_{i}$. The annotations $y_{i}$ represent the supervision, typically derived from some manual process which limits the potential amount of training data.\\n\\nIn certain problems, the annotations of the training data come from observations in interacting with the world, or at least a simulation of it. Google\\'s AlphaGo program was the first computer program to beat the world champion at Go. A position evaluation function is a scoring function that takes a board position and computes a number estimating how strong it is. AlphaGo\\'s position evaluation function was trained on all published games by human masters, but much more data was needed. The solution was, essentially, to build a position evaluator by training against itself. Position evaluation is substantially enhanced by search - looking several moves ahead before calling the evaluation function on each leaf. Trying to predict the post-search score without the search produces a stronger evaluation function. And generating this training data is just a result of computation: the program playing against itself.\\n\\nThis idea of learning from the environment is called reinforcement learning. It cannot be applied everywhere, but it is always worth looking for clever approaches to generate mechanically-annotated training data.\\n\\n\\\\subsection*{11.5.2 Unsupervised Learning}\\nUnsupervised methods try to find structure in the data, by providing labels (clusters) or values (rankings) without any trusted standard. They are best used for exploration, for making sense of a data set otherwise untouched by human hands.\\n\\n%---- Page End Break Here ---- Page : 372\\n\\nThe mother of all unsupervised learning methods is clustering\\\\index{clustering}, which we discussed extensively in Section 10.5. Note that clustering can be used to provide training data for classification even in the absence of labels. If we presume that the clusters found represent genuine phenomenon, we can then use the cluster ID as a label for all the elements in the given cluster. These can now serve as training data to build a classifier to predict the cluster ID. Predicting cluster IDs can be useful even if these concepts do not have a name associated with them, providing a reasonable label for any input record $q$.\\n\\n\\\\section*{topic modeling\\\\index{topic modeling}}\\nAnother important class of unsupervised methods is topic modeling, typically associated with documents drawn over a given vocabulary. Documents are written about topics, usually a mix of topics. This book is partitioned into chapters, each of which is about a different topic, but it also touches on subjects ranging from baseball to weddings. But what is a topic? Typically each topic is associated with a particular set of vocabulary words. Articles about baseball mention hits, pitchers, strikeouts, bases, and slugging. Married, engaged, groom, bride, love, and celebrate are words associated with the topic of wedding. Certain words can represent multiple topics. For example love is also associated with tennis, and hits with gangsters.\\n\\nOnce one has a set of topics $\\\\left(t_{1}, \\\\ldots, t_{k}\\\\right)$ and the words which define them, the problem of identifying the specific topics associated with any given document $d$ seems fairly straightforward. We count the number of word occurrences of $d$ in common with $t_{i}$, and report success whenever this is high enough. If given a set of documents manually labeled with topics, it seems reasonable to count the frequency of each word over every topic class, to construct the list of words most strongly associated with each topic.\\n\\nBut that is all very heavily supervised. Topic modeling is an unsupervised approach that infers the topics and the word lists from scratch, just given unlabeled documents. We can represent these texts by a $w \\\\times d$ frequency matrix\\\\index{matrix} $F$, where $w$ is the vocabulary size and $d$ the number of documents and $F[i, j]$ reflects how many times work $i$ appears in document $j$. Suppose we factor $F$ into $F \\\\approx W \\\\times D$, where $W$ is a $w \\\\times t$ word-topic matrix and $D$ is a $t \\\\times d$ topicdocument matrix. The largest entries in the $i$ th row of $W$ reflect the topics word $w_{i}$ is most strongly linked to, while the largest entries in the $j$ th column of $D$ reflect the topics best represented in document $d_{j}$.\\n\\nSuch a factorization would represent a completely unsupervised form of learning, with the exception of specifying the desired number of topics $t$. It seems a messy process to construct such an approximate factorization, but there are a variety of approaches to try to do so. Perhaps the most popular method for topic modeling is an approach called latent Dirichlet allocation\\\\index{latent Dirichlet allocation} (LDA), which produces a similar set of matrices $W$ and $D$, although not strictly produced by factorization.\\n\\nFigure 11.5 .2 presents a toy example of LDA in action. Three excellent books were analyzed, with the goal of seeing how they were organized among\\n\\n%---- Page End Break Here ---- Page : 373\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\\n\\\\hline\\n\\\\multirow[b]{2}{*}{Text} & \\\\multirow[b]{2}{*}{$T_{1}$} & \\\\multirow[b]{2}{*}{$T_{2}$} & \\\\multirow[b]{2}{*}{$T_{3}$} & \\\\multicolumn{2}{|c|}{$T_{1}$} & \\\\multicolumn{2}{|l|}{$T_{2}$} & \\\\multicolumn{2}{|c|}{$T_{3}$} \\\\\\\\\\n\\\\hline\\n &  &  &  & Term & Weight & Term & Weight & Term & Weight \\\\\\\\\\n\\\\hline\\nThe Bible & ${ }^{1}$ & ${ }^{\\\\text {L }}$ 2 & ${ }^{1}$ & God & 0.028 & CPU & 0.021 & past & 0.013 \\\\\\\\\\n\\\\hline\\nData Sci Manual & 0.73 0.05 & 0.83 & 0.26 0.12 & Jesus & 0.012 & computer & 0.010 & history & 0.011 \\\\\\\\\\n\\\\hline\\nWho\\'s Bigger? & 0.05 0.08 & 0.83 0.23 & 0.69 & pray & 0.006 & data & 0.005 & old & 0.006 \\\\\\\\\\n\\\\hline\\nWho s Bigger? & 0.08 & 0.23 & 0.69 & Israel & 0.003 & program & 0.003 & war & 0.004 \\\\\\\\\\n\\\\hline\\n &  &  &  & Moses & 0.001 & math & 0.002 & book & 0.002 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 11.11: Illustration of topic modeling (LDA). The three books are represented by their distribution of topics (left). Each topic is represented by a list of words, with the weight a measure of its importance to the topic (right). Documents are made up of words: the magic of LDA is that it simultaneously infers the topics and word assignments in an unsupervised manner.\\\\\\\\\\nthree latent topics. The LDA algorithm defined these topics in an unsupervised way, by assigning each word weights for how much it contributes to each topic. The results here are generally effective: the concept of each topic emerges from its most important words (on the right). And the word distribution within each book can then be readily partitioned among the three latent topics (on the left).\\n\\nNote that this factorization mindset can be applied beyond documents, to any feature matrix $F$. The matrix decomposition approaches we have previously discussed, like singular value decomposition and principle components analysis, are equally unsupervised, inducing structure inherent in the data sets without our lead in finding it.\\n\\n\\\\subsection*{11.5.3 semi-supervised learning\\\\index{semi-supervised learning}}\\nThe gap between supervised and unsupervised learning is filled by semi-supervised learning methods, which amplify small amounts of labeled training data into more. Turning small numbers of examples into larger numbers is often called bootstrapping\\\\index{bootstrapping}, from the notion of \"pulling yourself up from your bootstraps.\" Semi-supervised approaches personify the cunning which needs be deployed to build substantive training sets.\\n\\nWe assume that we are given a small number of labeled examples as $\\\\left(x_{i}, y_{i}\\\\right)$ pairs, backed by a large number of inputs $x_{j}$ of unknown label. Instead of directly building our model from the training set, we can use it to classify the mass of unlabeled instances. Perhaps we use a nearest neighbor approach to classify these unknowns, or any of the other approaches we have discussed here. But once we classify them, we assume the labels are correct and retrain on the larger set.\\n\\nSuch approaches benefit strongly from having a reliable evaluation set. We need to establish that the model trained on the bootstrapped examples performs better than one trained on what we started with. Adding billions of training examples is worthless if the labels are garbage.\\n\\nThere are other ways to generate training data without annotations. Often it seems easier to find positive examples than negative examples. Consider the\\\\\\\\\\nproblem of training a grammar corrector, meaning it distinguishes proper bits of writing from ill-formed stuff. It is easy to get a hold of large amounts of proper examples of English: whatever gets published in books and newspapers generally qualifies as good. But it seems harder to get a hold of a large corpora of incorrect writing. Still, we can observe that randomly adding, deleting, or substituting arbitrary words to any text almost always makes it worse $4_{4}^{4}$ By labeling all published text as correct and all random perturbations as incorrect, we can create as large a training set as we desire without hiring someone to annotate it.\\n\\nHow can we evaluate such a classifier? It is usually feasible to get enough genuine annotated data for evaluation purposes, because what we need for evaluation is typically much smaller than that for training. We can also use our classifier to suggest what to annotate. The most valuable examples for the annotator to vet are those that our classifier makes mistakes on: published sentences marked incorrect or random mutations that pass the test are worth passing to a human judge.\\n\\n\\\\subsection*{11.5.4 feature engineering\\\\index{feature engineering}}\\nFeature engineering is the fine art of applying domain knowledge to make it easier for machine learning algorithms\\\\index{algorithms} to do their intended job. In the context of our taxonomy here, feature engineering can be considered an important part of supervised learning, where the supervision applies to the feature vectors $x_{i}$ instead of the associated target annotations $y_{i}$.\\n\\nIt is important to ensure that features are presented to models in a way that the model can properly use them. Incorporating application-specific knowledge into the data instead of learning it sounds like cheating, to amateurs. But the pros understand that there are things that cannot be learned easily, and hence are better explicitly put into the feature set.\\n\\nConsider a model to price art at auctions. Auction houses make their money by charging a commission to the winning bidder, on top of what they pay the owner. Different houses charge different rates, but they can amount to a substantial bill. Since the total cost to the winner is split between purchase price and commission, higher commissions may well lower the purchase price, by cutting into what the bidder can afford to pay the owner.\\n\\nSo how can you represent the commission price in an art pricing model? I can think of at least three different approaches, some of which can have disastrous outcomes:\\n\\n\\\\begin{itemize}\\n  \\\\item Specify the commission percentage as a feature: Representing the house cut (say $10 \\\\%$ ) as a column in the feature set might not be usable in a linear model. The hit taken by the bidder is the product of the tax rate and the final price. It has a multiplicative effect, not an additive effect,\\n\\\\end{itemize}\\n\\n\\\\footnotetext{${ }^{4}$ Give it a try someplace on this page. Pick a word at random and replace it by red. Then again with the. And finally with defenestrate. Is my original text clearly better written than what you get after such a change?\\n\\n%---- Page End Break Here ---- Page : 375\\n}\\nand hence cannot be meaningfully exploited if the price range for the art spans from $\\\\$ 100$ to $\\\\$ 1,000,000$.\\n\\n\\\\begin{itemize}\\n  \\\\item Include the actual commission paid as a feature: Cheater. . . If you include the commission ultimately paid as a feature, you pollute the features with data not known at the time of the auction. Indeed, if all paintings were faced with a $10 \\\\%$ tax, and the tax paid was a feature, a perfectly accurate (and completely useless) model would predict the price as ten times the tax paid!\\n  \\\\item Set the regression target variable to be the total amount paid: Since the house commission rates and add-on fees are known to the buyer before they make the bid, the right target variable should be the total amount paid. Any given prediction of the total purchase price can be broken down later into the purchase price, commission, and taxes according to the rules of the house.\\n\\\\end{itemize}\\n\\nFeature engineering can be thought of as a domain-dependent version of data cleaning, so the techniques discussed in Section 3.3 all apply here. The most important of them will be reviewed here in context, now that we have finally reached the point of actually building data-driven models:\\n\\n\\\\begin{itemize}\\n  \\\\item Z-scores and normalization: Normally-distributed values over comparable numerical ranges make the best features, in general. To make the ranges comparable, turn the values into Z-scores, by subtracting off the mean and dividing by the standard deviation, $Z=(x-\\\\mu) / \\\\sigma$. To make a power law variable more normal, replace $x$ in the feature set with $\\\\log x$.\\n  \\\\item Impute missing values: Make sure there are no missing values in your data and, if so, replace them by a meaningful guess or estimate. Recording that someone\\'s weight equals -1 is an effortless way to mess up any model. The simplest imputation method replaces each missing value by the mean of the given column, and generally suffices, but stronger methods train a model to predict the missing value based on the other variables in the record. Review Section 3.3.3 for details.\\n  \\\\item Dimension reduction: Recall that regularization is a way of forcing models to discard irrelevant features to prevent overfitting. It is even more effective to eliminate irrelevant features before fitting your models, by removing them from the data set. When is a feature $x$ likely irrelevant for your model? Poor correlation with the target variable $y$, plus the lack of any qualitative reason you can give for why $x$ might impact $y$ are both excellent indicators.\\\\\\\\\\nDimension reduction techniques like singular-value decomposition are excellent ways to reduce large feature vectors to more powerful and concise representations. The benefits include faster training times, less overfitting, and noise reduction from observations.\\n \\n%---- Page End Break Here ---- Page : 376\\n \\\\item Explicit incorporation of non-linear combinations: Certain products or ratios of feature variables have natural interpretations in context. Area or volume are products of length, width, and height, yet cannot be part of any linear model unless explicitly made a column in the feature matrix. Aggregate totals, like career points scored in sports or total dollars earned in salary, are usually incomparable between items of different age or duration. But converting totals into rates (like points per game played or dollars per hour) usually make more meaningful features.\\n\\\\end{itemize}\\n\\nDefining these products and ratios requires domain-specific information, and careful thought during the feature engineering process. You are much more likely to know the right combinations than your non-linear classifier is to find it on its own.\\n\\nDon\\'t be shy here. The difference between a good model and a bad model usually comes down to quality of its feature engineering. Advanced machine learning algorithms are glamorous, but it is the data preparation that produces the results.\\n\\n\\\\subsection*{11.6 Deep Learning}\\nThe machine learning algorithms we have studied here do not really scale well to huge data sets, for several reasons. \\\\index{non-linearity}Models like linear regression generally have relatively few parameters, say one coefficient per column, and hence cannot really benefit from enormous numbers of training examples. If the data has a good linear fit, you will be able to find it with a small data set. And if doesn\\'t, well, you didn\\'t really want to find it anyway.\\n\\nDeep learning is an incredibly exciting recent development in machine learning. It is based on neural network\\\\index{network}s, a popular approach from the 1980s which then fell substantially out of style. But over the past five years something happened, and suddenly multi-layer (deep) networks\\\\index{networks} began wildly out-performing traditional approaches on classical problems in computer vision and natural language processing.\\n\\nExactly why this happened remains somewhat of a mystery. It doesn\\'t seem that there was a fundamental algorithmic breakthrough so much as that data volume and computational speeds crossed a threshold where the ability to exploit enormous amounts of training data overcame methods more effective at dealing with a scarce resource. But infrastructure is rapidly developing to leverage this advantage: new open source software frameworks like Google\\'s TensorFlow\\\\index{TensorFlow} make it easy to specify network architectures to special-purpose processors designed to speed training by orders of magnitude.\\n\\nWhat distinguishes deep learning from other approaches is that it generally avoids feature engineering. Each layer in a neural network generally accepts as its input the output of its previous layer, yielding progressively higher-level features as we move up towards the top of the network. This serves to define a hierarchy of understanding from the raw input to the final result, and indeed\\\\\\n%---- Page End Break Here ---- Page : 377\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-392}\\n\\nFigure 11.12: Deep learning\\\\index{learning} networks have hidden layers of parameters.\\\\\\\\\\nthe penultimate level of a network designed for one task often provides useful high-level features for related tasks.\\n\\nWhy are neural networks\\\\index{neural networks} so successful? Nobody really knows. There are indications that for many tasks the full weight of these networks are not really needed; that what they are doing will eventually be done using less opaque methods. Neural networks seem to work by overfitting, finding a way to use millions of examples to fit millions of parameters. Yet they generally manage to avoid the worst behavior of overfitting, perhaps by using less precise ways to encode knowledge. A system explicitly memorizing long strings of text to split out on demand will seem brittle and overfit, while one representing such phrases in a looser way is liable to be more flexible and generalizable.\\n\\nThis is a field which is advancing rapidly, enough so that I want to keep my treatment strictly at the idea level. What are the key properties of these networks? Why have they suddenly become so successful?\\n\\nTake-Home Lesson: Deep learning is a very exciting technology that has legs, although it is best suited for domains with enormous amounts of training data. Thus most data science models will continue to be built using the traditional classification and regression algorithms that we detailed earlier in this chapter.\\n\\n\\\\subsection*{11.6.1 Networks and depth\\\\index{depth}}\\nFigure 11.12 illustrates the architecture of a deep learning network. Each node $x$ represents a computational unit, which computes the value of a given simple function $f(x)$ over all inputs to it. For now, perhaps view it as a simple adder that adds all the inputs, then outputs the sum. Each directed edge ( $x, y$ ) connects the output of node $x$ to the input of a node $y$ higher in the network. Further, each such edge has an associated multiplier coefficient $w_{x, y}$. The value actually passed to $y$ is the $w_{x, y} \\\\cdot f(x)$, meaning node $y$ computes a weighted sum of its inputs.\\n\\nThe left column of Figure 11.12 represents a set of input variables, the values of which change whenever we ask the network to make a prediction. Think of this\\\\\\n%---- Page End Break Here ---- Page : 378\\n\\\\\\nas the interface to the network. Links from here to the next level propagate out this input value to all the nodes which will compute with it. On the right side are one or more output variables, presenting the final results of this computation. Between these input and output layers sit hidden layers of nodes. Given the weights of all the coefficients, the network structure, and the values of input variables, the computation is straightforward: compute the values of the lowest level in the network, propagate them forward, and repeat from the next level until you hit the top.\\n\\nLearning the network means setting the weights of the coefficient parameters $w_{x, y}$. The more edges there are, the more parameters we have to learn. In principle, learning means analyzing a training corpus of $\\\\left(x_{i}, y_{i}\\\\right)$ pairs, and adjusting weights of the edge parameters so that the output nodes generate something close to $y_{i}$ when fed input $x_{i}$.\\n\\n\\\\section*{Network Depth}\\nThe depth of the network should, in some sense, correspond to the conceptual hierarchy associated with the objects being modeled. The image we should have is the input being successively transformed, filtered, boiled down, and banged into better and better shape as we move up the network. Generally speaking, the number of nodes should progressively decrease as we move up to higher layers.\\n\\nWe can think of each layer as providing a level of abstraction. Consider a classification problem over images, perhaps deciding whether the image contains a picture of a cat or not. Thinking in terms of successive levels of abstraction, images can be said to be made from pixels, neighborhood patches, edges, textures, regions, simple objects, compound objects, and scenes. This is an argument that at least eight levels of abstraction could potentially be recognizable and usable by networks on images. Similar hierarchies exist in document understanding (characters, words, phrases, sentences, paragraphs, sections, documents) and any other artifacts of similar complexity.\\n\\nIndeed, deep learning networks trained for specific tasks can produce valuable general-purpose features, by exposing the outputs of lower levels in the network as powerful features for conventional classifiers. For example, Imagenet\\\\index{Imagenet} is a popular network for object recognition from images. One high-level layer of 1000 nodes measures the confidence that the image contains objects of each of 1000 different types. The patterns of what objects light up to what degree are generally useful for other tasks, such as measuring image similarity.\\n\\nWe do not impose any real vision of what each of these levels should represent, only to connect them so that the potential to recognize such complexity exists. Neighborhood patches are functions of small groups of connected pixels, while regions will be made up of small numbers of connected patches. Some sense of what we are trying to recognize goes into designing this topology, but the network does what it feels it has to do during training to minimize training error, or loss.\\n\\nThe disadvantages of deeper networks is that they become harder to train\\\\\\n%---- Page End Break Here ---- Page : 379\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-394}\\n\\nFigure 11.13: Addition networks do not benefit from depth. The two layer network (left) computes exactly the function as the equivalent one layer network (right).\\\\\\\\\\nthe larger and deeper they get. Each new layer adds a fresh set of edge-weight parameters, increasing the risks of overfitting. Properly ascribing the effect of prediction errors to edge-weights becomes increasingly difficult, as the number of intervening layers grows between the edge and the observed result. However, networks with over ten layers and millions of parameters have been successfully trained and, generally speaking, recognition performance increases with the complexity of the network.\\n\\nNetworks also get more computationally expensive to make predictions as the depth increases, since the computation takes time linear in the number of edges in the network. This is not terrible, especially since all the nodes on any given level can be evaluated in parallel on multiple cores to reduce the prediction time. Training time is where the real computational bottlenecks generally exist.\\n\\n\\\\section*{non-linearity\\\\index{non-linearity}}\\nThe image of recognizing increasing levels of abstraction up the hidden layers of a network is certainly a compelling one. It is fair to ask if it is real, however. Do extra layers in a network really give us additional computational power to do things we can\\'t with less?\\n\\nThe example of Figure 11.13 seems to argue the converse. It shows addition networks built with two and three layers of nodes, respectively, but both compute exactly the same function on all inputs. This suggests that the extra layer was unnecessary, except perhaps to reduce the engineering constraint of node degree, the number of edges entering as input.\\n\\nWhat it really shows is that we need more complicated, non-linear node activation function\\\\index{activation function}s $\\\\phi(v)$ to take advantage of depth. Non-linear functions cannot be composed in the same way that addition can be composed to yield addition. This nonlinear activation function $\\\\phi\\\\left(v_{i}\\\\right)$ typically operates on a weighted sum\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-395}\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-395(1)}\\n\\nFigure 11.14: The logistic (left) and ReLU (right) activation functions for nodes in neural networks.\\\\\\\\\\nof the inputs $x$, where\\n\\n$$\\nv_{i}=\\\\beta+\\\\sum_{i} w_{i} x_{i} .\\n$$\\n\\nHere $\\\\beta$ is a constant for the given node, perhaps to be learned in training. It is called the bias of\\\\index{bias of} the node because it defines the activation in the absence of other inputs.\\n\\nThat computing the output values of layer $l$ involves applying the activation function $\\\\phi$ to weighted sums of the values from layer $l-1$ has an important implication on performance. In particular, neural network evaluation basically just involves one matrix multiplication per level, where the weighted sums are obtained by multiplying an $\\\\left|V_{l}\\\\right| \\\\times\\\\left|V_{l-1}\\\\right|$ weight matrix $W$ by an $\\\\left|V_{l-1}\\\\right| \\\\times 1$ output vector $V_{l-1}$. Each element of the resulting $\\\\left|V_{l}\\\\right| \\\\times 1$ vector is then hit with the $\\\\phi$ function to prepare the output values for that layer. Fast libraries for matrix multiplication can perform the heart of this evaluation very efficiently.\\n\\nA suite of interesting, non-linear activation functions have been deployed in building networks. Two of the most prominent, shown in Figure 11.14 include:\\n\\n\\\\begin{itemize}\\n  \\\\item logit\\\\index{logit}: We have previously encountered the logistic function\\\\index{logistic function} or logit, in our discussion of logistic regression for classification. Here\\n\\\\end{itemize}\\n\\n$$\\nf(x)=\\\\frac{1}{1+e^{-x}}\\n$$\\n\\nThis unit has the property that the output is constrained to the range $[0,1]$, where $f(0)=1 / 2$. Further, the function is differentiable, so backpropagation can be used to train the resulting network.\\n\\n\\\\begin{itemize}\\n  \\\\item Rectified linear units (ReLU): A rectifier or diode in an electrical circuit lets current flow in only one direction. Its response function $f(x)$ is linear when $x$ is positive, but zero when $x$ is negative, as shown in Figure 11.14\\\\\\n%---- Page End Break Here ---- Page : 381\\n\\\\\\n(right).\\n\\\\end{itemize}\\n\\n$$\\n\\\\begin{aligned}\\nf(x) & =x \\\\text { when } x \\\\geq 0 \\\\\\\\\\n& =0 \\\\text { when } x<0\\n\\\\end{aligned}\\n$$\\n\\nThis kink at $x=0$ is enough to remove the linearity from the function, and provides a natural way to turn off the unit by driving it negative. The ReLU function remains differentiable, but has quite a different response than the logit, increasing monotonically and being unbounded on one side.\\n\\nI am not really aware of a theory explaining why certain functions should perform better in certain contexts. Specific activation functions presumably became popular because they worked well in experiments, with the choice of unit being something you can change if you don\\'t feel your network is performing as well as it should.\\n\\nGenerally speaking, adding one hidden layer adds considerable power to the network, with additional layers suffering from diminishing returns. The theory shows that networks without any hidden layers have the power to recognize linearly separable classes, but we turned to neural nets to build more powerful classifiers.\\n\\nTake-Home Lesson: Start with one hidden layer with a number of nodes between the size of the input and output layers, so they are forced to learn compressed representations that make for powerful features.\\n\\n\\\\subsection*{11.6.2 Backpropagation}\\nBackpropagation is the primary training procedure for neural networks, which achieves very impressive results by fitting large numbers of parameters incrementally on large training sets. It is quite reminiscent of stochastic gradient descent, which we introduced in Section 9.4\\n\\nOur basic problem is this. We are given a neural network with preliminary values for each parameter $w_{i j}^{l}$, meaning the multiplier that the output of node $v_{j}^{l-1}$ gets before being added to node $v_{i}^{l}$. We are also given a training set consisting of $n$ input vector-output value pairs $\\\\left(x_{a}, y_{a}\\\\right)$, where $1 \\\\leq a \\\\leq n$. In our network model, the vector $x_{i}$ represents the values to be assigned to the input layer $v^{1}$, and $y_{i}$ the desired response from the output layer $v_{l}$. Evaluating the current network on $x_{i}$ will result in an output vector $v_{l}$. The error $E_{l}$ of the network at layer $l$ can be measured, perhaps as\\n\\n$$\\nE_{l}=\\\\left\\\\|y_{i}-v^{l}\\\\right\\\\|^{2}=\\\\sum_{j}\\\\left(\\\\phi\\\\left(\\\\beta+\\\\sum_{j} w_{i j}^{l} v_{i j}^{l-1}\\\\right)-y_{i j}\\\\right)^{2}\\n$$\\n\\nWe would like to improve the values of the weight coefficients $w_{i j}^{l}$ so they better predict $y_{i}$ and minimize $E_{l}$. This equation above defines the loss $E_{l}$ as a function of the weight coefficients, since the input values from the previous\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{c|ccccc}\\nSource & 1 & 2 & 3 & 4 & 5 \\\\\\\\\\n\\\\hline\\nApple & iPhone & iPad & apple & MacBook & iPod \\\\\\\\\\napple & apples & blackberry & Apple & iphone & fruit \\\\\\\\\\ncar & cars & vehicle & automobile & truck & Car \\\\\\\\\\nchess & Chess & backgammon & mahjong & checkers & tournaments \\\\\\\\\\ndentist & dentists & dental & orthodontist & dentistry & Dentist \\\\\\\\\\ndog & dogs & puppy & pet & cat & puppies \\\\\\\\\\nMexico & Puerto & Peru & Guatemala & Colombia & Argentina \\\\\\\\\\nred & blue & yellow & purple & orange & pink \\\\\\\\\\nrunning & run & ran & runs & runing & start \\\\\\\\\\nwrite & writing & read & written & tell & Write \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nFigure 11.15: Nearest neighbors in word embeddings\\\\index{word embeddings} capture terms with similar roles and meaning.\\\\\\\\\\nlayer $v^{l-1}$ is fixed. As in stochastic gradient descent, the current value of the $w_{i j}^{l}$ defines a point $p$ on this error surface, and the derivative of $E_{l}$ at this point defines the direction of steepest descent reducing the errors. Walking down a distance $d$ in this direction defined by the current step size or learning rate\\\\index{learning rate} yields updated values of the coefficients, whose $v_{l}$ does a better job predicting $y_{a}$ from $x_{a}$.\\n\\nBut this only changes coefficients in the output layer. To move down to the previous layer, note that the previous evaluation of the network provided an output for each of these nodes as a function of the input. To repeat the same training procedure, we need a target value for each node in layer $l-1$ to play the role of $y_{a}$ from our training example. Given $y_{a}$ and the new weights to compute $v^{l}$, we can compute values for the outputs of these layers which would perfectly predict $y_{i}$. With these targets, we can modify the coefficient weights at this level, and keep propagating backwards until we hit the bottom of the network, at the input layer.\\n\\n\\\\subsection*{11.6.3 Word and Graph Embeddings}\\nThere is one particular unsupervised application of deep learning technology that I have found readily applicable to several problems of interest. This has the extra benefit of being accessible to a broader audience with no familiarity with neural networks. Word embeddings are distributed representations of what words actually mean or $d o$.\\n\\nEach word is denoted by a single point in, say, 100-dimensional space, so that words which play similar roles tend to be represented by nearby points. Figure 11.15 presents the five nearest neighbors of several characteristic English words according to the GloVe word embedding PSM14, and I trust you will agree that they capture an amazing amount of each word\\'s meaning by association.\\n\\nThe primary value of word embeddings is as general features to apply in\\\\\\n%---- Page End Break Here ---- Page : 383\\n\\\\\\nspecific machine learning applications. Let\\'s reconsider the problem of distinguishing spam from meaningful email messages. In the traditional bag of words representation, each message might be represented as a sparse vector $b$, where $b[i]$ might report the number of times vocabulary word $w_{i}$ appears in the message. A reasonable vocabulary size $v$ for English is 100,000 words, turning $b$ into a ghastly 100,000 -dimensional representation that does not capture the similarity between related terms. Word vector representations prove much less brittle, because of the lower dimensionality.\\n\\nWe have seen how algorithms like singular value decomposition (SVD) or principle components analysis can be used to compress an $n \\\\times m$ feature matrix $M$ to an $n \\\\times k$ matrix $M^{\\\\prime}$ (where $k \\\\ll m$ ) in such a way that $M^{\\\\prime}$ retains most of the information of $M$. Similarly, we can think of word embeddings as a compression of a $v \\\\times t$ word-text incidence matrix $M$, where $t$ is the number of documents in corpus, and $M[i, j]$ measures the relevance of word $i$ to document $j$. Compressing this matrix to $v \\\\times k$ would yield a form of word embedding.\\n\\nThat said, neural networks are the most popular approach to building word embeddings. Imagine a network where the input layer accepts the current embeddings of (say) five words, $w_{1}, \\\\ldots, w_{5}$, corresponding to a particular five word phrase from our document training corps. The network\\'s task might be to predict the embedding of the middle word $w_{3}$ from the embeddings of the flanking four words. Through backpropagation, we can adjust the weights of the nodes in the network so it improves the accuracy on this particular example. The key here is that we continue the backpropagation past the lowest level, so that we modify the actual input parameters! These parameters represented the embeddings for the words in the given phrase, so this step improves the embedding for the prediction task. Repeating this on a large number of training examples yields a meaningful embedding for the entire vocabulary.\\n\\nA major reason for the popularity of word embeddings is word2vec, a terrific implementation of this algorithm, which can rapidly train embeddings for hundreds of thousands of vocabulary words on gigabytes of text in a totally unsupervised manner. The most important parameter you must set is the desired number of dimensions $d$. If $d$ is too small, the embedding does not have the freedom to fully capture the meaning of the given symbol. If $d$ is too large, the representation becomes unwieldy and overfit. Generally speaking, the sweet spot lies somewhere between 50 and 300 dimensions.\\n\\n\\\\section*{Graph Embeddings}\\nSuppose we are given an $n \\\\times n$ pairwise similarity matrix $S$ defined over a universe of $n$ items. We can construct the adjacency matrix of similarity graph $G$ by declaring an edge $(x, y)$ whenever the similarity of $x$ and $y$ in $S$ is high enough. This large matrix $G$ might be compressed using singular value decomposition (SVD) or principle components analysis (PCA), but this proves expensive on large networks.\\n\\nPrograms like word2vec do an excellent job constructing representations from sequences of symbols in a training corpus. The key to applying them in new\\\\\\n%---- Page End Break Here ---- Page : 384\\n\\\\\\ndomains is mapping your particular data set to strings over an interesting vocabulary. DeepWalk\\\\index{DeepWalk} is an approach to building graph embeddings, point representations for each vertex such that \"similar\" vertices are placed close together in space.\\n\\nOur vocabulary can be chosen to be the set of distinct vertex IDs, from 1 to $n$. But what is the text that can represent the graph as a sequence of symbols? We can construct random walks over the network, where we start from an arbitrary vertex and repeatedly jump to a random neighbor. These walks can be thought of as \"sentences\" over our vocabulary of vertex-words. The resulting embeddings, after running word2vec on these random walks, prove very effective features in applications.\\n\\nDeepWalk is an excellent illustration of how word embeddings can be used to capture meaning from any large-scale corpus of sequences, irrespective of whether they are drawn from a natural language. The same idea plays an important role in the following war story.\\n\\n\\\\subsection*{11.7 War Story: The Name Game}\\nMy brother uses the name Thor Rabinowitz whenever he needs an alias for a restaurant reservation or online form. To understand this war story, you first have to appreciate why this is very funny.\\n\\n\\\\begin{itemize}\\n  \\\\item Thor is the name of an ancient Norse god, and more recent super-hero character. There are a small but not insignificant number of people in the world named Thor, the majority of whom presumably are Norwegian.\\n  \\\\item Rabinowitz is a Polish-Jewish surname, which means \"son of the rabbi.\" There are a small but not insignificant number of people in the world named Rabinowitz, essentially none of whom are Norwegian.\\n\\\\end{itemize}\\n\\nThe upshot is that there has never been a person with that name, a fact you can readily confirm by Googling \"Thor Rabinowitz\". Mentioning this name should trigger cognitive dissonance in any listener, because the two names are so culturally incompatible.\\n\\nThe specter of Thor Rabinowitz hangs over this tale. My colleague Yifan Hu was trying to find a way to prove that a user logging in from a suspicious machine was really who they said they were. If the next login attempt to my account suddenly comes from Nigeria after many years in New York, is it really me or a bad guy trying to steal my account?\\\\\\\\\\n\"The bad guy won\\'t know who your friends are,\" Yifan observed. \"What if we challenge you to recognize the names of two friends from your email contact list in a list of fake names. Only the real owner will know who they are.\"\\\\\\\\\\n\"How are you going to get the fake names?\" I asked. \"Maybe use the names of other people who are not contacts of the owner?\"\\\\\\\\\\n\"No way,\" said Yifan. \"Customers will get upset if we show their names to the bad guy. But we can just make up names by picking first and last names and sticking them together.\"\\\\\\n%---- Page End Break Here ---- Page : 385\\n\\\\\\n\"But Thor Rabinowitz wouldn\\'t fool anybody,\" I countered, explaining the need for cultural compatibility.\\n\\nWe needed a way to represent names so as to capture subtle cultural affinities. He suggested something like a word embedding could do the job, but we needed training text be that would encode this information.\\n\\nYifan rose to the occasion. He obtained a data set composed of the names of the most important email contacts for over 2 million people. Contact lists for representative individual $5^{5}$ might be:\\n\\n\\\\begin{itemize}\\n  \\\\item Brad Pitt: Angelina Jolie, Jennifer Aniston, George Clooney, Cate Blanchett, Julia Roberts.\\n  \\\\item Donald Trump: Mike Pence, Ivanika Trump, Paul Ryan, Vladimir Putin, Mitch McConnell.\\n  \\\\item Xi Jinping: Hu Jintao, Jiang Zemin, Peng Liyuan, Xi Mingze, Ke Lingling.\\n\\\\end{itemize}\\n\\nWe could treat each email contact list as a string of names, and then concatenate these strings to be sentences in a 2 million-line document. Feeding this to word2vec would train embeddings for each first/last name token appearing in the corpus. Since certain name tokens like John could appear either as first or last names, we created separate symbols to distinguish the cases of John/1 from John/2.\\n\\nWord2vec made short work of the task, creating a one hundred-dimensional vector for each name token with marvelous locality properties. First names associated with the same gender clustered near each other. Why? Men generally have more male friends in their contact list than women, and visa versa. These co-locations pulled the genders together. Within each gender we see clusterings of names by ethnic groupings: Chinese names near Chinese names and Turkish names near other Turkish names. The principle that birds of a feather flock together (homophily) holds here as well.\\n\\nNames regularly go in and out of fashion. We even see names clustering by age of popularity. My daughter\\'s friends all seem to have names like Brianna, Brittany, Jessica, and Samantha. Sure enough, these name embeddings cluster tightly together in space, because they do so in time: these kids tend to communicate most often with peers of similar age.\\n\\nWe see similar phenomena with last name tokens. Figure 11.16 presents a map of the 5000 most frequent last names, drawn by projecting our one hundred-dimensional name embeddings down to two dimensions. Names have been color-coded according to their dominant racial classification according to U.S. Census data. The cutouts in Figure 11.16 highlight the homogeneity of regions by cultural group. Overall the embedding clearly places White, Black, Hispanic, and Asian names in large contiguous regions. There are two distinct\\n\\n\\\\footnotetext{${ }^{5}$ Great care was taken throughout this project to preserve user privacy. The names of the email account owners were never included in the data, and all uncommon names were filtered out. To prevent any possible misinterpretation here, the examples shown are not really the contact lists for Brad Pitt, Donald Trump, and Xi Jinping.\\n\\n%---- Page End Break Here ---- Page : 386\\n}\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-401}\\n\\nFigure 11.16: Visualization of the name embedding for the most frequent 5000 last names from email contact data, showing a two-dimensional projection view of the embedding (left). Insets from left to right highlight British (center), and Hispanic (right) names.\\n\\nAsian regions in the map. Figure 11.17 presents insets for these two regions, revealing that one cluster consists of Chinese names and the other of Indian names.\\n\\nWith very few Thors corresponding with very few Rabinowitzes, these corresponding name tokens are destined to lie far apart in embedding space. But the first name tokens popular within a given demographic are likely to lie near the last names from the same demographic, since the same close linkages appear in individual contact lists. Thus the nearest last name token $y$ to a specific first name token $x$ is likely to be culturally compatible, making $x y$ a good candidate for a reasonable-sounding name.\\n\\nThe moral of this story is the power of word embeddings to effortlessly capture structure latent in any long sequence of symbols, where order matters. Programs like word2vec are great fun to play with, and remarkably easy to use. Experiment with any interesting data set you have, and you will be surprised at the properties it uncovers.\\n\\n\\\\subsection*{11.8 Chapter Notes}\\nGood introductions to machine learning include Bishop [Bis07] and Friedman et al. [FHT01]. Deep learning is currently the most exciting area of machine learning, with the book by Goodfellow, Bengio, and Courville [GBC16] serving as the most comprehensive treatment.\\n\\nWord embeddings were introduced by Mikolov et al. [MCCD13], along with their powerful implementation of word2vec. Goldberg and Levy [G14] have shown that word2vec is implicitly factoring the pointwise mutual information matrix of word co-locations. In fact, the neural network model is not really fundamental to what it is doing. Our DeepWalk approach to graph embeddings is described in Perozzi et al. [PaRS14].\\\\\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-402}\\n\\nFigure 11.17: The two distinct Asian clusters in name space reflect different cultural groups. On the left, an inset showing Chinese/South Asian names. On the right, an inset from the cluster of Indian family names.\\n\\nThe Titanic survival examples are derived from the Kaggle competition \\\\href{https://www.kaggle.com/c/titanic}{https://www.kaggle.com/c/titanic}. The war story on fake name generation is a result of work with Shuchu Han, Yifan Hu, Baris Coskun, and Meizhu Liu at Yahoo labs.\\n\\n\\\\subsection*{11.9 exercises\\\\index{exercises}}\\n\\\\section*{Classification}\\n11-1. [3] Using the naive Bayes classifier of Figure 11.2 decide whether (Cloudy,High, Normal) and (Sunny,Low,High) are beach days.\\\\\\\\[0pt]\\n11-2. [8] Apply the naive Bayes technique for multiclass text classification. Specifically, use The New York Times Developer API to fetch recent articles from several sections of the newspaper. Then, using the simple Bernoulli model for word presence, implement a classifier which, given the text of an article from The New York Times, predicts which section the article belongs to.\\\\\\\\[0pt]\\n11-3. [3] What is regularization, and what kind of problems with machine learning does it solve?\\n\\n\\\\section*{Decision Trees}\\n11-4. [3] Give decision trees to represent the following Boolean functions:\\\\\\\\\\n(a) $A$ and $\\\\bar{B}$.\\\\\\\\\\n(b) $A$ or $(B$ and $C)$.\\\\\\\\\\n(c) $(A$ and $B)$ or $(C$ and $D)$.\\n\\n11-5. [3] Suppose we are given an $n \\\\times d$ labeled classification data matrix, where each item has an associated label class $A$ or class $B$. Give a proof or a counterexample to each of the statements below:\\\\\\\\\\n(a) Does there always exist a decision tree classifier which perfectly separates $A$ from $B$ ?\\\\\\\\\\n(b) Does there always exist a decision tree classifier which perfectly separates $A$ from $B$ if the $n$ feature vectors are all distinct?\\\\\\\\\\n(c) Does there always exist a logistic regression classifier which perfectly separates $A$ from $B$ ?\\\\\\\\\\n(d) Does there always exist a logistic regression classifier which perfectly separates $A$ from $B$ if the $n$ feature vectors are all distinct?\\n\\n11-6. [3] Consider a set of $n$ labeled points in two dimensions. Is it possible to build a finite-sized decision tree classifier with tests of the form \"is $x>c$ ?\", \"is $x<c$ ?\", \"is $y>c$ ?\", and \"is $y<c$ ?\" which classifies each possible query exactly like a nearest neighbor classifier?\\n\\n\\\\section*{Support Vector Machines}\\n11-7. [3] Give a linear-time algorithm to find the maximum-width separating line in one dimension.\\\\\\\\[0pt]\\n11-8. [8] Give an $O\\\\left(n^{k+1}\\\\right)$ algorithm to find the maximum-width separating line in $k$ dimensions.\\\\\\\\[0pt]\\n11-9. [3] Suppose we use support vector machines to find a perfect separating line between a given set of $n$ red and blue points. Now suppose we delete all the points which are not support vectors, and use SVM to find the best separator of what remains. Might this separating line be different than the one before?\\n\\n\\\\section*{Neural Networks}\\n11-10. [5] Specify the network structure and node activation functions to enable a neural network model to implement linear regression.\\\\\\\\[0pt]\\n11-11. [5] Specify the network structure and node activation functions to enable a neural network model to implement logistic regression.\\n\\n\\\\section*{Implementation Projects}\\n11-12. [5] Find a data set involving an interesting sequence of symbols: perhaps text, color sequences in images, or event logs from some device. Use word2vec to construct symbol embeddings from them, and explore through nearest neighbor analysis. What interesting structures do the embeddings capture?\\\\\\\\[0pt]\\n11-13. [5] Experiment with different discounting methods estimating the frequency of words in English. In particular, evaluate the degree to which frequencies on short text files ( 1000 words, 10,000 words, 100,000 words, and $1,000,000$ words) reflect the frequencies over a large text corpora, say, $10,000,000$ words.\\n\\n\\\\section*{Interview Questions}\\n11-14. [5] What is deep learning? What are some of the characteristics that distinguish it from traditional machine learning\\n\\n%---- Page End Break Here ---- Page : 389\\n\\n11-15. [5] When would you use random forests vs. SVMs, and why?\\\\\\\\[0pt]\\n11-16. [5] Do you think fifty small decision trees are better than a large one? Why?\\\\\\\\[0pt]\\n11-17. [8] How would you come up with a program to identify plagiarism in documents?\\n\\n\\\\section*{Kaggle Challenges}\\n11-18. How relevant are given search results to the user?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/crowdflower-search-relevance}{https://www.kaggle.com/c/crowdflower-search-relevance}\\\\\\\\\\n11-19. Did a movie reviewer like or dislike the film?\\\\\\\\\\n\\\\href{https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews}{https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews}\\\\\\\\\\n11-20. From sensor data, determine which home appliance is currently in use. \\\\href{https://www.kaggle.com/c/belkin-energy-disaggregation-competition}\\n%---- Page End Break Here ---- Page : 390\\n{https://www.kaggle.com/c/belkin-energy-disaggregation-competition}\\n\\n\\\\section*{Chapter 12}\\n\\\\section*{big data\\\\index{big data}: Achieving Scale}\\nA change in quantity also entails a change in quality.\\n\\n\\\\begin{itemize}\\n  \\\\item Friedrich Engels\\n\\\\end{itemize}\\n\\nI was once interviewed on a television program, and asked the difference between data and big data. After some thought, I gave them an answer which I will still hold to today: \"size.\"\\n\\nbupkis\\\\index{bupkis} is a marvelous Yiddish word meaning \"too small to matter.\" Used in a sentence like \"He got paid bupkis for it,\" it is a complaint about a paltry sum of money. Perhaps the closest analogy in English vernacular would be the word \"peanuts.\"\\n\\nGenerally speaking, the data volumes we have dealt with thus far in this book all amount to bupkis. Human annotated training sets run in the hundreds to thousands of examples, but anything you must pay people to create has a hard time running into the millions. The log of all New York taxi rides for several years discussed in Section 1.6 came to 80 million records. Not bad, but still bupkis: you can store this easily on your laptop, and make a scan through the file to tabulate statistics in minutes.\\n\\nThe buzzword big data is perhaps reaching its expiration date, but presumes the analysis of truly massive data sets. What big means increases with time, but I will currently draw the starting line at around 1 terabyte.\\n\\nThis isn\\'t as impressive as it may sound. After all, as of this writing a terabyte-scale disk will set you back only $\\\\$ 100$, which is bupkis. But acquiring a meaningful data set to fill it will take some initiative, perhaps privileged access inside a major internet company\\\\index{Engels, Friedrich} or large volumes of video. There are plenty of organizations wrestling with petabytes and even exabytes of data on a regular basis.\\n\\nBig data requires a larger scale of infrastructure than the projects we have considered to date. Moving enormous volumes of data between machines requires fast networks and patience. We need to move away from sequential processing, even beyond multiple cores to large numbers of machines floating\\\\\\\\\\nin the clouds. These computations scale to the point where we must consider robustness, because of the near certainty that some hardware component will fail before we get our answer.\\n\\nWorking with data generally gets harder with size. In this section, I will try to sensitize you to the general issues associated with massive data sets. It is important to understand why size matters, so you will be able to contribute to projects that operate at that scale.\\n\\n\\\\subsection*{12.1 What is Big Data?}\\nHow big is big? Any number I give you will be out of date by the time I type it, but here are some 2016 statistics I stumbled across, primarily at http: \\\\href{//www.internetlivestats.com/:}{//www.internetlivestats.com/:}\\n\\n\\\\begin{itemize}\\n  \\\\item Twitter\\\\index{Twitter}: 600 million tweets per day.\\n  \\\\item Facebook: 600 terabytes of incoming data each day, from 1.6 billion active users.\\n  \\\\item Google: 3.5 billion search queries per day.\\n  \\\\item Instagram: 52 million new photos per day.\\n  \\\\item Apple: 130 billion total app downloads.\\n  \\\\item Netflix: 125 million hours of TV shows and movies streamed daily.\\n  \\\\item Email: 205 billion messages per day.\\n\\\\end{itemize}\\n\\nSize matters: we can do amazing things with this stuff. But other things also matter. This section will look at some of the technical and conceptual complexities of dealing with big data.\\n\\nTake-Home Lesson: Big data generally consists of massive numbers of rows (reco\\\\index{hypothesis driven}rds) over a relatively small number of columns (features). Thus big data is often overkill to accurately fit a single model for a given problem. The value generally comes from fitting many distinct models, as in training a custom model personalized for each distinct user.\\n\\n\\\\subsection*{12.1.1 Big Data as bad data\\\\index{bad data}}\\nMassive data sets are typically the result of opportunity, instead of design. In traditional hypothesis-driven science, we design an experiment to gather exactly the data we need to answer our specific question. But big data is more typically the product of some logging process recording discrete events, or perhaps distributed contributions from millions of people over social media\\\\index{social media}. The data\\\\\\n%---- Page End Break Here ---- Page : 392\\n\\\\\\nscientist generally has little or no control of the collection process, just a vague charter to turn all those bits into money.\\n\\nConsider the task of measuring popular opinion from the posts on a social media platform, or online review site. Big data\\\\index{data} can be a wonderful resource. But it is particularly prone to biases and limitations that make it difficult to draw accurate conclusions from, including:\\n\\n\\\\begin{itemize}\\n  \\\\item Unrepresentative participation: There are sampling biases inherent in any ambient data source. The data from any particular social media site does not reflect the people who don\\'t use it - and you must be careful not to overgeneralize.\\\\\\\\\\nAmazon users buy far more books than shoppers at Walmart. Their political affiliations and economic status differs as well. You get equally-biased but very different views of the world if analyzing data from Instagram (too young), The New York Times (too liberal), Fox News (too conservative), or The Wall Street Journal (too wealthy).\\n  \\\\item Spam and machine-generated content: Big data sources are worse than unrepresentative. Often they have been engineered to be deliberately misleading.\\n\\\\end{itemize}\\n\\nAny online platform large enough to generate enormous amounts of data is large enough for there to be economic incentives to pervert it. Armies of paid reviewers work each day writing fake and misleading product reviews. Bots churn out mechanically written tweets and longer texts in volume\\\\index{volume}, and even are the primary consumers of it: a sizable fraction of the hits reported on any website are from mechanical crawlers, instead of people. Fully $90 \\\\%$ of all email sent over networks is spam: the effectiveness of spam filters at several stages of the pipeline is the only reason you don\\'t see more of it.\\n\\nSpam filtering is an essential part of the data cleaning process on any social media analysis. If you don\\'t remove the spam, it will be lying to you instead of just misleading you.\\n\\n\\\\begin{itemize}\\n  \\\\item Too much redundancy: Many human activities follow a power law distribution, meaning that a very small percentage of the items account for a large percentage of the total activity. News and social media concentrates heavily on the latest missteps of the Kardashian\\'s and similar celebrities, covering them with articles by the thousands. Many of these will be almost exact duplicates of other articles. How much more does the full set of them tell you than any one of them would?\\\\\\\\\\nThis law of unequal coverage implies that much of the data we see through ambient sources is something we have seen before. Removing this duplication is an essential cleaning step for many applications. Any photo sharing site will contain thousands of images of the Empire State Building, but none of the building I work in. Training a classifier with such images will\\\\\\n%---- Page End Break Here ---- Page : 393\\n\\\\\\nproduce fabulous features for landmarks, that may or may not be useful for more general tasks.\\n  \\\\item Susceptibility to temporal bias: Products change in response to competition and changes in consumer demand. Often these improvements change the way people use these products. A time series resulting from ambient data collection might well encode several product/interface transitions, which make it hard to distinguish artifact from signal.\\\\\\\\\\nA notorious example revolves around Google Flu Trends\\\\index{Google Flu Trends}, which for several years successfully forecast flu outbreaks on the basis of search engine queries. But then the model started performing badly. One factor was that Google added an auto-complete mechanism, where it suggests relevant queries during your search process. This changed the distribution of search queries sufficiently to make time series data from before the change incomparable with the data that follows.\\\\\\\\\\nSome of these effects can be mitigated through careful normalization, but often they are baked so tightly into the data to prevent meaningful longitudinal analysis.\\n\\\\end{itemize}\\n\\nTake-Home Lesson: Big data is data we have. Good data is data appropriate to the challenge at hand. Big data is bad data if it cannot really answer the questions we care about.\\n\\n\\\\subsection*{12.1.2 The Three Vs}\\nManagement consulting types have latched onto a notion of the three Vs of big data as a means of explaining it: the properties of volume, variety\\\\index{variety}, and velocity\\\\index{velocity}. They provide a foundation to talk about what makes big data different. The Vs are:\\n\\n\\\\begin{itemize}\\n  \\\\item Volume: It goes without saying that big data is bigger than little data. The distinction is one of class. We leave the world where we can represent our data in a spreadsheet or process it on a single machine. This requires developing a more sophisticated computational infrastructure, and restricting our analysis to linear-time algorithms for efficiency.\\n  \\\\item Variety: Ambient data collection typically moves beyond the matrix to amass heterogeneous data, which often requires ad hoc integration techniques.\\\\\\\\\\nConsider social media. Posts may well include text, links, photos, and video. Depending upon our task, all of these may be relevant, but text processing requires vastly different techniques than network data and multimedia. Even images and videos are quite different beasts, not to be processed using the same pipeline. Meaningfully integrating these materials into a single data set for analysis requires substantial thought and effort.\\n \\n%---- Page End Break Here ---- Page : 394\\n \\\\item Velocity: Collecting data from ambient sources implies that the system is live, meaning it is always on, always collecting data. In contrast, the data sets we have studied to date have generally been dead, meaning collected once and stuffed into a file for later analysis.\\\\\\\\\\nlive data\\\\index{live data} means that infrastructure\\\\index{infrastructure}s must be built for collecting, indexing, accessing, and visualizing the results, typically through a dashboard system. Live data means that consumers want real-time access to the latest results, through graphs, charts, and APIs.\\n\\\\end{itemize}\\n\\nDepending upon the industry, real-time access may involve updating the state of the database within seconds or even milliseconds of actual events. In particular, the financial systems associated with high-frequency trading demand immediate access to the latest information. You are in a race against the other guy, and you profit only if you win.\\\\\\\\\\nData velocity is perhaps the place where data science differs most substantially from classical statistics. It is what stokes the demand for advanced system architectures, which require engineers who build for scale using the latest technologies.\\n\\nThe management set sometimes defines a fourth V: veracity\\\\index{veracity}, a measure for how much we trust the underlying data. Here we are faced with the problem of eliminating spam and other artifacts resulting from the collection process, beyond the level of normal cleaning.\\n\\n\\\\subsection*{12.2 War Story: Infrastructure Matters}\\nI should have recognized the depth of Mikhail\\'s distress the instant he lifted his eyebrow at me.\\n\\nMy Ph.D. student Mikhail Bautin is perhaps the best programmer I have ever seen. Or you have ever seen, probably. Indeed, he finished in 1st place at the 12th International Olympiad in Informatics with a perfect score, marking him as the best high school-level programmer in the world that year.\\n\\nAt this point, our Lydia news analysis project had a substantial infrastructure, running on a bunch of machines. Text spidered from news sources around the world was normalized and passed through a natural language processing (NLP) pipeline we had written for English, and the extracted entities and their sentiment were identified from the text and stored in a big database. With a series of SQL commands this data could be extracted to a format where you could display it on a webpage, or run it in a spreadsheet.\\n\\nI wanted us to study the degree to which machine translation preserved detectable sentiment. If so, it provided an easy way to generalize our sentiment analysis to languages beyond English. It would be low-hanging fruit to stick some third-party language translator into our pipeline and see what happened.\\n\\nI thought this was a timely and important study, and indeed, our subsequent paper [BVS08] has been cited 155 times as of this writing. So I gave the project\\\\\\n%---- Page End Break Here ---- Page : 395\\n\\\\\\nto my best Ph.D. student, and even offered him the services of a very able masters student to help with some of the technical issues. Quietly and obediently, he accepted the task. But he did raise his eyebrow at me.\\n\\nThree weeks later he stepped into my office. The infrastructure my lab had developed for maintaining the news analysis in our database was old-fashioned and crufty. It would not scale. It offended his sense of being. Unless I let him rewrite the entire thing from scratch using modern technology, he was leaving graduate school immediately. He had used his spare time during these three weeks to secure a very lucrative job offer from a world-class hedge fund, and was stopping by to say farewell.\\n\\nI can be a very reasonable man, once things are stated plainly enough. Yes, his dissertation could be on such an infrastructure. He turned around and immediately got down to work.\\n\\nThe first thing that had to go was the central MYSQL database where all our news and sentiment references were stored. It was a bottleneck. It could not be distributed across a cluster of machines. He was going to store everything in a distributed file system (HDFS) so that there was no single bottleneck: reads and writes could happen all over our cluster.\\n\\nThe second thing that had to go was our jury-rigged approach to coordinating the machines in our cluster on their various tasks. It was unreliable. There was no error-recovery mechanism. He was going to rewrite all our backend processing as MapReduce jobs using Hadoop.\\n\\nThe third thing that had to go was the ad h\\\\index{distributed ï¬le system}oc file format we used to represent news articles and their annotations. It was buggy. There were exceptions everywhere. Our parsers often broke on them for stupid reasons. This is why G-d had invented XML, to provide a way to rigorously express structured data, and efficient off-the-shelf tools to parse it. Any text that passed through his code was going to pass an XML validator first. He refused to touch the diseaseridden Perl scripts that did our NLP analysis, but isolated this code completely enough that the infection could be contained.\\n\\nWith so many moving parts, even Mikhail took some time to get his infrastructure right. Replacing our infrastructure meant that we couldn\\'t advance on any other project until it was complete. Whenever I fretted that we couldn\\'t get any experimental analysis done until he was ready, he quietly reminded me about the standing offer he had from the hedge fund, and continued right on with what he was doing.\\n\\nAnd of course, Mikhail was right. The scale of what we could do in the lab increased ten-fold with the new infrastructure. There was much less downtime, and scrambling to restore the database after a power-glitch became a thing of the past. The APIs he developed to regulate access to the data powered all our application analysis in a convenient and logical way. His infrastructure cleanly survived a porting to the Amazon Cloud environment, running every night to keep up with the world\\'s news.\\n\\nThe take-home lesson here is that infrastructure matters. Most of this book talks about higher-level concepts: statistics, machine learning, visualization and it is easy to get hoity-toity about what is science and what is plumbing.\\n\\n%---- Page End Break Here ---- Page : 396\\n\\nBut civilization does not run right without effective plumbing. Infrastructures that are clean, efficient, scalable, and maintainable, built using modern software technologies, are essential to effective data science. Operations that reduce technical debt like refactoring, and upgrading libraries/tools to currently supported versions are not no-ops or procrastinating, but the key to making it easier to do the stuff you really want to do.\\n\\n\\\\subsection*{12.3 Algorithmics for Big Data}\\nBig data requires efficient algorithms to work on it. In this section, we will delve briefly into the basic algorithmic issues associated with big data: asymptotic complexity, hashing, and streaming models to optimize I/O performance in large data files.\\n\\nI do not have the time or space here to provide a comprehensive introduction to the design and analysis of combinatorial algorithms. However, I can confidently recommend The Algorithm Design Manual [ki08] as an excellent book on these matters, if you happen to be looking for one.\\n\\n\\\\subsection*{12.3.1 big oh analysis\\\\index{big oh analysis}}\\nTraditional algorithm analysis\\\\index{algorithm analysis} is based on an abstract computer called the random access machine\\\\index{random access machine} or RAM. On such a model:\\n\\n\\\\begin{itemize}\\n  \\\\item Each simple operation takes exactly one step.\\n  \\\\item Each memory operation takes exactly one step.\\n\\\\end{itemize}\\n\\nHence counting up the operations performed over the course of the algorithm gives its running time.\\n\\nGenerally speaking, the number of operations performed by any algorithm is a function of the size of the input $n$ : a matrix with $n$ rows, a text with $n$ words, a point set with $n$ points. Algorithm analysis is the process of estimating or bounding the number of steps the algorithm takes as a function of $n$.\\n\\nFor algorithms defined by for-loops, such analysis is fairly straightforward. The depth of the nesting of these loops defines the complexity of the algorithm. A single loop from 1 to $n$ defines a linear-time or $O(n)$ algorithm, while two nested loops defines a quadratic-time or $O\\\\left(n^{2}\\\\right)$ algorithm. Two sequential forloops that do not nest are still linear, because $n+n=2 n$ steps are used instead of $n \\\\times n=n^{2}$ such operations.\\n\\nExamples of basic loop-structure algorithms include:\\n\\n\\\\begin{itemize}\\n  \\\\item Find the nearest neighbor\\\\index{nearest neighbor} of point $p$ : We need to compare $p$ against all $n$ points in a given array $a$. The distance computation between $p$ and point $a[i]$ requires subtracting and squaring $d$ terms, where $d$ is the dimensionality of $p$. Looping through all $n$ points and keeping track of the closest point takes $O(d \\\\cdot n)$ time. Since $d$ is typically small enough to be thought of as a constant, this is considered a linear-time algorithm.\\n \\n%---- Page End Break Here ---- Page : 397\\n \\\\item The closest pair of points\\\\index{closest pair of points} in a set: We need to compare every point $a[i]$ against every other point $a[j]$, where $1 \\\\leq i \\\\neq j \\\\leq n$. By the reasoning above, this takes $O\\\\left(d \\\\cdot n^{2}\\\\right)$ time, and would be considered a quadratic-time algorithm.\\n  \\\\item matrix multiplication\\\\index{matrix multiplication}: Multiplying an $x \\\\times y$ matrix times a $y \\\\times z$ matrix results in an $x \\\\times z$ matrix, where each of the $x \\\\cdot z$ terms is the dot product of two $y$-length vectors:\\n\\\\end{itemize}\\n\\n\\\\begin{verbatim}\\nC = numpy.zeros((x, z))\\nfor i in range(0,x-1):\\n    for j in range(0, z-1):\\n        for k in range(0, y-1):\\n            C[i][j] += A[i][k] * B[k][j]\\n\\\\end{verbatim}\\n\\nThis algorithm takes $x \\\\cdot y \\\\cdot z$ steps. If $n=\\\\max (x, y, z)$, then this takes at most $O\\\\left(n^{3}\\\\right)$ steps, and would be considered a cubic-time algorithm.\\n\\nFor algorithms which are defined by conditional while loops or recursion, the analysis often requires more sophistication. Examples, with very concise explanations, include:\\n\\n\\\\begin{itemize}\\n  \\\\item Adding two numbers: Very simple operations might have no conditionals, like adding two numbers together. There is no real value of $n$ here, only two, so this takes constant time or $O(1)$.\\n  \\\\item binary search\\\\index{binary search}: We seek to locate a given search key $k$ in a sorted array $A$, containing $n$ items. Think about searching for a name in the telephone book. We compare $k$ against the middle element $A[n / 2]$, and decide whether what we are looking for lies in the top half or the bottom half. The number of halvings until we get down to 1 is $\\\\log _{2}(n)$, as we discussed in Section 2.4. Thus binary search runs in $O(\\\\log n)$ time.\\n  \\\\item mergesort\\\\index{mergesort}: Two sorted lists with a total of $n$ items can be merged into a single sorted list in linear time: take out the smaller of the two head elements as first in sorted order, and repeat. Mergesort splits the $n$ elements into two halves, sorts each, and then merges them. The number of halvings until we get down to 1 is again $\\\\log _{2}(n)$ (do see Section 2.4), and merging all elements at all levels yields an $O(n \\\\log n)$ sorting algorithm.\\n\\\\end{itemize}\\n\\nThis was a very fast algorithmic review, perhaps too quick for comprehension, but it did manage to provide representatives of six different algorithm complexity classes. These complexity functions define a spectrum from fastest to slowest, defined by the following ordering:\\n\\n$$\\nO(1) \\\\ll O(\\\\log n) \\\\ll O(n) \\\\ll O(n \\\\log n) \\\\ll O\\\\left(n^{2}\\\\right) \\\\ll O\\\\left(n^{3}\\\\right)\\n$$\\n\\nTake-Home Lesson: Algorithms running on big data sets must be linear or near-linear, perhaps $O(n \\\\log n)$. Quadratic algorithms become impossible to contemplate for $n>10,000$.\\n\\n\\\\subsection*{12.3.2 hashing\\\\index{hashing}}\\nHashing is a technique which can often turn quadratic algorithms into lineartime algorithms, making them tractable for dealing with the scale of data we hope to work with.\\n\\nWe first discussed hash functions\\\\index{hash functions} in the context of locality-sensitive hashing (LSH) in Section 10.2.4 A hash function $h$ takes an object $x$ and maps it to a specific integer $h(x)$. The key idea is that whenever $x=y$, then $h(x)=h(y)$. Thus we can use $h(x)$ as an integer to index an array, and collect all similar objects in the same place. Different items are usually mapped to different places, assuming a well-designed hash function, but there are no guarantees.\\n\\nObjects that we seek to hash are often sequences of simpler elements. For example, files or text strings are just sequences of elementary characters. These elementary components usually have a natural mapping to numbers: character codes like Unicode by definition map symbols to numbers, for example. The first step to hash $x$ is to represent it as a sequence of such numbers, with no loss of information. Let us assume each of the $n=|S|$ character numbers of $x$ are integers between 0 and $\\\\alpha-1$.\\n\\nTurning the vector of numbers into a single representative number is the job of the hash function $h(x)$. A good way to do this is to think of the vector as a base- $\\\\alpha$ number, so\\n\\n$$\\nh(x)=\\\\sum_{i=0}^{n-1} \\\\alpha^{n-(i+1)} x_{i}(\\\\bmod m)\\n$$\\n\\nThe mod function $(x \\\\bmod m)$ returns the remainder of $x$ divided by $m$, and so yields a number between 0 and $m-1$. This $n$-digit, base- $\\\\alpha$ number is doomed to be huge, so taking the remainder gives us a way to get a representative code of modest size. The principle here is the same as a roulette wheel for gambling: the ball\\'s long path around the wheel ultimately ends in one of $m=38$ slots, as determined by the remainder of the path length divided by the circumference of the wheel.\\n\\nSuch hash functions are amazingly useful things. Major applications include:\\n\\n\\\\begin{itemize}\\n  \\\\item dictionary maintenance\\\\index{dictionary maintenance}: A hash table is an array-based data structure using $h(x)$ to define the position of object $x$, coupled with an appropriate collision-resolution method. Properly implemented, such hash tables yield constant time (or $O(1)$ ) search times in practice.\\\\\\\\\\nThis is much better than binary search, and hence hash tables are widely used in practice. Indeed, Python uses hashing below the hood to link variable names to the values they store. Hashing is also the fundamental\\\\\\n%---- Page End Break Here ---- Page : 399\\n\\\\\\nidea behind distributed computing systems like MapReduce, which will be discussed in Section 12.6\\n  \\\\item Frequency counting: A common task in analyzing logs is tabulating the frequencies of given events, such as word counts or page hits. The fastest/easiest approach is to set up a hash table with event types as the key, and increment the associated counter for each new event. Properly implemented, this algorithm is linear in the total number of events being analyzed.\\n  \\\\item Duplicate removal: An important data cleaning chore is identifying duplicate records in a data stream and removing them. Perhaps these are all the email addresses we have of our customers, and want to make sure we only spam each of them once. Alternately, we may seek to construct the complete vocabulary of a given language from large volumes of text.\\\\\\\\\\nThe basic algorithm is simple. For each item in the stream, check whether it is already in the hash table. If not insert it, if so ignore it. Properly implemented, this algorithm takes time linear in the total number of records being analyzed.\\n  \\\\item Canonization: Often the same object can be referred to by multiple different names. Vocabulary words are generally case-insensitive, meaning that \"The\" is equivalent to \"the.\" Determining the vocabulary of a language requires unifying alternate forms, mapping them to a single key.\\\\\\\\\\nThis process of constructing a canonical representation can be interpreted as hashing. Generally speaking, this requires a domain-specific simplification function doing such things as reduction to lower case, white space removal, stop word elimination, and abbreviation expansion. These canonical keys can then be hashed, using conventional hash functions.\\n  \\\\item Cryptographic hashing: By constructing concise and uninvertible representations, hashing can be used to monitor and constrain human behavior. How can you prove that an input file remains unchanged since you last analyzed it? Construct a hash code or checksum for the file when you worked on it, and save this code for comparison with the file hash at any point in the future. They will be the same if the file is unchanged, and almost surely differ if any alterations have occurred.\\\\\\\\\\nSuppose you want to commit to a bid on a specific item, but not reveal the actual price you will pay until all bids are in. Hash your bid using a given cryptographic hash function, and submit the resulting hash code. After the deadline, send your bid in again, this time without encryption. Any suspicious mind can hash your now open bid, and confirm the value matches your previously submitted hash code. The key is that it be difficult to produce collisions with the given hash function, meaning you cannot readily construct another message which will hash to the same code. Otherwise you could submit the second message instead of the first, changing your bid after the deadline.\\n\\n%---- Page End Break Here ---- Page : 400\\n\\\\end{itemize}\\n\\n\\\\subsection*{12.3.3 Exploiting the storage hierarchy\\\\index{storage hierarchy}}\\nBig data algorithms are often storage-bound or bandwidth-bound rather than compute-bound. This means that the cost of waiting around for data to arrive where it is needed exceeds that of algorithmically manipulating it to get the desired results. It still takes half an hour to just to read 1 terabyte of data from a modern disk. Achieving good performance can rest more on smart data management than sophisticated algorithmics.\\n\\nTo be available for analysis, data must be stored somewhere in a computing system. There are several possible types of devices to put it on, which differ greatly in speed, capacity, and latency. The performance differences between different levels of the storage hierarchy is so enormous that we cannot ignore it in our abstraction of the RAM machine. Indeed, the ratio of the access speed from disk to cache memory\\\\index{cache memory} is roughly the same $\\\\left(10^{6}\\\\right)$ as the speed of a tortoise to the exit velocity of the earth!\\n\\nThe major levels of the storage hierarchy are:\\n\\n\\\\begin{itemize}\\n  \\\\item Cache memory: Modern computer architectures feature a complex system of registers and caches to store working copies of the data actively being used. Some of this is used for prefetching\\\\index{prefetching}: grabbing larger blocks of data around memory locations which have been recently accessed, in anticipation of them being needed later. Cache sizes are typically measured in megabytes, with access times between five and one hundred times faster than main memory\\\\index{main memory}. This performance makes it very advantageous for computations to exploit locality\\\\index{locality}, to use particular data items intensively in concentrated bursts, rather than intermittently over a long computation.\\n  \\\\item Main memory: This is what holds the general state of the computation, and where large data structures are hosted and maintained. Main memory is generally measured in gigabytes, and runs hundreds to thousands of times faster than disk storage\\\\index{disk storage}. To the greatest extent possible, we need data structures that fit into main memory and avoid the paging behavior of virtual memory.\\n  \\\\item Main memory on another machine: Latency times on a local area network run into the low-order milliseconds, making it generally faster than secondary storage devices like disks. This means that distributed data structures like hash tables can be meaningfully maintained across networks of machines, but with access times that can be hundreds of times slower than main memory.\\n  \\\\item Disk storage: Secondary storage devices can be measured in terabytes, providing the capacity that enables big data to get big. Physical devices like spinning disks take considerable time to move the read head to the position where the data is. Once there, it is relatively quick to read a large block of data. This motivates pre-fetching, copying large chunks of files into memory under the assumption that they will be needed later.\\n\\n%---- Page End Break Here ---- Page : 401\\n\\\\end{itemize}\\n\\nLatency issues generally act like a volume discount: we pay a lot for the first item we access, but then get a bunch more very cheaply. We need to organize our computations to take advantage of this, using techniques like:\\n\\n\\\\begin{itemize}\\n  \\\\item Process files and data structures in streams: It is important to access files and data structures sequentially whenever possible, to exploit prefetching. This mean\\\\index{mean}s arrays are better than linked structures, because logically-neighboring elements sit near each other on the storage device. It means making entire passes over data files that read each item once, and then perform all necessary computations with it before moving on. Much of the advantage of sorting data is that we can jump to the appropriate location in question. Realize that such random access is expensive: think sweeping instead of searching.\\n  \\\\item Think big files instead of directories: One can organize a corpus of documents such that each is in its own file. This is logical for humans but slow for machines, when there are millions of tiny files. Much better is to organize them in one large file to efficiently sweep through all examples, instead of requiring a separate disk access for each one.\\n  \\\\item packing data\\\\index{packing data} concisely: The cost of decompressing data being held in main memory is generally much smaller than the extra transfer costs for larger files. This is an argument that it pays to represent large data files concisely whenever you can. This might mean explicit file compression schemes, with small enough file sizes so that they can be expanded in memory.\\\\\\\\\\nIt does mean designing file formats and data structures to be concisely encoded. Consider representing DNA sequences\\\\index{DNA sequences}, which are long strings on a four-letter alphabet. Each letter/base can be represented in 2 bits, meaning that four bases can be represented in a single 8 -bit byte and thirty-two bases in a 64 -bit word. Such data-size reductions can greatly reduce transfer times, and are worth the computational effort to pack and unpack.\\\\\\\\\\nWe have previously touted the importance of readability in file formats in Section 3.1.2, and hold to that opinion here. Minor size reductions are likely not worth the loss of readability or ease of parsing. But cutting a file size in half is equivalent to doubling your transfer rate, which may matter in a big data environment.\\n\\\\end{itemize}\\n\\n\\\\subsection*{12.3.4 streaming\\\\index{streaming} and single-pass algorithm\\\\index{single-pass algorithm}s}\\nData is not necessarily stored forever. Or even at all. In applications with a very high volume of updates and activity, it may pay to compute statistics on the fly as the data emerges so we can then throw the original away.\\n\\nIn a streaming or single-pass algorithm, we get only one chance to view each element of the input. We can assume some memory, but not enough to store the\\\\\\n%---- Page End Break Here ---- Page : 402\\n\\\\\\nbulk of the individual records. We need to decide what to do with each element when we see it, and then it is gone.\\n\\nFor example, suppose we seek to compute the mean of a stream of numbers as it passes by. This is not a hard problem: we can keep two variables: $s$ representing the running sum to date, and $n$ the number of items we have seen so far. For each new observation $a_{i}$, we add it to $s$ and increment $n$. Whenever someone needs to know the current mean $\\\\mu=\\\\bar{A}$ of the stream $A$, we report the value of $s / n$.\\n\\nWhat about computing the variance\\\\index{variance} or standard deviation\\\\index{standard deviation} of the stream? This seems harder. Recall that\\n\\n$$\\nV(A)=\\\\sigma^{2}=\\\\frac{\\\\sum_{i=1}^{n}\\\\left(a_{i}-\\\\bar{A}\\\\right)^{2}}{n-1}\\n$$\\n\\nThe problem is that the sequence mean $\\\\bar{A}$ cannot be known until we hit the end of the stream, at which point we have lost the original elements to subtract against the mean.\\n\\nBut all is not lost. Recall that there is an alternate formula for the variance, the mean of the square minus the square of the mean:\\n\\n$$\\nV(a)=\\\\left(\\\\frac{1}{n} \\\\sum_{i=1}^{n}\\\\left(a_{i}\\\\right)^{2}\\\\right)-(\\\\bar{A})^{2}\\n$$\\n\\nThus by keeping track of a running sum of squares of the elements in addition to $n$ and $s$, we have all the material we need to compute the variance on demand.\\n\\nMany quantities cannot be computed exactly under the streaming model. An example would be finding the media\\\\index{ï¬ltering}n element of a long sequence. Suppose we don\\'t have enough memory to store half the elements of the full stream. The first element that we chose to delete, whatever it is, could be made to be the median\\\\index{median} by a carefully-designed stream of elements yet unseen. We need to have all the data simultaneously available to us to solve certain problems.\\n\\nBut even if we cannot compute something exactly, we can often come up with an estimate that is good enough for government work. Important problems of this type include identifying the most frequent items in a stream, the number of distinct elements, or even estimating element frequency when we do not have enough memory to keep an exact counter.\\n\\nsketching\\\\index{sketching} involves using what storage we do have to keep track of a partial representation of the sequence. Perhaps this is a frequency histogram of items binned by value, or a small hash table of values we have seen to date. The quality of our estimate increases with the amount of memory we have to store our sketch. random sampling\\\\index{random sampling}\\\\index{sampling}\\\\index{sampling} is an immensely useful tool for constructing sketches, and is the focus of Section 12.4\\n\\n\\\\subsection*{12.4 Filtering and Sampling}\\nOne important benefit of big data is that with sufficient volume you can afford to throw most of your data away. And this can be quite worthwhile, to make\\\\\\n%---- Page End Break Here ---- Page : 403\\n\\\\\\nyour analysis cleaner and easier.\\\\\\\\\\nI distinguish between two distinct ways to throw data away, filtering and sampling. Filtering means selecting a relevant subset of data based on a specific criteria. For example, suppose we wanted to build a language model for an application in the United States, and we wanted to train it on data from Twitter\\\\index{Twitter}\\\\index{Twitter}. English accounts for only about one third of all tweets on Twitter, so filtering out all other languages leaves enough for meaningful analysis.\\n\\nWe can think of filtering as a special form of cleaning, where we remove data not because it is erroneous but because it is distracting to the matter at hand. Filtering away irrelevant or hard-to-interpret data requires application-specific knowledge. English is indeed the primary language in use in the United States, making the decision to filter the data in this way perfectly reasonable.\\n\\nBut filtering introduces biases. Over $10 \\\\%$ of the U.S. population speaks Spanish. Shouldn\\'t they be represented in the language model, amigo? It is important to select the right filtering criteria to achieve the outcome we seek. Perhaps we might better filter tweets based on location of origin, instead of language.\\n\\nIn contrast, sampling means selecting an appropriate size subset in an arbitrary manner, without domain-specific criteria. There are several reasons why we may want to subsample good, relevant data:\\n\\n\\\\begin{itemize}\\n  \\\\item right-sizing training data\\\\index{right-sizing training data}: Simple, robust models generally have few parameters, making big data unnecessary to fit them. Subsampling your data in an unbiased way leads to efficient model fitting, but is still representative of the entire data set.\\n  \\\\item data partition\\\\index{data partition}ing: Model-building hygiene requires cleanly separating training, testing, and evaluation data, typically in a $60 \\\\%, 20 \\\\%$, and $20 \\\\%$ mix. Constructing these partitions in an unbiased manner is necessary for the veracity of this process.\\n  \\\\item Exploratory data analysis\\\\index{data analysis} and visualization\\\\index{visualization}: Spreadsheet-sized data sets are fast and easy to explore. An unbiased sample is representative of the whole while remaining comprehensible.\\n\\\\end{itemize}\\n\\nSampling $n$ records in an efficient and unbiased manner is a more subtle task than it may appear at first. There are two general approaches, deterministic and randomized, which detailed in the following sections.\\n\\n\\\\subsection*{12.4.1 deterministic sampling algorithms\\\\index{deterministic sampling algorithms}}\\nOur straw man sampling algorithm will be sampling by truncation\\\\index{truncation}\\\\index{by truncation}, which simply takes the first $n$ records in the file as the desired sample. This is simple, and has the property that it is readily reproducible, meaning someone else with the full data file could easily reconstruct the sample.\\n\\nHowever, the order of records in a file often encodes semantic information, meaning that truncated samples often contain subtle effects from factors such as:\\n\\n%---- Page End Break Here ---- Page : 404\\n\\n\\\\begin{itemize}\\n  \\\\item temporal\\\\index{temporal} biases: Log files are typically constructed by appending new records to the end of the file. Thus the first $n$ records would be the oldest available, and will not reflect recent regime changes.\\n  \\\\item lexicographic\\\\index{lexicographic} biases: Many files are sorted according to the primary key, which means that the first $n$ records are biased to a particular population. Imagine a personnel roster sorted by name. The first $n$ records might consist only of the $A \\\\mathrm{~s}$, which means that we will probably over-sample Arabic names from the general population, and under-sample Chinese ones.\\n  \\\\item numerical\\\\index{numerical} biases: Often files are sorted by identity numbers, which may appear to be arbitrarily defined. But ID numbers can encode meaning. Consider sorting the personnel records by their U.S. social security numbers. In fact, the first five digits of social security numbers are generally a function of the year and place of birth. Thus truncation leads to a geographically and age-biased sample.\\\\\\\\\\nOften data files are constructed by concatenating smaller files together, some of which may be far more enriched in positive examples than others. In particularly pathological cases, the record number might completely encode the class variable, meaning that an accurate but totally useless classifier may follow from using the class ID as a feature.\\n\\\\end{itemize}\\n\\nSo truncation is generally a bad idea. A somewhat better approach is uniform sampling\\\\index{uniform sampling}. Suppose we seek to sample $n / m$ records out of $n$ from a given file. A straightforward approach is to start from the $i$ th record, where $i$ is some value between 1 and $m$, and then sample every $m$ th record starting from $i$. Another way of saying this is that we output the $j$ th record if $j(\\\\bmod m)=i$. Such uniform sampling provides a way to balance many concerns:\\n\\n\\\\begin{itemize}\\n  \\\\item We obtain exactly the desired number of records for our sample.\\n  \\\\item It is quick and reproducible by anyone given the file and the values of $i$ and $m$.\\n  \\\\item It is easy to construct multiple disjoint samples. If we repeat the process with a different offset $i$, we get an independent sample.\\n\\\\end{itemize}\\n\\nTwitter uses this method to govern API services that provide access to tweets. The free level of access (the spritzer hose) rations out $1 \\\\%$ of the stream by giving every 100th tweet. Professional levels of access dispense every tenth tweet or even more, depending upon what you are willing to pay for.\\n\\nThis is generally better than truncation, but there still exist potential periodic temporal biases. If you sample every $m$ th record in the log, perhaps every item you see will be associated with an event from a Tuesday, or at 11PM each night. On files sorted by numbers, you are in danger of ending up with items with the same lower order digits. Telephone numbers ending in \" 000 \" or repeat\\\\\\n%---- Page End Break Here ---- Page : 405\\n\\\\\\ndigits like \" 8888 \" are often reserved for business use instead of residential, thus biasing the sample. You can minimize the chances of such phenomenon by forcing $m$ to be a large-enough prime number, but the only certain way to avoid sampling biases is to use randomization.\\n\\n\\\\subsection*{12.4.2 Randomized and Stream Sampling}\\nRandomly sampling records with a probability $p$ results in a selection of an expected $p \\\\cdot n$ items, without any explicit biases. Typical random number generators return a value between 0 and 1 , drawn from a uniform distribution\\\\index{uniform distribution}. We can use the sampling probability $p$ as a threshold. As we scan each new record, generate a new random number $r$. When $r \\\\leq p$, we accept this record into our sample, but when $r>p$ we ignore it.\\n\\nrandom sampling\\\\index{random sampling} is a generally sound methodology, but it comes with certain technical quirks. Statistical discrepancies ensure that certain regions or demographics will be over-sampled relative to population, however in an unbiased manner and to a predictable extent. Multiple random samples will not be disjoint, and random sampling is not reproducible without the seed and random generator.\\n\\nBecause the ultimate number of sampled records depends upon randomness, we may end up with slightly too many or too few items. If we need exactly $k$ items, we can construct a random permutation of the items and truncate it after the first $k$. Algorithms for constructing random permutations were discussed in Section 5.5.1. These are simple, but require large amounts of irregular data movement, making them potentially bad news for large files. A simpler approach is to append a new random number field to each record, and sort with this as the key. Taking the first $k$ records from this sorted file is equivalent to randomly sampling exactly $k$ records.\\n\\nObtaining a fixed size random sample from a stream is a trickier problem, because we cannot store all the items until the end. Indeed, we don\\'t even know how $\\\\operatorname{big} n$ will ultimately be.\\n\\nTo solve this problem, we will maintain a uniformly selected sample in an array of size $k$, updated as each new element arrives from the stream. The probability that the $n$th stream element belongs in the sample is $k / n$, and so we will insert it into our array if random number $r \\\\leq k / n$. Doing so must kick a current resident out of the table, and selecting which current array element is the victim can be done with another call to the random number generator.\\n\\n\\\\subsection*{12.5 parallelism\\\\index{parallelism}}\\nTwo heads are better than one, and a hundred heads better than two. Computing technology has matured in ways that make it increasingly feasible to commandeer multiple processing elements on demand for your application. Microprocessors routinely have 4 cores and beyond, making it worth thinking about parallelism even on individual machines. The advent of data centers and cloud\\\\\\n%---- Page End Break Here ---- Page : 406\\n\\\\\\ncomputing has made it easy to rent large numbers of machines on demand, enabling even small-time operators to take advantage of big distributed infrastructures.\\n\\nThere are two distinct approaches to simultaneously computing with multiple machines, namely parallel and distributed computing. The distinction here is how tightly coupled the machines are, and whether the tasks are CPU-bound or memory/IO-bound. Roughly:\\n\\n\\\\begin{itemize}\\n  \\\\item parallel processing\\\\index{parallel processing} happens on one machine, involving multiple cores and/or processors that communicate through threads and operating system resources. Such tightly-coupled computation is often CPU-bound, limited more by the number of cycles than the movement of data through the machine. The emphasis is solving a particular computing problem faster than one could sequentially.\\n  \\\\item distributed processing\\\\index{distributed processing} happens on many machines, using network communication\\\\index{communication}. The potential scale here is enormous, but most appropriate to loosely-coupled jobs which do not communicate much. Often the goal of distributed processing involves sharing resources like memory and secondary storage across multiple machines, more so than exploiting multiple CPUs. Whenever the speed of reading data from a disk is the bottleneck, we are better off having many machines reading as many different disks as possible, simultaneously.\\n\\\\end{itemize}\\n\\nIn this section, we introduce the basic principles of parallel computing, and two relatively simple ways to exploit it: data parallelism and grid search. MapReduce\\\\index{MapReduce} is the primary paradigm for distributed computing on big data, and will be the topic of Section 12.6\\n\\n\\\\subsection*{12.5.1 One, Two, Many}\\nPrimitive cultures were not very numerically savvy, and supposedly only counted using the words one, two, and many. This is actually a very good way to think about parallel and distributed computing, because the complexity increases very rapidly with the number of machines:\\n\\n\\\\begin{itemize}\\n  \\\\item One: Try to keep all the cores of your box busy, but you are working on one computer. This isn\\'t distributed computing.\\n  \\\\item Two: Perhaps you will try to manually divide the work between a few machines on your local network. This is barely distributed computing, and is generally managed through ad hoc techniques.\\n  \\\\item Many: To take advantage of dozens or even hundreds of machines, perhaps in the cloud, we have no choice but to employ a system like MapReduce that can efficiently manage these resources.\\n\\n%---- Page End Break Here ---- Page : 407\\n\\\\end{itemize}\\n\\nComplexity increases hand-in-hand with the number of agents being coordinated towards a task. Consider what changes as social gatherings scale in size. There is a continual trend of making do with looser coordination\\\\index{coordination} as size increases, and a greater chance of unexpected and catastrophic events occurring, until they become so likely that the unexpected must be expected:\\n\\n\\\\begin{itemize}\\n  \\\\item 1 person: A date is easy to arrange using personal communication.\\n  \\\\item $>2$ persons: A dinner among friends requires active coordination.\\n  \\\\item     \\\\begin{displayquote}\\n10 persons: A group meeting requires that there be a leader in charge.\\n    \\\\end{displayquote}\\n\\n  \\\\item     \\\\begin{displayquote}\\n100 persons: A wedding dinner requires a fixed menu, because the kitchen cannot manage the diversity of possible orders.\\n    \\\\end{displayquote}\\n\\n  \\\\item     \\\\begin{displayquote}\\n1000 persons: At any community festival or parade, no one knows the majority of attendees.\\n    \\\\end{displayquote}\\n\\n  \\\\item     \\\\begin{displayquote}\\n10,000 persons: After any major political demonstration, somebody is going to spend the night in the hospital, even if the march is peaceful.\\n    \\\\end{displayquote}\\n\\n  \\\\item     \\\\begin{displayquote}\\n100, 000 persons: At any large sporting event, one of the spectators will presumably die that day, either through a heart attack [ $\\\\mathrm{BSC}^{+} 11$ ] or an accident on the drive home.\\n    \\\\end{displayquote}\\n\\n\\\\end{itemize}\\n\\nIf some of these sound unrealistic to you, recall that the length of a typical human life is 80 years $\\\\times 365$ days/year $=29,200$ days. But perhaps this sheds light on some of the challenges of parallelization and distributed computing:\\n\\n\\\\begin{itemize}\\n  \\\\item Coordination: How do we assign work units to processors, particularly when we have more work units than workers? How do we aggregate or combine each worker\\'s efforts into a single result?\\n  \\\\item Communication: To what extent can workers share partial results? How can we know when all the workers have finished their tasks?\\n  \\\\item fault tolerance\\\\index{fault tolerance}: How do we reassign tasks if workers quit or die? Must we protect against malicious and systematic attacks, or just random failures?\\n\\\\end{itemize}\\n\\nTake-Home Lesson: Parallel computing works when we can minimize communication and coordination complexity, and complete the task with low probability of failure.\\n\\n%---- Page End Break Here ---- Page : 408\\n\\n\\\\subsection*{12.5.2 data parallelism\\\\index{data parallelism}}\\nData parallelism involves partitioning and replicating the data among multiple processors and disks, running the same algorithm on each piece, and then collecting the results together to produce the final results. We assume a master machine divvying out tasks to a bunch of slaves, and collecting the results.\\n\\nA representative task is aggregating statistics from a large collection of files, say, counting how often words appear in a massive text corpus. The counts for each file can be computed independently as partial results towards the whole, and the task of merging these resulting count files easily computed by a single machine at the end. The primary advantage of this is simplicity, because all counting processes are running the same program. The inter-processor communication is straightforward: moving the files to the appropriate machine, starting the job, and then reporting the results back to the master machine.\\n\\nThe most straightforward approach to multicore computing involves data parallelism. Data naturally forms partitions established by time, clustering algorithms, or natural categories. For most aggregation problems, records can be partitioned arbitrarily, provided all subproblems will be merged together at the end, as shown in Figure 12.1\\n\\nFor more complicated problems, it takes additional work to combine the results of these runs together later. Recall the $k$-means clustering algorithm (Section 10.5.1), which has two steps:\\n\\n\\\\begin{enumerate}\\n  \\\\item For each point, identifies which current cluster center is closest to it.\\n  \\\\item Computes the new centroid of the points now associate\\\\index{k-means clustering}d with it.\\n\\\\end{enumerate}\\n\\nAssuming the points have been spread across multiple machines, the first step requires the master to communicate all current centers to each machine, while the second step requires each slave to report back to the master the new centroids of the points in its partition. The master then appropriately computes the averages of these centroids to end the iteration.\\n\\n\\\\subsection*{12.5.3 grid search\\\\index{grid search}}\\nA second approach to exploit parallelism involves multiple independent runs on the same data. We have seen that many machine learning methods involve parameters which impact the quality of the ultimate result, such as selecting the right number of clusters $k$ for $k$-means clustering. Picking the best one means trying them all, and each of these runs can be conducted simultaneously on different machines.\\n\\nGrid search is the quest for the right meta-parameters in training. It is difficult to predict exactly how varying the learning rate or batch size in stochasticgradient descent affects the quality of the final model. Multiple independent fits can be run in parallel, and in the end we take the best one according to our evaluation.\\n\\nEffectively searching the space over $k$ different parameters is difficult because of interactions: identifying the best value single of each parameter separately\\\\\\n%---- Page End Break Here ---- Page : 409\\n\\\\\\ndoes not necessarily produce the best parameter set when combined. Typically, reasonable minimum and maximum values for each parameter $p_{i}$ are established by the user, as well as the number of values $t_{i}$ for this parameter to be tested at. Each interval is partitioned into equally-spaced values governed by this $t_{i}$. We then try all parameter sets which can be formed by picking one value per interval, establishing the grid in grid search.\\n\\nHow much should we believe that the best model in a grid search is really better than the others? Often there is simple variance that explains the small differences in performance on a given test set, turning grid search into cherrypicking for the number which makes our performance sound best. If you have the computational resources available to conduct a grid search for your model, feel free to go ahead, but recognize the limits of what trial-and-error can do.\\n\\n\\\\subsection*{12.5.4 cloud computing services\\\\index{cloud computing services}}\\nPlatforms such as Amazon AWS, Google Cloud, and Microsoft Azure make it easy to rent large (or small) numbers of machines for short-term (or longterm) jobs. They provide you with the ability to get access to exactly the right computing resources when you need them, provided that you can pay for them, of course.\\n\\nThe cost models for these service providers are somewhat complicated, however. They will typically be hourly charges for each virtual machine, as a function of the processor type, number of cores, and main memory involved. Reasonable machines will rent for between 10 and 50 cents/hour. You will pay for the amount of long-term storage as a function of gigabyte/months, with different cost tiers depending upon access patterns. Further, you pay bandwidth charges covering the volume of data transfer between machines and over the web.\\n\\nSpot pricing and reserved instances can lead to lower hourly costs for special usage patterns, but with extra caveats. Under spot pricing, machines go to the highest bidder, so your job is at risk of being interrupted if someone else needs it more than you do. With reserved instances, you pay a certain amount up front in order to get a lower hourly price. This makes sense if you will be needing one computer $24 / 7$ for a year, but not if you need a hundred computers each for one particular day.\\n\\nFortunately, it can be free to experiment. All the major cloud providers provide some free time to new users, so you can play with the setup and decide on their dime whether it is appropriate for you.\\n\\n\\\\subsection*{12.6 MapReduce\\\\index{MapReduce}}\\nGoogle\\'s MapReduce paradigm for distributed computing has spread widely through open-source implementations like Hadoop and Spark. It offers a simple programming model with several benefits, including straightforward scaling to\\\\\\n%---- Page End Break Here ---- Page : 410\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-425}\\n\\nFigure 12.1: divide and conquer\\\\index{divide and conquer} is the algorithmic paradigm of distributed computing.\\\\\\\\\\nhundreds or even thousands of machines, and fault tolerance through redundancy.\\n\\nThe level of abstraction of programming\\\\index{programming} models steadily increases over time, as reflected by more powerful tools and systems that hide implementation details from the user. If you are doing data science on a multiple-computer scale, MapReduce computing is probably going on under the hood, even if you are not explicitly programming it.\\n\\nAn important class of large-scale data science tasks have the following basic structure:\\n\\n\\\\begin{itemize}\\n  \\\\item Iterate over a large number of items, be they data records, text strings, or directories of files.\\n  \\\\item Extract something of interest from each item, be it the value of a particular field, frequency counts of each word, or the presence/absence of particular patterns in each file.\\n  \\\\item Aggregate these intermediate results over all items, and generate an appropriate combined result.\\n\\\\end{itemize}\\n\\nRepresentatives of these class of problems include word frequency counting, $k$-means clustering, and PageRank computations. All are solvable through straightforward iterative algorithms, whose running times scale linearly in the size of the input. But this can be inadequate for inputs of massive size, where the files don\\'t naturally fit in the memory of a single machine. Think about webscale problems, like word-frequency counting over billions of tweets, $k$-means clustering on hundred of millions of Facebook profiles, and PageRank over all websites on the Internet.\\n\\nThe typical solution here is divide and conquer. Partition the input files among $m$ different machines, perform the computations in parallel on each of them, and then combine the results on the appropriate machine. Such a solution\\\\\\n%---- Page End Break Here ---- Page : 411\\n\\\\\\nworks, in principle, for word counting, because even enormous text corpora will ultimately reduce to relatively small files of distinct vocabulary words with associated frequency counts, which can then be readily added together to produce the total counts.\\n\\nBut consider a PageRank computation, where for each node $v$ we need to sum up the PageRank from all nodes $x$ where $x$ points to $v$. There is no way we can cut the graph into separate pieces such that all these $x$ vertices will sit on the same machine as $v$. Getting things in the right place to work with them is at the heart of what MapReduce is all about.\\n\\n\\\\subsection*{12.6.1 Map-Reduce Programming}\\nThe key to distributing such computations is setting up a distributed hash table of buckets, where all the items with the same key get mapped to the same bucket:\\n\\n\\\\begin{itemize}\\n  \\\\item Word count: For counting the total frequency of a particular word $w$ across a set of files, we need to collect the frequency counts for all the files in a single bucket associated with $w$. There they can be added together to produce the final total.\\n  \\\\item $k$-means clustering: The critical step in $k$-means clustering is updating the new centroid $c^{\\\\prime}$ of the points closest to the current centroid $c$. After hashing all the points $p$ closest to $c$ to a single bucket associated with $c$, we can compute $c^{\\\\prime}$ in a single sweep through this bucket.\\n  \\\\item PageRank: The new PageRank of vertex $v$ is the sum of old PageRank for all neighboring vertices $x$, where $(x, v)$ is a directed edge in the graph. Hashing the PageRank of $x$ to the bucket for all adjacent vertices $v$ collects all relevant information in the right place, so we can update the PageRank in one sweep through them.\\n\\\\end{itemize}\\n\\nThese algorithms can be specified through two programmer-written functions, map and reduce:\\n\\n\\\\begin{itemize}\\n  \\\\item Map: Make a sweep through each input file, hashing or emitting keyvalue pairs as appropriate. Consider the following pseudocode for the word count mapper:\\n\\\\end{itemize}\\n\\nMap(String docid, String text):\\\\\\\\\\nfor each word w in text:\\\\\\\\\\nEmit(w, 1);\\n\\n\\\\begin{itemize}\\n  \\\\item Reduce: Make a sweep through the set of values $v$ associated with a specific key $k$, aggregating and processing accordingly. Pseudocode for the word count reducer is:\\\\\\n%---- Page End Break Here ---- Page : 412\\n\\\\\\n\\\\includegraphics[max width=\\\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-427}\\n\\\\end{itemize}\\n\\nFigure 12.2: Word count in action. Count combination has been performed before mapping to reduce the size of the map files.\\n\\n\\\\begin{displayquote}\\nReduce(String term, Iterator $<$ Int $>$ values): int sum $=0 ;$\\\\\\\\\\nfor each v in values:\\\\\\\\\\n$\\\\quad$ sum $+=\\\\mathrm{v}$;\\n\\\\end{displayquote}\\n\\nThe efficiency of a MapReduce program depends upon many things, but one important objective is keeping the number of emits to a minimum. Emitting a count for each word triggers a message across machines, and this communication and associated writes to the bucket prove costly in large quantities. The more stuff that is mapped, the more that must eventually be reduced.\\n\\nThe ideal is to combine counts from particular input streams locally before, and then emit only the total for each distinct word per file. This could be done by adding extra logic/data structures to the map function. An alternate idea is to run mini-reducers in memory after the map phase, but before interprocessor communication, as an optimization to reduce network traffic. We note that optimization for in-memory computation is one of the major performance advantages of Spark over Hadoop for MapReduce-style programming.\\n\\nFigure 12.2 illustrates the flow of a MapReduce job for word counting, using three mappers and two reducers. Combination has been done locally, so the counts for each word used more than once in an input file (here doc and be) have been tabulated prior to emitting them to the reducers.\\n\\nOne problem illustrated by Figure 12.2 is that of mapping skew\\\\index{skew}, the natural imbalance in the amount of work assigned to each reduce task. In this toy example, the top reducer has been assigned map files with $33 \\\\%$ more words and $60 \\\\%$ larger counts than its partner. For a task with a serial running time of $T$, perfect parallelization with $n$ processors would yield a running time of $T / n$. But the running time of a MapReduce job is determined by the largest, slowest piece. Mapper skew dooms us to a largest piece that is often substantially higher than the average size.\\n\\n%---- Page End Break Here ---- Page : 413\\n\\nOne source of mapper skew is the luck of the draw, that it is rare to flip $n$ coins and end up with exactly as many heads as tails. But a more serious problem is that key frequency is often power law distributed, so the most frequent key will come to dominate the counts. Consider the word count problem, and assume that word frequency observes Zipf\\'s law from Section 5.1.5 Then the frequency of the most popular word (the) should be greater than the sum of the thousand words ranked from 1000 to 2000 . Whichever bucket the ends up in is likely to prove the hardest one to digest $\\\\cdot \\\\frac{1}{1}$\\n\\n\\\\subsection*{12.6.2 MapReduce under the Hood}\\nAll this is fine. But how does a MapReduce implementation like Hadoop ensure that all mapped items go to the right place? And how does it assign work to processors and synchronize the MapReduce operations, all in a fault-tolerant way?\\n\\nThere are two major components: the distributed hash table (or file system), and the run-time system handling coordination and managing resources. Both of these are detailed below.\\n\\n\\\\section*{Distributed File Systems}\\nLarge collections of computers can contribute their memory space (RAM) and local disk storage to attack a job, not just their CPUs. A distributed file system such as the Hadoop Distributed File System (HDFS) can be implemented as a distributed hash table. After a collection of machines register their available memory with the coordinating runtime system, each can be assigned a certain hash table range that it will be responsible for. Each process doing mapping can then ensure that the emitted items are forwarded to the appropriate bucket on the app\\\\index{Hadoop distributed ï¬le system}ropriate machine.\\n\\nBecause large numbers of items might be mapped to a single bucket, we may choose to represent buckets as disk files, with the new items appended to the end. Disk access is slow, but disk throughput is reasonable, so linear scans through files are generally manageable.\\n\\nOne problem with such a distributed hash table is fault tolerance: a single machine crash could lose enough values to invalidate the entire computation. The solution is to replicate everything for reliability on commodity hardware. In particular, the run-time system will replicate each item on three different machines, to minimize the chances of losing data in a hardware failure. Once the runtime system senses that a machine or disk is down, it gets to work replicating the lost data from these copies to restore the health of the file system.\\n\\n\\\\footnotetext{${ }^{1}$ This is one of the reasons the most frequent words in a language are declared stop words\\\\index{stop words}, and often omitted as features in text analysis problems.\\n\\n%---- Page End Break Here ---- Page : 414\\n}\\\\section*{MapReduce runtime system\\\\index{MapReduce runtime system}}\\nThe other major component of MapReduce environments for Hadoop or Spark is their runtime system, the layer of software which regulates such tasks as:\\n\\n\\\\begin{itemize}\\n  \\\\item Processor scheduling: Which cores get assigned to running which map and reduce tasks, and on which input files? The programmer can help by suggesting how many mappers and reducers should be active at any one time, but the assignment of jobs to cores is up to the runtime system.\\n  \\\\item Data distribution: This might involve moving data to an available processor that can deal with it, but recall that typical map and reduce operations require simple linear sweeps through potentially large files. Thus moving a file might be more expensive than just doing the computation we desire locally.\\\\\\\\\\nThus it is better to move processes to data. The runtime system should have configuration of which resources are available on which machine, and the general layout of the network. It can make an appropriate decision of which processes should run where.\\n  \\\\item Synchronization: Reducers can\\'t run until something has been mapped to them, and can\\'t complete until after the mapping is done. Spark permits more complicated work flows, beyond synchronized rounds of map and reduce. It is the runtime system that handles this synchronization.\\n  \\\\item Error and fault tolerance: The reliability of MapReduce requires recovering gracefully from hardware and communication\\\\index{communication}s failures. When the runtime system detects a worker failure, it attempts to restart the computation. When this fails, it transfers the uncompleted tasks to other workers. That this all happens seamlessly, without the involvement of the programmer, enables us to scale computations to large networks of machines, on the scale where hiccups become likely instead of rare events.\\n\\\\end{itemize}\\n\\n\\\\section*{Layers upon Layers}\\nSystems like HDFS and Hadoop are merely layers of software that other systems can build on. Although Spark can be thought of as a competitor for Hadoop, in fact it can leverage the Hadoop distributed file system and is often most efficient when doing so. These days, my students seem to spend less time writing low-level MapReduce jobs, because they instead use software layers working at higher levels of abstraction.\\n\\nThe full big data ecosystem consists of many different species. An important class are NoSQL databases\\\\index{NoSQL databases}, which permit the distribution of structured data over a distributed network of machines, enabling you to combine the RAM and disk from multiple machines. Further, these systems are typically designed so you can add additional machines and resources as you need them. The cost of this flexibility is that they usually support simpler query languages than full SQL, but still rich enough for many applications.\\n\\n%---- Page End Break Here ---- Page : 415\\n\\nThe big data software ecosystem evolves much more rapidly than the foundational matters discussed in this book. Google searches and a scan of the O\\'Reilly book catalog should reveal the latest technologies when you are ready to get down to business.\\n\\n\\\\subsection*{12.7 Societal and ethical implications\\\\index{ethical implications}}\\nOur ability to get into serious trouble increases with size. A car can cause a more serious accident than a bicycle, and an airplane more serious carnage than an automobile.\\n\\nBig data can do great things for the world, but it also holds the power to hurt individuals and society at large. Behaviors that are harmless on a small scale, like scraping, become intellectual property theft in the large. Describing the accuracy of your model in an excessively favorable light is common for PowerPoint presentations, but has real implications when your model then governs credit authorization or access to medical treatment. Losing access to your email account is a bonehead move, but not properly securing personal data for 100 million customers becomes potentially criminal.\\n\\nI end this book with a brief survey of common ethical concerns in the world of big data, to help sensitize you to the types of things the public worries about or should worry about:\\n\\n\\\\begin{itemize}\\n  \\\\item Integrity in communications and modeling\\\\index{modeling}: The data scientist serves as the conduit between their analysis and their employer or the general public. There is a great temptation to make our results seem stronger than they really are, by using a variety of time-tested techniques:\\n  \\\\item We can report a correlation or precision level, without comparing it to a baseline or reporting a $p$-value.\\n  \\\\item We can cherry pick among multiple experiments, and present only the best results we get, instead of presenting a more accurate picture.\\n  \\\\item We can use visualization techniques to obscure information, instead of reveal it.\\n\\\\end{itemize}\\n\\nEmbedded within every model are assumptions and weaknesses. A good modeler knows what the limitations of their model are: what they trust it to be able to do and where they start to feel less certain. An honest modeler communicates the full picture of their work: what they know and what they are not so sure of.\\\\\\\\\\nConflicts of interest are a genuine concern in data science. Often, one knows what the \"right answer\" is before the study, particularly the result that the boss wants most to hear. Perhaps your results will be used to influence public opinion, or appear in testimony before legal or governmental authorities. Accurate reporting and dissemination of results are essential behavior for ethical data scientists.\\n\\n%---- Page End Break Here ---- Page : 416\\n\\n\\\\begin{itemize}\\n  \\\\item transparency\\\\index{transparency} and ownership\\\\index{ownership}: Typically companies and research organizations publish data use and retention policies to demonstrate that they can be trusted with their customer\\'s data. Such transparency is important, but has proven to be subject to change just as soon as the commercial value of the data becomes apparent. It is often easier to get forgiveness than to get permission.\\\\\\\\\\nTo what extent do users own the data that they have generated? Ownership means that they should have the right to see what information has been collected from them, and the ability to prevent the future use of this material. These issues can get difficult, both technically and ethically. Should a criminal be able to demand all references to their crime be struck from a search engine like Google? Should my daughter be able to request removal of images of her posted by others without her permission?\\\\\\\\\\ndata errors\\\\index{data errors} can propagate and harm individuals, without allowing a mechanism to people to access and understand what information has been collected about them. Incorrect or incomplete financial information can ruin somebody\\'s credit rating, but credit agencies are forced by law to make each person\\'s record available to them and provide a mechanism to correct errors. However, data provenance is generally lost in the course of merging files, so these updates do not necessarily get back to all derivative products which were built from defective data. Without it, how can your customers discover and fix the incorrect information that you have about them?\\n  \\\\item Uncorrectable decisions and feedback loops: Employing models as hard screening criteria can be dangerous, particularly in domains where the model is just a proxy for what you really want to measure. Correlation is not causation. But consider a model suggesting that it is risky to hire a particular job candidate because people like him who live in lower-class neighborhoods are more likely to be arrested. If all employers use such models, these people simply won\\'t get hired, and are driven deeper into poverty through no fault of their own.\\\\\\\\\\nThese problems are particularly insidious because they are generally uncorrectable. The victim of the model typically has no means of appeal. And the owner of the model has no way to know what they are missing, i.e. how many good candidates were screened away without further consideration.\\n  \\\\item model-driven\\\\index{model-driven} bias\\\\index{bias} and filters: Big data permits the customization of products to best fit each individual user. Google, Facebook, and others analyze your data so as to show you the results their algorithms think you most want to see.\\\\\\\\\\nBut these algorithms may contain inadvertent biases picked up from machine learning algorithms on dubious training sets. Perhaps the search engine will show good job opportunities to men much more often than to women, or discriminate on other criteria.\\n\\n%---- Page End Break Here ---- Page : 417\\n\\\\end{itemize}\\n\\nShowing you exactly what you say you want to see may prevent you from seeing information that you really need to see. Such filters may have some responsibility for political polarization in our society: do you see opposing viewpoints, or just an echo chamber for your own thoughts?\\n\\n\\\\begin{itemize}\\n  \\\\item Maintaining the security\\\\index{security} of large data sets: Big data presents a bigger target for hackers than a spreadsheet on your hard drive. We have declared files with 100 million records to be bupkis, but that might represent personal data on $30 \\\\%$ of the population of the United States. Data breaches of this magnitude occur with distressing frequency.\\\\\\\\\\nMaking 100 million people change their password costs 190 man-years of wasted effort, even if each correction takes only one minute. But most information cannot be changed so readily: addresses, ID numbers, and account information persist for years if not a lifetime, making the damage from batch releases of data impossible to ever fully mitigate.\\\\\\\\\\nData scientists have obligations to fully adhere to the security practices of their organizations and identify potential weaknesses. They also have a responsibility to minimize the dangers of security breaches through encryption and anonymization. But perhaps most important is to avoid requesting fields and records you don\\'t need, and (this is absolutely the most difficult thing to do) deleting data once your project\\'s need for it has expired.\\n  \\\\item Maintaining privacy\\\\index{privacy} in aggregated data: It is not enough to delete names, addresses, and identity numbers to maintain privacy in a data set. Even anonymized data can be effectively de-anonymized in clever ways, by using orthogonal data sources. Consider the taxi data set we introduced in Section 1.6. It never contained any passenger identifier information in the first place. Yet it does provide pickup GPS coordinates to a resolution which might pinpoint a particular house as the source, and a particular strip joint as the destination. Now we have a pretty good idea who made that trip, and an equally good idea who might be interested in this information if the bloke were married.\\\\\\\\\\nA related experiment identified particular taxi trips taken by celebrities, so as to figure out their destination and how well they tipped Gay14. By using Google to find paparazzi photographs of celebrities getting into taxis and extracting the time and place they were taken, it was easy to identify the record corresponding to that exact pickup as containing the desired target.\\n\\\\end{itemize}\\n\\nEthical issues in data science are serious enough that professional organizations have weighed in on best practices, including the Data Science Code of Professional Conduct (\\\\href{http://www.datascienceassn.org/code-of-conduct.html}{http://www.datascienceassn.org/code-of-conduct.html}) of the Data Science Association and the Ethical Guidelines for Statistical Practices (\\\\href{http://www.amstat.org/about/ethicalguidelines.cfm}{http://www.amstat.org/about/ethicalguidelines.cfm}) of the American Statistical Association.\\n\\n%---- Page End Break Here ---- Page : 418\\n\\nI encourage you to read these documents to help you develop your sense of ethical issues and standards of professional behavior. Recall that people turn to data scientists for wisdom and consul, more than just code. Do what you can to prove worthy of this trust.\\n\\n\\\\subsection*{12.8 Chapter Notes}\\nThere exist no shortage of books on the topic of big data analysis. Leskovec, Rajarman and Ullman [RU14 is perhaps the most comprehensive of these, and a good place to turn for a somewhat deeper treatment of the topics we discuss here. This book and some companion videos are available at http: \\\\href{//www.mmds.org}{//www.mmds.org}.\\n\\nMy favorite hands-on resources on software technologies are generally books from O\\'Reilly Media. In the context of this chapter, I recommend their books on data analytics with Hadoop [BK16] and Spark RLOW15].\\n\\nO\\'Neil O\\'N16 provides a thought-provoking look at the social dangers of big data analysis, emphasizing the misuse of opaque models relying on proxy data sources that create feedback loops which exacerbate the problems they are trying to solve.\\n\\nThe analogy of disk/cache speeds to tortoise/escape velocity is due to Michael Bender.\\n\\n\\\\subsection*{12.9 exercises\\\\index{exercises}}\\n\\\\section*{Parallel and Distributed Processing}\\n12-1. [3] What is the difference between parallel processing and distributed processing?\\n\\n12-2. [3] What are the benefits of MapReduce?\\\\\\\\[0pt]\\n12-3. [5] Design MapReduce algorithms to take large files of integers and compute:\\n\\n\\\\begin{itemize}\\n  \\\\item The largest integer.\\n  \\\\item The average of all the integers.\\n  \\\\item The number of distinct integers in the input.\\n  \\\\item The mode of the integers.\\n  \\\\item The median of the integers.\\n\\\\end{itemize}\\n\\n12-4. [3] Would we expect map skew to be a bigger problem when there are ten reducers or a hundred reducers?\\\\\\\\[0pt]\\n12-5. [3] Would we expect the problem of map skew to increase or decrease when we combine counts from each file before emitting them?\\n\\n12-6. [5] For each of the following The Quant Shop prediction challenges dream up the most massive possible data source that might reasonably exist, who might have it, and what biases might lurk in its view of the world.\\\\\\n%---- Page End Break Here ---- Page : 419\\n\\\\\\n(a) Miss Universe.\\\\\\\\\\n(b) Movie gross.\\\\\\\\\\n(c) Baby weight.\\\\\\\\\\n(d) Art auction price.\\\\\\\\\\n(e) White Christmas.\\\\\\\\\\n(f) Football champions.\\\\\\\\\\n(g) Ghoul pool.\\\\\\\\\\n(h) Gold/oil prices.\\n\\n\\\\section*{Ethics}\\n12-7. [3] What are five practical ways one can go about protecting privacy in big data?\\\\\\\\[0pt]\\n12-8. [3] What do you consider to be acceptable boundaries for Facebook to use the data it has about you? Give examples of uses which would be unacceptable to you. Are these forbidden by their data usage agreement?\\\\\\\\[0pt]\\n12-9. [3] Give examples of decision making where you would trust an algorithm to make as good or better decisions as a person. For what tasks would you trust human judgment more than an algorithm? Why?\\n\\n\\\\section*{Implementation Projects}\\n12-10. [5] Do the stream sampling methods we discussed really produce uniform random samples from the desired distribution? Implement them, draw samples, and run them through the appropriate statistical test.\\\\\\\\[0pt]\\n12-11. [5] Set up a Hadoop or Spark cluster that spans two or more machines. Run a basic task like word counting. Does it really run faster than a simple job on one machine? How many machines/cores do you need in order to win?\\\\\\\\[0pt]\\n12-12. [5] Find a big enough data source which you have access to, that you can justify processing with more than a single machine. Do something interesting with it.\\n\\n\\\\section*{Interview Questions}\\n12-13. [3] What is your definition of big data?\\\\\\\\[0pt]\\n12-14. [5] What is the largest data set that you have processed? What did you do, and what were the results?\\\\\\\\[0pt]\\n12-15. [8] Give five predictions about what will happen in the world over the next twenty years?\\\\\\\\[0pt]\\n12-16. [5] Give some examples of best practices in data science.\\\\\\\\[0pt]\\n12-17. [5] How might you detect bogus reviews, or bogus Facebook accounts used for bad purposes?\\\\\\\\[0pt]\\n12-18. [5] What do the map function and the reduce function do, under the Map-Reduce paradigm? What do the combiner and partitioner do?\\\\\\\\[0pt]\\n12-19. [5] Do you think that the typed login/password will eventually disappear? How might they be replaced?\\\\\\\\[0pt]\\n12-20. [5] When a data scientist cannot draw any conclusion from a data set, what should they say to their boss/customer?\\n\\n%---- Page End Break Here ---- Page : 420\\n\\n12-21. [3] What are hash table collisions? How can they be avoided? How frequently do they occur?\\n\\n\\\\section*{Kaggle Challenges}\\n12-22. Which customers will become repeat buyers? \\\\href{https://www.kaggle.com/c/acquire-valued-shoppers-challenge}{https://www.kaggle.com/c/acquire-valued-shoppers-challenge}\\\\\\\\\\n$12-23$. Which customers are worth sending junk mail to? \\\\href{https://www.kaggle.com/c/springleaf-marketing-response}{https://www.kaggle.com/c/springleaf-marketing-response}\\n\\n12-24. Which hotel should you recommend to a given traveler? \\\\href{https://www.kaggle.com/c/expedia-hotel-recommendations}\\n%---- Page End Break Here ---- Page : 421\\n{https://www.kaggle.com/c/expedia-hotel-recommendations}\\n\\n\\\\section*{Chapter 13}\\n\\\\section*{Coda}\\n\"Begin at the beginning,\" the King said, gravely, \"and go on till you come to the end: then stop.\"\\n\\n\\\\begin{itemize}\\n  \\\\item Lewis Carroll\\n\\\\end{itemize}\\n\\nHopefully you the reader have been at least partially enlightened by this book, and remain excited by the power of data. The most common path to employing these skills is to take a job in industry. This is a noble calling, but be aware there are also other possibilities.\\n\\n\\\\subsection*{13.1 Get a Job!}\\nThere are very rosy predictions of the job prospects for future data scientists. The McKinsey Global Institute projects that demand for \"deep analytical talent in the United States could be $50 \\\\%$ to $60 \\\\%$ greater than its projected supply by 2018.\" The job placement site \\\\href{http://www.glassdoor.com}{www.glassdoor.com} informs me that as of today, the average data scientist salary is precisely $\\\\$ 113,436$. Harvard Business Review declared that being a data scientist is \"the sexiest job of the 21st century\" DP12. That sounds like the place where I want to be!\\n\\nBut all this testimony would be much more convincing if there was some widely shared understanding of what exactly a data scientist is. It is less obvious to me that there are ever destined to be vast numbers of jobs with the official title of data scientist the way there are for, say software engineer or computer programmer. But don\\'t panic.\\n\\nIt is fair to say that there are several different types of jobs that relate to data science, distinguished by the relative importance of applications knowledge and technical strength. I see the following basic career tracks related to data science:\\n\\n\\\\begin{itemize}\\n  \\\\item Software engineering for data science: A substantial fraction of high-end software development positions are at big data companies like Google,\\n\\\\end{itemize}\\n\\nFacebook, and Amazon, or data-centric companies in the financial sector, like banks and hedge funds. These jobs revolve around building largescale software infrastructures for managing data, and generally require a degree in computer science to acquire the necessary technical skills and experience.\\n\\n\\\\begin{itemize}\\n  \\\\item Statistician/data scientists: There has always been a diverse job market for trained statisticians, especially in health care, manufacturing, business, education, and the government/non-profit sectors. This world will continue to grow and thrive, although I suspect it will demand stronger computational skills than in the past. These computational-oriented statistical analysts will have training or experience in data science, building on a strong foundation in statistics.\\n  \\\\item Quantitative business analysts: A large cohort of business professionals work in marketing, sales, advertising, and management, providing essential functions at any product-based or consulting company. These careers require a greater degree of business domain knowledge than the previous two categories, but increasingly expect quantitative skills. They may be hiring you to work in marketing, but demand a background or experience in data science/analytics. Or they hire you to work in human resources, but expect you to be able to develop metrics for job performance and satisfaction.\\n\\\\end{itemize}\\n\\nThe material covered in this book is essential for all three of these career tracks, but obviously you have more to learn. The careers which are easiest to train for also prove to be the quickest to saturate, so keep developing your skills through coursework, projects, and practice.\\n\\n\\\\subsection*{13.2 Go to Graduate School!}\\nIf you find the ideas and methods presented in this book interesting, perhaps you are the kind of person who should think about going to graduate school. Technical skills age quickly without advanced training, and it can be difficult to find the time for professional training after joining the working world.\\n\\nGraduate programs in data science are rapidly emerging from host departments of computer science, statistics, business, applied mathematics, and the like. Which type of program is most appropriate for you depends upon your undergraduate training and life experiences. Depending upon their focus, data science programs will differ wildly in the computational and statistical background they expect. Generally speaking, the technically-hardest programs in terms of programming, machine learning, and statistics provide the best preparation for the future. Be aware of grandiose claims from programs which minimize these demands.\\n\\n\\\\footnotetext{${ }^{1}$ Good for you if you are already there!\\n}Most of these programs are at the masters level, but outstanding students who are able to make the life commitment should consider the possibility of undertaking a Ph.D degree. Graduate study in computer science, machine learning, or statistics involves courses in advanced topics that build upon what you learned as an undergraduate, but more importantly you will be doing new and original research in the area of your choice. All reasonable American doctoral programs will pay tuition and fees for all accepted Ph.D students, plus enough of a stipend to live comfortably if not lavishly.\\n\\nIf you have a strong computer science background and the right stuff, I would encourage you to continue your studies, ideally by coming to work with us at Stony Brook! My group does research in a variety of interesting topics in data science, as you can tell from the war stories. Please check us out at \\\\href{http://www.data-manual.com/gradstudy}{http://www.data-manual.com/gradstudy}.\\n\\n\\\\subsection*{13.3 Professional Consulting Services}\\nAlgorist Technologies is a consulting firm that provides its clients with shortterm, expert help in data science and algorithm design. Typically, an Algorist consultant is called in for one to three days worth of intensive on site discussion and analysis with the client\\'s own development staff. Algorist has built an impressive record of performance improvements with several companies and applications, as well as expert witness services and longer-term consulting.\\n\\nVisit \\\\href{http://www.algorist.com/consulting}{www.algorist.com/consulting} for more information on the services provided by Algorist Technologies.\\n\\n%---- Page End Break Here ---- Page : 425\\n\\n\\\\section*{Chapter 14}\\n\\\\section*{Bibliography}\\n[Abe13] Andrew Abela. Advanced Presentations by Design: Creating Communication that Drives Action. Pfeiffer, 2nd edition, 2013.\\\\\\\\[0pt]\\n[Ans73] Francis J Anscombe. Graphs in statistical analysis. The American Statistician, 27(1):17-21, 1973.\\\\\\\\[0pt]\\n[Bab11] Charles Babbage. Passages from the Life of a Philosopher. Cambridge University Press, 2011.\\\\\\\\[0pt]\\n[Bar10] James Barron. Apple\\'s new device looks like a winner. From 1988. The New York Times, January 28, 2010.\\\\\\\\[0pt]\\n[Ben12] Edward A Bender. An Introduction to Mathematical Modeling. Courier Corporation, 2012.\\\\\\\\[0pt]\\n[Bis07] Christopher Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics. Springer, New York, 2007.\\\\\\\\[0pt]\\n[BK07] Robert M. Bell and Yehuda Koren. Lessons from the Netflix prize challenge. ACM SIGKDD Explorations Newsletter, 9(2):75-79, 2007.\\\\\\\\[0pt]\\n[BK16] Benjamin Bengfort and Jenny Kim. Data Analytics with Hadoop: An Introduction for Data Scientists. O\\'Reilly Media, Inc., 2016.\\\\\\\\[0pt]\\n[BP98] Serge Brin and Larry Page. The Anatomy of a Large-Scale Hypertextual Web Search Engine. In Proc. 7th Int. Conf. on World Wide Web ( $W W W$ ), pages 107-117, 1998.\\\\\\\\[0pt]\\n[Bra99] Ronald Bracewell. The Fourier Transform and its Applications. McGraw-Hill, 3rd edition, 1999.\\\\\\\\[0pt]\\n[Bri88] E. Oran Brigham. The Fast Fourier Transform. Prentice Hall, Englewood Cliffs NJ, facimile edition, 1988.\\\\\\\\\\n$\\\\left[\\\\mathrm{BSC}^{+} 11\\\\right] \\\\quad$ M. Borjesson, L. Serratosa, F. Carre, D. Corrado, J. Drezner, D. Dugmore, H. Heidbuchel, K. Mellwig, N. Panhuyzen-Goedkoop, M. Papadakis, H. Rasmusen, S. Sharma, E. Solberg, F. van Buuren, and A. Pelliccia. Consensus document regarding cardiovascular safety at sports arenas. European Heart Journal, 32:2119-2124, 2011.\\\\\\\\[0pt]\\n[BT08] Dimitri Bertsekas and John Tsitsklis. Introduction to Probability. Athena Scientific, 2nd edition, 2008.\\\\\\\\[0pt]\\n[BVS08] Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena. International Sentiment Analysis for News and Blogs. In Proceedings of the International Conference on Weblogs and Social Media, Seattle, WA, April 2008.\\\\\\\\[0pt]\\n[BWPS10] Mikhail Bautin, Charles B Ward, Akshay Patil, and Steven S Skiena. Access: news and blog analysis for the social sciences. In Proceedings of the 19th International Conference on World Wide Web, pages 12291232. ACM, 2010.\\\\\\\\[0pt]\\n[ $\\\\left.\\\\mathrm{CPS}^{+} 08\\\\right] \\\\mathrm{J}$ Robert Coleman, Dimitris Papamichail, Steven Skiena, Bruce Futcher, Eckard Wimmer, and Steffen Mueller. Virus attenuation by genome-scale changes in codon pair bias. Science, 320(5884):1784-1787, 2008.\\\\\\\\[0pt]\\n[CPS15] Yanqing Chen, Bryan Perozzi, and Steven Skiena. Vector-based similarity measurements for historical figures. In International Conference on Similarity Search and Applications, pages 179-190. Springer, 2015.\\\\\\\\[0pt]\\n[dBvKOS00] Mark de Berg, Mark van Kreveld, Mark Overmars, and Otfried Schwarzkopf. Computational Geometry: Algorithms and Applications. Springer, 2nd edition, 2000.\\\\\\\\[0pt]\\n[DDKN11] Sebastian Deterding, Dan Dixon, Rilla Khaled, and Lennart Nacke. From game design elements to gamefulness: defining gamification. In Proceedings of the 15th International Academic MindTrek Conference: Envisioning future media environments, pages 9-15. ACM, 2011.\\\\\\\\[0pt]\\n[Don15] David Donoho. 50 years of data science. Tukey Centennial Workshop, Princeton NJ, 2015.\\\\\\\\[0pt]\\n[DP12] Thomas H Davenport and DJ Patil. Data scientist. Harvard Business Review, 90(5):70-76, 2012.\\\\\\\\[0pt]\\n[EK10] David Easley and Jon Kleinberg. Networks, Crowds, and Markets: Reasoning about a highly connected world. Cambridge University Press, 2010.\\\\\\\\[0pt]\\n[ELLS11] Brian Everitt, Sabine Landau, Mmorven Leese, and Daniel Stahl. Cluster Analysis. Wiley, 5th edition, 2011.\\\\\\\\[0pt]\\n[ELS93] Peter Eades, X. Lin, and William F. Smyth. A fast and effective heuristic for the feedback arc set problem. Information Processing Letters, 47:319323, 1993.\\\\\\\\[0pt]\\n[ \\\\$\\\\textbackslash mathrm\\\\{FCH\\\\}\\\\^{}\\\\{+\\\\}\\\\$14] Matthew Faulkner, Robert Clayton, Thomas Heaton, K Mani Chandy, Monica Kohler, Julian Bunn, Richard Guy, Annie Liu, Michael Olson, MingHei Cheng, et al. Community sense and response systems: Your phone as quake detector. Communications of the ACM, 57(7):66-75, 2014.\\\\\\\\[0pt]\\n[Few09] Stephen Few. Now You See It: simple visualization techniques for quantitative analysis. Analytics Press, Oakland CA, 2009.\\\\\\\\[0pt]\\n[FHT01] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning. Springer, 2001.\\\\\\\\[0pt]\\n[FPP07] David Freedman, Robert Pisani, and Roger Purves. Statistics. WW Norton \\\\& Co, New York, 2007.\\\\\\\\[0pt]\\n[Gay14] C. Gayomali. NYC taxi data blunder reveals which celebs don\\'t tip and who frequents strip clubs. \\\\href{http://www.fastcompany.com/3036573/}{http://www.fastcompany.com/3036573/}, October 2. 2014.\\\\\\\\[0pt]\\n[GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\\\\\\\\[0pt]\\n[GFH13] Frank R. Giordano, William P. Fox, and Steven B. Horton. A First Course in Mathematical Modeling. Nelson Education, 2013.\\\\\\\\[0pt]\\n[Gle96] James Gleick. A bug and a crash: sometimes a bug is more than a nuisance. The New York Times Magazine, December 1, 1996.\\\\\\\\[0pt]\\n[GMP \\\\$\\\\{ \\\\}\\\\^{}\\\\{+\\\\}\\\\$09] Jeremy Ginsberg, Matthew H Mohebbi, Rajan S Patel, Lynnette Brammer, Mark S Smolinski, and Larry Brilliant. Detecting influenza epidemics using search engine query data. Nature, 457(7232):1012-1014, 2009.\\\\\\\\[0pt]\\n[Gol16] David Goldenberg. The biggest dinosaur in history may never have existed. FiveThirtyEight, \\\\href{http://fivethirtyeight.com/features/the-biggest-dinosaur-in-history-may-never-have-existed/}{http://fivethirtyeight.com/features/the-biggest-dinosaur-in-history-may-never-have-existed/}, January 11, 2016.\\\\\\\\[0pt]\\n[Gru15] Joel Grus. Data Science from Scratch: First principles with Python. O\\'Reilly Media, Inc., 2015.\\\\\\\\[0pt]\\n[GSS07] Namrata Godbole, Manja Srinivasaiah, and Steven Skiena. Large-scale sentiment analysis for news and blogs. Int. Conf. Weblogs and Social Media, 7:21, 2007.\\\\\\\\[0pt]\\n[HC88] Diane F Halpern and Stanley Coren. Do right-handers live longer? Nature, 333:213, 1988.\\\\\\\\[0pt]\\n[HC91] Diane F Halpern and Stanley Coren. Handedness and life span. N Engl J Med, 324(14):998-998, 1991.\\\\\\\\[0pt]\\n[HS10] Yancheng Hong and Steven Skiena. The wisdom of bookies? sentiment analysis vs. the NFL point spread. In Int. Conf. on Weblogs and Social Media, 2010.\\\\\\\\[0pt]\\n[Huf10] Darrell Huff. How to Lie with Statistics. WW Norton \\\\& Company, 2010.\\\\\\\\[0pt]\\n[Ind04] Piotr Indyk. Nearest neighbors in high-dimensional spaces. In J. Goodman and J. O\\'Rourke, editors, Handbook of Discrete and Computational Geometry, pages 877-892. CRC Press, 2004.\\\\\\\\[0pt]\\n[Jam10] Bill James. The New Bill James Historical Baseball Abstract. Simon and Schuster, 2010.\\\\\\\\[0pt]\\n[JLSI99] Vic Jennings, Bill Lloyd-Smith, and Duncan Ironmonger. Household size and the Poisson distribution. J. Australian Population Association, 16:65-84, 1999.\\\\\\\\[0pt]\\n[Joa02] Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 133-142. ACM, 2002.\\\\\\\\[0pt]\\n[Joh07] Steven Johnson. The Ghost Map: The story of London\\'s most terrifying epidemic - and how it changed science, cities, and the modern world. Riverhead Books, 2007.\\\\\\\\[0pt]\\n[JWHT13] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning. Springer-Verlag, sixth edition, 2013.\\\\\\\\[0pt]\\n[Kap12] Karl M Kapp. The Gamification of Learning and Instruction: Gamebased methods and strategies for training and education. Wiley, 2012.\\\\\\n%---- Page End Break Here ---- Page : 429\\n\\\\[0pt]\\n[KARPS15] Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. Statistically significant detection of linguistic change. In Proceedings of the 24th International Conference on World Wide Web, pages 625-635. ACM, 2015.\\\\\\\\[0pt]\\n[KCS08] Aniket Kittur, Ed H Chi, and Bongwon Suh. Crowdsourcing user studies with mechanical turk. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 453-456. ACM, 2008.\\\\\\\\\\n$\\\\left[\\\\mathrm{KKK}^{+} 10\\\\right]$ Slava Kisilevich, Milos Krstajic, Daniel Keim, Natalia Andrienko, and Gennady Andrienko. Event-based analysis of people\\'s activities and behavior using flickr and panoramio geotagged photo collections. In 2010 14th International Conference Information Visualisation, pages 289-296. IEEE, 2010.\\\\\\\\[0pt]\\n[Kle13] Phillip Klein. Coding the Matrix: Linear Algebra through Computer Science Applications. Newtonian Press, 2013.\\\\\\\\[0pt]\\n[KSG13] Michal Kosinski, David Stillwell, and Thore Graepel. Private traits and attributes are predictable from digital records of human behavior. Proc. National Academy of Sciences, 110(15):5802-5805, 2013.\\\\\\\\[0pt]\\n[KTDS17] Vivek Kulkarni, Yingtao Tian, Parth Dandiwala, and Steven Skiena. Dating documents: A domain independent approach to predict year of authorship. Submitted for publication, 2017.\\\\\\\\[0pt]\\n[Lei07] David J. Leinweber. Stupid data miner tricks: overfitting the S\\\\&P 500. The Journal of Investing, 16(1):15-22, 2007.\\\\\\\\[0pt]\\n[Lew04] Michael Lewis. Moneyball: The art of winning an unfair game. WW Norton \\\\& Company, 2004.\\\\\\\\[0pt]\\n[LG14] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177-2185, 2014.\\\\\\\\[0pt]\\n[LKKV14] David Lazer, Ryan Kennedy, Gary King, and Alessandro Vespignani. The parable of Google flu: traps in big data analysis. Science, 343(6176):1203-1205, 2014.\\\\\\\\[0pt]\\n[LKS05] Levon Lloyd, Dimitrios Kechagias, and Steven Skiena. Lydia: A system for large-scale news analysis. In SPIRE, pages 161-166, 2005.\\\\\\\\[0pt]\\n[LLM15] David Lay, Steven Lay, and Judi McDonald. Linear Algebra and its Applications. Pearson, 5th edition, 2015.\\\\\\\\[0pt]\\n[LM12] Amy Langville and Carl Meyer. Who\\'s \\\\#1? The Science of Rating and Ranking. Princeton Univ. Press, 2012.\\\\\\\\[0pt]\\n[LRU14] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. Mining of Massive Datasets. Cambridge University Press, 2014.\\\\\\\\[0pt]\\n[Ma199] Burton Gordon Malkiel. A Random Walk Down Wall Street: Including a life-cycle guide to personal investing. WW Norton \\\\& Company, 1999.\\\\\\\\\\n$\\\\left[\\\\mathrm{MAV}^{+} 11\\\\right]$ J. Michel, Y. Shen A. Aiden, A. Veres, M. Gray, Google Books Team, J. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker, M. Nowak, and E. Aiden. Quantitative analysis of culture using millions of digitized books. Science, 331:176-182, 2011.\\\\\\n%---- Page End Break Here ---- Page : 430\\n\\\\[0pt]\\n[MBL \\\\$\\\\{ \\\\}\\\\^{}\\\\{+\\\\}\\\\$06] Andrew Mehler, Yunfan Bao, Xin Li, Yue Wang, and Steven Skiena. Spatial Analysis of News Sources. In IEEE Trans. Vis. Comput. Graph., volume 12, pages 765-772, 2006.\\\\\\\\[0pt]\\n[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\\\\\\\[0pt]\\n[McK12] Wes McKinney. Python for Data Analysis: Data wrangling with Pandas, NumPy, and IPython. O\\'Reilly Media, Inc., 2012.\\\\\\\\[0pt]\\n[McM04] Chris McManus. Right Hand, Left Hand: The origins of asymmetry in brains, bodies, atoms and cultures. Harvard University Press, 2004.\\\\\\\\\\n$\\\\left[\\\\mathrm{MCP}^{+} 10\\\\right]$ Steffen Mueller, J Robert Coleman, Dimitris Papamichail, Charles B Ward, Anjaruwee Nimnual, Bruce Futcher, Steven Skiena, and Eckard Wimmer. Live attenuated influenza virus vaccines by computer-aided rational design. Nature Biotechnology, 28(7):723-726, 2010.\\\\\\\\\\n$\\\\left[M_{M O R}{ }^{+88]}\\\\right.$ Bartlett W Mel, Stephen M Omohundro, Arch D Robison, Steven S Skiena, Kurt H. Thearling, Luke T. Young, and Stephen Wolfram. Tablet: personal computer of the year 2000. Communications of the ACM, 31(6):638-648, 1988.\\\\\\\\[0pt]\\n[NYC15] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 427-436. IEEE, 2015.\\\\\\\\[0pt]\\n[O\\'N16] Cathy O\\'Neil. Weapons of Math Destruction: How big data increases inequality and threatens democracy. Crown Publishing Group, 2016.\\\\\\\\[0pt]\\n[O\\'R01] Joseph O\\'Rourke. Computational Geometry in C. Cambridge University Press, New York, 2nd edition, 2001.\\\\\\\\[0pt]\\n[Pad15] Sydney Padua. The Thrilling Adventures of Lovelace and Babbage: The (mostly) true story of the first computer. Penguin, 2015.\\\\\\\\[0pt]\\n[PaRS14] Bryan Perozzi, Rami al Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 701-710. ACM, 2014.\\\\\\\\[0pt]\\n[PFTV07] William Press, Brian Flannery, Saul Teukolsky, and William T. Vetterling. Numerical Recipes: The art of scientific computing. Cambridge University Press, 3rd edition, 2007.\\\\\\\\[0pt]\\n[PSM14] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543, 2014.\\\\\\\\[0pt]\\n[RD01] Ed Reingold and Nachum Dershowitz. Calendrical Calculations: The Millennium Edition. Cambridge University Press, New York, 2001.\\\\\\\\[0pt]\\n[RLOW15] Sandy Ryza, Uri Laserson, Sean Owen, and Josh Wills. Advanced Analytics with Spark: Patterns for Learning from Data at Scale. O\\'Reilly Media, Inc., 2015.\\\\\\\\[0pt]\\n[Sam05] H. Samet. Multidimensional spatial data structures. In D. Mehta and S. Sahni, editors, Handbook of Data Structures and Applications, pages 16:1-16:29. Chapman and Hall/CRC, 2005.\\\\\\n%---- Page End Break Here ---- Page : 431\\n\\\\[0pt]\\n[Sam06] Hanan Samet. Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann, 2006.\\\\\\\\[0pt]\\n[SAMS97] George N Sazaklis, Esther M Arkin, Joseph SB Mitchell, and Steven S Skiena. Geometric decision trees for optical character recognition. In Proceedings of the 13th Annual Symposium on Computational Geometry, pages 394-396. ACM, 1997.\\\\\\\\[0pt]\\n[SF12] Gail M. Sullivan and Richard Feinn. Using effect size: or why the $p$ value is not enough. J. Graduate Medical Education, 4:279282, 2012.\\\\\\\\[0pt]\\n[Sil12] Nate Silver. The Signal and the Noise: Why so many predictions fail-but some don\\'t. Penguin, 2012.\\\\\\\\[0pt]\\n[Ski01] S. Skiena. Calculated Bets: Computers, Gambling, and Mathematical Modeling to Win. Cambridge University Press, New York, 2001.\\\\\\\\[0pt]\\n[Ski08] S. Skiena. The Algorithm Design Manual. Springer-Verlag, London, second edition, 2008.\\\\\\\\[0pt]\\n[Ski12] Steven Skiena. Redesigning viral genomes. Computer, 45(3):0047-53, 2012.\\\\\\\\\\n$\\\\left[\\\\mathrm{SMB}^{+} 99\\\\right]$ Arthur G Stephenson, Daniel R Mulville, Frank H Bauer, Greg A Dukeman, Peter Norvig, Lia S LaPiana, Peter J Rutledge, David Folta, and Robert Sackheim. Mars climate orbiter mishap investigation board phase i report. NASA, Washington, $D C$, page 44, 1999.\\\\\\\\\\n$\\\\left[\\\\mathrm{SRS}^{+} 14\\\\right]$ Paolo Santi, Giovanni Resta, Michael Szell, Stanislav Sobolevsky, Steven H Strogatz, and Carlo Ratti. Quantifying the benefits of vehicle pooling with shareability networks. Proceedings of the National Academy of Sciences, 111(37):13290-13294, 2014.\\\\\\\\[0pt]\\n[SS15] Oleksii Starov and Steven Skiena. GIS technology supports taxi tip prediction. Esri Map Book, 2014 User Conference, July 14-17, San Diego, 2015.\\\\\\\\[0pt]\\n[Str11] Gilbert Strang. Introduction to Linear Algebra. Wellesley-Cambridge Press, 2011.\\\\\\\\[0pt]\\n[Sur05] James Surowiecki. The wisdom of crowds. Anchor, 2005.\\\\\\\\[0pt]\\n[SW13] Steven S. Skiena and Charles B. Ward. Who\\'s Bigger?: Where Historical Figures Really Rank. Cambridge University Press, 2013.\\\\\\\\[0pt]\\n[Tij12] Henk Tijms. Understanding Probability. Cambridge University Press, 2012.\\\\\\\\[0pt]\\n[Tuc88] Alan Tucker. A Unified Introduction to Linear Algebra: Models, methods, and theory. Macmillan, 1988.\\\\\\\\[0pt]\\n[Tuf83] Edward R Tufte. The Visual Display of Quantitative Information. Graphics Press, Cheshire, CT, 1983.\\\\\\\\[0pt]\\n[Tuf90] Edward R Tufte. Envisioning Information. Graphics Press, Cheshire, CT, 1990.\\\\\\\\[0pt]\\n[Tuf97] Edward R Tufte. Visual Explanations. Graphics Press, Cheshire, CT, 1997.\\\\\\\\\\n$\\\\left[\\\\mathrm{VAMM}^{+} 08\\\\right]$ Luis Von Ahn, Benjamin Maurer, Colin McMillen, David Abraham, and Manuel Blum. recaptcha: Human-based character recognition via web security measures. Science, 321(5895):1465-1468, 2008.\\\\\\n%---- Page End Break Here ---- Page : 432\\n\\\\[0pt]\\n[Vig15] Tyler Vigen. Spurious Correlations. Hatchette Books, 2015.\\\\\\\\[0pt]\\n[Wat16] Thayer Watkins. Arrow\\'s impossibility theorem for aggregating individual preferences into social preferences. \\\\href{http://www.sjsu.edu/faculty/watkins/arrow.htm}{http://www.sjsu.edu/faculty/watkins/arrow.htm}, 2016.\\\\\\\\[0pt]\\n[Wea82] Warren Weaver. Lady Luck. Dover Publications, 1982.\\\\\\\\[0pt]\\n[Wei05] Sanford Weisberg. Applied linear regression, volume 528. Wiley, 2005.\\\\\\\\[0pt]\\n[Wes00] Doug West. Introduction to Graph Theory. Prentice-Hall, Englewood Cliffs NJ, second edition, 2000.\\\\\\\\[0pt]\\n[Whe13] Charles Wheelan. Naked Statistics: Stripping the dread from the data. WW Norton \\\\& Company, 2013.\\\\\\\\[0pt]\\n[ZS09] Wenbin Zhang and Steven Skiena. Improving movie gross prediction through news analysis. In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology-Volume 01, pages 301-304. IEEE Computer Society, 2009.\\\\\\\\[0pt]\\n[ZS10] Wenbin Zhang and Steven Skiena. Trading strategies to exploit blog and news sentiment. In Proc. Int. Conf. Weblogs and Social Media (ICWSM), 2010.\\n\\n%---- Page End Break Here ---- Page : 433\\n\\n\\\\section*{Index}\\nA/B testing, 86\\\\\\\\\\nAaron Schwartz case, 68\\\\\\\\\\nAB testing, 137\\\\\\\\\\nacademic data, 66\\\\\\\\\\naccuracy, 215, 228\\\\\\\\\\nactivation function, 380\\\\\\\\\\nAdaBoost, 364\\\\\\\\\\nadd-one discounting, 357\\\\\\\\\\nagglomerative cluster trees, 338\\\\\\\\\\naggregation mechanisms, 83\\\\\\\\\\nAkaike information criterion, 289, 335\\\\\\\\\\nalgorithm analysis, 397\\\\\\\\\\nAmazon Turk, 67, 84\\\\\\\\\\ntasks assigned, 85\\\\\\\\\\nTurkers, 84\\\\\\\\\\nAmerican basketball players, 97\\\\\\\\\\nanalogies, 312\\\\\\\\\\nanchoring, 82\\\\\\\\\\nangular distance, 310\\\\\\\\\\nAnscombe\\'s Quartet, 159\\\\\\\\\\nAOL, 64\\\\\\\\\\nAPI, 65\\\\\\\\\\nApple iPhone sales, 34\\\\\\\\\\napplication program interfaces, 65\\\\\\\\\\narea under the ROC curve, 219\\\\\\\\\\nAristotle, 326, 327\\\\\\\\\\nArrow\\'s impossibility theorem, 84, 114\\\\\\\\\\nartifacts, 69\\\\\\\\\\nAscombe quartet, 272\\\\\\\\\\nasking interesting questions, 4\\\\\\\\\\nassociativity, 244\\\\\\\\\\nautocorrelation, 46\\\\\\\\\\naverage link, 340\\\\\\\\\\nBabbage, Charles\\\\index{Babbage, Charles}, 57, 90\\\\\\\\\\nbackpropagation, 382\\\\\\\\\\nBacon, Kevin, 9\\\\\\\\\\nbag of words, 14\\\\\\\\\\nbagging, 362\\\\\\\\\\nbalanced training classes, 295\\\\\\\\\\nbar charts, 179\\\\\\\\\\nbest practices, 181\\\\\\\\\\nstacked, 181\\\\\\\\\\nBarzun, Jacques, 5\\\\\\\\\\nbaseball encyclopedia, 5\\\\\\\\\\nbaseline models, 210\\\\\\\\\\nfor classification, 210\\\\\\\\\\nfor value prediction, 212\\\\\\\\\\nBayes\\' theorem, 150, 205, 299\\\\\\\\\\nBaysian information criteria, 289\\\\\\\\\\nbell-shaped, 123, 141\\\\\\\\\\ndistribution, 101\\\\\\\\\\nbias, 202, 417\\\\\\\\\\nlexicographic, 405\\\\\\\\\\nnumerical, 405\\\\\\\\\\ntemporal, 405\\\\\\\\\\nbias-variance trade-off, 202\\\\\\\\\\nbig data, 391\\\\\\\\\\nalgorithms, 397\\\\\\\\\\nbad data, 392\\\\\\\\\\nstatistics, 392\\\\\\\\\\nbig data engineer, 4\\\\\\\\\\nbig oh analysis, 397\\\\\\\\\\nbinary relations, 320\\\\\\\\\\nbinary search, 398\\\\\\\\\\nbinomial distribution\\\\index{binomial distribution}, 123\\\\\\\\\\nÂ© The Author(s) 2017\\\\\\\\\\nS.S. Skiena, The Data Science Design Manual,\\n\\nTexts in Computer Science, \\\\href{https://doi.org/10.1007/978-3-319-55444-0}\\n%---- Page End Break Here ---- Page : 435\\n\\n%---- Page End Break Here ---- Page : 427\\n{https://doi.org/10.1007/978-3-319-55444-0}\\\\\\\\\\nbioinformatician, 50\\\\\\\\\\nBlumenstock, Josh, 27\\\\\\\\\\nBody Mass Index, 96, 177\\\\\\\\\\nBonferroni correction, 141\\\\\\\\\\nboosting, 363, 364\\\\\\\\\\nalgorithms, 364\\\\\\\\\\nbootstrapping, 374\\\\\\\\\\nBorda\\'s method, 108\\\\\\\\\\nbox plots, 175\\\\\\\\\\nBox, George, 201\\\\\\\\\\nbox-and-whisker plots, 176\\\\\\\\\\nbubble plots, 179\\\\\\\\\\nbupkis, 391\\\\\\\\\\nBush, George W., 326\\n\\nC-language, 59\\\\\\\\\\ncache memory, 401\\\\\\\\\\ncanonical representation, 400\\\\\\\\\\ncanonization, 400\\\\\\\\\\nCAPTCHAs, 89\\\\\\\\\\nCarroll, Lewis\\\\index{Carroll, Lewis}, 423\\\\\\\\\\ncartograms, 189\\\\\\\\\\nCenter for Disease Control, 204\\\\\\\\\\ncenter of mass, 332\\\\\\\\\\ncentrality measures, 34\\\\\\\\\\ncentroids, 331\\\\\\\\\\ncharacter code unification, 74\\\\\\\\\\ncharacteristic equation, 256\\\\\\\\\\ncharacterizing distributions, 39\\\\\\\\\\nchart types, 170\\\\\\\\\\nbar charts, 179\\\\\\\\\\ndata maps, 187\\\\\\\\\\ndot and line plots, 174\\\\\\\\\\nhistograms, 183\\\\\\\\\\npie charts, 179\\\\\\\\\\nscatter plots, 177\\\\\\\\\\ntabular data, 170\\\\\\\\\\ncicada, 46\\\\\\\\\\nclassification, 16, 210, 289\\\\\\\\\\nbinary, 290, 314\\\\\\\\\\nmulti-class, 297\\\\\\\\\\nregression, 290\\\\\\\\\\nclassification and regression trees, 357\\\\\\\\\\nclassifiers\\\\\\\\\\nbalanced, 217\\\\\\\\\\nevaluating, 213\\\\\\\\\\none-vs.-all, 298\\\\\\\\\\nperfect, 218\\\\\\\\\\nClinton, Bill, 326\\\\\\\\[0pt]\\nClinton, Bill], 326\\\\\\\\\\nclosest pair of points, 398\\\\\\\\\\ncloud computing services, 410\\\\\\\\\\ncluster\\\\\\\\\\nconductance, 343\\\\\\\\\\ndistance, 337\\\\\\\\\\nclustering, 327, 373\\\\\\\\\\nagglomerative, 336\\\\\\\\\\napplications, 328\\\\\\\\\\nbiological, 337\\\\\\\\\\ncut-based, 341\\\\\\\\\\nk-means, 330\\\\\\\\\\nsingle link, 339\\\\\\\\\\nvisualization of, 337\\\\\\\\\\nclusters\\\\\\\\\\nnumber of, 333\\\\\\\\\\norganization of, 337\\\\\\\\\\nClyde, 111, 112\\\\\\\\\\ncoefficient vector, 270\\\\\\\\\\nCohen\\'s d, 136\\\\\\\\\\ncollaborative filtering, 9\\\\\\\\\\ncommunication, 408, 416\\\\\\\\\\ncommutativity, 243\\\\\\\\\\ncompany data, 64\\\\\\\\\\ncomputer scientist, 2\\\\\\\\\\nconditional probability, 30, 31\\\\\\\\\\nCondorcet jury theorem, 84\\\\\\\\\\nconfusion matrix, 214, 220\\\\\\\\\\nconnected components, 324\\\\\\\\\\ncontingency table, 214\\\\\\\\\\nconvex, 280\\\\\\\\\\nconvex hull, 368\\\\\\\\\\ncoordination, 408\\\\\\\\\\ncorrelation\\\\\\\\\\nanalysis, 40\\\\\\\\\\ninterpretation, 43\\\\\\\\\\nsignificance, 45\\\\\\\\\\ncorrelation and causation, 45,135\\\\\\\\\\ncosine similarity, 309\\\\\\\\\\ncross validation, 227\\\\\\\\\\nadvantages, 227\\\\\\\\\\nCrowdFlower, 67, 84, 86\\\\\\\\\\ncrowdsourcing, 67, 80\\\\\\\\\\nbad uses, 87\\\\\\\\\\ncrowdsourcing services, 84\\\\\\\\\\ncryptographic hashing, 400\\\\\\\\\\nCSV files, 62\\\\\\\\\\ncumulative density function, 33 , 132, 186\\\\\\\\\\ncurrency conversion, 75\\\\\\\\\\ncut, 343\\\\\\\\\\ndamping factor, 325\\\\\\\\\\nDarwin, Charles, 81\\\\\\\\\\ndata, 237, 391\\\\\\\\\\nquantitative vs. categorical, 15\\\\\\\\\\nbig vs. little, 15\\\\\\\\\\ncleaning, 69\\\\\\\\\\ncollecting, 64\\\\\\\\\\ncompatibility, 72\\\\\\\\\\nerrors, 69\\\\\\\\\\nfor evaluation, 225\\\\\\\\\\nfor testing, 225\\\\\\\\\\nfor training, 225\\\\\\\\\\nlogging, 68\\\\\\\\\\nproperties, 14\\\\\\\\\\nscraping, 67\\\\\\\\\\nstructured, 14\\\\\\\\\\ntypes, 14\\\\\\\\\\nunstructured, 14\\\\\\\\\\nvisualizing, 155\\\\\\\\\\ndata analysis, 404\\\\\\\\\\ndata centrism, 2\\\\\\\\\\ndata cleaning, 376\\\\\\\\\\ndata errors, 417\\\\\\\\\\ndata formats, 61\\\\\\\\\\ndata hygiene\\\\\\\\\\nevaluation, 225\\\\\\\\\\ndata munging, 57\\\\\\\\\\ndata parallelism, 409\\\\\\\\\\ndata partition, 404\\\\\\\\\\ndata processing, 10\\\\\\\\\\ndata reduction, 329\\\\\\\\\\ndata science, 1\\\\\\\\\\nlanguages, 57\\\\\\\\\\nmodels, 210\\\\\\\\\\ndata science television, 17\\\\\\\\\\ndata scientist, 2\\\\\\\\\\ndata sources, 64\\\\\\\\\\ndata visualization, 155\\\\\\\\\\ndata-driven, 156\\\\\\\\\\nmodels, 207\\\\\\\\\\nde MÃ©rÃ©, Chevalier, 29\\\\\\\\\\ndecision boundaries, 291\\\\\\\\\\ndecision tree classifiers, 299, 357\\\\\\\\\\ndecision trees, 357\\\\\\\\\\nadvantages, 358\\\\\\\\\\nconstruction, 359\\\\\\\\\\nensembles of, 362\\\\\\\\\\ndeep learning, 202, 352, 377\\\\\\\\\\nmodels, 209\\\\\\\\\\nnetwork, 378\\\\\\\\\\nDeepWalk, 385\\\\\\\\\\ndegree of vertex, 325\\\\\\\\\\ndepth, 378\\\\\\\\\\nderivative, 281\\\\\\\\\\npartial, 282\\\\\\\\\\nsecond, 281\\\\\\\\\\ndescriptive statistics, 34\\\\\\\\\\ndeterministic sampling algorithms, 404\\\\\\\\\\ndeveloping scoring systems, 99\\\\\\\\\\ndictionary maintenance, 399\\\\\\\\\\nDiMaggio, 148\\\\\\\\\\ndimension reduction, 277, 376\\\\\\\\\\ndimensional egalitarianism, 308\\\\\\\\\\ndimensions, 384\\\\\\\\\\ndinosaur vertebra, 78\\\\\\\\\\ndirected acyclic graph, 110\\\\\\\\\\ndirected graph, 321\\\\\\\\\\ndiscounting, 209, 356\\\\\\\\\\ndisk storage, 401\\\\\\\\\\ndistance methods, 303\\\\\\\\\\ndistance metrics, 304\\\\\\\\\\n$L_{k}, 305$\\\\\\\\\\neuclidean, 305\\\\\\\\\\nmanhattan distance, 305\\\\\\\\\\nmaximum component, 305\\\\\\\\\\ndistances, 319\\\\\\\\\\nmeasuring, 303\\\\\\\\\\ndistributed file system, 396\\\\\\\\\\ndistributed processing, 407\\\\\\\\\\ndivide and conquer, 411\\\\\\\\\\nDNA sequences, 402\\\\\\\\\\ndot product, 241\\\\\\\\\\nduality, 268\\\\\\\\\\nduplicate removal, 400\\\\\\\\\\nE-step, 335\\\\\\\\\\nedge cuts, 324\\\\\\\\\\nedges, 319\\\\\\\\\\neffect size, 136\\\\\\\\\\neigenvalues, 255\\\\\\\\\\ncomputation, 256\\\\\\\\\\ndecomposition, 257\\\\\\\\\\nproperties of, 255\\\\\\\\\\nElizabeth II, 326\\\\\\\\\\nElo rankings, 104\\\\\\\\\\nembedded graph, 323\\\\\\\\\\nembedding, 321\\\\\\\\\\nEmoji Dick, 86\\\\\\\\\\nEngels, Friedrich, 391\\\\\\\\\\nensemble learning, 363\\\\\\\\\\nentropy, 310\\\\\\\\\\nequations\\\\\\\\\\ndetermined, 251\\\\\\\\\\nunderdetermined, 251\\\\\\\\\\nerror, 202, 221\\\\\\\\\\nabsolute, 221\\\\\\\\\\ndetection, 155\\\\\\\\\\nmean squared, 223,331\\\\\\\\\\nrelative, 222\\\\\\\\\\nresidual, 269\\\\\\\\\\nroot mean squared, 223\\\\\\\\\\nsquared, 222\\\\\\\\\\nstatistics, 221\\\\\\\\\\nerrors vs. artifacts, 69\\\\\\\\\\nethical implications, 416\\\\\\\\\\nEuclidean metric, 303\\\\\\\\\\nevaluation\\\\\\\\\\nenvironments, 224\\\\\\\\\\nstatistics, 214\\\\\\\\\\nevent, 28\\\\\\\\\\nExcel, 59\\\\\\\\\\nexclusive or, 361\\\\\\\\\\nexercises, $23,53,90,119,151,199$, $234,263,301,346,388$, 419\\\\\\\\\\nexpectation maximization, 335\\\\\\\\\\nexpected value, 28\\\\\\\\\\nexperiment, 27\\\\\\\\\\nexploratory data analysis, 155,156\\\\\\\\\\nvisualization, 160\\\\\\\\\\nF-score, 216\\\\\\\\\\nFacebook, 21\\\\\\\\\\nfalse negatives, 214\\\\\\\\\\nfalse positives, 214\\\\\\\\\\nfast Fourier transform, 47\\\\\\\\\\nfault tolerance, 408\\\\\\\\\\nfeature engineering, 375\\\\\\\\\\nfeature scaling, 274\\\\\\\\\\nsublinear, 275\\\\\\\\\\nz-scores, 275\\\\\\\\\\nfeatures\\\\\\\\\\nhighly-correlated, 277\\\\\\\\\\nFermat, Pierre de\\\\index{Fermat, Pierre de}, 30\\\\\\\\\\nFeynman, Richard, 229\\\\\\\\\\nFFT, 47\\\\\\\\\\nfiltering, 403\\\\\\\\\\nfinancial market, 126\\\\\\\\\\nfinancial unification, 75\\\\\\\\\\nfit and complexity, 288\\\\\\\\\\nFoldIt, 89\\\\\\\\\\nfootball, 111\\\\\\\\\\nAmerican players, 97\\\\\\\\\\ngame prediction, 111\\\\\\\\\\nforecasting\\\\\\\\\\ntime series, 212\\\\\\\\\\nformulation, 354\\\\\\\\\\nFreedom of Information Act, 12, 65\\\\\\\\\\nfrequency counting, 400\\\\\\\\\\nfrequency distributions, 184\\\\\\\\\\nfurthest link, 340\\\\\\\\\\nGalton, Francis, 81\\\\\\\\\\ngames with a purpose, 88\\\\\\\\\\ngamification, 88\\\\\\\\\\ngarbage in, garbage out, 3, 69\\\\\\\\\\nGates, Bill, 130\\\\\\\\\\nGaussian\\\\\\\\\\nelimination, 250\\\\\\\\\\nGaussian distribution, 124\\\\\\\\\\nGaussian noise, 125\\\\\\\\\\nGeneral Sentiment, 20\\\\\\\\\\ngenius, 19\\\\\\\\\\ngeometric mean, 35\\\\\\\\\\ngeometric point sets, 238\\\\\\\\\\ngeometry, 240\\\\\\\\\\nGini impurity, 360\\\\\\\\\\nGlobal Positioning System, 12\\\\\\\\\\ngold standards, 99\\\\\\\\\\ngood scoring functions, 101\\\\\\\\\\nGoodhart\\'s law, 303\\\\\\\\\\nGoodhart, Charles, 303\\\\\\\\\\nGoogle\\\\\\\\\\nAlphaGo, 372\\\\\\\\\\nTensorFlow, 377\\\\\\\\\\nGoogle Flu Trends, 394\\\\\\\\\\nGoogle News, 219\\\\\\\\\\nGoogle Ngrams, 10\\\\\\\\\\nGoogle Scholar, 66\\\\\\\\\\ngovernment data, 65\\\\\\\\\\ngradient boosted decision trees, 359, 366\\\\\\\\\\ngradient descent search, 281\\\\\\\\\\ngraph embeddings, 384\\\\\\\\\\ngraph theory, 323\\\\\\\\\\ngraphs, 238, 319, 321\\\\\\\\\\ncuts, 342\\\\\\\\\\ndense, 322\\\\\\\\\\ndirected, 321\\\\\\\\\\nembedded, 323\\\\\\\\\\nlabeled, 323\\\\\\\\\\nnon-simple, 322\\\\\\\\\\nsimple, 322\\\\\\\\\\nsparse, 322\\\\\\\\\\ntopological, 323\\\\\\\\\\nundirected, 321\\\\\\\\\\nunlabeled, 323\\\\\\\\\\nunweighted, 322\\\\\\\\\\nweighted, 320,322\\\\\\\\\\nGray, Dorian\\\\index{Gray, Dorian}, 251\\\\\\\\\\ngrid indexes, 315\\\\\\\\\\ngrid search, 409\\\\\\\\\\nHadoop distributed file system, 414\\\\\\\\\\nHamming, Richard W., 1\\\\\\\\\\nhash functions, 399\\\\\\\\\\napplications, 399\\\\\\\\\\nhashing, 399\\\\\\\\\\nheatmaps, 178\\\\\\\\\\nhedge fund, 1\\\\\\\\\\nhierarchy, 298\\\\\\\\\\nhigher dimensions, 307, 370\\\\\\\\\\nhistograms, 32, 183, 222\\\\\\\\\\nbest practices, 186\\\\\\\\\\nbin size, 184\\\\\\\\\\nHitler, Adolf, 326\\\\\\\\\\nHTML, 67\\\\\\\\\\nhypothesis development, 328\\\\\\\\\\nhypothesis driven, 156, 392\\\\\\\\\\nImagenet, 379\\\\\\\\\\nIMDb, 7\\\\\\\\\\nimputation\\\\\\\\\\nby interpolation, 78\\\\\\\\\\nby mean value, 77\\\\\\\\\\nby nearest neighbor, 78\\\\\\\\\\nby random value, 77\\\\\\\\\\nheuristic-based, 77\\\\\\\\\\nindependence, 30, 123, 354\\\\\\\\\\ninflation rates, 76\\\\\\\\\\ninformation gain, 360\\\\\\\\\\ninformation theoretic entropy, 360\\\\\\\\\\ninfrastructure, 396\\\\\\\\\\ninner product, 243\\\\\\\\\\ninstitutional review board, 88\\\\\\\\\\nInternet Movie Database, 7\\\\\\\\\\nInternet of Things, 68\\\\\\\\\\ninverse transform sampling, 132\\\\\\\\\\nIPython, 61\\\\\\\\\\nIQ testing, 89\\\\\\\\\\nJaccard distance, 341\\\\\\\\\\nJaccard similarity, 341\\\\\\\\[0pt]\\nJackson, Michael [136]\\\\index{Jackson, Michael [136]}, 262\\\\\\\\\\nJava, 59\\\\\\\\\\nJesus, 326\\\\\\\\\\nJSON, 63\\\\\\\\\\nk-means clustering, 343, 409\\\\\\\\\\nk-mediods algorithm, 332\\\\\\\\\\nk-nearest neighbors, 313\\\\\\\\\\nKaggle, viii\\\\\\\\\\nkd-trees, 316\\\\\\\\\\nkernels, 371\\\\\\\\\\nKolmogorov-Smirnov test, 139\\n\\nKruskal\\'s algorithm, 339\\\\\\\\\\nKullback-Leibler divergence, 311\\\\\\\\\\nlabeled graphs, 323\\\\\\\\\\nLang, Andrew, 267\\\\\\\\\\nLaplace, 356\\\\\\\\\\nLaplacian, 343\\\\\\\\\\nlarge-scale question, 9\\\\\\\\\\nlatent Dirichlet allocation, 373\\\\\\\\\\nlearning rate, 283, 383\\\\\\\\\\nlearning to rank, 119\\\\\\\\\\nleast squares regression, 270\\\\\\\\\\nlie factor, 164\\\\\\\\\\nLincoln, Abraham, 242\\\\\\\\\\nline charts, 174\\\\\\\\\\nadvantages, 174\\\\\\\\\\nbest practices, 175\\\\\\\\\\nline hatchings, 177\\\\\\\\\\nlinear algebra, 237\\\\\\\\\\npower of, 237\\\\\\\\\\nlinear algebraic formulae\\\\\\\\\\ninterpretation, 238\\\\\\\\\\nlinear equation, 238\\\\\\\\\\nlinear programming, 369\\\\\\\\\\nlinear regression, 212, 267\\\\\\\\\\nerror, 269\\\\\\\\\\nsolving, 270\\\\\\\\\\nlinear support vector machines, 369\\\\\\\\\\nlinear systems, 250\\\\\\\\\\nLinnaeus, Carl, 326\\\\\\\\\\nlive data, 395\\\\\\\\\\nlocality, 401\\\\\\\\\\nlocality sensitive hashing, 317\\\\\\\\\\nlogarithm, 47\\\\\\\\\\nlogistic classification issues, 295\\\\\\\\\\nlogistic function, 381\\\\\\\\\\nlogistic regression, 366\\\\\\\\\\nlogit, 381\\\\\\\\\\nlogit function, 106, 292\\\\\\\\\\nloss function, 280, 294\\\\\\\\\\nLU decomposition, 254\\\\\\\\\\nlumpers, 329\\\\\\\\\\nM-step, 335\\\\\\\\\\nmachine learning, 351\\\\\\\\\\nalgorithms, 375\\\\\\\\\\nclassifiers, 85\\\\\\\\\\nmodels, 208\\\\\\\\\\nmain memory, 401\\\\\\\\\\nmajor league baseball, 6\\\\\\\\\\nMapReduce, 407, 410\\\\\\\\\\nprogramming, 412\\\\\\\\\\nMapReduce runtime system, 415\\\\\\\\\\nmatchings, 324\\\\\\\\\\nMathematica, 59, 61\\\\\\\\\\nMatlab, 58\\\\\\\\\\nmatrix, 14, 237, 270, 373\\\\\\\\\\naddition, 242\\\\\\\\\\nadjacency, 246, 320\\\\\\\\\\ncovariance, 245, 257, 271\\\\\\\\\\ndeterminant, 249, 254\\\\\\\\\\neigenvalues, 255\\\\\\\\\\neigenvectors, 255\\\\\\\\\\nfactoring, 252\\\\\\\\\\nidentity, 246, 248\\\\\\\\\\ninversion, 248\\\\\\\\\\nlinear combinations of, 242\\\\\\\\\\nmultiplication, 243\\\\\\\\\\nmultiplicative inverse of, 249\\\\\\\\\\nnon-singular, 249\\\\\\\\\\npermutation, 247\\\\\\\\\\nrank, 251\\\\\\\\\\nreasons for factoring, 252\\\\\\\\\\nrotation, 248\\\\\\\\\\nsingular, 249\\\\\\\\\\ntranspose of, 242\\\\\\\\\\ntriangular, 254\\\\\\\\\\nunderdetermined, 256\\\\\\\\\\nmatrix multiplication, 243, 398\\\\\\\\\\napplications, 244\\\\\\\\\\nmatrix operations\\\\\\\\\\nvisualizing, 241\\\\\\\\\\nmaximum margin separator, 367\\\\\\\\\\nmean, $34,83,125,132,138,212$, 227, 403\\\\\\\\\\narithmetic, 35\\\\\\\\\\ngeometric, 35\\\\\\\\\\nmeasurement error, 125\\\\\\\\\\nmedian, 35, 83, 132, 212, 403\\\\\\\\\\nmergesort, 398\\\\\\\\\\nmetadata, 7\\\\\\\\\\nmethod centrism, 2\\\\\\\\\\nmetric, 304\\\\\\\\\\nidentity, 304\\\\\\\\\\npositivity, 304\\\\\\\\\\nsymmetry, 304\\\\\\\\\\ntriangle inequality, 304\\\\\\\\\\nminima\\\\\\\\\\nglobal, 284\\\\\\\\\\nlocal, 284\\\\\\\\\\nminimum spanning tree, 324,339\\\\\\\\\\nmissing values, 76,376\\\\\\\\\\nmixture model, 333\\\\\\\\\\nMoby Dick, 86\\\\\\\\\\nmode, 36\\\\\\\\\\nmodel-driven, 417\\\\\\\\\\nmodeling, 201, 328, 416\\\\\\\\\\nphilosophies of, 201\\\\\\\\\\nprinciples for effectiveness, 203\\\\\\\\\\nmodels\\\\\\\\\\nad hoc, 208\\\\\\\\\\nbaseline, 210\\\\\\\\\\nblackbox, 206\\\\\\\\\\ndata science, 210\\\\\\\\\\ndata-driven, 207\\\\\\\\\\ndeep learning, 209\\\\\\\\\\ndescriptive, 206\\\\\\\\\\ndeterministic, 208\\\\\\\\\\nevaluating, 212\\\\\\\\\\nfirst-principle, 207\\\\\\\\\\nflat, 209\\\\\\\\\\nGoogle\\'s forecasting, 204\\\\\\\\\\nhierarchical, 209\\\\\\\\\\nlinear, 206\\\\\\\\\\nlive, 204\\\\\\\\\\nmachine learning, 208\\\\\\\\\\nneural network, 206\\\\\\\\\\nnon-linear, 206\\\\\\\\\\noverfit, 203\\\\\\\\\\nsimplifying, 286\\\\\\\\\\nsimulation, 229\\\\\\\\\\nstochastic, 208\\\\\\\\\\ntaxonomy of, 205\\\\\\\\\\nunderfit, 202\\\\\\\\\\nMoneyball, 5\\\\\\\\\\nmonkey, 215\\\\\\\\\\nmonotonic, 307\\n\\nMonte Carlo\\\\\\\\\\nsampling, 134\\\\\\\\\\nsimulations, 229\\\\\\\\\\nMosteller, Frederick, 121\\\\\\\\\\nmulticlass systems\\\\\\\\\\nevaluating, 219\\\\\\\\\\nmultiedge, 322\\\\\\\\\\nmultinomial regression, 299\\\\\\\\\\nmultiplying probabilities, 48\\\\\\\\\\nnaive Bayes, 354, 363\\\\\\\\\\nname unification, 74\\\\\\\\\\nNapoleon, 326\\\\\\\\\\nNASA, 73\\\\\\\\\\nNational Football League, 112\\\\\\\\\\nnatural language processing, 20\\\\\\\\\\nnearest centroid, 340\\\\\\\\\\nnearest neighbor, 339, 397\\\\\\\\\\nnearest neighbor classification, 311\\\\\\\\\\nadvantages, 311\\\\\\\\\\nnearest neighbors\\\\\\\\\\nfinding, 315\\\\\\\\\\nnegative class, 213\\\\\\\\\\nNetflix prize, 9\\\\\\\\\\nnetwork\\\\\\\\\\ndepth, 379\\\\\\\\\\nnetwork methods, 303\\\\\\\\\\nnetworks, 109, 238, 319, 378\\\\\\\\\\ninduced, 320\\\\\\\\\\nlearning, 379\\\\\\\\\\nneural networks, 377\\\\\\\\\\nnew data set, 156\\\\\\\\\\nNew York, 277\\\\\\\\\\nNixon, Richard, 326\\\\\\\\\\nNLP-based system, 21\\\\\\\\\\nno free lunch theorem, 353\\\\\\\\\\nnode\\\\\\\\\\nbias of, 381\\\\\\\\\\nnon-linear classifiers, 366\\\\\\\\\\nnon-linear functions\\\\\\\\\\nfitting, 273\\\\\\\\\\nnon-linear support vector machines, 369\\\\\\\\\\nnon-linearity, 358, 377, 380\\\\\\\\\\nnorm, 287\\\\\\\\\\nnormal, 125\\\\\\\\\\nnormal distribution, 79, 109, 124, 141\\\\\\\\\\nimplications, 126\\\\\\\\\\nnormality testing, 141\\\\\\\\\\nnormalization, 103, 376\\\\\\\\\\nnormalizing skewed distribution, 49\\\\\\\\\\nnorms, 309\\\\\\\\\\nNoSQL databases, 415\\\\\\\\\\nnotebook environments, 59\\\\\\\\\\nnumerical conversions, 73\\\\\\\\\\nObama, Barack, 326\\\\\\\\[0pt]\\nObama, Barack [91], 327\\\\\\\\[0pt]\\nObama, Barack], 326\\\\\\\\\\nOccam\\'s razor, 201, 211, 286\\\\\\\\\\nOccam, William of, 202\\\\\\\\\\nOh G-d, 177\\\\\\\\\\noptimization\\\\\\\\\\nlocal, 284\\\\\\\\\\nOracle of Bacon, 9\\\\\\\\\\noutlier, 118\\\\\\\\\\ndetection, 78\\\\\\\\\\noutlier detection, 329\\\\\\\\\\noutliers\\\\\\\\\\nremoving, 272\\\\\\\\\\noverfitting, 202, 296\\\\\\\\\\noverlap percentage, 137\\\\\\\\\\nownership, 417\\\\\\\\\\np-values, 145\\\\\\\\\\npacking data, 402\\\\\\\\\\nPageRank, 100, 111, 325\\\\\\\\\\npairwise correlations, 158\\\\\\\\\\nparallel processing, 407\\\\\\\\\\nparallelism, 406\\\\\\\\\\nparameter fitting, 279\\\\\\\\\\nparameter spaces convex, 280\\\\\\\\\\npartition function, 299\\\\\\\\\\nPascal\\'s triangle, 123\\\\\\\\\\nPascal, Blaise\\\\index{Pascal, Blaise}, 30\\\\\\\\\\npaths, 246\\\\\\\\\\nPearson correlation coefficient, 41, 136\\\\\\\\\\npenalty function, 293\\\\\\\\\\nperformance of models\\\\index{performance of models}, 39\\\\\\\\\\nperiodic table, 188\\\\\\\\\\nPerl, 58\\\\\\\\\\npermutation, 246\\\\\\\\\\nrandomly generating, 147\\\\\\\\\\ntests, 145\\\\\\\\\\npersonal wealth, 130\\\\\\\\\\npie charts, 179\\\\\\\\\\nbad examples, 183\\\\\\\\\\nbest practices, 181\\\\\\\\\\npoint spread, 112\\\\\\\\\\npoints\\\\\\\\\\nrotating, 248\\\\\\\\\\npoints vs. vectors, 309\\\\\\\\\\nPoisson distribution, 127\\\\\\\\\\nposition evaluation function, 372\\\\\\\\\\npositive class, 213\\\\\\\\\\npower law distribution, 129\\\\\\\\\\npower law function, 276\\\\\\\\\\nprecision, $3,215,221$\\\\\\\\\\nprefetching, 401\\\\\\\\\\nprincipal components, 260\\\\\\\\\\nanalysis, 260\\\\\\\\\\nprior probability, 354\\\\\\\\\\nprivacy, 418\\\\\\\\\\nprobabilistic, 203\\\\\\\\\\nprobability, 27, 29, 354\\\\\\\\\\nprobability density function, 32 , 132,186\\\\\\\\\\nprobability distribution, 32\\\\\\\\\\nprobability of an event, 28\\\\\\\\\\nprobability of an outcome, 28\\\\\\\\\\nprobability vs. statistics, 29\\\\\\\\\\nprogram flow graph, 322\\\\\\\\\\nprogramming languages, 57\\\\\\\\\\nprotocol buffers, 63\\\\\\\\\\nproxies, 99\\\\\\\\\\npsychologists, 89\\\\\\\\\\nPubmed, 70\\\\\\\\\\npure partition, 360\\\\\\\\\\nPythagorean theorem, 306\\\\\\\\\\nPython, 58, 67\\\\\\\\\\nQuant Shop, 17\\\\\\\\\\nR, 58\\\\\\\\\\nRand index, 341\\\\\\\\\\nrandom access machine, 397\\\\\\\\\\nrandom sampling, 403, 406\\\\\\\\\\nrandom variable, 28\\\\\\\\\\nranking systems\\\\\\\\\\nclass rank, 100\\\\\\\\\\nsearch results, 100\\\\\\\\\\ntop sports teams, 100\\\\\\\\\\nuniversity rankings, 100\\\\\\\\\\nrankings, 95\\\\\\\\\\ndigraph-based, 109\\\\\\\\\\nhistorical, 117\\\\\\\\\\nmerging, 108\\\\\\\\\\ntechniques, 104\\\\\\\\\\nratio, 48\\\\\\\\\\nray\\\\\\\\\\nunit, 241\\\\\\\\\\nReagan, Ronald, 326\\\\\\\\\\nrearrangement operations, 238\\\\\\\\\\nrecall, 216, 221\\\\\\\\\\nreceiver-operator characteristic curve, 218\\\\\\\\\\nrectified linear units, 381\\\\\\\\\\nrectifier, 381\\\\\\\\\\nredundancy, 393\\\\\\\\\\nregression, 16\\\\\\\\\\napplication, 359\\\\\\\\\\nfor classification, 290\\\\\\\\\\nLASSO, 287\\\\\\\\\\nlogistic, 289, 292\\\\\\\\\\nridge, 286\\\\\\\\\\nregression models, 272\\\\\\\\\\nremoving outliers, 272\\\\\\\\\\nregularization, 286, 376\\\\\\\\\\nreinforcement learning, 372\\\\\\\\\\nRichter scale, 131\\\\\\\\\\nright-sizing training data, 404\\\\\\\\\\nroad network, 322\\\\\\\\\\nrobustness, $3,96,359$\\\\\\\\\\nRoosevelt, Franklin D., 326\\\\\\\\\\nRota, Gian-Carlo, 237\\\\\\\\\\nsabermetrics, 22\\\\\\\\\\nsample space, 27\\\\\\\\\\nsampling, 132, 403, 404\\\\\\\\\\nbeyond one dimension, 133\\\\\\\\\\nby truncation, 404\\\\\\\\\\nscalar\\\\\\\\\\nmultiplication, 242\\\\\\\\\\nscale invariant, 132\\\\\\\\\\nscales\\\\\\\\\\nLikert, 298\\\\\\\\\\nordinal, 297\\\\\\\\\\nscaling constant, 293\\\\\\\\\\nscatter plots, 98,177\\\\\\\\\\nbest practices, 177\\\\\\\\\\nthree-dimensional, 179\\\\\\\\\\nSchaumann, Jan, 351\\\\\\\\\\nscientist, 2\\\\\\\\\\nscores, 95\\\\\\\\\\nthreshold, 218\\\\\\\\\\nscores vs. rankings, 100\\\\\\\\\\nscoring functions, 95\\\\\\\\\\nsecurity, 418\\\\\\\\\\nself-loop, 322\\\\\\\\\\nsemi-supervised learning, 374\\\\\\\\\\nShakespeare, William, 326\\\\\\\\\\nsharp, 215\\\\\\\\\\nSheep Market, 86\\\\\\\\\\nshortest paths, 324\\\\\\\\\\nsignal to noise ratio, 37\\\\\\\\\\nsignificance level, 139\\\\\\\\\\nSilver, Nate, 203\\\\\\\\\\nsimilarity graphs, 341\\\\\\\\\\nsimilarity matrix, 342\\\\\\\\\\nsimple graph, 322\\\\\\\\\\nsingle-command program, 224\\\\\\\\\\nsingle-pass algorithm, 402\\\\\\\\\\nsingular value decomposition, 258\\\\\\\\\\nsketching, 403\\\\\\\\\\nskew, 413\\\\\\\\\\nSkiena, Len, ix\\\\\\\\\\nsmall evaluation set, 226\\\\\\\\\\nsocial media, 392\\\\\\\\\\nanalysis, 21\\\\\\\\\\ndata, 394\\\\\\\\\\nSocial Network-movie, 105\\\\\\\\\\nspam, 393\\\\\\\\\\nspam filtering, 393\\\\\\\\\\nSpearman rank correlation coefficient, 42, 108\\\\\\\\[0pt]\\nSpears, Britney [566]\\\\index{Spears, Britney [566]}, 262\\\\\\\\\\nspectral clustering, 343\\\\\\\\\\nspidering, 67\\\\\\\\\\nsplitters, 329\\\\\\\\\\nsports performance, 38\\\\\\\\\\nSQL databases, 63\\\\\\\\\\nstandard deviation, $36,125,132$, 138, 227, 403\\\\\\\\\\nstatistical analysis, 121, 230\\\\\\\\\\nstatistical distributions, 122\\\\\\\\\\nstatistical proxy, 7\\\\\\\\\\nstatistical significance, 135\\\\\\\\\\nstatistics, 29\\\\\\\\\\nstochastic gradient descent, 285, 296, 382\\\\\\\\\\nstock market, 37, 79, 126\\\\\\\\\\nStony Brook University, 20\\\\\\\\\\nstop words, 414\\\\\\\\\\nstorage hierarchy, 401\\\\\\\\\\nstreaming, 402\\\\\\\\\\nsummary statistics, 157,159\\\\\\\\\\nsupervised learning, 372\\\\\\\\\\nsupervision, 372\\\\\\\\\\ndegrees of, 372\\\\\\\\\\nsupport vector machines, 352,366\\\\\\\\\\nsupport vectors, 368\\\\\\\\\\nSurowiecki, James, 82\\\\\\\\\\nsweat equity, 66\\\\\\\\\\nT-test, 137\\\\\\\\\\ntangent line, 282\\\\\\\\\\ntarget scaling, 274\\\\\\\\\\nsublinear, 276\\\\\\\\\\ntaxi\\\\\\\\\\nrecords from New York, 11\\\\\\\\\\ntipping model, 286\\\\\\\\\\ntipping rate, 13,277\\\\\\\\\\ntaxi driver, 277\\\\\\\\\\nterms of service, 68\\\\\\\\\\ntest statistic, 138\\\\\\\\\\ntext analysis, 253\\\\\\\\\\ntheory of relativity, 144\\\\\\\\\\nTikhonov regularization, 287\\\\\\\\\\ntime unification, 75\\\\\\\\\\nTitanic, 183, 358\\\\\\\\\\ntop-k success rate, 219\\\\\\\\\\ntopic modeling, 373\\\\\\\\\\ntopological graph, 323\\\\\\\\\\ntopological sorting, 110, 324\\\\\\\\\\ntransparency, 417\\\\\\\\\\ntree, 298, 336\\\\\\\\\\ntrees\\\\\\\\\\nagglomerative, 337\\\\\\\\\\ntrue negatives, 214\\\\\\\\\\ntrue positives, 214\\\\\\\\\\ntruncation, 405\\\\\\\\\\nTufte, Edward, 155, 162\\\\\\\\\\nTwitter, 392, 404, 405\\\\\\\\\\nU.S. presidential elections, 187, 203\\\\\\\\\\nuncertainty, 175\\\\\\\\\\nundirected graph, 321\\\\\\\\\\nuniform distribution, 406\\\\\\\\\\nuniform sampling, 405\\\\\\\\\\nuninvertible, 400\\\\\\\\\\nunit conversions, 72\\\\\\\\\\nUNIX time, 75\\\\\\\\\\nunlabeled graphs, 323\\\\\\\\\\nunrepresentative participation, 393\\\\\\\\\\nunsupervised learning, 372\\\\\\\\\\nunweighted graph, 322\\\\\\\\\\nurban transportation network, 11\\\\\\\\\\nvalidation data, 95\\\\\\\\\\nvalue prediction, 210\\\\\\\\\\nvalue prediction models evaluating, 221\\\\\\\\\\nvariability measures, 36\\\\\\\\\\nvariance, $36,202,403$\\\\\\\\\\ninterpretation, 37\\\\\\\\\\nvariation coefficient, 137\\\\\\\\\\nvariety, 394\\\\\\\\\\nvectors, 240\\\\\\\\\\nunit, 240\\\\\\\\\\nvelocity, 394\\\\\\\\\\nveracity, 395\\\\\\\\\\nvertex, 319\\\\\\\\\\nvertices, 319\\\\\\\\\\nvisualization, 404\\\\\\\\\\nchart types, 170\\\\\\\\\\ncritiquing\\\\index{critiquing}, 189\\\\\\\\\\ninteractive, 195\\\\\\\\\\ntools, 160\\\\\\\\\\nvisualization aesthetic, 162\\\\\\\\\\nchartjunk, 162, 165\\\\\\\\\\ncolors, 163, 168\\\\\\\\\\ndata-ink ratio, 162, 163\\\\\\\\\\nlie factor, 162,164\\\\\\\\\\nrepetition, 163, 169\\\\\\\\\\nscaling and labeling, 162, 167\\\\\\\\\\nvolume, 394\\\\\\\\\\nVoronoi diagrams, 315\\\\\\\\\\nvoting, 363\\\\\\\\\\nwith classifiers, 363\\\\\\\\\\nweb crawling, 68\\\\\\\\\\nweighted average, 84\\\\\\\\\\nweighted graph, 322\\\\\\\\\\nWelch\\'s t-statistic, 138\\\\\\\\\\nWikipedia, 20, 79, 116, 326\\\\\\\\\\nwisdom, 19\\\\\\\\\\nwisdom of crowds, 81\\\\\\\\\\nWolfram Alpha, 59, 144\\\\\\\\\\nword embeddings, 254, 383\\\\\\\\\\nword2vec, 384\\\\\\\\\\nXML, 62\\\\\\\\\\nZ-score, 103, 308, 376\\\\\\\\\\nZipf\\'s law, 131, 357\\n\\n\\n\\\\end{document\\\\index{American basketball players}\\\\index{bell-shaped}\\\\index{binomial distribution}\\\\index{boosting}\\\\index{box plots}\\\\index{histograms}\\\\index{conditional probability}\\\\index{cosine similarity}\\\\index{construction}\\\\index{DiMaggio}\\\\index{discounting}\\\\index{distance metrics}\\\\index{decomposition}\\\\index{ensemble learning}\\\\index{error}\\\\index{residual}\\\\index{event}\\\\index{expected value}\\\\index{exploratory data analysis}\\\\index{ï¬nancial market}\\\\index{American players}\\\\index{frequency distributions}\\\\index{Gates, Bill}\\\\index{gradient boosted decision trees}\\\\index{bin size}\\\\index{hypothesis driven}\\\\index{independence}\\\\index{Laplace}\\\\index{LU decomposition}\\\\index{covariance}\\\\index{determinant}\\\\index{identity}\\\\index{triangular}\\\\index{metric}\\\\index{positivity}\\\\index{symmetry}\\\\index{triangle inequality}\\\\index{naive Bayes}\\\\index{new data set}\\\\index{no free lunch theorem}\\\\index{implications}\\\\index{norms}\\\\index{Pascalâs triangle}\\\\index{personal wealth}\\\\index{bad examples}\\\\index{points vs. vectors}\\\\index{power law distribution}\\\\index{probability of an event}\\\\index{probability of an outcome}\\\\index{random variable}\\\\index{application}\\\\index{robustness}\\\\index{singular value decomposition}\\\\index{coeï¬cient}\\\\index{statistical distributions}\\\\index{stock market}\\\\index{support vector machines}\\\\index{Titanic}\\\\index{uncertainty}\\\\index{interactive}\\\\index{colors}\\\\index{voting}\\\\index{with classiï¬ers}\\\\index{word embeddings}}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tex_file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_text_data(page_number, span_counter, text_data, doc):\n",
    "    page = doc[page_number]\n",
    "    # print(page)\n",
    "\n",
    "    # Read page text as a dictionary, suppressing extra spaces in CJK fonts\n",
    "    blocks = page.get_text(\"dict\", flags=0)[\"blocks\"]\n",
    "    # print(blocks)\n",
    "    line_number_in_page = 0\n",
    "    span_number_in_page = 0\n",
    "    # print(\"--Old--\")\n",
    "    for block_number, b in enumerate(blocks):  # Iterate through the text blocks\n",
    "        span_number_in_block = 0  # Initialize span counter for the block\n",
    "\n",
    "        # print(b[\"lines\"])\n",
    "\n",
    "        for l in b[\"lines\"]:  # Iterate through the text lines\n",
    "            # print(l)\n",
    "            line_number_in_page += 1\n",
    "            span_number_in_line = 0  # Initialize span counter for the line\n",
    "            # print(\"Spans : \"+ str(len(l[\"spans\"])))\n",
    "            for s in l[\"spans\"]:  # Iterate through the text spans\n",
    "                 # Create a deep copy of the original span dictionary to preserve all its properties\n",
    "                span_data = copy.deepcopy(s)\n",
    "\n",
    "                # Temporary removal to check hwo it works\n",
    "                del span_data[\"size\"]\n",
    "                # del span_data[\"flags\"]\n",
    "                del span_data[\"bidi\"]\n",
    "                del span_data[\"char_flags\"]\n",
    "                del span_data[\"ascender\"]\n",
    "                del span_data[\"descender\"]\n",
    "                del span_data['origin']\n",
    "                del span_data['bbox']\n",
    "                del span_data['color']\n",
    "                del span_data['font']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Add additional properties if needed\n",
    "                # span_data[\"page_number\"] = page_number\n",
    "                # span_data[\"span_number_overall\"] = span_counter\n",
    "                # span_data[\"span_number_in_line\"] = span_number_in_line\n",
    "                # span_data[\"span_number_in_block\"] = span_number_in_block\n",
    "                # span_data[\"span_number_in_page\"] = span_number_in_page\n",
    "                # span_data[\"block_number\"] = block_number\n",
    "\n",
    "                # Extract and store bounding box information\n",
    "                # x0, y0, x1, y1 = span_data[\"bbox\"]\n",
    "                # span_data[\"indent_left\"] = x0\n",
    "                # span_data[\"indent_top\"] = y0\n",
    "                # span_data[\"x1\"] = x1\n",
    "                # span_data[\"y1\"] = y1\n",
    "\n",
    "                # Decompose flags to determine font styles\n",
    "                decomposed_flags = flags_decomposer(span_data[\"flags\"])\n",
    "                span_data[\"is_italic\"] = \"italic\" in decomposed_flags\n",
    "                span_data[\"is_bold\"] = \"bold\" in decomposed_flags\n",
    "                span_data[\"is_superscript\"] = \"superscript\" in decomposed_flags\n",
    "\n",
    "                del span_data[\"flags\"]\n",
    "\n",
    "                # Append the dictionary to the text_data list\n",
    "                text_data.append(span_data)\n",
    "                # Increase the overall counters\n",
    "                span_counter += 1\n",
    "                span_number_in_line += 1  # Increase the span counter within the line\n",
    "                span_number_in_block += 1  # Increase the span counter within the block\n",
    "                span_number_in_page += 1\n",
    "    # print(\"---Old End---\")\n",
    "    return text_data, span_counter\n",
    "\n",
    "def flags_decomposer(flags):\n",
    "    \"\"\"Make font flags human readable.\"\"\"\n",
    "    l = []\n",
    "    if flags & 2 ** 0:\n",
    "        l.append(\"superscript\")\n",
    "    if flags & 2 ** 1:\n",
    "        l.append(\"italic\")\n",
    "    if flags & 2 ** 2:\n",
    "        l.append(\"serifed\")\n",
    "    else:\n",
    "        l.append(\"sans\")\n",
    "    if flags & 2 ** 3:\n",
    "        l.append(\"monospaced\")\n",
    "    else:\n",
    "        l.append(\"proportional\")\n",
    "    if flags & 2 ** 4:\n",
    "        l.append(\"bold\")\n",
    "    return \", \".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length : 772\n"
     ]
    }
   ],
   "source": [
    "text_data = []\n",
    "span_counter = 0\n",
    "for i in range(16, 16+16):\n",
    "  text_data, span_counter = get_page_text_data(i, span_counter, text_data, doc)\n",
    "  # text_data, span_counter = get_page_text_data(i, span_counter, doc)\n",
    "\n",
    "print(\"Length : \" + str(len(text_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', tex_file_contents)\n",
    "page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', tex_file_contents))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n     ```\\n     \\x08egin{figure}[h]\\n         \\\\centering\\n         \\\\includegraphics{filename}\\n         \\\\caption{Caption text}\\n         \\\\label{fig:label}\\n     \\\\end{figure}\\n     ```  \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_page_command = \"\"\"\n",
    "You will receive an unformatted LaTeX (.tex) file part of a book along with a separate JSON file containing formatting instructions.  \n",
    "Your task is to format the LaTeX file part according to the JSON data while ensuring proper structure and presentation for a book.  \n",
    "\n",
    "### **Formatting Guidelines:**  \n",
    "\n",
    "**1. Apply JSON Formatting Instructions:**  \n",
    "   - Modify only the necessary parts based on JSON data.  \n",
    "   - Do **not** make arbitrary changesâonly apply specified formatting corrections.  \n",
    "\n",
    "**2. Book Structure:**  \n",
    "   - Organize content into proper **chapters, sections, and subsections** only if explicitly marked in the `.tex` file.  \n",
    "   - **Do not assume chapter starts based on recurring text** (e.g., headers repeated on every page).  \n",
    "   - If chapter names and numbers appear on every page in the JSON, **ignore them** when determining chapter breaks.  \n",
    "   - **Remove hardcoded numbering** for chapters and sections, allowing LaTeX to handle it automatically.  \n",
    "   - Make the Contents Page dynamically if contents is present in the .tex file part. Do not hardcode the table of contents.\n",
    "\n",
    "**3. Image Handling:**  \n",
    "   - Convert all instances of `\\includegraphics{}` into a proper `figure` environment:  \n",
    "\n",
    "**4. Table Formatting:**  \n",
    "   - Ensure tables are properly structured with appropriate spacing, alignment, and captions for readability.  \n",
    "\n",
    "**5. Italics Handling:**  \n",
    "   - Apply italics **only** to content explicitly marked as italicized in the JSON data.  \n",
    "\n",
    "**6. Document Setup:**  \n",
    "   - This is the **first part of the book**, so include **all necessary LaTeX imports and the document class**.  \n",
    "   - **Do not modify LaTeX package imports unless explicitly required in the JSON file.** \n",
    "   - Do **not** manually start or end the document unless such commands are explicitly present.  \n",
    "\n",
    "**7. Strict Output Requirements:**  \n",
    "   - The output **must be pure LaTeX code**â**no explanations, comments, or markdown syntax.**  \n",
    "   - The formatted output will be **directly appended** to the `.tex` file, so it must be immediately compilable.  \n",
    "\n",
    "**8. Accuracy and Consistency:**  \n",
    "   - Since the book is processed in parts, formatting should be **consistent across all sections**.  \n",
    "   - **Do not introduce new formatting styles** that conflict with previous or upcoming sections.  \n",
    "   - Ensure that all content is preserved and formatted correctlyâno missing text, no misinterpretations.  \n",
    "\n",
    "**Final Note:**  \n",
    "Errors in formatting can **significantly affect the compiled document.** Ensure precise execution of all instructions while preserving the document's original meaning and intent.  \n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "next_pages_prompt = \"\"\"\n",
    "You will receive a portion of a LaTeX (.tex) file part of a book along with a separate JSON file containing formatting instructions.  \n",
    "Your task is to format this LaTeX file part according to the provided JSON data while maintaining consistency with previous sections.  \n",
    "\n",
    "### **Formatting Guidelines:**  \n",
    "\n",
    "**1. Apply JSON Formatting Instructions:**  \n",
    "   - Modify only the necessary parts as specified in the JSON data.  \n",
    "   - Do **not** assume formattingâonly apply explicit corrections.  \n",
    "\n",
    "**2. Maintain Book Structure:**  \n",
    "   - Organize content into proper **chapters, sections, and subsections** only if explicitly marked in the `.tex` file.  \n",
    "   - **Do not assume chapter starts based on recurring text** (e.g., headers repeated on every page).  \n",
    "   - If chapter names and numbers appear on every page in the JSON, **ignore them** when determining chapter breaks.  \n",
    "   - **Remove hardcoded numbering** on chapters, sections and subsections and rely on LaTeXâs automatic numbering system strictly.  \n",
    "   - Make the Contents Page dynamically if contents is present in the .tex file part. Do not hardcode the table of contents.\n",
    "**3. Image Handling:**  \n",
    "   - Convert `\\includegraphics{}` into a properly formatted `figure` environment:  \n",
    "\n",
    "\n",
    "**4. Table Formatting:**  \n",
    "   - Ensure tables are properly structured, aligned, and formatted for readability.  \n",
    "\n",
    "**5. Italics Handling:**  \n",
    "   - Apply italics **only** to content explicitly marked as italicized in the JSON data.  \n",
    "\n",
    "**6. Document Integrity:**  \n",
    "   - **Do not add any LaTeX preamble, document class, or import statements.**  \n",
    "   - **Do not modify LaTeX package imports unless explicitly required in the JSON file.** \n",
    "   - **Do not include `\\begin{document}` or `\\end{document}`** unless explicitly present in the provided `.tex` file.  \n",
    "\n",
    "**7. Strict Output Requirements:**  \n",
    "   - The output **must be pure LaTeX code**âno explanations, comments, or markdown syntax.  \n",
    "   - The formatted output will be **directly appended** to an existing `.tex` file, so it must be immediately compilable.  \n",
    "\n",
    "**8. Accuracy and Consistency:**  \n",
    "   - Ensure formatting is **consistent with previous sections** of the book.  \n",
    "   - **Do not introduce new formatting styles** that conflict with earlier parts.  \n",
    "   - Ensure **all content is retained**, formatted correctly, and adheres to the documentâs original intent.  \n",
    "\n",
    "**Final Note:**  \n",
    "Errors in formatting can **significantly impact the final compiled document.** Follow the instructions precisely to maintain a high-quality, structured LaTeX book.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "     ```\n",
    "     \\begin{figure}[h]\n",
    "         \\centering\n",
    "         \\includegraphics{filename}\n",
    "         \\caption{Caption text}\n",
    "         \\label{fig:label}\n",
    "     \\end{figure}\n",
    "     ```  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(command, data, prev_response, temperature=1):\n",
    "  first_page_prompt = f\"{command} \\n {data}\"\n",
    "  default_page_prompt = f\"\"\"{command} \\n{data}\"\"\"\n",
    "  prompt_content = first_page_prompt if prev_response == \"\" else default_page_prompt\n",
    "  response =  client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You convert PDF documents to LaTeX.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt_content}\"}\n",
    "    ],\n",
    "  temperature=temperature\n",
    "  )\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_page_data = {}\n",
    "page_numbers = []\n",
    "for i in range(len(doc)):\n",
    "    page = doc[i]\n",
    "    page_numbers.append(page.get_label())\n",
    "    book_page_data[i] = page.get_text(\"text\").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '28',\n",
       " '30',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '41',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '51',\n",
       " '52',\n",
       " '55',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '92',\n",
       " '93',\n",
       " '97',\n",
       " '99',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '122',\n",
       " '125',\n",
       " '126',\n",
       " '129',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '147',\n",
       " '148',\n",
       " '152',\n",
       " '153',\n",
       " '156',\n",
       " '158',\n",
       " '159',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '168',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '174',\n",
       " '177',\n",
       " '178',\n",
       " '179',\n",
       " '180',\n",
       " '183',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '191',\n",
       " '192',\n",
       " '197',\n",
       " '199',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '220',\n",
       " '221',\n",
       " '222',\n",
       " '223',\n",
       " '224',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '230',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '236',\n",
       " '238',\n",
       " '239',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '250',\n",
       " '251',\n",
       " '253',\n",
       " '254',\n",
       " '257',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '264',\n",
       " '265',\n",
       " '268',\n",
       " '269',\n",
       " '272',\n",
       " '273',\n",
       " '274',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '280',\n",
       " '281',\n",
       " '282',\n",
       " '283',\n",
       " '284',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '289',\n",
       " '291',\n",
       " '292',\n",
       " '293',\n",
       " '295',\n",
       " '296',\n",
       " '297',\n",
       " '298',\n",
       " '299',\n",
       " '300',\n",
       " '301',\n",
       " '304',\n",
       " '306',\n",
       " '308',\n",
       " '309',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '314',\n",
       " '315',\n",
       " '316',\n",
       " '317',\n",
       " '318',\n",
       " '319',\n",
       " '320',\n",
       " '321',\n",
       " '322',\n",
       " '323',\n",
       " '324',\n",
       " '325',\n",
       " '326',\n",
       " '327',\n",
       " '328',\n",
       " '329',\n",
       " '330',\n",
       " '331',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '340',\n",
       " '341',\n",
       " '343',\n",
       " '344',\n",
       " '345',\n",
       " '346',\n",
       " '348',\n",
       " '352',\n",
       " '356',\n",
       " '358',\n",
       " '362',\n",
       " '363',\n",
       " '365',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '369',\n",
       " '371',\n",
       " '372',\n",
       " '373',\n",
       " '375',\n",
       " '376',\n",
       " '377',\n",
       " '378',\n",
       " '379',\n",
       " '381',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '386',\n",
       " '389',\n",
       " '390',\n",
       " '392',\n",
       " '393',\n",
       " '394',\n",
       " '395',\n",
       " '396',\n",
       " '397',\n",
       " '399',\n",
       " '400',\n",
       " '401',\n",
       " '402',\n",
       " '403',\n",
       " '404',\n",
       " '405',\n",
       " '406',\n",
       " '407',\n",
       " '408',\n",
       " '409',\n",
       " '410',\n",
       " '411',\n",
       " '412',\n",
       " '413',\n",
       " '414',\n",
       " '415',\n",
       " '416',\n",
       " '417',\n",
       " '418',\n",
       " '419',\n",
       " '420',\n",
       " '421',\n",
       " '425',\n",
       " '429',\n",
       " '430',\n",
       " '431',\n",
       " '432',\n",
       " '433',\n",
       " '435',\n",
       " '427']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_numbers.index(page_breaks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_data(start_indx, end_indx, doc):\n",
    "    text_data = []\n",
    "    span_counter = 0\n",
    "    for i in range(start_indx, end_indx+1):\n",
    "        text_data, span_counter = get_page_text_data(i, span_counter, text_data, doc)\n",
    "    # text_data, span_counter = get_page_text_data(i, span_counter, doc)\n",
    "\n",
    "    # print(\"Length : \" + str(len(text_data)))\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 323/323 [53:02<00:00,  9.85s/it]  \n"
     ]
    }
   ],
   "source": [
    "start_indx = 0\n",
    "\n",
    "tex_start_pos = 0\n",
    "tex_end_pos = 0\n",
    "\n",
    "first_part = 1\n",
    "\n",
    "parts = len(page_breaks)\n",
    "counter = 1\n",
    "for page in tqdm(page_breaks[:parts]):\n",
    "    \n",
    "    end_indx = page_numbers.index(page)\n",
    "    text_data = get_pages_data(start_indx, end_indx, doc)\n",
    "\n",
    "    tex_end_pos = page_positions[int(page)]\n",
    "    tex_contents = tex_file_contents[tex_start_pos:tex_end_pos]\n",
    "\n",
    "    # gpt api call\n",
    "    combined_data = (\n",
    "    \"Below is pre-generated TeX code without proper formatting.\\n\\n\"\n",
    "    f\"{tex_contents}\\n\\n\"\n",
    "    \"Below is the JSON  data which contains formatting :\\n\\n\"\n",
    "    f\"{text_data}\"\n",
    "    )\n",
    "    if counter == parts:\n",
    "      combined_data += \"\\n\\n\"\n",
    "      combined_data += \"This was the last part, close the latex document with end document. Before that, make an index using \\makeindex command and similarly make a bibliography.\"\n",
    "    else:\n",
    "       combined_data += \"\\n\\n\"\n",
    "       combined_data += f\"This is the {counter} part of the book, do not close the latex document with end document.\"\n",
    "\n",
    "    command = first_page_command if first_part==1 else next_pages_prompt\n",
    "    response = generate_response(combined_data, command, \"\") # reversed the combined_data and command\n",
    "        \n",
    "    first_part = 0\n",
    "    counter+=1\n",
    "    with open(OUTPUT_TEX_FILE, 'a') as f:\n",
    "      f.write(response + \"\\n\")\n",
    "      f.write(f\"%---- Page End Break Here ---- Page : {page}\\n\")\n",
    "\n",
    "\n",
    "    # update positions\n",
    "    tex_start_pos = tex_end_pos+1\n",
    "    start_indx = end_indx+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 323/323 [30:39<00:00,  5.69s/it]\n"
     ]
    }
   ],
   "source": [
    "start_indx = 0\n",
    "\n",
    "tex_start_pos = 0\n",
    "tex_end_pos = 0\n",
    "\n",
    "first_part = 1\n",
    "\n",
    "parts = len(page_breaks)\n",
    "counter = 1\n",
    "\n",
    "skipper = 1\n",
    "for page in tqdm(page_breaks[:parts]):\n",
    "\n",
    "    if skipper != 10:\n",
    "       skipper += 1\n",
    "       continue\n",
    "\n",
    "    skipper = 1\n",
    "    \n",
    "    end_indx = page_numbers.index(page)\n",
    "    text_data = get_pages_data(start_indx, end_indx, doc)\n",
    "\n",
    "    tex_end_pos = page_positions[int(page)]\n",
    "    tex_contents = tex_file_contents[tex_start_pos:tex_end_pos]\n",
    "\n",
    "    # gpt api call\n",
    "    combined_data = (\n",
    "    \"Below is pre-generated TeX code without proper formatting.\\n\\n\"\n",
    "    f\"{tex_contents}\\n\\n\"\n",
    "    \"Below is the JSON  data which contains formatting :\\n\\n\"\n",
    "    f\"{text_data}\"\n",
    "    )\n",
    "    if counter == parts:\n",
    "      combined_data += \"\\n\\n\"\n",
    "      combined_data += \"This was the last part, close the latex document with end document. Before that, make an index using \\makeindex command and similarly make a bibliography.\"\n",
    "    else:\n",
    "       combined_data += \"\\n\\n\"\n",
    "       combined_data += f\"This is the {counter} part of the book, do not close the latex document with end document.\"\n",
    "\n",
    "    command = first_page_command if first_part==1 else next_pages_prompt\n",
    "    response = generate_response(combined_data, command, \"\") # reversed the combined_data and command\n",
    "        \n",
    "    first_part = 0\n",
    "    counter+=1\n",
    "    with open(OUTPUT_TEX_FILE, 'a') as f:\n",
    "      f.write(response + \"\\n\")\n",
    "      f.write(f\"%---- Page End Break Here ---- Page : {page}\\n\")\n",
    "\n",
    "\n",
    "    # update positions\n",
    "    tex_start_pos = tex_end_pos+1\n",
    "    start_indx = end_indx+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
