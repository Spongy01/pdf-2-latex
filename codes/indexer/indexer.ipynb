{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import pymupdf\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import fitz\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from fuzzysearch import find_near_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ROOT = \"../../\"\n",
    "\n",
    "BOOK_PATH =  ROOT + \"files/data-science_book/data-science-content.pdf\"\n",
    "TEX_PATH = ROOT + \"files/data-science_book/outputs/data-science_cleaned_2p_bib_first_index_after_parallel.tex\"\n",
    "INDEX_PATH = ROOT + \"files/data-science_book/data-science-index.pdf\"\n",
    "OUTPUT_TEX_PATH = ROOT + \"files/data-science_book/outputs/data-science_cleaned_2p_bib_first_index_after_parallel_indexed.tex\"\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table of contents\n",
    "TOC_PATH  = ROOT + \"files/data-science_book/data-science_toc.pdf\"\n",
    "all_toc_text = \"\"\n",
    "for page_num in range(len(toc)):\n",
    "    page = toc[page_num]\n",
    "    text = page.get_text()\n",
    "    all_toc_text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "    all_toc_text += text.strip() + \"\\n\"\n",
    "\n",
    "toc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = pymupdf.open(\"../../files/data-science_book/data-science-content.pdf\")\n",
    "\n",
    "table = toc.get_toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, '1\\rWhat is Data Science?', 1],\n",
       " [2, '1.1 Computer Science, Data Science, and Real', 2],\n",
       " [2, 'Science', 2],\n",
       " [2, '1.2 Asking Interesting Questions from Data', 4],\n",
       " [3, '1.2.1 The Baseball Encyclopedia', 5],\n",
       " [3, '1.2.2 The Internet Movie Database (IMDb)', 7],\n",
       " [3, '1.2.3 Google Ngrams', 10],\n",
       " [3, '1.2.4 New York Taxi Records', 11],\n",
       " [2, '1.3 Properties of Data', 14],\n",
       " [3, '1.3.1 Structured vs. Unstructured Data', 14],\n",
       " [3, '1.3.2 Quantitative vs. Categorical Data', 15],\n",
       " [3, '1.3.3 Big Data vs. Little Data', 15],\n",
       " [2, '1.4 Classi\\x0ccation and Regression', 16],\n",
       " [2, '1.5 Data Science Television: The Quant Shop', 17],\n",
       " [3, '1.5.1 Kaggle Challenges', 19],\n",
       " [2, '1.6 About the War Stories', 19],\n",
       " [2, '1.7 War Story: Answering the Right Question', 21],\n",
       " [2, '1.8 Chapter Notes', 22],\n",
       " [2, '1.9 Exercises', 23],\n",
       " [1, '2\\rMathematical Preliminaries', 26],\n",
       " [2, '2.1 Probability', 26],\n",
       " [3, '2.1.1 Probability vs. Statistics', 28],\n",
       " [3, '2.1.2 Compound Events and Independence', 29],\n",
       " [3, '2.1.3 Conditional Probability', 30],\n",
       " [3, '2.1.4 Probability Distributions', 31],\n",
       " [2, '2.2 Descriptive Statistics', 33],\n",
       " [3, '2.2.1 Centrality Measures', 33],\n",
       " [3, '2.2.2 Variability Measures', 35],\n",
       " [3, '2.2.3 Interpreting Variance', 36],\n",
       " [3, '2.2.4 Characterizing Distributions', 38],\n",
       " [2, '2.3 Correlation Analysis', 39],\n",
       " [3, '2.3.1 Correlation Coe\\x0ecients: Pearson and Spearman Rank', 40],\n",
       " [3, '2.3.2 The Power and Signi\\x0ccance of Correlation', 42],\n",
       " [3, '2.3.3 Correlation Does Not Imply Causation!', 44],\n",
       " [3, '2.3.4 Detecting Periodicities by Autocorrelation', 45],\n",
       " [2, '2.4 Logarithms', 46],\n",
       " [3, '2.4.1 Logarithms and Multiplying Probabilities', 47],\n",
       " [3, '2.4.2 Logarithms and Ratios', 47],\n",
       " [3, '2.4.3 Logarithms and Normalizing Skewed Distributions', 48],\n",
       " [2, '2.5 War Story: Fitting Designer Genes', 49],\n",
       " [2, '2.6 Chapter Notes', 51],\n",
       " [2, '2.7 Exercises', 52],\n",
       " [1, '3\\rData Munging', 56],\n",
       " [2, '3.1 Languages for Data Science', 56],\n",
       " [3, '3.1.1 The Importance of Notebook Environments', 58],\n",
       " [3, '3.1.2 Standard Data Formats', 60],\n",
       " [2, '3.2 Collecting Data', 63],\n",
       " [3, '3.2.1 Hunting', 63],\n",
       " [3, '3.2.2 Scraping', 66],\n",
       " [3, '3.2.3 Logging', 67],\n",
       " [2, '3.3 Cleaning Data', 68],\n",
       " [3, '3.3.1 Errors vs. Artifacts', 68],\n",
       " [3, '3.3.2 Data Compatibility', 71],\n",
       " [3, '3.3.3 Dealing with Missing Values', 75],\n",
       " [3, '3.3.4 Outlier Detection', 77],\n",
       " [2, '3.4 War Story: Beating the Market', 78],\n",
       " [2, '3.5 Crowdsourcing', 79],\n",
       " [3, '3.5.1 The Penny Demo', 80],\n",
       " [3, '3.5.2 When is the Crowd Wise?', 81],\n",
       " [3, '3.5.3 Mechanisms for Aggregation', 82],\n",
       " [3, '3.5.4 Crowdsourcing Services', 83],\n",
       " [3, '3.5.5 Gami\\x0ccation', 87],\n",
       " [2, '3.6 Chapter Notes', 89],\n",
       " [2, '3.7 Exercises', 89],\n",
       " [1, '4\\rScores and Rankings', 93],\n",
       " [2, '4.1 The Body Mass Index (BMI)', 94],\n",
       " [2, '4.2 Developing Scoring Systems', 97],\n",
       " [3, '4.2.1 Gold Standards and Proxies', 97],\n",
       " [3, '4.2.2 Scores vs. Rankings', 98],\n",
       " [3, '4.2.3 Recognizing Good Scoring Functions', 99],\n",
       " [2, '4.3 Z-scores and Normalization', 101],\n",
       " [2, '4.4 Advanced Ranking Techniques', 102],\n",
       " [3, '4.4.1 Elo Rankings', 102],\n",
       " [3, '4.4.2 Merging Rankings', 106],\n",
       " [3, '4.4.3 Digraph-based Rankings', 107],\n",
       " [3, '4.4.4 PageRank', 109],\n",
       " [2, \"4.5 War Story: Clyde's Revenge\", 109],\n",
       " [2, \"4.6 Arrow's Impossibility Theorem\", 112],\n",
       " [2, \"4.7 War Story: Who's Bigger?\", 113],\n",
       " [2, '4.8 Chapter Notes', 116],\n",
       " [2, '4.9 Exercises', 117],\n",
       " [1, '5\\rStatistical Analysis', 119],\n",
       " [2, '5.1 Statistical Distributions', 120],\n",
       " [3, '5.1.1 The Binomial Distribution', 121],\n",
       " [3, '5.1.2 The Normal Distribution', 122],\n",
       " [3, '5.1.3 Implications of the Normal Distribution', 124],\n",
       " [3, '5.1.4 Poisson Distribution', 125],\n",
       " [3, '5.1.5 Power Law Distributions', 127],\n",
       " [2, '5.2 Sampling from Distributions', 130],\n",
       " [3, '5.2.1 Random Sampling beyond One Dimension', 131],\n",
       " [2, '5.3 Statistical Signi\\x0ccance', 133],\n",
       " [3, '5.3.1 The Signi\\x0ccance of Signi\\x0ccance', 133],\n",
       " [3, '5.3.2 The T-test: Comparing Population Means', 135],\n",
       " [3, '5.3.3 The Kolmogorov-Smirnov Test', 137],\n",
       " [3, '5.3.4 The Bonferroni Correction', 139],\n",
       " [3, '5.3.5 False Discovery Rate', 140],\n",
       " [2, '5.4 War Story: Discovering the Fountain of Youth?', 141],\n",
       " [2, '5.5 Permutation Tests and P-values', 143],\n",
       " [3, '5.5.1 Generating Random Permutations', 145],\n",
       " [3, \"5.5.2 DiMaggio's Hitting Streak\", 146],\n",
       " [2, '5.6 Bayesian Reasoning', 148],\n",
       " [2, '5.7 Chapter Notes', 149],\n",
       " [2, '5.8 Exercises', 149],\n",
       " [1, '6\\rVisualizing Data', 153],\n",
       " [2, '6.1 Exploratory Data Analysis', 154],\n",
       " [3, '6.1.1 Confronting a New Data Set', 154],\n",
       " [3, \"6.1.2 Summary Statistics and Anscombe's Quartet\", 157],\n",
       " [3, '6.1.3 Visualization Tools', 158],\n",
       " [2, '6.2 Developing a Visualization Aesthetic', 160],\n",
       " [3, '6.2.1 Maximizing Data-Ink Ratio', 161],\n",
       " [3, '6.2.2 Minimizing the Lie Factor', 162],\n",
       " [3, '6.2.3 Minimizing Chartjunk', 163],\n",
       " [3, '6.2.4 Proper Scaling and Labeling', 165],\n",
       " [3, '6.2.5 E\\x0bective Use of Color and Shading', 166],\n",
       " [3, '6.2.6 The Power of Repetition', 167],\n",
       " [2, '6.3 Chart Types', 168],\n",
       " [3, '6.3.1 Tabular Data', 168],\n",
       " [3, '6.3.2 Dot and Line Plots', 172],\n",
       " [3, '6.3.3 Scatter Plots', 175],\n",
       " [3, '6.3.4 Bar Plots and Pie Charts', 177],\n",
       " [3, '6.3.5 Histograms', 181],\n",
       " [3, '6.3.6 Data Maps', 185],\n",
       " [2, '6.4 Great Visualizations', 187],\n",
       " [3, \"6.4.1 Marey's Train Schedule\", 187],\n",
       " [3, \"6.4.2 Snow's Cholera Map\", 189],\n",
       " [3, \"6.4.3 New York's Weather Year\", 190],\n",
       " [2, '6.5 Reading Graphs', 190],\n",
       " [3, '6.5.1 The Obscured Distribution', 191],\n",
       " [3, '6.5.2 Overinterpreting Variance', 191],\n",
       " [2, '6.6 Interactive Visualization', 193],\n",
       " [2, '6.7 War Story: TextMapping the World', 194],\n",
       " [2, '6.8 Chapter Notes', 196],\n",
       " [2, '6.9 Exercises', 197],\n",
       " [1, '7\\rMathematical Models', 199],\n",
       " [2, '7.1 Philosophies of Modeling', 199],\n",
       " [3, \"7.1.1 Occam's Razor\", 199],\n",
       " [3, '7.1.2 Bias{Variance Trade-O\\x0bs', 200],\n",
       " [3, '7.1.3 What Would Nate Silver Do?', 201],\n",
       " [2, '7.2 A Taxonomy of Models', 203],\n",
       " [3, '7.2.1 Linear vs. Non-Linear Models', 204],\n",
       " [3, '7.2.2 Blackbox vs. Descriptive Models', 204],\n",
       " [3, '7.2.3 First-Principle vs. Data-Driven Models', 205],\n",
       " [3, '7.2.4 Stochastic vs. Deterministic Models', 206],\n",
       " [3, '7.2.5 Flat vs. Hierarchical Models', 207],\n",
       " [2, '7.3 Baseline Models', 208],\n",
       " [3, '7.3.1 Baseline Models for Classi\\x0ccation', 208],\n",
       " [3, '7.3.2 Baseline Models for Value Prediction', 210],\n",
       " [2, '7.4 Evaluating Models', 210],\n",
       " [3, '7.4.1 Evaluating Classi\\x0cers', 211],\n",
       " [3, '7.4.2 Receiver-Operator Characteristic (ROC) Curves', 216],\n",
       " [3, '7.4.3 Evaluating Multiclass Systems', 217],\n",
       " [3, '7.4.4 Evaluating Value Prediction Models', 219],\n",
       " [2, '7.5 Evaluation Environments', 222],\n",
       " [3, '7.5.1 Data Hygiene for Evaluation', 223],\n",
       " [3, '7.5.2 Amplifying Small Evaluation Sets', 224],\n",
       " [2, '7.6 War Story: 100% Accuracy', 226],\n",
       " [2, '7.7 Simulation Models', 227],\n",
       " [2, '7.8 War Story: Calculated Bets', 228],\n",
       " [2, '7.9 Chapter Notes', 231],\n",
       " [2, '7.10 Exercises', 232],\n",
       " [1, '8\\rLinear Algebra', 235],\n",
       " [2, '8.1 The Power of Linear Algebra', 235],\n",
       " [3, '8.1.1 Interpreting Linear Algebraic Formulae', 236],\n",
       " [3, '8.1.2 Geometry and Vectors', 238],\n",
       " [2, '8.2 Visualizing Matrix Operations', 239],\n",
       " [3, '8.2.1 Matrix Addition', 240],\n",
       " [3, '8.2.2 Matrix Multiplication', 241],\n",
       " [3, '8.2.3 Applications of Matrix Multiplication', 242],\n",
       " [3, '8.2.4 Identity Matrices and Inversion', 246],\n",
       " [3, '8.2.5 Matrix Inversion and Linear Systems', 248],\n",
       " [3, '8.2.6 Matrix Rank', 249],\n",
       " [2, '8.3 Factoring Matrices', 250],\n",
       " [3, '8.3.1 Why Factor Feature Matrices?', 250],\n",
       " [3, '8.3.2 LU Decomposition and Determinants', 252],\n",
       " [2, '8.4 Eigenvalues and Eigenvectors', 253],\n",
       " [3, '8.4.1 Properties of Eigenvalues', 253],\n",
       " [3, '8.4.2 Computing Eigenvalues', 254],\n",
       " [2, '8.5 Eigenvalue Decomposition', 255],\n",
       " [3, '8.5.1 Singular Value Decomposition', 256],\n",
       " [3, '8.5.2 Principal Components Analysis', 258],\n",
       " [2, '8.6 War Story: The Human Factors', 260],\n",
       " [2, '8.7 Chapter Notes', 261],\n",
       " [2, '8.8 Exercises', 261],\n",
       " [1, '9 Linear and Logistic Regression', 264],\n",
       " [2, '9.1 Linear Regression', 265],\n",
       " [3, '9.1.1 Linear Regression and Duality', 265],\n",
       " [3, '9.1.2 Error in Linear Regression', 266],\n",
       " [3, '9.1.3 Finding the Optimal Fit', 267],\n",
       " [2, '9.2 Better Regression Models', 269],\n",
       " [3, '9.2.1 Removing Outliers', 269],\n",
       " [3, '9.2.2 Fitting Non-Linear Functions', 270],\n",
       " [3, '9.2.3 Feature and Target Scaling', 271],\n",
       " [3, '9.2.4 Dealing with Highly-Correlated Features', 274],\n",
       " [2, '9.3 War Story: Taxi Deriver', 274],\n",
       " [2, '9.4 Regression as Parameter Fitting', 276],\n",
       " [3, '9.4.1 Convex Parameter Spaces', 277],\n",
       " [3, '9.4.2 Gradient Descent Search', 278],\n",
       " [3, '9.4.3 What is the Right Learning Rate?', 280],\n",
       " [3, '9.4.4 Stochastic Gradient Descent', 282],\n",
       " [2, '9.5 Simplifying Models through Regularization', 283],\n",
       " [3, '9.5.1 Ridge Regression', 283],\n",
       " [3, '9.5.2 LASSO Regression', 284],\n",
       " [3, '9.5.3 Trade-O\\x0bs between Fit and Complexity', 285],\n",
       " [2, '9.6 Classi\\x0ccation and Logistic Regression', 286],\n",
       " [3, '9.6.1 Regression for Classi\\x0ccation', 287],\n",
       " [3, '9.6.2 Decision Boundaries', 288],\n",
       " [3, '9.6.3 Logistic Regression', 289],\n",
       " [2, '9.7 Issues in Logistic Classi\\x0ccation', 292],\n",
       " [3, '9.7.1 Balanced Training Classes', 292],\n",
       " [3, '9.7.2 Multi-Class Classi\\x0ccation', 294],\n",
       " [3, '9.7.3 Hierarchical Classi\\x0ccation', 295],\n",
       " [3, '9.7.4 Partition Functions and Multinomial Regression', 296],\n",
       " [2, '9.8 Chapter Notes', 297],\n",
       " [2, '9.9 Exercises', 298],\n",
       " [1, '10\\rDistance and Network Methods', 300],\n",
       " [2, '10.1 Measuring Distances', 300],\n",
       " [3, '10.1.1 Distance Metrics', 301],\n",
       " [3, '10.1.2 The', 302],\n",
       " [3, 'Distance Metric', 302],\n",
       " [3, '10.1.3 Working in Higher Dimensions', 304],\n",
       " [3, '10.1.4 Dimensional Egalitarianism', 305],\n",
       " [3, '10.1.5 Points vs. Vectors', 306],\n",
       " [3, '10.1.6 Distances between Probability Distributions', 307],\n",
       " [2, '10.2 Nearest Neighbor Classi\\x0ccation', 308],\n",
       " [3, '10.2.1 Seeking Good Analogies', 309],\n",
       " [3, '10.2.2', 310],\n",
       " [3, '-Nearest Neighbors', 310],\n",
       " [3, '10.2.3 Finding Nearest Neighbors', 312],\n",
       " [3, '10.2.4 Locality Sensitive Hashing', 314],\n",
       " [2, '10.3 Graphs, Networks, and Distances', 316],\n",
       " [3, '10.3.1 Weighted Graphs and Induced Networks', 317],\n",
       " [3, '10.3.2 Talking About Graphs', 318],\n",
       " [3, '10.3.3 Graph Theory', 320],\n",
       " [2, '10.4 PageRank', 322],\n",
       " [2, '10.5 Clustering', 324],\n",
       " [3, '10.5.1', 327],\n",
       " [3, '-means Clustering', 327],\n",
       " [3, '10.5.2 Agglomerative Clustering', 333],\n",
       " [3, '10.5.3 Comparing Clusterings', 338],\n",
       " [3, '10.5.4 Similarity Graphs and Cut-Based Clustering', 338],\n",
       " [2, '10.6 War Story: Cluster Bombing', 341],\n",
       " [2, '10.7 Chapter Notes', 342],\n",
       " [2, '10.8 Exercises', 343],\n",
       " [1, '11\\rMachine Learning', 347],\n",
       " [2, '11.1 Naive Bayes', 350],\n",
       " [3, '11.1.1 Formulation', 350],\n",
       " [3, '11.1.2 Dealing with Zero Counts (Discounting)', 352],\n",
       " [2, '11.2 Decision Tree Classi\\x0cers', 353],\n",
       " [3, '11.2.1 Constructing Decision Trees', 355],\n",
       " [3, '11.2.2 Realizing Exclusive Or', 357],\n",
       " [3, '11.2.3 Ensembles of Decision Trees', 358],\n",
       " [2, '11.3 Boosting and Ensemble Learning', 359],\n",
       " [3, '11.3.1 Voting with Classi\\x0cers', 359],\n",
       " [3, '11.3.2 Boosting Algorithms', 360],\n",
       " [2, '11.4 Support Vector Machines', 362],\n",
       " [3, '11.4.1 Linear SVMs', 365],\n",
       " [3, '11.4.2 Non-linear SVMs', 365],\n",
       " [3, '11.4.3 Kernels', 367],\n",
       " [2, '11.5 Degrees of Supervision', 368],\n",
       " [3, '11.5.1 Supervised Learning', 368],\n",
       " [3, '11.5.2 Unsupervised Learning', 368],\n",
       " [3, '11.5.3 Semi-supervised Learning', 370],\n",
       " [3, '11.5.4 Feature Engineering', 371],\n",
       " [2, '11.6 Deep Learning', 373],\n",
       " [3, '11.6.1 Networks and Depth', 374],\n",
       " [3, '11.6.2 Backpropagation', 378],\n",
       " [3, '11.6.3 Word and Graph Embeddings', 379],\n",
       " [2, '11.7 War Story: The Name Game', 381],\n",
       " [2, '11.8 Chapter Notes', 383],\n",
       " [2, '11.9 Exercises', 384],\n",
       " [1, '12\\rBig Data: Achieving Scale', 387],\n",
       " [2, '12.1 What is Big Data?', 388],\n",
       " [3, '12.1.1 Big Data as Bad Data', 388],\n",
       " [3, '12.1.2 The Three Vs', 390],\n",
       " [2, '12.2 War Story: Infrastructure Matters', 391],\n",
       " [2, '12.3 Algorithmics for Big Data', 393],\n",
       " [3, '12.3.1 Big Oh Analysis', 393],\n",
       " [3, '12.3.2 Hashing', 395],\n",
       " [3, '12.3.3 Exploiting the Storage Hierarchy', 397],\n",
       " [3, '12.3.4 Streaming and Single-Pass Algorithms', 398],\n",
       " [2, '12.4 Filtering and Sampling', 399],\n",
       " [3, '12.4.1 Deterministic Sampling Algorithms', 400],\n",
       " [3, '12.4.2 Randomized and Stream Sampling', 402],\n",
       " [2, '12.5 Parallelism', 402],\n",
       " [3, '12.5.1 One, Two, Many', 403],\n",
       " [3, '12.5.2 Data Parallelism', 405],\n",
       " [3, '12.5.3 Grid Search', 405],\n",
       " [3, '12.5.4 Cloud Computing Services', 406],\n",
       " [2, '12.6 MapReduce', 406],\n",
       " [3, '12.6.1 Map-Reduce Programming', 408],\n",
       " [3, '12.6.2 MapReduce under the Hood', 410],\n",
       " [2, '12.7 Societal and Ethical Implications', 412],\n",
       " [2, '12.8 Chapter Notes', 415],\n",
       " [2, '12.9 Exercises', 415],\n",
       " [1, '13\\rCoda', 418],\n",
       " [2, '13.1 Get a Job!', 418],\n",
       " [2, '13.2 Go to Graduate School!', 419],\n",
       " [2, '13.3 Professional Consulting Services', 420],\n",
       " [1, 'Bibliography', 421],\n",
       " [1, 'Index', 428]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_merge_toc(toc):\n",
    "    cleaned_toc = []\n",
    "\n",
    "    def remove_numbering(title):\n",
    "        title = title.replace('\\r', '').replace('\\x0c', 'fi')\n",
    "        return re.sub(r'^\\s*\\d+(\\.\\d+)*\\s*', '', title).strip()\n",
    "\n",
    "    for entry in toc:\n",
    "        level, title, page = entry\n",
    "        title_clean = title.replace('\\r', '').replace('\\x0c', 'fi').strip()\n",
    "\n",
    "        if cleaned_toc:\n",
    "            prev_level, prev_title, prev_page = cleaned_toc[-1]\n",
    "            # If same level and same page, and this one doesn’t start with a number — join\n",
    "            if level == prev_level and page == prev_page and not re.match(r'^\\s*\\d+(\\.\\d+)*', title_clean):\n",
    "                cleaned_toc[-1] = (prev_level, prev_title + \" \" + title_clean, prev_page)\n",
    "                continue\n",
    "\n",
    "        cleaned_title = remove_numbering(title_clean)\n",
    "        cleaned_toc.append((level, cleaned_title, page))\n",
    "\n",
    "    return cleaned_toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original title: 1What is Data Science?\n",
      "After replacing: 1What is Data Science?\n",
      "Original title: 1.1 Computer Science, Data Science, and Real\n",
      "After replacing: 1.1 Computer Science, Data Science, and Real\n",
      "Original title: 1.2 Asking Interesting Questions from Data\n",
      "After replacing: 1.2 Asking Interesting Questions from Data\n",
      "Original title: 1.2.1 The Baseball Encyclopedia\n",
      "After replacing: 1.2.1 The Baseball Encyclopedia\n",
      "Original title: 1.2.2 The Internet Movie Database (IMDb)\n",
      "After replacing: 1.2.2 The Internet Movie Database (IMDb)\n",
      "Original title: 1.2.3 Google Ngrams\n",
      "After replacing: 1.2.3 Google Ngrams\n",
      "Original title: 1.2.4 New York Taxi Records\n",
      "After replacing: 1.2.4 New York Taxi Records\n",
      "Original title: 1.3 Properties of Data\n",
      "After replacing: 1.3 Properties of Data\n",
      "Original title: 1.3.1 Structured vs. Unstructured Data\n",
      "After replacing: 1.3.1 Structured vs. Unstructured Data\n",
      "Original title: 1.3.2 Quantitative vs. Categorical Data\n",
      "After replacing: 1.3.2 Quantitative vs. Categorical Data\n",
      "Original title: 1.3.3 Big Data vs. Little Data\n",
      "After replacing: 1.3.3 Big Data vs. Little Data\n",
      "Original title: 1.4 Classification and Regression\n",
      "After replacing: 1.4 Classification and Regression\n",
      "Original title: 1.5 Data Science Television: The Quant Shop\n",
      "After replacing: 1.5 Data Science Television: The Quant Shop\n",
      "Original title: 1.5.1 Kaggle Challenges\n",
      "After replacing: 1.5.1 Kaggle Challenges\n",
      "Original title: 1.6 About the War Stories\n",
      "After replacing: 1.6 About the War Stories\n",
      "Original title: 1.7 War Story: Answering the Right Question\n",
      "After replacing: 1.7 War Story: Answering the Right Question\n",
      "Original title: 1.8 Chapter Notes\n",
      "After replacing: 1.8 Chapter Notes\n",
      "Original title: 1.9 Exercises\n",
      "After replacing: 1.9 Exercises\n",
      "Original title: 2Mathematical Preliminaries\n",
      "After replacing: 2Mathematical Preliminaries\n",
      "Original title: 2.1 Probability\n",
      "After replacing: 2.1 Probability\n",
      "Original title: 2.1.1 Probability vs. Statistics\n",
      "After replacing: 2.1.1 Probability vs. Statistics\n",
      "Original title: 2.1.2 Compound Events and Independence\n",
      "After replacing: 2.1.2 Compound Events and Independence\n",
      "Original title: 2.1.3 Conditional Probability\n",
      "After replacing: 2.1.3 Conditional Probability\n",
      "Original title: 2.1.4 Probability Distributions\n",
      "After replacing: 2.1.4 Probability Distributions\n",
      "Original title: 2.2 Descriptive Statistics\n",
      "After replacing: 2.2 Descriptive Statistics\n",
      "Original title: 2.2.1 Centrality Measures\n",
      "After replacing: 2.2.1 Centrality Measures\n",
      "Original title: 2.2.2 Variability Measures\n",
      "After replacing: 2.2.2 Variability Measures\n",
      "Original title: 2.2.3 Interpreting Variance\n",
      "After replacing: 2.2.3 Interpreting Variance\n",
      "Original title: 2.2.4 Characterizing Distributions\n",
      "After replacing: 2.2.4 Characterizing Distributions\n",
      "Original title: 2.3 Correlation Analysis\n",
      "After replacing: 2.3 Correlation Analysis\n",
      "Original title: 2.3.1 Correlation Coe\u000ecients: Pearson and Spearman Rank\n",
      "After replacing: 2.3.1 Correlation Coe\u000ecients: Pearson and Spearman Rank\n",
      "Original title: 2.3.2 The Power and Significance of Correlation\n",
      "After replacing: 2.3.2 The Power and Significance of Correlation\n",
      "Original title: 2.3.3 Correlation Does Not Imply Causation!\n",
      "After replacing: 2.3.3 Correlation Does Not Imply Causation!\n",
      "Original title: 2.3.4 Detecting Periodicities by Autocorrelation\n",
      "After replacing: 2.3.4 Detecting Periodicities by Autocorrelation\n",
      "Original title: 2.4 Logarithms\n",
      "After replacing: 2.4 Logarithms\n",
      "Original title: 2.4.1 Logarithms and Multiplying Probabilities\n",
      "After replacing: 2.4.1 Logarithms and Multiplying Probabilities\n",
      "Original title: 2.4.2 Logarithms and Ratios\n",
      "After replacing: 2.4.2 Logarithms and Ratios\n",
      "Original title: 2.4.3 Logarithms and Normalizing Skewed Distributions\n",
      "After replacing: 2.4.3 Logarithms and Normalizing Skewed Distributions\n",
      "Original title: 2.5 War Story: Fitting Designer Genes\n",
      "After replacing: 2.5 War Story: Fitting Designer Genes\n",
      "Original title: 2.6 Chapter Notes\n",
      "After replacing: 2.6 Chapter Notes\n",
      "Original title: 2.7 Exercises\n",
      "After replacing: 2.7 Exercises\n",
      "Original title: 3Data Munging\n",
      "After replacing: 3Data Munging\n",
      "Original title: 3.1 Languages for Data Science\n",
      "After replacing: 3.1 Languages for Data Science\n",
      "Original title: 3.1.1 The Importance of Notebook Environments\n",
      "After replacing: 3.1.1 The Importance of Notebook Environments\n",
      "Original title: 3.1.2 Standard Data Formats\n",
      "After replacing: 3.1.2 Standard Data Formats\n",
      "Original title: 3.2 Collecting Data\n",
      "After replacing: 3.2 Collecting Data\n",
      "Original title: 3.2.1 Hunting\n",
      "After replacing: 3.2.1 Hunting\n",
      "Original title: 3.2.2 Scraping\n",
      "After replacing: 3.2.2 Scraping\n",
      "Original title: 3.2.3 Logging\n",
      "After replacing: 3.2.3 Logging\n",
      "Original title: 3.3 Cleaning Data\n",
      "After replacing: 3.3 Cleaning Data\n",
      "Original title: 3.3.1 Errors vs. Artifacts\n",
      "After replacing: 3.3.1 Errors vs. Artifacts\n",
      "Original title: 3.3.2 Data Compatibility\n",
      "After replacing: 3.3.2 Data Compatibility\n",
      "Original title: 3.3.3 Dealing with Missing Values\n",
      "After replacing: 3.3.3 Dealing with Missing Values\n",
      "Original title: 3.3.4 Outlier Detection\n",
      "After replacing: 3.3.4 Outlier Detection\n",
      "Original title: 3.4 War Story: Beating the Market\n",
      "After replacing: 3.4 War Story: Beating the Market\n",
      "Original title: 3.5 Crowdsourcing\n",
      "After replacing: 3.5 Crowdsourcing\n",
      "Original title: 3.5.1 The Penny Demo\n",
      "After replacing: 3.5.1 The Penny Demo\n",
      "Original title: 3.5.2 When is the Crowd Wise?\n",
      "After replacing: 3.5.2 When is the Crowd Wise?\n",
      "Original title: 3.5.3 Mechanisms for Aggregation\n",
      "After replacing: 3.5.3 Mechanisms for Aggregation\n",
      "Original title: 3.5.4 Crowdsourcing Services\n",
      "After replacing: 3.5.4 Crowdsourcing Services\n",
      "Original title: 3.5.5 Gamification\n",
      "After replacing: 3.5.5 Gamification\n",
      "Original title: 3.6 Chapter Notes\n",
      "After replacing: 3.6 Chapter Notes\n",
      "Original title: 3.7 Exercises\n",
      "After replacing: 3.7 Exercises\n",
      "Original title: 4Scores and Rankings\n",
      "After replacing: 4Scores and Rankings\n",
      "Original title: 4.1 The Body Mass Index (BMI)\n",
      "After replacing: 4.1 The Body Mass Index (BMI)\n",
      "Original title: 4.2 Developing Scoring Systems\n",
      "After replacing: 4.2 Developing Scoring Systems\n",
      "Original title: 4.2.1 Gold Standards and Proxies\n",
      "After replacing: 4.2.1 Gold Standards and Proxies\n",
      "Original title: 4.2.2 Scores vs. Rankings\n",
      "After replacing: 4.2.2 Scores vs. Rankings\n",
      "Original title: 4.2.3 Recognizing Good Scoring Functions\n",
      "After replacing: 4.2.3 Recognizing Good Scoring Functions\n",
      "Original title: 4.3 Z-scores and Normalization\n",
      "After replacing: 4.3 Z-scores and Normalization\n",
      "Original title: 4.4 Advanced Ranking Techniques\n",
      "After replacing: 4.4 Advanced Ranking Techniques\n",
      "Original title: 4.4.1 Elo Rankings\n",
      "After replacing: 4.4.1 Elo Rankings\n",
      "Original title: 4.4.2 Merging Rankings\n",
      "After replacing: 4.4.2 Merging Rankings\n",
      "Original title: 4.4.3 Digraph-based Rankings\n",
      "After replacing: 4.4.3 Digraph-based Rankings\n",
      "Original title: 4.4.4 PageRank\n",
      "After replacing: 4.4.4 PageRank\n",
      "Original title: 4.5 War Story: Clyde's Revenge\n",
      "After replacing: 4.5 War Story: Clyde's Revenge\n",
      "Original title: 4.6 Arrow's Impossibility Theorem\n",
      "After replacing: 4.6 Arrow's Impossibility Theorem\n",
      "Original title: 4.7 War Story: Who's Bigger?\n",
      "After replacing: 4.7 War Story: Who's Bigger?\n",
      "Original title: 4.8 Chapter Notes\n",
      "After replacing: 4.8 Chapter Notes\n",
      "Original title: 4.9 Exercises\n",
      "After replacing: 4.9 Exercises\n",
      "Original title: 5Statistical Analysis\n",
      "After replacing: 5Statistical Analysis\n",
      "Original title: 5.1 Statistical Distributions\n",
      "After replacing: 5.1 Statistical Distributions\n",
      "Original title: 5.1.1 The Binomial Distribution\n",
      "After replacing: 5.1.1 The Binomial Distribution\n",
      "Original title: 5.1.2 The Normal Distribution\n",
      "After replacing: 5.1.2 The Normal Distribution\n",
      "Original title: 5.1.3 Implications of the Normal Distribution\n",
      "After replacing: 5.1.3 Implications of the Normal Distribution\n",
      "Original title: 5.1.4 Poisson Distribution\n",
      "After replacing: 5.1.4 Poisson Distribution\n",
      "Original title: 5.1.5 Power Law Distributions\n",
      "After replacing: 5.1.5 Power Law Distributions\n",
      "Original title: 5.2 Sampling from Distributions\n",
      "After replacing: 5.2 Sampling from Distributions\n",
      "Original title: 5.2.1 Random Sampling beyond One Dimension\n",
      "After replacing: 5.2.1 Random Sampling beyond One Dimension\n",
      "Original title: 5.3 Statistical Significance\n",
      "After replacing: 5.3 Statistical Significance\n",
      "Original title: 5.3.1 The Significance of Significance\n",
      "After replacing: 5.3.1 The Significance of Significance\n",
      "Original title: 5.3.2 The T-test: Comparing Population Means\n",
      "After replacing: 5.3.2 The T-test: Comparing Population Means\n",
      "Original title: 5.3.3 The Kolmogorov-Smirnov Test\n",
      "After replacing: 5.3.3 The Kolmogorov-Smirnov Test\n",
      "Original title: 5.3.4 The Bonferroni Correction\n",
      "After replacing: 5.3.4 The Bonferroni Correction\n",
      "Original title: 5.3.5 False Discovery Rate\n",
      "After replacing: 5.3.5 False Discovery Rate\n",
      "Original title: 5.4 War Story: Discovering the Fountain of Youth?\n",
      "After replacing: 5.4 War Story: Discovering the Fountain of Youth?\n",
      "Original title: 5.5 Permutation Tests and P-values\n",
      "After replacing: 5.5 Permutation Tests and P-values\n",
      "Original title: 5.5.1 Generating Random Permutations\n",
      "After replacing: 5.5.1 Generating Random Permutations\n",
      "Original title: 5.5.2 DiMaggio's Hitting Streak\n",
      "After replacing: 5.5.2 DiMaggio's Hitting Streak\n",
      "Original title: 5.6 Bayesian Reasoning\n",
      "After replacing: 5.6 Bayesian Reasoning\n",
      "Original title: 5.7 Chapter Notes\n",
      "After replacing: 5.7 Chapter Notes\n",
      "Original title: 5.8 Exercises\n",
      "After replacing: 5.8 Exercises\n",
      "Original title: 6Visualizing Data\n",
      "After replacing: 6Visualizing Data\n",
      "Original title: 6.1 Exploratory Data Analysis\n",
      "After replacing: 6.1 Exploratory Data Analysis\n",
      "Original title: 6.1.1 Confronting a New Data Set\n",
      "After replacing: 6.1.1 Confronting a New Data Set\n",
      "Original title: 6.1.2 Summary Statistics and Anscombe's Quartet\n",
      "After replacing: 6.1.2 Summary Statistics and Anscombe's Quartet\n",
      "Original title: 6.1.3 Visualization Tools\n",
      "After replacing: 6.1.3 Visualization Tools\n",
      "Original title: 6.2 Developing a Visualization Aesthetic\n",
      "After replacing: 6.2 Developing a Visualization Aesthetic\n",
      "Original title: 6.2.1 Maximizing Data-Ink Ratio\n",
      "After replacing: 6.2.1 Maximizing Data-Ink Ratio\n",
      "Original title: 6.2.2 Minimizing the Lie Factor\n",
      "After replacing: 6.2.2 Minimizing the Lie Factor\n",
      "Original title: 6.2.3 Minimizing Chartjunk\n",
      "After replacing: 6.2.3 Minimizing Chartjunk\n",
      "Original title: 6.2.4 Proper Scaling and Labeling\n",
      "After replacing: 6.2.4 Proper Scaling and Labeling\n",
      "Original title: 6.2.5 E\u000bective Use of Color and Shading\n",
      "After replacing: 6.2.5 E\u000bective Use of Color and Shading\n",
      "Original title: 6.2.6 The Power of Repetition\n",
      "After replacing: 6.2.6 The Power of Repetition\n",
      "Original title: 6.3 Chart Types\n",
      "After replacing: 6.3 Chart Types\n",
      "Original title: 6.3.1 Tabular Data\n",
      "After replacing: 6.3.1 Tabular Data\n",
      "Original title: 6.3.2 Dot and Line Plots\n",
      "After replacing: 6.3.2 Dot and Line Plots\n",
      "Original title: 6.3.3 Scatter Plots\n",
      "After replacing: 6.3.3 Scatter Plots\n",
      "Original title: 6.3.4 Bar Plots and Pie Charts\n",
      "After replacing: 6.3.4 Bar Plots and Pie Charts\n",
      "Original title: 6.3.5 Histograms\n",
      "After replacing: 6.3.5 Histograms\n",
      "Original title: 6.3.6 Data Maps\n",
      "After replacing: 6.3.6 Data Maps\n",
      "Original title: 6.4 Great Visualizations\n",
      "After replacing: 6.4 Great Visualizations\n",
      "Original title: 6.4.1 Marey's Train Schedule\n",
      "After replacing: 6.4.1 Marey's Train Schedule\n",
      "Original title: 6.4.2 Snow's Cholera Map\n",
      "After replacing: 6.4.2 Snow's Cholera Map\n",
      "Original title: 6.4.3 New York's Weather Year\n",
      "After replacing: 6.4.3 New York's Weather Year\n",
      "Original title: 6.5 Reading Graphs\n",
      "After replacing: 6.5 Reading Graphs\n",
      "Original title: 6.5.1 The Obscured Distribution\n",
      "After replacing: 6.5.1 The Obscured Distribution\n",
      "Original title: 6.5.2 Overinterpreting Variance\n",
      "After replacing: 6.5.2 Overinterpreting Variance\n",
      "Original title: 6.6 Interactive Visualization\n",
      "After replacing: 6.6 Interactive Visualization\n",
      "Original title: 6.7 War Story: TextMapping the World\n",
      "After replacing: 6.7 War Story: TextMapping the World\n",
      "Original title: 6.8 Chapter Notes\n",
      "After replacing: 6.8 Chapter Notes\n",
      "Original title: 6.9 Exercises\n",
      "After replacing: 6.9 Exercises\n",
      "Original title: 7Mathematical Models\n",
      "After replacing: 7Mathematical Models\n",
      "Original title: 7.1 Philosophies of Modeling\n",
      "After replacing: 7.1 Philosophies of Modeling\n",
      "Original title: 7.1.1 Occam's Razor\n",
      "After replacing: 7.1.1 Occam's Razor\n",
      "Original title: 7.1.2 Bias{Variance Trade-O\u000bs\n",
      "After replacing: 7.1.2 Bias{Variance Trade-O\u000bs\n",
      "Original title: 7.1.3 What Would Nate Silver Do?\n",
      "After replacing: 7.1.3 What Would Nate Silver Do?\n",
      "Original title: 7.2 A Taxonomy of Models\n",
      "After replacing: 7.2 A Taxonomy of Models\n",
      "Original title: 7.2.1 Linear vs. Non-Linear Models\n",
      "After replacing: 7.2.1 Linear vs. Non-Linear Models\n",
      "Original title: 7.2.2 Blackbox vs. Descriptive Models\n",
      "After replacing: 7.2.2 Blackbox vs. Descriptive Models\n",
      "Original title: 7.2.3 First-Principle vs. Data-Driven Models\n",
      "After replacing: 7.2.3 First-Principle vs. Data-Driven Models\n",
      "Original title: 7.2.4 Stochastic vs. Deterministic Models\n",
      "After replacing: 7.2.4 Stochastic vs. Deterministic Models\n",
      "Original title: 7.2.5 Flat vs. Hierarchical Models\n",
      "After replacing: 7.2.5 Flat vs. Hierarchical Models\n",
      "Original title: 7.3 Baseline Models\n",
      "After replacing: 7.3 Baseline Models\n",
      "Original title: 7.3.1 Baseline Models for Classification\n",
      "After replacing: 7.3.1 Baseline Models for Classification\n",
      "Original title: 7.3.2 Baseline Models for Value Prediction\n",
      "After replacing: 7.3.2 Baseline Models for Value Prediction\n",
      "Original title: 7.4 Evaluating Models\n",
      "After replacing: 7.4 Evaluating Models\n",
      "Original title: 7.4.1 Evaluating Classifiers\n",
      "After replacing: 7.4.1 Evaluating Classifiers\n",
      "Original title: 7.4.2 Receiver-Operator Characteristic (ROC) Curves\n",
      "After replacing: 7.4.2 Receiver-Operator Characteristic (ROC) Curves\n",
      "Original title: 7.4.3 Evaluating Multiclass Systems\n",
      "After replacing: 7.4.3 Evaluating Multiclass Systems\n",
      "Original title: 7.4.4 Evaluating Value Prediction Models\n",
      "After replacing: 7.4.4 Evaluating Value Prediction Models\n",
      "Original title: 7.5 Evaluation Environments\n",
      "After replacing: 7.5 Evaluation Environments\n",
      "Original title: 7.5.1 Data Hygiene for Evaluation\n",
      "After replacing: 7.5.1 Data Hygiene for Evaluation\n",
      "Original title: 7.5.2 Amplifying Small Evaluation Sets\n",
      "After replacing: 7.5.2 Amplifying Small Evaluation Sets\n",
      "Original title: 7.6 War Story: 100% Accuracy\n",
      "After replacing: 7.6 War Story: 100% Accuracy\n",
      "Original title: 7.7 Simulation Models\n",
      "After replacing: 7.7 Simulation Models\n",
      "Original title: 7.8 War Story: Calculated Bets\n",
      "After replacing: 7.8 War Story: Calculated Bets\n",
      "Original title: 7.9 Chapter Notes\n",
      "After replacing: 7.9 Chapter Notes\n",
      "Original title: 7.10 Exercises\n",
      "After replacing: 7.10 Exercises\n",
      "Original title: 8Linear Algebra\n",
      "After replacing: 8Linear Algebra\n",
      "Original title: 8.1 The Power of Linear Algebra\n",
      "After replacing: 8.1 The Power of Linear Algebra\n",
      "Original title: 8.1.1 Interpreting Linear Algebraic Formulae\n",
      "After replacing: 8.1.1 Interpreting Linear Algebraic Formulae\n",
      "Original title: 8.1.2 Geometry and Vectors\n",
      "After replacing: 8.1.2 Geometry and Vectors\n",
      "Original title: 8.2 Visualizing Matrix Operations\n",
      "After replacing: 8.2 Visualizing Matrix Operations\n",
      "Original title: 8.2.1 Matrix Addition\n",
      "After replacing: 8.2.1 Matrix Addition\n",
      "Original title: 8.2.2 Matrix Multiplication\n",
      "After replacing: 8.2.2 Matrix Multiplication\n",
      "Original title: 8.2.3 Applications of Matrix Multiplication\n",
      "After replacing: 8.2.3 Applications of Matrix Multiplication\n",
      "Original title: 8.2.4 Identity Matrices and Inversion\n",
      "After replacing: 8.2.4 Identity Matrices and Inversion\n",
      "Original title: 8.2.5 Matrix Inversion and Linear Systems\n",
      "After replacing: 8.2.5 Matrix Inversion and Linear Systems\n",
      "Original title: 8.2.6 Matrix Rank\n",
      "After replacing: 8.2.6 Matrix Rank\n",
      "Original title: 8.3 Factoring Matrices\n",
      "After replacing: 8.3 Factoring Matrices\n",
      "Original title: 8.3.1 Why Factor Feature Matrices?\n",
      "After replacing: 8.3.1 Why Factor Feature Matrices?\n",
      "Original title: 8.3.2 LU Decomposition and Determinants\n",
      "After replacing: 8.3.2 LU Decomposition and Determinants\n",
      "Original title: 8.4 Eigenvalues and Eigenvectors\n",
      "After replacing: 8.4 Eigenvalues and Eigenvectors\n",
      "Original title: 8.4.1 Properties of Eigenvalues\n",
      "After replacing: 8.4.1 Properties of Eigenvalues\n",
      "Original title: 8.4.2 Computing Eigenvalues\n",
      "After replacing: 8.4.2 Computing Eigenvalues\n",
      "Original title: 8.5 Eigenvalue Decomposition\n",
      "After replacing: 8.5 Eigenvalue Decomposition\n",
      "Original title: 8.5.1 Singular Value Decomposition\n",
      "After replacing: 8.5.1 Singular Value Decomposition\n",
      "Original title: 8.5.2 Principal Components Analysis\n",
      "After replacing: 8.5.2 Principal Components Analysis\n",
      "Original title: 8.6 War Story: The Human Factors\n",
      "After replacing: 8.6 War Story: The Human Factors\n",
      "Original title: 8.7 Chapter Notes\n",
      "After replacing: 8.7 Chapter Notes\n",
      "Original title: 8.8 Exercises\n",
      "After replacing: 8.8 Exercises\n",
      "Original title: 9 Linear and Logistic Regression\n",
      "After replacing: 9 Linear and Logistic Regression\n",
      "Original title: 9.1 Linear Regression\n",
      "After replacing: 9.1 Linear Regression\n",
      "Original title: 9.1.1 Linear Regression and Duality\n",
      "After replacing: 9.1.1 Linear Regression and Duality\n",
      "Original title: 9.1.2 Error in Linear Regression\n",
      "After replacing: 9.1.2 Error in Linear Regression\n",
      "Original title: 9.1.3 Finding the Optimal Fit\n",
      "After replacing: 9.1.3 Finding the Optimal Fit\n",
      "Original title: 9.2 Better Regression Models\n",
      "After replacing: 9.2 Better Regression Models\n",
      "Original title: 9.2.1 Removing Outliers\n",
      "After replacing: 9.2.1 Removing Outliers\n",
      "Original title: 9.2.2 Fitting Non-Linear Functions\n",
      "After replacing: 9.2.2 Fitting Non-Linear Functions\n",
      "Original title: 9.2.3 Feature and Target Scaling\n",
      "After replacing: 9.2.3 Feature and Target Scaling\n",
      "Original title: 9.2.4 Dealing with Highly-Correlated Features\n",
      "After replacing: 9.2.4 Dealing with Highly-Correlated Features\n",
      "Original title: 9.3 War Story: Taxi Deriver\n",
      "After replacing: 9.3 War Story: Taxi Deriver\n",
      "Original title: 9.4 Regression as Parameter Fitting\n",
      "After replacing: 9.4 Regression as Parameter Fitting\n",
      "Original title: 9.4.1 Convex Parameter Spaces\n",
      "After replacing: 9.4.1 Convex Parameter Spaces\n",
      "Original title: 9.4.2 Gradient Descent Search\n",
      "After replacing: 9.4.2 Gradient Descent Search\n",
      "Original title: 9.4.3 What is the Right Learning Rate?\n",
      "After replacing: 9.4.3 What is the Right Learning Rate?\n",
      "Original title: 9.4.4 Stochastic Gradient Descent\n",
      "After replacing: 9.4.4 Stochastic Gradient Descent\n",
      "Original title: 9.5 Simplifying Models through Regularization\n",
      "After replacing: 9.5 Simplifying Models through Regularization\n",
      "Original title: 9.5.1 Ridge Regression\n",
      "After replacing: 9.5.1 Ridge Regression\n",
      "Original title: 9.5.2 LASSO Regression\n",
      "After replacing: 9.5.2 LASSO Regression\n",
      "Original title: 9.5.3 Trade-O\u000bs between Fit and Complexity\n",
      "After replacing: 9.5.3 Trade-O\u000bs between Fit and Complexity\n",
      "Original title: 9.6 Classification and Logistic Regression\n",
      "After replacing: 9.6 Classification and Logistic Regression\n",
      "Original title: 9.6.1 Regression for Classification\n",
      "After replacing: 9.6.1 Regression for Classification\n",
      "Original title: 9.6.2 Decision Boundaries\n",
      "After replacing: 9.6.2 Decision Boundaries\n",
      "Original title: 9.6.3 Logistic Regression\n",
      "After replacing: 9.6.3 Logistic Regression\n",
      "Original title: 9.7 Issues in Logistic Classification\n",
      "After replacing: 9.7 Issues in Logistic Classification\n",
      "Original title: 9.7.1 Balanced Training Classes\n",
      "After replacing: 9.7.1 Balanced Training Classes\n",
      "Original title: 9.7.2 Multi-Class Classification\n",
      "After replacing: 9.7.2 Multi-Class Classification\n",
      "Original title: 9.7.3 Hierarchical Classification\n",
      "After replacing: 9.7.3 Hierarchical Classification\n",
      "Original title: 9.7.4 Partition Functions and Multinomial Regression\n",
      "After replacing: 9.7.4 Partition Functions and Multinomial Regression\n",
      "Original title: 9.8 Chapter Notes\n",
      "After replacing: 9.8 Chapter Notes\n",
      "Original title: 9.9 Exercises\n",
      "After replacing: 9.9 Exercises\n",
      "Original title: 10Distance and Network Methods\n",
      "After replacing: 10Distance and Network Methods\n",
      "Original title: 10.1 Measuring Distances\n",
      "After replacing: 10.1 Measuring Distances\n",
      "Original title: 10.1.1 Distance Metrics\n",
      "After replacing: 10.1.1 Distance Metrics\n",
      "Original title: 10.1.2 The\n",
      "After replacing: 10.1.2 The\n",
      "Original title: 10.1.3 Working in Higher Dimensions\n",
      "After replacing: 10.1.3 Working in Higher Dimensions\n",
      "Original title: 10.1.4 Dimensional Egalitarianism\n",
      "After replacing: 10.1.4 Dimensional Egalitarianism\n",
      "Original title: 10.1.5 Points vs. Vectors\n",
      "After replacing: 10.1.5 Points vs. Vectors\n",
      "Original title: 10.1.6 Distances between Probability Distributions\n",
      "After replacing: 10.1.6 Distances between Probability Distributions\n",
      "Original title: 10.2 Nearest Neighbor Classification\n",
      "After replacing: 10.2 Nearest Neighbor Classification\n",
      "Original title: 10.2.1 Seeking Good Analogies\n",
      "After replacing: 10.2.1 Seeking Good Analogies\n",
      "Original title: 10.2.2\n",
      "After replacing: 10.2.2\n",
      "Original title: 10.2.3 Finding Nearest Neighbors\n",
      "After replacing: 10.2.3 Finding Nearest Neighbors\n",
      "Original title: 10.2.4 Locality Sensitive Hashing\n",
      "After replacing: 10.2.4 Locality Sensitive Hashing\n",
      "Original title: 10.3 Graphs, Networks, and Distances\n",
      "After replacing: 10.3 Graphs, Networks, and Distances\n",
      "Original title: 10.3.1 Weighted Graphs and Induced Networks\n",
      "After replacing: 10.3.1 Weighted Graphs and Induced Networks\n",
      "Original title: 10.3.2 Talking About Graphs\n",
      "After replacing: 10.3.2 Talking About Graphs\n",
      "Original title: 10.3.3 Graph Theory\n",
      "After replacing: 10.3.3 Graph Theory\n",
      "Original title: 10.4 PageRank\n",
      "After replacing: 10.4 PageRank\n",
      "Original title: 10.5 Clustering\n",
      "After replacing: 10.5 Clustering\n",
      "Original title: 10.5.1\n",
      "After replacing: 10.5.1\n",
      "Original title: 10.5.2 Agglomerative Clustering\n",
      "After replacing: 10.5.2 Agglomerative Clustering\n",
      "Original title: 10.5.3 Comparing Clusterings\n",
      "After replacing: 10.5.3 Comparing Clusterings\n",
      "Original title: 10.5.4 Similarity Graphs and Cut-Based Clustering\n",
      "After replacing: 10.5.4 Similarity Graphs and Cut-Based Clustering\n",
      "Original title: 10.6 War Story: Cluster Bombing\n",
      "After replacing: 10.6 War Story: Cluster Bombing\n",
      "Original title: 10.7 Chapter Notes\n",
      "After replacing: 10.7 Chapter Notes\n",
      "Original title: 10.8 Exercises\n",
      "After replacing: 10.8 Exercises\n",
      "Original title: 11Machine Learning\n",
      "After replacing: 11Machine Learning\n",
      "Original title: 11.1 Naive Bayes\n",
      "After replacing: 11.1 Naive Bayes\n",
      "Original title: 11.1.1 Formulation\n",
      "After replacing: 11.1.1 Formulation\n",
      "Original title: 11.1.2 Dealing with Zero Counts (Discounting)\n",
      "After replacing: 11.1.2 Dealing with Zero Counts (Discounting)\n",
      "Original title: 11.2 Decision Tree Classifiers\n",
      "After replacing: 11.2 Decision Tree Classifiers\n",
      "Original title: 11.2.1 Constructing Decision Trees\n",
      "After replacing: 11.2.1 Constructing Decision Trees\n",
      "Original title: 11.2.2 Realizing Exclusive Or\n",
      "After replacing: 11.2.2 Realizing Exclusive Or\n",
      "Original title: 11.2.3 Ensembles of Decision Trees\n",
      "After replacing: 11.2.3 Ensembles of Decision Trees\n",
      "Original title: 11.3 Boosting and Ensemble Learning\n",
      "After replacing: 11.3 Boosting and Ensemble Learning\n",
      "Original title: 11.3.1 Voting with Classifiers\n",
      "After replacing: 11.3.1 Voting with Classifiers\n",
      "Original title: 11.3.2 Boosting Algorithms\n",
      "After replacing: 11.3.2 Boosting Algorithms\n",
      "Original title: 11.4 Support Vector Machines\n",
      "After replacing: 11.4 Support Vector Machines\n",
      "Original title: 11.4.1 Linear SVMs\n",
      "After replacing: 11.4.1 Linear SVMs\n",
      "Original title: 11.4.2 Non-linear SVMs\n",
      "After replacing: 11.4.2 Non-linear SVMs\n",
      "Original title: 11.4.3 Kernels\n",
      "After replacing: 11.4.3 Kernels\n",
      "Original title: 11.5 Degrees of Supervision\n",
      "After replacing: 11.5 Degrees of Supervision\n",
      "Original title: 11.5.1 Supervised Learning\n",
      "After replacing: 11.5.1 Supervised Learning\n",
      "Original title: 11.5.2 Unsupervised Learning\n",
      "After replacing: 11.5.2 Unsupervised Learning\n",
      "Original title: 11.5.3 Semi-supervised Learning\n",
      "After replacing: 11.5.3 Semi-supervised Learning\n",
      "Original title: 11.5.4 Feature Engineering\n",
      "After replacing: 11.5.4 Feature Engineering\n",
      "Original title: 11.6 Deep Learning\n",
      "After replacing: 11.6 Deep Learning\n",
      "Original title: 11.6.1 Networks and Depth\n",
      "After replacing: 11.6.1 Networks and Depth\n",
      "Original title: 11.6.2 Backpropagation\n",
      "After replacing: 11.6.2 Backpropagation\n",
      "Original title: 11.6.3 Word and Graph Embeddings\n",
      "After replacing: 11.6.3 Word and Graph Embeddings\n",
      "Original title: 11.7 War Story: The Name Game\n",
      "After replacing: 11.7 War Story: The Name Game\n",
      "Original title: 11.8 Chapter Notes\n",
      "After replacing: 11.8 Chapter Notes\n",
      "Original title: 11.9 Exercises\n",
      "After replacing: 11.9 Exercises\n",
      "Original title: 12Big Data: Achieving Scale\n",
      "After replacing: 12Big Data: Achieving Scale\n",
      "Original title: 12.1 What is Big Data?\n",
      "After replacing: 12.1 What is Big Data?\n",
      "Original title: 12.1.1 Big Data as Bad Data\n",
      "After replacing: 12.1.1 Big Data as Bad Data\n",
      "Original title: 12.1.2 The Three Vs\n",
      "After replacing: 12.1.2 The Three Vs\n",
      "Original title: 12.2 War Story: Infrastructure Matters\n",
      "After replacing: 12.2 War Story: Infrastructure Matters\n",
      "Original title: 12.3 Algorithmics for Big Data\n",
      "After replacing: 12.3 Algorithmics for Big Data\n",
      "Original title: 12.3.1 Big Oh Analysis\n",
      "After replacing: 12.3.1 Big Oh Analysis\n",
      "Original title: 12.3.2 Hashing\n",
      "After replacing: 12.3.2 Hashing\n",
      "Original title: 12.3.3 Exploiting the Storage Hierarchy\n",
      "After replacing: 12.3.3 Exploiting the Storage Hierarchy\n",
      "Original title: 12.3.4 Streaming and Single-Pass Algorithms\n",
      "After replacing: 12.3.4 Streaming and Single-Pass Algorithms\n",
      "Original title: 12.4 Filtering and Sampling\n",
      "After replacing: 12.4 Filtering and Sampling\n",
      "Original title: 12.4.1 Deterministic Sampling Algorithms\n",
      "After replacing: 12.4.1 Deterministic Sampling Algorithms\n",
      "Original title: 12.4.2 Randomized and Stream Sampling\n",
      "After replacing: 12.4.2 Randomized and Stream Sampling\n",
      "Original title: 12.5 Parallelism\n",
      "After replacing: 12.5 Parallelism\n",
      "Original title: 12.5.1 One, Two, Many\n",
      "After replacing: 12.5.1 One, Two, Many\n",
      "Original title: 12.5.2 Data Parallelism\n",
      "After replacing: 12.5.2 Data Parallelism\n",
      "Original title: 12.5.3 Grid Search\n",
      "After replacing: 12.5.3 Grid Search\n",
      "Original title: 12.5.4 Cloud Computing Services\n",
      "After replacing: 12.5.4 Cloud Computing Services\n",
      "Original title: 12.6 MapReduce\n",
      "After replacing: 12.6 MapReduce\n",
      "Original title: 12.6.1 Map-Reduce Programming\n",
      "After replacing: 12.6.1 Map-Reduce Programming\n",
      "Original title: 12.6.2 MapReduce under the Hood\n",
      "After replacing: 12.6.2 MapReduce under the Hood\n",
      "Original title: 12.7 Societal and Ethical Implications\n",
      "After replacing: 12.7 Societal and Ethical Implications\n",
      "Original title: 12.8 Chapter Notes\n",
      "After replacing: 12.8 Chapter Notes\n",
      "Original title: 12.9 Exercises\n",
      "After replacing: 12.9 Exercises\n",
      "Original title: 13Coda\n",
      "After replacing: 13Coda\n",
      "Original title: 13.1 Get a Job!\n",
      "After replacing: 13.1 Get a Job!\n",
      "Original title: 13.2 Go to Graduate School!\n",
      "After replacing: 13.2 Go to Graduate School!\n",
      "Original title: 13.3 Professional Consulting Services\n",
      "After replacing: 13.3 Professional Consulting Services\n",
      "Original title: Bibliography\n",
      "After replacing: Bibliography\n",
      "Original title: Index\n",
      "After replacing: Index\n"
     ]
    }
   ],
   "source": [
    "toc = clean_and_merge_toc(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'What is Data Science?', 1),\n",
       " (2, 'Computer Science, Data Science, and Real Science', 2),\n",
       " (2, 'Asking Interesting Questions from Data', 4),\n",
       " (3, 'The Baseball Encyclopedia', 5),\n",
       " (3, 'The Internet Movie Database (IMDb)', 7),\n",
       " (3, 'Google Ngrams', 10),\n",
       " (3, 'New York Taxi Records', 11),\n",
       " (2, 'Properties of Data', 14),\n",
       " (3, 'Structured vs. Unstructured Data', 14),\n",
       " (3, 'Quantitative vs. Categorical Data', 15),\n",
       " (3, 'Big Data vs. Little Data', 15),\n",
       " (2, 'Classication and Regression', 16),\n",
       " (2, 'Data Science Television: The Quant Shop', 17),\n",
       " (3, 'Kaggle Challenges', 19),\n",
       " (2, 'About the War Stories', 19),\n",
       " (2, 'War Story: Answering the Right Question', 21),\n",
       " (2, 'Chapter Notes', 22),\n",
       " (2, 'Exercises', 23),\n",
       " (1, 'Mathematical Preliminaries', 26),\n",
       " (2, 'Probability', 26),\n",
       " (3, 'Probability vs. Statistics', 28),\n",
       " (3, 'Compound Events and Independence', 29),\n",
       " (3, 'Conditional Probability', 30),\n",
       " (3, 'Probability Distributions', 31),\n",
       " (2, 'Descriptive Statistics', 33),\n",
       " (3, 'Centrality Measures', 33),\n",
       " (3, 'Variability Measures', 35),\n",
       " (3, 'Interpreting Variance', 36),\n",
       " (3, 'Characterizing Distributions', 38),\n",
       " (2, 'Correlation Analysis', 39),\n",
       " (3, 'Correlation Coe\\x0ecients: Pearson and Spearman Rank', 40),\n",
       " (3, 'The Power and Signicance of Correlation', 42),\n",
       " (3, 'Correlation Does Not Imply Causation!', 44),\n",
       " (3, 'Detecting Periodicities by Autocorrelation', 45),\n",
       " (2, 'Logarithms', 46),\n",
       " (3, 'Logarithms and Multiplying Probabilities', 47),\n",
       " (3, 'Logarithms and Ratios', 47),\n",
       " (3, 'Logarithms and Normalizing Skewed Distributions', 48),\n",
       " (2, 'War Story: Fitting Designer Genes', 49),\n",
       " (2, 'Chapter Notes', 51),\n",
       " (2, 'Exercises', 52),\n",
       " (1, 'Data Munging', 56),\n",
       " (2, 'Languages for Data Science', 56),\n",
       " (3, 'The Importance of Notebook Environments', 58),\n",
       " (3, 'Standard Data Formats', 60),\n",
       " (2, 'Collecting Data', 63),\n",
       " (3, 'Hunting', 63),\n",
       " (3, 'Scraping', 66),\n",
       " (3, 'Logging', 67),\n",
       " (2, 'Cleaning Data', 68),\n",
       " (3, 'Errors vs. Artifacts', 68),\n",
       " (3, 'Data Compatibility', 71),\n",
       " (3, 'Dealing with Missing Values', 75),\n",
       " (3, 'Outlier Detection', 77),\n",
       " (2, 'War Story: Beating the Market', 78),\n",
       " (2, 'Crowdsourcing', 79),\n",
       " (3, 'The Penny Demo', 80),\n",
       " (3, 'When is the Crowd Wise?', 81),\n",
       " (3, 'Mechanisms for Aggregation', 82),\n",
       " (3, 'Crowdsourcing Services', 83),\n",
       " (3, 'Gamication', 87),\n",
       " (2, 'Chapter Notes', 89),\n",
       " (2, 'Exercises', 89),\n",
       " (1, 'Scores and Rankings', 93),\n",
       " (2, 'The Body Mass Index (BMI)', 94),\n",
       " (2, 'Developing Scoring Systems', 97),\n",
       " (3, 'Gold Standards and Proxies', 97),\n",
       " (3, 'Scores vs. Rankings', 98),\n",
       " (3, 'Recognizing Good Scoring Functions', 99),\n",
       " (2, 'Z-scores and Normalization', 101),\n",
       " (2, 'Advanced Ranking Techniques', 102),\n",
       " (3, 'Elo Rankings', 102),\n",
       " (3, 'Merging Rankings', 106),\n",
       " (3, 'Digraph-based Rankings', 107),\n",
       " (3, 'PageRank', 109),\n",
       " (2, \"War Story: Clyde's Revenge\", 109),\n",
       " (2, \"Arrow's Impossibility Theorem\", 112),\n",
       " (2, \"War Story: Who's Bigger?\", 113),\n",
       " (2, 'Chapter Notes', 116),\n",
       " (2, 'Exercises', 117),\n",
       " (1, 'Statistical Analysis', 119),\n",
       " (2, 'Statistical Distributions', 120),\n",
       " (3, 'The Binomial Distribution', 121),\n",
       " (3, 'The Normal Distribution', 122),\n",
       " (3, 'Implications of the Normal Distribution', 124),\n",
       " (3, 'Poisson Distribution', 125),\n",
       " (3, 'Power Law Distributions', 127),\n",
       " (2, 'Sampling from Distributions', 130),\n",
       " (3, 'Random Sampling beyond One Dimension', 131),\n",
       " (2, 'Statistical Signicance', 133),\n",
       " (3, 'The Signicance of Signicance', 133),\n",
       " (3, 'The T-test: Comparing Population Means', 135),\n",
       " (3, 'The Kolmogorov-Smirnov Test', 137),\n",
       " (3, 'The Bonferroni Correction', 139),\n",
       " (3, 'False Discovery Rate', 140),\n",
       " (2, 'War Story: Discovering the Fountain of Youth?', 141),\n",
       " (2, 'Permutation Tests and P-values', 143),\n",
       " (3, 'Generating Random Permutations', 145),\n",
       " (3, \"DiMaggio's Hitting Streak\", 146),\n",
       " (2, 'Bayesian Reasoning', 148),\n",
       " (2, 'Chapter Notes', 149),\n",
       " (2, 'Exercises', 149),\n",
       " (1, 'Visualizing Data', 153),\n",
       " (2, 'Exploratory Data Analysis', 154),\n",
       " (3, 'Confronting a New Data Set', 154),\n",
       " (3, \"Summary Statistics and Anscombe's Quartet\", 157),\n",
       " (3, 'Visualization Tools', 158),\n",
       " (2, 'Developing a Visualization Aesthetic', 160),\n",
       " (3, 'Maximizing Data-Ink Ratio', 161),\n",
       " (3, 'Minimizing the Lie Factor', 162),\n",
       " (3, 'Minimizing Chartjunk', 163),\n",
       " (3, 'Proper Scaling and Labeling', 165),\n",
       " (3, 'E\\x0bective Use of Color and Shading', 166),\n",
       " (3, 'The Power of Repetition', 167),\n",
       " (2, 'Chart Types', 168),\n",
       " (3, 'Tabular Data', 168),\n",
       " (3, 'Dot and Line Plots', 172),\n",
       " (3, 'Scatter Plots', 175),\n",
       " (3, 'Bar Plots and Pie Charts', 177),\n",
       " (3, 'Histograms', 181),\n",
       " (3, 'Data Maps', 185),\n",
       " (2, 'Great Visualizations', 187),\n",
       " (3, \"Marey's Train Schedule\", 187),\n",
       " (3, \"Snow's Cholera Map\", 189),\n",
       " (3, \"New York's Weather Year\", 190),\n",
       " (2, 'Reading Graphs', 190),\n",
       " (3, 'The Obscured Distribution', 191),\n",
       " (3, 'Overinterpreting Variance', 191),\n",
       " (2, 'Interactive Visualization', 193),\n",
       " (2, 'War Story: TextMapping the World', 194),\n",
       " (2, 'Chapter Notes', 196),\n",
       " (2, 'Exercises', 197),\n",
       " (1, 'Mathematical Models', 199),\n",
       " (2, 'Philosophies of Modeling', 199),\n",
       " (3, \"Occam's Razor\", 199),\n",
       " (3, 'Bias{Variance Trade-O\\x0bs', 200),\n",
       " (3, 'What Would Nate Silver Do?', 201),\n",
       " (2, 'A Taxonomy of Models', 203),\n",
       " (3, 'Linear vs. Non-Linear Models', 204),\n",
       " (3, 'Blackbox vs. Descriptive Models', 204),\n",
       " (3, 'First-Principle vs. Data-Driven Models', 205),\n",
       " (3, 'Stochastic vs. Deterministic Models', 206),\n",
       " (3, 'Flat vs. Hierarchical Models', 207),\n",
       " (2, 'Baseline Models', 208),\n",
       " (3, 'Baseline Models for Classication', 208),\n",
       " (3, 'Baseline Models for Value Prediction', 210),\n",
       " (2, 'Evaluating Models', 210),\n",
       " (3, 'Evaluating Classiers', 211),\n",
       " (3, 'Receiver-Operator Characteristic (ROC) Curves', 216),\n",
       " (3, 'Evaluating Multiclass Systems', 217),\n",
       " (3, 'Evaluating Value Prediction Models', 219),\n",
       " (2, 'Evaluation Environments', 222),\n",
       " (3, 'Data Hygiene for Evaluation', 223),\n",
       " (3, 'Amplifying Small Evaluation Sets', 224),\n",
       " (2, 'War Story: 100% Accuracy', 226),\n",
       " (2, 'Simulation Models', 227),\n",
       " (2, 'War Story: Calculated Bets', 228),\n",
       " (2, 'Chapter Notes', 231),\n",
       " (2, 'Exercises', 232),\n",
       " (1, 'Linear Algebra', 235),\n",
       " (2, 'The Power of Linear Algebra', 235),\n",
       " (3, 'Interpreting Linear Algebraic Formulae', 236),\n",
       " (3, 'Geometry and Vectors', 238),\n",
       " (2, 'Visualizing Matrix Operations', 239),\n",
       " (3, 'Matrix Addition', 240),\n",
       " (3, 'Matrix Multiplication', 241),\n",
       " (3, 'Applications of Matrix Multiplication', 242),\n",
       " (3, 'Identity Matrices and Inversion', 246),\n",
       " (3, 'Matrix Inversion and Linear Systems', 248),\n",
       " (3, 'Matrix Rank', 249),\n",
       " (2, 'Factoring Matrices', 250),\n",
       " (3, 'Why Factor Feature Matrices?', 250),\n",
       " (3, 'LU Decomposition and Determinants', 252),\n",
       " (2, 'Eigenvalues and Eigenvectors', 253),\n",
       " (3, 'Properties of Eigenvalues', 253),\n",
       " (3, 'Computing Eigenvalues', 254),\n",
       " (2, 'Eigenvalue Decomposition', 255),\n",
       " (3, 'Singular Value Decomposition', 256),\n",
       " (3, 'Principal Components Analysis', 258),\n",
       " (2, 'War Story: The Human Factors', 260),\n",
       " (2, 'Chapter Notes', 261),\n",
       " (2, 'Exercises', 261),\n",
       " (1, 'Linear and Logistic Regression', 264),\n",
       " (2, 'Linear Regression', 265),\n",
       " (3, 'Linear Regression and Duality', 265),\n",
       " (3, 'Error in Linear Regression', 266),\n",
       " (3, 'Finding the Optimal Fit', 267),\n",
       " (2, 'Better Regression Models', 269),\n",
       " (3, 'Removing Outliers', 269),\n",
       " (3, 'Fitting Non-Linear Functions', 270),\n",
       " (3, 'Feature and Target Scaling', 271),\n",
       " (3, 'Dealing with Highly-Correlated Features', 274),\n",
       " (2, 'War Story: Taxi Deriver', 274),\n",
       " (2, 'Regression as Parameter Fitting', 276),\n",
       " (3, 'Convex Parameter Spaces', 277),\n",
       " (3, 'Gradient Descent Search', 278),\n",
       " (3, 'What is the Right Learning Rate?', 280),\n",
       " (3, 'Stochastic Gradient Descent', 282),\n",
       " (2, 'Simplifying Models through Regularization', 283),\n",
       " (3, 'Ridge Regression', 283),\n",
       " (3, 'LASSO Regression', 284),\n",
       " (3, 'Trade-O\\x0bs between Fit and Complexity', 285),\n",
       " (2, 'Classication and Logistic Regression', 286),\n",
       " (3, 'Regression for Classication', 287),\n",
       " (3, 'Decision Boundaries', 288),\n",
       " (3, 'Logistic Regression', 289),\n",
       " (2, 'Issues in Logistic Classication', 292),\n",
       " (3, 'Balanced Training Classes', 292),\n",
       " (3, 'Multi-Class Classication', 294),\n",
       " (3, 'Hierarchical Classication', 295),\n",
       " (3, 'Partition Functions and Multinomial Regression', 296),\n",
       " (2, 'Chapter Notes', 297),\n",
       " (2, 'Exercises', 298),\n",
       " (1, 'Distance and Network Methods', 300),\n",
       " (2, 'Measuring Distances', 300),\n",
       " (3, 'Distance Metrics', 301),\n",
       " (3, 'The Distance Metric', 302),\n",
       " (3, 'Working in Higher Dimensions', 304),\n",
       " (3, 'Dimensional Egalitarianism', 305),\n",
       " (3, 'Points vs. Vectors', 306),\n",
       " (3, 'Distances between Probability Distributions', 307),\n",
       " (2, 'Nearest Neighbor Classication', 308),\n",
       " (3, 'Seeking Good Analogies', 309),\n",
       " (3, ' -Nearest Neighbors', 310),\n",
       " (3, 'Finding Nearest Neighbors', 312),\n",
       " (3, 'Locality Sensitive Hashing', 314),\n",
       " (2, 'Graphs, Networks, and Distances', 316),\n",
       " (3, 'Weighted Graphs and Induced Networks', 317),\n",
       " (3, 'Talking About Graphs', 318),\n",
       " (3, 'Graph Theory', 320),\n",
       " (2, 'PageRank', 322),\n",
       " (2, 'Clustering', 324),\n",
       " (3, ' -means Clustering', 327),\n",
       " (3, 'Agglomerative Clustering', 333),\n",
       " (3, 'Comparing Clusterings', 338),\n",
       " (3, 'Similarity Graphs and Cut-Based Clustering', 338),\n",
       " (2, 'War Story: Cluster Bombing', 341),\n",
       " (2, 'Chapter Notes', 342),\n",
       " (2, 'Exercises', 343),\n",
       " (1, 'Machine Learning', 347),\n",
       " (2, 'Naive Bayes', 350),\n",
       " (3, 'Formulation', 350),\n",
       " (3, 'Dealing with Zero Counts (Discounting)', 352),\n",
       " (2, 'Decision Tree Classiers', 353),\n",
       " (3, 'Constructing Decision Trees', 355),\n",
       " (3, 'Realizing Exclusive Or', 357),\n",
       " (3, 'Ensembles of Decision Trees', 358),\n",
       " (2, 'Boosting and Ensemble Learning', 359),\n",
       " (3, 'Voting with Classiers', 359),\n",
       " (3, 'Boosting Algorithms', 360),\n",
       " (2, 'Support Vector Machines', 362),\n",
       " (3, 'Linear SVMs', 365),\n",
       " (3, 'Non-linear SVMs', 365),\n",
       " (3, 'Kernels', 367),\n",
       " (2, 'Degrees of Supervision', 368),\n",
       " (3, 'Supervised Learning', 368),\n",
       " (3, 'Unsupervised Learning', 368),\n",
       " (3, 'Semi-supervised Learning', 370),\n",
       " (3, 'Feature Engineering', 371),\n",
       " (2, 'Deep Learning', 373),\n",
       " (3, 'Networks and Depth', 374),\n",
       " (3, 'Backpropagation', 378),\n",
       " (3, 'Word and Graph Embeddings', 379),\n",
       " (2, 'War Story: The Name Game', 381),\n",
       " (2, 'Chapter Notes', 383),\n",
       " (2, 'Exercises', 384),\n",
       " (1, 'Big Data: Achieving Scale', 387),\n",
       " (2, 'What is Big Data?', 388),\n",
       " (3, 'Big Data as Bad Data', 388),\n",
       " (3, 'The Three Vs', 390),\n",
       " (2, 'War Story: Infrastructure Matters', 391),\n",
       " (2, 'Algorithmics for Big Data', 393),\n",
       " (3, 'Big Oh Analysis', 393),\n",
       " (3, 'Hashing', 395),\n",
       " (3, 'Exploiting the Storage Hierarchy', 397),\n",
       " (3, 'Streaming and Single-Pass Algorithms', 398),\n",
       " (2, 'Filtering and Sampling', 399),\n",
       " (3, 'Deterministic Sampling Algorithms', 400),\n",
       " (3, 'Randomized and Stream Sampling', 402),\n",
       " (2, 'Parallelism', 402),\n",
       " (3, 'One, Two, Many', 403),\n",
       " (3, 'Data Parallelism', 405),\n",
       " (3, 'Grid Search', 405),\n",
       " (3, 'Cloud Computing Services', 406),\n",
       " (2, 'MapReduce', 406),\n",
       " (3, 'Map-Reduce Programming', 408),\n",
       " (3, 'MapReduce under the Hood', 410),\n",
       " (2, 'Societal and Ethical Implications', 412),\n",
       " (2, 'Chapter Notes', 415),\n",
       " (2, 'Exercises', 415),\n",
       " (1, 'Coda', 418),\n",
       " (2, 'Get a Job!', 418),\n",
       " (2, 'Go to Graduate School!', 419),\n",
       " (2, 'Professional Consulting Services', 420),\n",
       " (1, 'Bibliography', 421),\n",
       " (1, 'Index', 428)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_dict = {title.lower(): level for level, title, _ in toc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'classification and regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m toc_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification and regression\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'classification and regression'"
     ]
    }
   ],
   "source": [
    "toc_dict['classification and regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification and Regression'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"Classi\\x0ccation and Regression\"\n",
    "title_clean = title.replace('\\r', '').replace('\\x0c', 'fi').strip()\n",
    "title_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--- Page 1 ---\\nContents\\n1\\nWhat is Data Science?\\n1\\n1.1\\nComputer Science, Data Science, and Real Science . . . . . . . .\\n2\\n1.2\\nAsking Interesting Questions from Data . . . . . . . . . . . . . .\\n4\\n1.2.1\\nThe Baseball Encyclopedia\\n. . . . . . . . . . . . . . . . .\\n5\\n1.2.2\\nThe Internet Movie Database (IMDb) . . . . . . . . . . .\\n7\\n1.2.3\\nGoogle Ngrams . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n1.2.4\\nNew York Taxi Records . . . . . . . . . . . . . . . . . . .\\n11\\n1.3\\nProperties of Data . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n1.3.1\\nStructured vs. Unstructured Data\\n. . . . . . . . . . . . .\\n14\\n1.3.2\\nQuantitative vs. Categorical Data\\n. . . . . . . . . . . . .\\n15\\n1.3.3\\nBig Data vs. Little Data . . . . . . . . . . . . . . . . . . .\\n15\\n1.4\\nClassiﬁcation and Regression\\n. . . . . . . . . . . . . . . . . . . .\\n16\\n1.5\\nData Science Television: The Quant Shop . . . . . . . . . . . . .\\n17\\n1.5.1\\nKaggle Challenges\\n. . . . . . . . . . . . . . . . . . . . . .\\n19\\n1.6\\nAbout the War Stories . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n1.7\\nWar Story: Answering the Right Question . . . . . . . . . . . . .\\n21\\n1.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n1.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n2\\nMathematical Preliminaries\\n27\\n2.1\\nProbability\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n2.1.1\\nProbability vs. Statistics . . . . . . . . . . . . . . . . . . .\\n29\\n2.1.2\\nCompound Events and Independence . . . . . . . . . . . .\\n30\\n2.1.3\\nConditional Probability . . . . . . . . . . . . . . . . . . .\\n31\\n2.1.4\\nProbability Distributions\\n. . . . . . . . . . . . . . . . . .\\n32\\n2.2\\nDescriptive Statistics . . . . . . . . . . . . . . . . . . . . . . . . .\\n34\\n2.2.1\\nCentrality Measures . . . . . . . . . . . . . . . . . . . . .\\n34\\n2.2.2\\nVariability Measures . . . . . . . . . . . . . . . . . . . . .\\n36\\n2.2.3\\nInterpreting Variance\\n. . . . . . . . . . . . . . . . . . . .\\n37\\n2.2.4\\nCharacterizing Distributions\\n. . . . . . . . . . . . . . . .\\n39\\n2.3\\nCorrelation Analysis . . . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n2.3.1\\nCorrelation Coeﬃcients: Pearson and Spearman Rank . .\\n41\\n2.3.2\\nThe Power and Signiﬁcance of Correlation . . . . . . . . .\\n43\\n2.3.3\\nCorrelation Does Not Imply Causation!\\n. . . . . . . . . .\\n45\\nxi\\n\\n--- Page 2 ---\\nxii\\nCONTENTS\\n2.3.4\\nDetecting Periodicities by Autocorrelation . . . . . . . . .\\n46\\n2.4\\nLogarithms\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n2.4.1\\nLogarithms and Multiplying Probabilities . . . . . . . . .\\n48\\n2.4.2\\nLogarithms and Ratios . . . . . . . . . . . . . . . . . . . .\\n48\\n2.4.3\\nLogarithms and Normalizing Skewed Distributions . . . .\\n49\\n2.5\\nWar Story: Fitting Designer Genes . . . . . . . . . . . . . . . . .\\n50\\n2.6\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n2.7\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n3\\nData Munging\\n57\\n3.1\\nLanguages for Data Science . . . . . . . . . . . . . . . . . . . . .\\n57\\n3.1.1\\nThe Importance of Notebook Environments . . . . . . . .\\n59\\n3.1.2\\nStandard Data Formats . . . . . . . . . . . . . . . . . . .\\n61\\n3.2\\nCollecting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n3.2.1\\nHunting . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n3.2.2\\nScraping . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n3.2.3\\nLogging . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n3.3\\nCleaning Data\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n3.3.1\\nErrors vs. Artifacts\\n. . . . . . . . . . . . . . . . . . . . .\\n69\\n3.3.2\\nData Compatibility . . . . . . . . . . . . . . . . . . . . . .\\n72\\n3.3.3\\nDealing with Missing Values . . . . . . . . . . . . . . . . .\\n76\\n3.3.4\\nOutlier Detection . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n3.4\\nWar Story: Beating the Market . . . . . . . . . . . . . . . . . . .\\n79\\n3.5\\nCrowdsourcing\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n3.5.1\\nThe Penny Demo . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n3.5.2\\nWhen is the Crowd Wise? . . . . . . . . . . . . . . . . . .\\n82\\n3.5.3\\nMechanisms for Aggregation\\n. . . . . . . . . . . . . . . .\\n83\\n3.5.4\\nCrowdsourcing Services\\n. . . . . . . . . . . . . . . . . . .\\n84\\n3.5.5\\nGamiﬁcation\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n3.6\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n3.7\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n4\\nScores and Rankings\\n95\\n4.1\\nThe Body Mass Index (BMI) . . . . . . . . . . . . . . . . . . . .\\n96\\n4.2\\nDeveloping Scoring Systems . . . . . . . . . . . . . . . . . . . . .\\n99\\n4.2.1\\nGold Standards and Proxies . . . . . . . . . . . . . . . . .\\n99\\n4.2.2\\nScores vs. Rankings\\n. . . . . . . . . . . . . . . . . . . . . 100\\n4.2.3\\nRecognizing Good Scoring Functions . . . . . . . . . . . . 101\\n4.3\\nZ-scores and Normalization . . . . . . . . . . . . . . . . . . . . . 103\\n4.4\\nAdvanced Ranking Techniques\\n. . . . . . . . . . . . . . . . . . . 104\\n4.4.1\\nElo Rankings . . . . . . . . . . . . . . . . . . . . . . . . . 104\\n4.4.2\\nMerging Rankings\\n. . . . . . . . . . . . . . . . . . . . . . 108\\n4.4.3\\nDigraph-based Rankings . . . . . . . . . . . . . . . . . . . 109\\n4.4.4\\nPageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n4.5\\nWar Story: Clyde’s Revenge . . . . . . . . . . . . . . . . . . . . . 111\\n4.6\\nArrow’s Impossibility Theorem . . . . . . . . . . . . . . . . . . . 114\\n\\n--- Page 3 ---\\nCONTENTS\\nxiii\\n4.7\\nWar Story: Who’s Bigger? . . . . . . . . . . . . . . . . . . . . . . 115\\n4.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n4.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n5\\nStatistical Analysis\\n121\\n5.1\\nStatistical Distributions . . . . . . . . . . . . . . . . . . . . . . . 122\\n5.1.1\\nThe Binomial Distribution . . . . . . . . . . . . . . . . . . 123\\n5.1.2\\nThe Normal Distribution\\n. . . . . . . . . . . . . . . . . . 124\\n5.1.3\\nImplications of the Normal Distribution . . . . . . . . . . 126\\n5.1.4\\nPoisson Distribution . . . . . . . . . . . . . . . . . . . . . 127\\n5.1.5\\nPower Law Distributions . . . . . . . . . . . . . . . . . . . 129\\n5.2\\nSampling from Distributions . . . . . . . . . . . . . . . . . . . . . 132\\n5.2.1\\nRandom Sampling beyond One Dimension . . . . . . . . . 133\\n5.3\\nStatistical Signiﬁcance . . . . . . . . . . . . . . . . . . . . . . . . 135\\n5.3.1\\nThe Signiﬁcance of Signiﬁcance . . . . . . . . . . . . . . . 135\\n5.3.2\\nThe T-test: Comparing Population Means . . . . . . . . . 137\\n5.3.3\\nThe Kolmogorov-Smirnov Test . . . . . . . . . . . . . . . 139\\n5.3.4\\nThe Bonferroni Correction . . . . . . . . . . . . . . . . . . 141\\n5.3.5\\nFalse Discovery Rate . . . . . . . . . . . . . . . . . . . . . 142\\n5.4\\nWar Story: Discovering the Fountain of Youth? . . . . . . . . . . 143\\n5.5\\nPermutation Tests and P-values . . . . . . . . . . . . . . . . . . . 145\\n5.5.1\\nGenerating Random Permutations . . . . . . . . . . . . . 147\\n5.5.2\\nDiMaggio’s Hitting Streak . . . . . . . . . . . . . . . . . . 148\\n5.6\\nBayesian Reasoning\\n. . . . . . . . . . . . . . . . . . . . . . . . . 150\\n5.7\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n5.8\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n6\\nVisualizing Data\\n155\\n6.1\\nExploratory Data Analysis . . . . . . . . . . . . . . . . . . . . . . 156\\n6.1.1\\nConfronting a New Data Set\\n. . . . . . . . . . . . . . . . 156\\n6.1.2\\nSummary Statistics and Anscombe’s Quartet . . . . . . . 159\\n6.1.3\\nVisualization Tools . . . . . . . . . . . . . . . . . . . . . . 160\\n6.2\\nDeveloping a Visualization Aesthetic . . . . . . . . . . . . . . . . 162\\n6.2.1\\nMaximizing Data-Ink Ratio . . . . . . . . . . . . . . . . . 163\\n6.2.2\\nMinimizing the Lie Factor . . . . . . . . . . . . . . . . . . 164\\n6.2.3\\nMinimizing Chartjunk . . . . . . . . . . . . . . . . . . . . 165\\n6.2.4\\nProper Scaling and Labeling\\n. . . . . . . . . . . . . . . . 167\\n6.2.5\\nEﬀective Use of Color and Shading . . . . . . . . . . . . . 168\\n6.2.6\\nThe Power of Repetition . . . . . . . . . . . . . . . . . . . 169\\n6.3\\nChart Types\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n6.3.1\\nTabular Data . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n6.3.2\\nDot and Line Plots . . . . . . . . . . . . . . . . . . . . . . 174\\n6.3.3\\nScatter Plots . . . . . . . . . . . . . . . . . . . . . . . . . 177\\n6.3.4\\nBar Plots and Pie Charts . . . . . . . . . . . . . . . . . . 179\\n6.3.5\\nHistograms . . . . . . . . . . . . . . . . . . . . . . . . . . 183\\n6.3.6\\nData Maps\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 187\\n\\n--- Page 4 ---\\nxiv\\nCONTENTS\\n6.4\\nGreat Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . 189\\n6.4.1\\nMarey’s Train Schedule\\n. . . . . . . . . . . . . . . . . . . 189\\n6.4.2\\nSnow’s Cholera Map . . . . . . . . . . . . . . . . . . . . . 191\\n6.4.3\\nNew York’s Weather Year . . . . . . . . . . . . . . . . . . 192\\n6.5\\nReading Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n6.5.1\\nThe Obscured Distribution\\n. . . . . . . . . . . . . . . . . 193\\n6.5.2\\nOverinterpreting Variance . . . . . . . . . . . . . . . . . . 193\\n6.6\\nInteractive Visualization . . . . . . . . . . . . . . . . . . . . . . . 195\\n6.7\\nWar Story: TextMapping the World\\n. . . . . . . . . . . . . . . . 196\\n6.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\n6.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\\n7\\nMathematical Models\\n201\\n7.1\\nPhilosophies of Modeling . . . . . . . . . . . . . . . . . . . . . . . 201\\n7.1.1\\nOccam’s Razor . . . . . . . . . . . . . . . . . . . . . . . . 201\\n7.1.2\\nBias–Variance Trade-Oﬀs\\n. . . . . . . . . . . . . . . . . . 202\\n7.1.3\\nWhat Would Nate Silver Do? . . . . . . . . . . . . . . . . 203\\n7.2\\nA Taxonomy of Models\\n. . . . . . . . . . . . . . . . . . . . . . . 205\\n7.2.1\\nLinear vs. Non-Linear Models . . . . . . . . . . . . . . . . 206\\n7.2.2\\nBlackbox vs. Descriptive Models\\n. . . . . . . . . . . . . . 206\\n7.2.3\\nFirst-Principle vs. Data-Driven Models . . . . . . . . . . . 207\\n7.2.4\\nStochastic vs. Deterministic Models\\n. . . . . . . . . . . . 208\\n7.2.5\\nFlat vs. Hierarchical Models . . . . . . . . . . . . . . . . . 209\\n7.3\\nBaseline Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\\n7.3.1\\nBaseline Models for Classiﬁcation . . . . . . . . . . . . . . 210\\n7.3.2\\nBaseline Models for Value Prediction . . . . . . . . . . . . 212\\n7.4\\nEvaluating Models . . . . . . . . . . . . . . . . . . . . . . . . . . 212\\n7.4.1\\nEvaluating Classiﬁers\\n. . . . . . . . . . . . . . . . . . . . 213\\n7.4.2\\nReceiver-Operator Characteristic (ROC) Curves\\n. . . . . 218\\n7.4.3\\nEvaluating Multiclass Systems\\n. . . . . . . . . . . . . . . 219\\n7.4.4\\nEvaluating Value Prediction Models . . . . . . . . . . . . 221\\n7.5\\nEvaluation Environments\\n. . . . . . . . . . . . . . . . . . . . . . 224\\n7.5.1\\nData Hygiene for Evaluation\\n. . . . . . . . . . . . . . . . 225\\n7.5.2\\nAmplifying Small Evaluation Sets\\n. . . . . . . . . . . . . 226\\n7.6\\nWar Story: 100% Accuracy\\n. . . . . . . . . . . . . . . . . . . . . 228\\n7.7\\nSimulation Models . . . . . . . . . . . . . . . . . . . . . . . . . . 229\\n7.8\\nWar Story: Calculated Bets . . . . . . . . . . . . . . . . . . . . . 230\\n7.9\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\\n7.10 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\\n8\\nLinear Algebra\\n237\\n8.1\\nThe Power of Linear Algebra\\n. . . . . . . . . . . . . . . . . . . . 237\\n8.1.1\\nInterpreting Linear Algebraic Formulae\\n. . . . . . . . . . 238\\n8.1.2\\nGeometry and Vectors . . . . . . . . . . . . . . . . . . . . 240\\n8.2\\nVisualizing Matrix Operations . . . . . . . . . . . . . . . . . . . . 241\\n8.2.1\\nMatrix Addition\\n. . . . . . . . . . . . . . . . . . . . . . . 242\\n\\n--- Page 5 ---\\nCONTENTS\\nxv\\n8.2.2\\nMatrix Multiplication\\n. . . . . . . . . . . . . . . . . . . . 243\\n8.2.3\\nApplications of Matrix Multiplication\\n. . . . . . . . . . . 244\\n8.2.4\\nIdentity Matrices and Inversion . . . . . . . . . . . . . . . 248\\n8.2.5\\nMatrix Inversion and Linear Systems . . . . . . . . . . . . 250\\n8.2.6\\nMatrix Rank\\n. . . . . . . . . . . . . . . . . . . . . . . . . 251\\n8.3\\nFactoring Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 252\\n8.3.1\\nWhy Factor Feature Matrices?\\n. . . . . . . . . . . . . . . 252\\n8.3.2\\nLU Decomposition and Determinants\\n. . . . . . . . . . . 254\\n8.4\\nEigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . 255\\n8.4.1\\nProperties of Eigenvalues\\n. . . . . . . . . . . . . . . . . . 255\\n8.4.2\\nComputing Eigenvalues\\n. . . . . . . . . . . . . . . . . . . 256\\n8.5\\nEigenvalue Decomposition . . . . . . . . . . . . . . . . . . . . . . 257\\n8.5.1\\nSingular Value Decomposition . . . . . . . . . . . . . . . . 258\\n8.5.2\\nPrincipal Components Analysis . . . . . . . . . . . . . . . 260\\n8.6\\nWar Story: The Human Factors . . . . . . . . . . . . . . . . . . . 262\\n8.7\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n8.8\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n9\\nLinear and Logistic Regression\\n267\\n9.1\\nLinear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\\n9.1.1\\nLinear Regression and Duality\\n. . . . . . . . . . . . . . . 268\\n9.1.2\\nError in Linear Regression . . . . . . . . . . . . . . . . . . 269\\n9.1.3\\nFinding the Optimal Fit . . . . . . . . . . . . . . . . . . . 270\\n9.2\\nBetter Regression Models\\n. . . . . . . . . . . . . . . . . . . . . . 272\\n9.2.1\\nRemoving Outliers . . . . . . . . . . . . . . . . . . . . . . 272\\n9.2.2\\nFitting Non-Linear Functions . . . . . . . . . . . . . . . . 273\\n9.2.3\\nFeature and Target Scaling\\n. . . . . . . . . . . . . . . . . 274\\n9.2.4\\nDealing with Highly-Correlated Features . . . . . . . . . . 277\\n9.3\\nWar Story: Taxi Deriver . . . . . . . . . . . . . . . . . . . . . . . 277\\n9.4\\nRegression as Parameter Fitting\\n. . . . . . . . . . . . . . . . . . 279\\n9.4.1\\nConvex Parameter Spaces . . . . . . . . . . . . . . . . . . 280\\n9.4.2\\nGradient Descent Search . . . . . . . . . . . . . . . . . . . 281\\n9.4.3\\nWhat is the Right Learning Rate? . . . . . . . . . . . . . 283\\n9.4.4\\nStochastic Gradient Descent . . . . . . . . . . . . . . . . . 285\\n9.5\\nSimplifying Models through Regularization\\n. . . . . . . . . . . . 286\\n9.5.1\\nRidge Regression . . . . . . . . . . . . . . . . . . . . . . . 286\\n9.5.2\\nLASSO Regression . . . . . . . . . . . . . . . . . . . . . . 287\\n9.5.3\\nTrade-Oﬀs between Fit and Complexity . . . . . . . . . . 288\\n9.6\\nClassiﬁcation and Logistic Regression\\n. . . . . . . . . . . . . . . 289\\n9.6.1\\nRegression for Classiﬁcation . . . . . . . . . . . . . . . . . 290\\n9.6.2\\nDecision Boundaries . . . . . . . . . . . . . . . . . . . . . 291\\n9.6.3\\nLogistic Regression . . . . . . . . . . . . . . . . . . . . . . 292\\n9.7\\nIssues in Logistic Classiﬁcation . . . . . . . . . . . . . . . . . . . 295\\n9.7.1\\nBalanced Training Classes . . . . . . . . . . . . . . . . . . 295\\n9.7.2\\nMulti-Class Classiﬁcation . . . . . . . . . . . . . . . . . . 297\\n9.7.3\\nHierarchical Classiﬁcation . . . . . . . . . . . . . . . . . . 298\\n\\n--- Page 6 ---\\nxvi\\nCONTENTS\\n9.7.4\\nPartition Functions and Multinomial Regression\\n. . . . . 299\\n9.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\\n9.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\\n10 Distance and Network Methods\\n303\\n10.1 Measuring Distances . . . . . . . . . . . . . . . . . . . . . . . . . 303\\n10.1.1 Distance Metrics . . . . . . . . . . . . . . . . . . . . . . . 304\\n10.1.2 The Lk Distance Metric . . . . . . . . . . . . . . . . . . . 305\\n10.1.3 Working in Higher Dimensions\\n. . . . . . . . . . . . . . . 307\\n10.1.4 Dimensional Egalitarianism . . . . . . . . . . . . . . . . . 308\\n10.1.5 Points vs. Vectors\\n. . . . . . . . . . . . . . . . . . . . . . 309\\n10.1.6 Distances between Probability Distributions . . . . . . . . 310\\n10.2 Nearest Neighbor Classiﬁcation . . . . . . . . . . . . . . . . . . . 311\\n10.2.1 Seeking Good Analogies . . . . . . . . . . . . . . . . . . . 312\\n10.2.2 k-Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . 313\\n10.2.3 Finding Nearest Neighbors\\n. . . . . . . . . . . . . . . . . 315\\n10.2.4 Locality Sensitive Hashing . . . . . . . . . . . . . . . . . . 317\\n10.3 Graphs, Networks, and Distances . . . . . . . . . . . . . . . . . . 319\\n10.3.1 Weighted Graphs and Induced Networks . . . . . . . . . . 320\\n10.3.2 Talking About Graphs . . . . . . . . . . . . . . . . . . . . 321\\n10.3.3 Graph Theory\\n. . . . . . . . . . . . . . . . . . . . . . . . 323\\n10.4 PageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\\n10.5 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\\n10.5.1 k-means Clustering . . . . . . . . . . . . . . . . . . . . . . 330\\n10.5.2 Agglomerative Clustering . . . . . . . . . . . . . . . . . . 336\\n10.5.3 Comparing Clusterings . . . . . . . . . . . . . . . . . . . . 341\\n10.5.4 Similarity Graphs and Cut-Based Clustering\\n. . . . . . . 341\\n10.6 War Story: Cluster Bombing\\n. . . . . . . . . . . . . . . . . . . . 344\\n10.7 Chapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\\n10.8 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\\n11 Machine Learning\\n351\\n11.1 Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\n11.1.1 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\n11.1.2 Dealing with Zero Counts (Discounting) . . . . . . . . . . 356\\n11.2 Decision Tree Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . 357\\n11.2.1 Constructing Decision Trees . . . . . . . . . . . . . . . . . 359\\n11.2.2 Realizing Exclusive Or . . . . . . . . . . . . . . . . . . . . 361\\n11.2.3 Ensembles of Decision Trees . . . . . . . . . . . . . . . . . 362\\n11.3 Boosting and Ensemble Learning . . . . . . . . . . . . . . . . . . 363\\n11.3.1 Voting with Classiﬁers . . . . . . . . . . . . . . . . . . . . 363\\n11.3.2 Boosting Algorithms . . . . . . . . . . . . . . . . . . . . . 364\\n11.4 Support Vector Machines\\n. . . . . . . . . . . . . . . . . . . . . . 366\\n11.4.1 Linear SVMs . . . . . . . . . . . . . . . . . . . . . . . . . 369\\n11.4.2 Non-linear SVMs . . . . . . . . . . . . . . . . . . . . . . . 369\\n11.4.3 Kernels\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n\\n--- Page 7 ---\\nCONTENTS\\nxvii\\n11.5 Degrees of Supervision . . . . . . . . . . . . . . . . . . . . . . . . 372\\n11.5.1 Supervised Learning . . . . . . . . . . . . . . . . . . . . . 372\\n11.5.2 Unsupervised Learning . . . . . . . . . . . . . . . . . . . . 372\\n11.5.3 Semi-supervised Learning . . . . . . . . . . . . . . . . . . 374\\n11.5.4 Feature Engineering . . . . . . . . . . . . . . . . . . . . . 375\\n11.6 Deep Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\\n11.6.1 Networks and Depth . . . . . . . . . . . . . . . . . . . . . 378\\n11.6.2 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . 382\\n11.6.3 Word and Graph Embeddings . . . . . . . . . . . . . . . . 383\\n11.7 War Story: The Name Game\\n. . . . . . . . . . . . . . . . . . . . 385\\n11.8 Chapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\\n11.9 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\\n12 Big Data: Achieving Scale\\n391\\n12.1 What is Big Data? . . . . . . . . . . . . . . . . . . . . . . . . . . 392\\n12.1.1 Big Data as Bad Data . . . . . . . . . . . . . . . . . . . . 392\\n12.1.2 The Three Vs . . . . . . . . . . . . . . . . . . . . . . . . . 394\\n12.2 War Story: Infrastructure Matters\\n. . . . . . . . . . . . . . . . . 395\\n12.3 Algorithmics for Big Data . . . . . . . . . . . . . . . . . . . . . . 397\\n12.3.1 Big Oh Analysis\\n. . . . . . . . . . . . . . . . . . . . . . . 397\\n12.3.2 Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\\n12.3.3 Exploiting the Storage Hierarchy . . . . . . . . . . . . . . 401\\n12.3.4 Streaming and Single-Pass Algorithms . . . . . . . . . . . 402\\n12.4 Filtering and Sampling . . . . . . . . . . . . . . . . . . . . . . . . 403\\n12.4.1 Deterministic Sampling Algorithms . . . . . . . . . . . . . 404\\n12.4.2 Randomized and Stream Sampling . . . . . . . . . . . . . 406\\n12.5 Parallelism\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406\\n12.5.1 One, Two, Many . . . . . . . . . . . . . . . . . . . . . . . 407\\n12.5.2 Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . 409\\n12.5.3 Grid Search . . . . . . . . . . . . . . . . . . . . . . . . . . 409\\n12.5.4 Cloud Computing Services . . . . . . . . . . . . . . . . . . 410\\n12.6 MapReduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\\n12.6.1 Map-Reduce Programming\\n. . . . . . . . . . . . . . . . . 412\\n12.6.2 MapReduce under the Hood . . . . . . . . . . . . . . . . . 414\\n12.7 Societal and Ethical Implications . . . . . . . . . . . . . . . . . . 416\\n12.8 Chapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\\n12.9 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\\n13 Coda\\n423\\n13.1 Get a Job! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423\\n13.2 Go to Graduate School!\\n. . . . . . . . . . . . . . . . . . . . . . . 424\\n13.3 Professional Consulting Services\\n. . . . . . . . . . . . . . . . . . 425\\n14 Bibliography\\n427\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_toc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--- Page 1 ---\\nContents\\n1\\nWhat is Data Science?\\n1\\n1.1\\nComputer Science, Data Science, and Real Science . . . . . . . .\\n2\\n1.2\\nAsking Interesting Questions from Data . . . . . . . . . . . . . .\\n4\\n1.2.1\\nThe Baseball Encyclopedia\\n. . . . . . . . . . . . . . . . .\\n5\\n1.2.2\\nThe Internet Movie Database (IMDb) . . . . . . . . . . .\\n7\\n1.2.3\\nGoogle Ngrams . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n1.2.4\\nNew York Taxi Records . . . . . . . . . . . . . . . . . . .\\n11\\n1.3\\nProperties of Data . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n1.3.1\\nStructured vs. Unstructured Data\\n. . . . . . . . . . . . .\\n14\\n1.3.2\\nQuantitative vs. Categorical Data\\n. . . . . . . . . . . . .\\n15\\n1.3.3\\nBig Data vs. Little Data . . . . . . . . . . . . . . . . . . .\\n15\\n1.4\\nClassiﬁcation and Regression\\n. . . . . . . . . . . . . . . . . . . .\\n16\\n1.5\\nData Science Television: The Quant Shop . . . . . . . . . . . . .\\n17\\n1.5.1\\nKaggle Challenges\\n. . . . . . . . . . . . . . . . . . . . . .\\n19\\n1.6\\nAbout the War Stories . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n1.7\\nWar Story: Answering the Right Question . . . . . . . . . . . . .\\n21\\n1.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n1.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n2\\nMathematical Preliminaries\\n27\\n2.1\\nProbability\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n2.1.1\\nProbability vs. Statistics . . . . . . . . . . . . . . . . . . .\\n29\\n2.1.2\\nCompound Events and Independence . . . . . . . . . . . .\\n30\\n2.1.3\\nConditional Probability . . . . . . . . . . . . . . . . . . .\\n31\\n2.1.4\\nProbability Distributions\\n. . . . . . . . . . . . . . . . . .\\n32\\n2.2\\nDescriptive Statistics . . . . . . . . . . . . . . . . . . . . . . . . .\\n34\\n2.2.1\\nCentrality Measures . . . . . . . . . . . . . . . . . . . . .\\n34\\n2.2.2\\nVariability Measures . . . . . . . . . . . . . . . . . . . . .\\n36\\n2.2.3\\nInterpreting Variance\\n. . . . . . . . . . . . . . . . . . . .\\n37\\n2.2.4\\nCharacterizing Distributions\\n. . . . . . . . . . . . . . . .\\n39\\n2.3\\nCorrelation Analysis . . . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n2.3.1\\nCorrelation Coeﬃcients: Pearson and Spearman Rank . .\\n41\\n2.3.2\\nThe Power and Signiﬁcance of Correlation . . . . . . . . .\\n43\\n2.3.3\\nCorrelation Does Not Imply Causation!\\n. . . . . . . . . .\\n45\\nxi\\n\\n--- Page 2 ---\\nxii\\nCONTENTS\\n2.3.4\\nDetecting Periodicities by Autocorrelation . . . . . . . . .\\n46\\n2.4\\nLogarithms\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n2.4.1\\nLogarithms and Multiplying Probabilities . . . . . . . . .\\n48\\n2.4.2\\nLogarithms and Ratios . . . . . . . . . . . . . . . . . . . .\\n48\\n2.4.3\\nLogarithms and Normalizing Skewed Distributions . . . .\\n49\\n2.5\\nWar Story: Fitting Designer Genes . . . . . . . . . . . . . . . . .\\n50\\n2.6\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n2.7\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n3\\nData Munging\\n57\\n3.1\\nLanguages for Data Science . . . . . . . . . . . . . . . . . . . . .\\n57\\n3.1.1\\nThe Importance of Notebook Environments . . . . . . . .\\n59\\n3.1.2\\nStandard Data Formats . . . . . . . . . . . . . . . . . . .\\n61\\n3.2\\nCollecting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n3.2.1\\nHunting . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n3.2.2\\nScraping . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n3.2.3\\nLogging . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n3.3\\nCleaning Data\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n3.3.1\\nErrors vs. Artifacts\\n. . . . . . . . . . . . . . . . . . . . .\\n69\\n3.3.2\\nData Compatibility . . . . . . . . . . . . . . . . . . . . . .\\n72\\n3.3.3\\nDealing with Missing Values . . . . . . . . . . . . . . . . .\\n76\\n3.3.4\\nOutlier Detection . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n3.4\\nWar Story: Beating the Market . . . . . . . . . . . . . . . . . . .\\n79\\n3.5\\nCrowdsourcing\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n3.5.1\\nThe Penny Demo . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n3.5.2\\nWhen is the Crowd Wise? . . . . . . . . . . . . . . . . . .\\n82\\n3.5.3\\nMechanisms for Aggregation\\n. . . . . . . . . . . . . . . .\\n83\\n3.5.4\\nCrowdsourcing Services\\n. . . . . . . . . . . . . . . . . . .\\n84\\n3.5.5\\nGamiﬁcation\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n3.6\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n3.7\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n4\\nScores and Rankings\\n95\\n4.1\\nThe Body Mass Index (BMI) . . . . . . . . . . . . . . . . . . . .\\n96\\n4.2\\nDeveloping Scoring Systems . . . . . . . . . . . . . . . . . . . . .\\n99\\n4.2.1\\nGold Standards and Proxies . . . . . . . . . . . . . . . . .\\n99\\n4.2.2\\nScores vs. Rankings\\n. . . . . . . . . . . . . . . . . . . . . 100\\n4.2.3\\nRecognizing Good Scoring Functions . . . . . . . . . . . . 101\\n4.3\\nZ-scores and Normalization . . . . . . . . . . . . . . . . . . . . . 103\\n4.4\\nAdvanced Ranking Techniques\\n. . . . . . . . . . . . . . . . . . . 104\\n4.4.1\\nElo Rankings . . . . . . . . . . . . . . . . . . . . . . . . . 104\\n4.4.2\\nMerging Rankings\\n. . . . . . . . . . . . . . . . . . . . . . 108\\n4.4.3\\nDigraph-based Rankings . . . . . . . . . . . . . . . . . . . 109\\n4.4.4\\nPageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n4.5\\nWar Story: Clyde’s Revenge . . . . . . . . . . . . . . . . . . . . . 111\\n4.6\\nArrow’s Impossibility Theorem . . . . . . . . . . . . . . . . . . . 114\\n\\n--- Page 3 ---\\nCONTENTS\\nxiii\\n4.7\\nWar Story: Who’s Bigger? . . . . . . . . . . . . . . . . . . . . . . 115\\n4.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n4.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n5\\nStatistical Analysis\\n121\\n5.1\\nStatistical Distributions . . . . . . . . . . . . . . . . . . . . . . . 122\\n5.1.1\\nThe Binomial Distribution . . . . . . . . . . . . . . . . . . 123\\n5.1.2\\nThe Normal Distribution\\n. . . . . . . . . . . . . . . . . . 124\\n5.1.3\\nImplications of the Normal Distribution . . . . . . . . . . 126\\n5.1.4\\nPoisson Distribution . . . . . . . . . . . . . . . . . . . . . 127\\n5.1.5\\nPower Law Distributions . . . . . . . . . . . . . . . . . . . 129\\n5.2\\nSampling from Distributions . . . . . . . . . . . . . . . . . . . . . 132\\n5.2.1\\nRandom Sampling beyond One Dimension . . . . . . . . . 133\\n5.3\\nStatistical Signiﬁcance . . . . . . . . . . . . . . . . . . . . . . . . 135\\n5.3.1\\nThe Signiﬁcance of Signiﬁcance . . . . . . . . . . . . . . . 135\\n5.3.2\\nThe T-test: Comparing Population Means . . . . . . . . . 137\\n5.3.3\\nThe Kolmogorov-Smirnov Test . . . . . . . . . . . . . . . 139\\n5.3.4\\nThe Bonferroni Correction . . . . . . . . . . . . . . . . . . 141\\n5.3.5\\nFalse Discovery Rate . . . . . . . . . . . . . . . . . . . . . 142\\n5.4\\nWar Story: Discovering the Fountain of Youth? . . . . . . . . . . 143\\n5.5\\nPermutation Tests and P-values . . . . . . . . . . . . . . . . . . . 145\\n5.5.1\\nGenerating Random Permutations . . . . . . . . . . . . . 147\\n5.5.2\\nDiMaggio’s Hitting Streak . . . . . . . . . . . . . . . . . . 148\\n5.6\\nBayesian Reasoning\\n. . . . . . . . . . . . . . . . . . . . . . . . . 150\\n5.7\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n5.8\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n6\\nVisualizing Data\\n155\\n6.1\\nExploratory Data Analysis . . . . . . . . . . . . . . . . . . . . . . 156\\n6.1.1\\nConfronting a New Data Set\\n. . . . . . . . . . . . . . . . 156\\n6.1.2\\nSummary Statistics and Anscombe’s Quartet . . . . . . . 159\\n6.1.3\\nVisualization Tools . . . . . . . . . . . . . . . . . . . . . . 160\\n6.2\\nDeveloping a Visualization Aesthetic . . . . . . . . . . . . . . . . 162\\n6.2.1\\nMaximizing Data-Ink Ratio . . . . . . . . . . . . . . . . . 163\\n6.2.2\\nMinimizing the Lie Factor . . . . . . . . . . . . . . . . . . 164\\n6.2.3\\nMinimizing Chartjunk . . . . . . . . . . . . . . . . . . . . 165\\n6.2.4\\nProper Scaling and Labeling\\n. . . . . . . . . . . . . . . . 167\\n6.2.5\\nEﬀective Use of Color and Shading . . . . . . . . . . . . . 168\\n6.2.6\\nThe Power of Repetition . . . . . . . . . . . . . . . . . . . 169\\n6.3\\nChart Types\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n6.3.1\\nTabular Data . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n6.3.2\\nDot and Line Plots . . . . . . . . . . . . . . . . . . . . . . 174\\n6.3.3\\nScatter Plots . . . . . . . . . . . . . . . . . . . . . . . . . 177\\n6.3.4\\nBar Plots and Pie Charts . . . . . . . . . . . . . . . . . . 179\\n6.3.5\\nHistograms . . . . . . . . . . . . . . . . . . . . . . . . . . 183\\n6.3.6\\nData Maps\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 187\\n\\n--- Page 4 ---\\nxiv\\nCONTENTS\\n6.4\\nGreat Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . 189\\n6.4.1\\nMarey’s Train Schedule\\n. . . . . . . . . . . . . . . . . . . 189\\n6.4.2\\nSnow’s Cholera Map . . . . . . . . . . . . . . . . . . . . . 191\\n6.4.3\\nNew York’s Weather Year . . . . . . . . . . . . . . . . . . 192\\n6.5\\nReading Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n6.5.1\\nThe Obscured Distribution\\n. . . . . . . . . . . . . . . . . 193\\n6.5.2\\nOverinterpreting Variance . . . . . . . . . . . . . . . . . . 193\\n6.6\\nInteractive Visualization . . . . . . . . . . . . . . . . . . . . . . . 195\\n6.7\\nWar Story: TextMapping the World\\n. . . . . . . . . . . . . . . . 196\\n6.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\n6.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\\n7\\nMathematical Models\\n201\\n7.1\\nPhilosophies of Modeling . . . . . . . . . . . . . . . . . . . . . . . 201\\n7.1.1\\nOccam’s Razor . . . . . . . . . . . . . . . . . . . . . . . . 201\\n7.1.2\\nBias–Variance Trade-Oﬀs\\n. . . . . . . . . . . . . . . . . . 202\\n7.1.3\\nWhat Would Nate Silver Do? . . . . . . . . . . . . . . . . 203\\n7.2\\nA Taxonomy of Models\\n. . . . . . . . . . . . . . . . . . . . . . . 205\\n7.2.1\\nLinear vs. Non-Linear Models . . . . . . . . . . . . . . . . 206\\n7.2.2\\nBlackbox vs. Descriptive Models\\n. . . . . . . . . . . . . . 206\\n7.2.3\\nFirst-Principle vs. Data-Driven Models . . . . . . . . . . . 207\\n7.2.4\\nStochastic vs. Deterministic Models\\n. . . . . . . . . . . . 208\\n7.2.5\\nFlat vs. Hierarchical Models . . . . . . . . . . . . . . . . . 209\\n7.3\\nBaseline Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\\n7.3.1\\nBaseline Models for Classiﬁcation . . . . . . . . . . . . . . 210\\n7.3.2\\nBaseline Models for Value Prediction . . . . . . . . . . . . 212\\n7.4\\nEvaluating Models . . . . . . . . . . . . . . . . . . . . . . . . . . 212\\n7.4.1\\nEvaluating Classiﬁers\\n. . . . . . . . . . . . . . . . . . . . 213\\n7.4.2\\nReceiver-Operator Characteristic (ROC) Curves\\n. . . . . 218\\n7.4.3\\nEvaluating Multiclass Systems\\n. . . . . . . . . . . . . . . 219\\n7.4.4\\nEvaluating Value Prediction Models . . . . . . . . . . . . 221\\n7.5\\nEvaluation Environments\\n. . . . . . . . . . . . . . . . . . . . . . 224\\n7.5.1\\nData Hygiene for Evaluation\\n. . . . . . . . . . . . . . . . 225\\n7.5.2\\nAmplifying Small Evaluation Sets\\n. . . . . . . . . . . . . 226\\n7.6\\nWar Story: 100% Accuracy\\n. . . . . . . . . . . . . . . . . . . . . 228\\n7.7\\nSimulation Models . . . . . . . . . . . . . . . . . . . . . . . . . . 229\\n7.8\\nWar Story: Calculated Bets . . . . . . . . . . . . . . . . . . . . . 230\\n7.9\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\\n7.10 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\\n8\\nLinear Algebra\\n237\\n8.1\\nThe Power of Linear Algebra\\n. . . . . . . . . . . . . . . . . . . . 237\\n8.1.1\\nInterpreting Linear Algebraic Formulae\\n. . . . . . . . . . 238\\n8.1.2\\nGeometry and Vectors . . . . . . . . . . . . . . . . . . . . 240\\n8.2\\nVisualizing Matrix Operations . . . . . . . . . . . . . . . . . . . . 241\\n8.2.1\\nMatrix Addition\\n. . . . . . . . . . . . . . . . . . . . . . . 242\\n\\n--- Page 5 ---\\nCONTENTS\\nxv\\n8.2.2\\nMatrix Multiplication\\n. . . . . . . . . . . . . . . . . . . . 243\\n8.2.3\\nApplications of Matrix Multiplication\\n. . . . . . . . . . . 244\\n8.2.4\\nIdentity Matrices and Inversion . . . . . . . . . . . . . . . 248\\n8.2.5\\nMatrix Inversion and Linear Systems . . . . . . . . . . . . 250\\n8.2.6\\nMatrix Rank\\n. . . . . . . . . . . . . . . . . . . . . . . . . 251\\n8.3\\nFactoring Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 252\\n8.3.1\\nWhy Factor Feature Matrices?\\n. . . . . . . . . . . . . . . 252\\n8.3.2\\nLU Decomposition and Determinants\\n. . . . . . . . . . . 254\\n8.4\\nEigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . 255\\n8.4.1\\nProperties of Eigenvalues\\n. . . . . . . . . . . . . . . . . . 255\\n8.4.2\\nComputing Eigenvalues\\n. . . . . . . . . . . . . . . . . . . 256\\n8.5\\nEigenvalue Decomposition . . . . . . . . . . . . . . . . . . . . . . 257\\n8.5.1\\nSingular Value Decomposition . . . . . . . . . . . . . . . . 258\\n8.5.2\\nPrincipal Components Analysis . . . . . . . . . . . . . . . 260\\n8.6\\nWar Story: The Human Factors . . . . . . . . . . . . . . . . . . . 262\\n8.7\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n8.8\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n9\\nLinear and Logistic Regression\\n267\\n9.1\\nLinear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\\n9.1.1\\nLinear Regression and Duality\\n. . . . . . . . . . . . . . . 268\\n9.1.2\\nError in Linear Regression . . . . . . . . . . . . . . . . . . 269\\n9.1.3\\nFinding the Optimal Fit . . . . . . . . . . . . . . . . . . . 270\\n9.2\\nBetter Regression Models\\n. . . . . . . . . . . . . . . . . . . . . . 272\\n9.2.1\\nRemoving Outliers . . . . . . . . . . . . . . . . . . . . . . 272\\n9.2.2\\nFitting Non-Linear Functions . . . . . . . . . . . . . . . . 273\\n9.2.3\\nFeature and Target Scaling\\n. . . . . . . . . . . . . . . . . 274\\n9.2.4\\nDealing with Highly-Correlated Features . . . . . . . . . . 277\\n9.3\\nWar Story: Taxi Deriver . . . . . . . . . . . . . . . . . . . . . . . 277\\n9.4\\nRegression as Parameter Fitting\\n. . . . . . . . . . . . . . . . . . 279\\n9.4.1\\nConvex Parameter Spaces . . . . . . . . . . . . . . . . . . 280\\n9.4.2\\nGradient Descent Search . . . . . . . . . . . . . . . . . . . 281\\n9.4.3\\nWhat is the Right Learning Rate? . . . . . . . . . . . . . 283\\n9.4.4\\nStochastic Gradient Descent . . . . . . . . . . . . . . . . . 285\\n9.5\\nSimplifying Models through Regularization\\n. . . . . . . . . . . . 286\\n9.5.1\\nRidge Regression . . . . . . . . . . . . . . . . . . . . . . . 286\\n9.5.2\\nLASSO Regression . . . . . . . . . . . . . . . . . . . . . . 287\\n9.5.3\\nTrade-Oﬀs between Fit and Complexity . . . . . . . . . . 288\\n9.6\\nClassiﬁcation and Logistic Regression\\n. . . . . . . . . . . . . . . 289\\n9.6.1\\nRegression for Classiﬁcation . . . . . . . . . . . . . . . . . 290\\n9.6.2\\nDecision Boundaries . . . . . . . . . . . . . . . . . . . . . 291\\n9.6.3\\nLogistic Regression . . . . . . . . . . . . . . . . . . . . . . 292\\n9.7\\nIssues in Logistic Classiﬁcation . . . . . . . . . . . . . . . . . . . 295\\n9.7.1\\nBalanced Training Classes . . . . . . . . . . . . . . . . . . 295\\n9.7.2\\nMulti-Class Classiﬁcation . . . . . . . . . . . . . . . . . . 297\\n9.7.3\\nHierarchical Classiﬁcation . . . . . . . . . . . . . . . . . . 298\\n\\n--- Page 6 ---\\nxvi\\nCONTENTS\\n9.7.4\\nPartition Functions and Multinomial Regression\\n. . . . . 299\\n9.8\\nChapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\\n9.9\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\\n10 Distance and Network Methods\\n303\\n10.1 Measuring Distances . . . . . . . . . . . . . . . . . . . . . . . . . 303\\n10.1.1 Distance Metrics . . . . . . . . . . . . . . . . . . . . . . . 304\\n10.1.2 The Lk Distance Metric . . . . . . . . . . . . . . . . . . . 305\\n10.1.3 Working in Higher Dimensions\\n. . . . . . . . . . . . . . . 307\\n10.1.4 Dimensional Egalitarianism . . . . . . . . . . . . . . . . . 308\\n10.1.5 Points vs. Vectors\\n. . . . . . . . . . . . . . . . . . . . . . 309\\n10.1.6 Distances between Probability Distributions . . . . . . . . 310\\n10.2 Nearest Neighbor Classiﬁcation . . . . . . . . . . . . . . . . . . . 311\\n10.2.1 Seeking Good Analogies . . . . . . . . . . . . . . . . . . . 312\\n10.2.2 k-Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . 313\\n10.2.3 Finding Nearest Neighbors\\n. . . . . . . . . . . . . . . . . 315\\n10.2.4 Locality Sensitive Hashing . . . . . . . . . . . . . . . . . . 317\\n10.3 Graphs, Networks, and Distances . . . . . . . . . . . . . . . . . . 319\\n10.3.1 Weighted Graphs and Induced Networks . . . . . . . . . . 320\\n10.3.2 Talking About Graphs . . . . . . . . . . . . . . . . . . . . 321\\n10.3.3 Graph Theory\\n. . . . . . . . . . . . . . . . . . . . . . . . 323\\n10.4 PageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\\n10.5 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\\n10.5.1 k-means Clustering . . . . . . . . . . . . . . . . . . . . . . 330\\n10.5.2 Agglomerative Clustering . . . . . . . . . . . . . . . . . . 336\\n10.5.3 Comparing Clusterings . . . . . . . . . . . . . . . . . . . . 341\\n10.5.4 Similarity Graphs and Cut-Based Clustering\\n. . . . . . . 341\\n10.6 War Story: Cluster Bombing\\n. . . . . . . . . . . . . . . . . . . . 344\\n10.7 Chapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\\n10.8 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\\n11 Machine Learning\\n351\\n11.1 Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\n11.1.1 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\n11.1.2 Dealing with Zero Counts (Discounting) . . . . . . . . . . 356\\n11.2 Decision Tree Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . 357\\n11.2.1 Constructing Decision Trees . . . . . . . . . . . . . . . . . 359\\n11.2.2 Realizing Exclusive Or . . . . . . . . . . . . . . . . . . . . 361\\n11.2.3 Ensembles of Decision Trees . . . . . . . . . . . . . . . . . 362\\n11.3 Boosting and Ensemble Learning . . . . . . . . . . . . . . . . . . 363\\n11.3.1 Voting with Classiﬁers . . . . . . . . . . . . . . . . . . . . 363\\n11.3.2 Boosting Algorithms . . . . . . . . . . . . . . . . . . . . . 364\\n11.4 Support Vector Machines\\n. . . . . . . . . . . . . . . . . . . . . . 366\\n11.4.1 Linear SVMs . . . . . . . . . . . . . . . . . . . . . . . . . 369\\n11.4.2 Non-linear SVMs . . . . . . . . . . . . . . . . . . . . . . . 369\\n11.4.3 Kernels\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n\\n--- Page 7 ---\\nCONTENTS\\nxvii\\n11.5 Degrees of Supervision . . . . . . . . . . . . . . . . . . . . . . . . 372\\n11.5.1 Supervised Learning . . . . . . . . . . . . . . . . . . . . . 372\\n11.5.2 Unsupervised Learning . . . . . . . . . . . . . . . . . . . . 372\\n11.5.3 Semi-supervised Learning . . . . . . . . . . . . . . . . . . 374\\n11.5.4 Feature Engineering . . . . . . . . . . . . . . . . . . . . . 375\\n11.6 Deep Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\\n11.6.1 Networks and Depth . . . . . . . . . . . . . . . . . . . . . 378\\n11.6.2 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . 382\\n11.6.3 Word and Graph Embeddings . . . . . . . . . . . . . . . . 383\\n11.7 War Story: The Name Game\\n. . . . . . . . . . . . . . . . . . . . 385\\n11.8 Chapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\\n11.9 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\\n12 Big Data: Achieving Scale\\n391\\n12.1 What is Big Data? . . . . . . . . . . . . . . . . . . . . . . . . . . 392\\n12.1.1 Big Data as Bad Data . . . . . . . . . . . . . . . . . . . . 392\\n12.1.2 The Three Vs . . . . . . . . . . . . . . . . . . . . . . . . . 394\\n12.2 War Story: Infrastructure Matters\\n. . . . . . . . . . . . . . . . . 395\\n12.3 Algorithmics for Big Data . . . . . . . . . . . . . . . . . . . . . . 397\\n12.3.1 Big Oh Analysis\\n. . . . . . . . . . . . . . . . . . . . . . . 397\\n12.3.2 Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\\n12.3.3 Exploiting the Storage Hierarchy . . . . . . . . . . . . . . 401\\n12.3.4 Streaming and Single-Pass Algorithms . . . . . . . . . . . 402\\n12.4 Filtering and Sampling . . . . . . . . . . . . . . . . . . . . . . . . 403\\n12.4.1 Deterministic Sampling Algorithms . . . . . . . . . . . . . 404\\n12.4.2 Randomized and Stream Sampling . . . . . . . . . . . . . 406\\n12.5 Parallelism\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406\\n12.5.1 One, Two, Many . . . . . . . . . . . . . . . . . . . . . . . 407\\n12.5.2 Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . 409\\n12.5.3 Grid Search . . . . . . . . . . . . . . . . . . . . . . . . . . 409\\n12.5.4 Cloud Computing Services . . . . . . . . . . . . . . . . . . 410\\n12.6 MapReduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\\n12.6.1 Map-Reduce Programming\\n. . . . . . . . . . . . . . . . . 412\\n12.6.2 MapReduce under the Hood . . . . . . . . . . . . . . . . . 414\\n12.7 Societal and Ethical Implications . . . . . . . . . . . . . . . . . . 416\\n12.8 Chapter Notes\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\\n12.9 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\\n13 Coda\\n423\\n13.1 Get a Job! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423\\n13.2 Go to Graduate School!\\n. . . . . . . . . . . . . . . . . . . . . . . 424\\n13.3 Professional Consulting Services\\n. . . . . . . . . . . . . . . . . . 425\\n14 Bibliography\\n427\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_book_pdf = pymupdf.open(BOOK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(og_book_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_pdf = pymupdf.open(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page 0 of ../../files/data-science_book/data-science-index.pdf"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_pdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling subindex entries\n",
    "\n",
    "def extract_index_entries(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    index = defaultdict(list)\n",
    "    current_main_term = None\n",
    "    counter = 1\n",
    "    for page in doc:\n",
    "        text_dict = page.get_text(\"dict\")\n",
    "        # print(\"Text Dict:\", text_dict)\n",
    "\n",
    "        for block in text_dict[\"blocks\"]:\n",
    "            # print(\"Block:\", block)\n",
    "\n",
    "            for line in block[\"lines\"]:\n",
    "                # print(\"Line:\", line)\n",
    "                for span in line[\"spans\"]:\n",
    "                    text = span[\"text\"].strip()\n",
    "                    if text:\n",
    "                        print(\"Span: \", span) # this has index entry\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "    \n",
    "        continue\n",
    "        blocks.sort(key=lambda b: (b[0], b[1]))  # sort top-down, then left-right\n",
    "        print(\"Page number:\", counter)\n",
    "        counter += 1\n",
    "        for block in blocks:\n",
    "            text = block[4].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            print(block)\n",
    "            # Attempt to extract page numbers from end of line\n",
    "            \n",
    "            \n",
    "            for match in pattern.finditer(text):\n",
    "                term = match.group(1).strip()\n",
    "                pages = [int(p) for p in re.findall(r\"\\d+\", match.group(2))]\n",
    "            \n",
    "                is_subentry = block[0] > 50  # x-coordinate threshold (tweak as needed)\n",
    "                if is_subentry and current_main_term:\n",
    "                    subentry = f\"{current_main_term}!{term}\"\n",
    "                    index[subentry].extend(pages)\n",
    "                else:\n",
    "                    current_main_term = term\n",
    "                    index[term].extend(pages)\n",
    "                   \n",
    "            \n",
    "            # match = re.match(pattern, text)\n",
    "            # if match:\n",
    "            #     entry, pages = match.groups()\n",
    "            #     page_numbers = [int(p) for p in re.findall(r\"\\d+\", pages)]\n",
    "\n",
    "            #     is_subentry = block[0] > 50  # x-coordinate threshold (tweak as needed)\n",
    "\n",
    "            #     if is_subentry and current_main_term:\n",
    "            #         subentry = f\"{current_main_term}!{entry}\"\n",
    "            #         index[subentry].extend(page_numbers)\n",
    "            #     else:\n",
    "            #         current_main_term = entry\n",
    "            #         index[entry].extend(page_numbers)\n",
    "\n",
    "\n",
    "    return dict(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span:  {'size': 24.787099838256836, 'flags': 20, 'bidi': 0, 'char_flags': 16, 'font': 'RfrbdtDgcwxnCMBX12', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Index', 'origin': (79.93990325927734, 106.10400390625), 'bbox': (79.93990325927734, 86.73210144042969, 148.72410583496094, 111.51920318603516)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'A/B testing, 86', 'origin': (79.93990325927734, 169.864990234375), 'bbox': (79.93990325927734, 162.0789031982422, 148.16378784179688, 172.04150390625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Aaron Schwartz case, 68', 'origin': (79.93990325927734, 181.82000732421875), 'bbox': (79.93990325927734, 174.03392028808594, 186.25079345703125, 183.99652099609375)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'AB testing, 137', 'origin': (79.93990325927734, 193.7750244140625), 'bbox': (79.93990325927734, 185.9889373779297, 148.16378784179688, 195.9515380859375)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'academic data, 66', 'origin': (79.93990325927734, 205.73001098632812), 'bbox': (79.93990325927734, 197.9439239501953, 158.48504638671875, 207.90652465820312)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'accuracy, 215, 228', 'origin': (79.93990325927734, 217.68502807617188), 'bbox': (79.93990325927734, 209.89894104003906, 159.06283569335938, 219.86154174804688)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'activation function, 380', 'origin': (79.93990325927734, 229.64102172851562), 'bbox': (79.93990325927734, 221.8549346923828, 182.78378295898438, 231.81753540039062)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'AdaBoost, 364', 'origin': (79.93990325927734, 241.59600830078125), 'bbox': (79.93990325927734, 233.80992126464844, 144.029296875, 243.77252197265625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'add-one discounting, 357', 'origin': (79.93990325927734, 253.551025390625), 'bbox': (79.93990325927734, 245.7649383544922, 188.11376953125, 255.7275390625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'agglomerative cluster trees, 338', 'origin': (79.93990325927734, 265.5060119628906), 'bbox': (79.93990325927734, 257.7199401855469, 217.264404296875, 267.6825256347656)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'aggregation mechanisms, 83', 'origin': (79.93990325927734, 277.4610290527344), 'bbox': (79.93990325927734, 269.6749572753906, 201.48358154296875, 279.6375427246094)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Akaike information criterion, 289,', 'origin': (79.93990325927734, 289.416015625), 'bbox': (79.93990325927734, 281.62994384765625, 226.8184814453125, 291.592529296875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': '335', 'origin': (119.79090118408203, 301.37200927734375), 'bbox': (119.79090118408203, 293.5859375, 134.73477172851562, 303.54852294921875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'algorithm analysis, 397', 'origin': (79.93990325927734, 313.3270263671875), 'bbox': (79.93990325927734, 305.54095458984375, 180.45254516601562, 315.5035400390625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Amazon Turk, 67, 84', 'origin': (79.93990325927734, 325.2820129394531), 'bbox': (79.93990325927734, 317.4959411621094, 172.07400512695312, 327.4585266113281)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'tasks assigned, 85', 'origin': (99.86589813232422, 337.2370300292969), 'bbox': (99.86589813232422, 329.4509582519531, 177.23541259765625, 339.4135437011719)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Turkers, 84', 'origin': (99.86589813232422, 349.1920166015625), 'bbox': (99.86589813232422, 341.40594482421875, 148.91177368164062, 351.3685302734375)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'American basketball players, 97', 'origin': (79.93990325927734, 361.1470031738281), 'bbox': (79.93990325927734, 353.3609313964844, 218.35028076171875, 363.3235168457031)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'analogies, 312', 'origin': (79.93990325927734, 373.10302734375), 'bbox': (79.93990325927734, 365.31695556640625, 140.28335571289062, 375.279541015625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'anchoring, 82', 'origin': (79.93990325927734, 385.0580139160156), 'bbox': (79.93990325927734, 377.2719421386719, 138.31076049804688, 387.2345275878906)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'angular distance, 310', 'origin': (79.93990325927734, 397.0130310058594), 'bbox': (79.93990325927734, 389.2269592285156, 172.38284301757812, 399.1895446777344)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Anscombe’s Quartet, 159', 'origin': (79.93990325927734, 408.968017578125), 'bbox': (79.93990325927734, 401.18194580078125, 189.85723876953125, 411.14453125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'AOL, 64', 'origin': (79.93990325927734, 420.9230041503906), 'bbox': (79.93990325927734, 413.1369323730469, 117.15022277832031, 423.0995178222656)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'API, 65', 'origin': (79.93990325927734, 432.8780212402344), 'bbox': (79.93990325927734, 425.0919494628906, 113.82270812988281, 435.0545349121094)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Apple iPhone sales, 34', 'origin': (79.93990325927734, 444.8340148925781), 'bbox': (79.93990325927734, 437.0479431152344, 178.3504638671875, 447.0105285644531)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'application program interfaces, 65', 'origin': (79.93990325927734, 456.78900146484375), 'bbox': (79.93990325927734, 449.0029296875, 228.27301025390625, 458.96551513671875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'area under the ROC curve, 219', 'origin': (79.93990325927734, 468.7440185546875), 'bbox': (79.93990325927734, 460.95794677734375, 216.48727416992188, 470.9205322265625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Aristotle, 326, 327', 'origin': (79.93990325927734, 480.69903564453125), 'bbox': (79.93990325927734, 472.9129638671875, 159.9395751953125, 482.87554931640625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Arrow’s impossibility theorem, 84,', 'origin': (79.93990325927734, 492.6540222167969), 'bbox': (79.93990325927734, 484.8679504394531, 229.74752807617188, 494.8305358886719)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': '114', 'origin': (119.79090118408203, 504.6100158691406), 'bbox': (119.79090118408203, 496.8239440917969, 134.73477172851562, 506.7865295410156)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'artifacts, 69', 'origin': (79.93990325927734, 516.5650024414062), 'bbox': (79.93990325927734, 508.7789306640625, 131.71554565429688, 518.7415161132812)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Ascombe quartet, 272', 'origin': (79.93990325927734, 528.52001953125), 'bbox': (79.93990325927734, 520.7339477539062, 175.1524658203125, 530.696533203125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'asking interesting questions, 4', 'origin': (79.93990325927734, 540.4750366210938), 'bbox': (79.93990325927734, 532.68896484375, 210.93814086914062, 542.6515502929688)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'associativity, 244', 'origin': (79.93990325927734, 552.4300537109375), 'bbox': (79.93990325927734, 544.6439819335938, 153.86239624023438, 554.6065673828125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'autocorrelation, 46', 'origin': (79.93990325927734, 564.385009765625), 'bbox': (79.93990325927734, 556.5989379882812, 162.11138916015625, 566.5615234375)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'average link, 340', 'origin': (269.2298889160156, 169.864990234375), 'bbox': (269.2298889160156, 162.0789031982422, 342.2557373046875, 172.04150390625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Babbage, Charles, 57, 90', 'origin': (269.2298889160156, 192.50601196289062), 'bbox': (269.2298889160156, 184.7199249267578, 377.58306884765625, 194.68252563476562)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'backpropagation, 382', 'origin': (269.2298889160156, 204.50201416015625), 'bbox': (269.2298889160156, 196.71592712402344, 362.1708984375, 206.67852783203125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Bacon, Kevin, 9', 'origin': (269.2298889160156, 216.49801635742188), 'bbox': (269.2298889160156, 208.71192932128906, 339.04779052734375, 218.67453002929688)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'bag of words, 14', 'origin': (269.2298889160156, 228.4940185546875), 'bbox': (269.2298889160156, 220.7079315185547, 340.671630859375, 230.6705322265625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'bagging, 362', 'origin': (269.2298889160156, 240.49002075195312), 'bbox': (269.2298889160156, 232.7039337158203, 323.9942321777344, 242.66653442382812)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'balanced training classes, 295', 'origin': (269.2298889160156, 252.48703002929688), 'bbox': (269.2298889160156, 244.70094299316406, 397.71746826171875, 254.66354370117188)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'bar charts, 179', 'origin': (269.2298889160156, 264.4830017089844), 'bbox': (269.2298889160156, 256.6969299316406, 334.3254699707031, 266.6595153808594)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'best practices, 181', 'origin': (289.1558837890625, 276.47900390625), 'bbox': (289.1558837890625, 268.69293212890625, 369.7532958984375, 278.655517578125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'stacked, 181', 'origin': (289.1558837890625, 288.4750061035156), 'bbox': (289.1558837890625, 280.6889343261719, 342.0273742675781, 290.6515197753906)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Barzun, Jacques, 5', 'origin': (269.2298889160156, 300.47100830078125), 'bbox': (269.2298889160156, 292.6849365234375, 351.4313049316406, 302.64752197265625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'baseball encyclopedia, 5', 'origin': (269.2298889160156, 312.4670104980469), 'bbox': (269.2298889160156, 304.6809387207031, 373.787353515625, 314.6435241699219)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'baseline models, 210', 'origin': (269.2298889160156, 324.4630126953125), 'bbox': (269.2298889160156, 316.67694091796875, 358.1062316894531, 326.6395263671875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'for classiﬁcation, 210', 'origin': (289.1558837890625, 336.4590148925781), 'bbox': (289.1558837890625, 328.6729431152344, 380.2638244628906, 338.6355285644531)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'for value prediction, 212', 'origin': (289.1558837890625, 348.45501708984375), 'bbox': (289.1558837890625, 340.6689453125, 394.8191833496094, 350.63153076171875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Bayes’ theorem, 150, 205, 299', 'origin': (269.2298889160156, 360.4520263671875), 'bbox': (269.2298889160156, 352.66595458984375, 398.88311767578125, 362.6285400390625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'Baysian information criteria, 289', 'origin': (269.2298889160156, 372.4480285644531), 'bbox': (269.2298889160156, 364.6619567871094, 412.7212219238281, 374.6245422363281)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'bell-shaped, 123, 141', 'origin': (269.2298889160156, 384.44403076171875), 'bbox': (269.2298889160156, 376.657958984375, 360.53704833984375, 386.62054443359375)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'distribution, 101', 'origin': (289.1558837890625, 396.44000244140625), 'bbox': (289.1558837890625, 388.6539306640625, 361.105712890625, 398.61651611328125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'bias, 202, 417', 'origin': (269.2298889160156, 408.4360046386719), 'bbox': (269.2298889160156, 400.6499328613281, 328.4774169921875, 410.6125183105469)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'lexicographic, 405', 'origin': (289.1558837890625, 420.4320068359375), 'bbox': (289.1558837890625, 412.64593505859375, 366.8741149902344, 422.6085205078125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'numerical, 405', 'origin': (289.1558837890625, 432.4280090332031), 'bbox': (289.1558837890625, 424.6419372558594, 352.4980773925781, 434.6045227050781)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'temporal, 405', 'origin': (289.1558837890625, 444.42401123046875), 'bbox': (289.1558837890625, 436.637939453125, 349.19049072265625, 446.60052490234375)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'bias–variance trade-oﬀ, 202', 'origin': (269.2298889160156, 456.4210205078125), 'bbox': (269.2298889160156, 448.63494873046875, 388.2430725097656, 458.5975341796875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'big data, 391', 'origin': (269.2298889160156, 468.4170227050781), 'bbox': (269.2298889160156, 460.6309509277344, 326.2059326171875, 470.5935363769531)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'algorithms, 397', 'origin': (289.1558837890625, 480.41302490234375), 'bbox': (289.1558837890625, 472.626953125, 356.1543273925781, 482.58953857421875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'bad data, 392', 'origin': (289.1558837890625, 492.40899658203125), 'bbox': (289.1558837890625, 484.6229248046875, 348.9014892578125, 494.58551025390625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'statistics, 392', 'origin': (289.1558837890625, 504.405029296875), 'bbox': (289.1558837890625, 496.61895751953125, 348.47314453125, 506.58154296875)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'big data engineer, 4', 'origin': (269.2298889160156, 516.4010009765625), 'bbox': (269.2298889160156, 508.61492919921875, 355.52587890625, 518.5775146484375)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'big oh analysis, 397', 'origin': (269.2298889160156, 528.3970336914062), 'bbox': (269.2298889160156, 520.6109619140625, 354.78863525390625, 530.5735473632812)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'binary relations, 320', 'origin': (269.2298889160156, 540.3930053710938), 'bbox': (269.2298889160156, 532.60693359375, 358.6441955566406, 542.5695190429688)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'binary search, 398', 'origin': (269.2298889160156, 552.3890380859375), 'bbox': (269.2298889160156, 544.6029663085938, 348.4225769042969, 554.5655517578125)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': 'binomial distribution, 123', 'origin': (269.2298889160156, 564.385986328125), 'bbox': (269.2298889160156, 556.5999145507812, 382.0960998535156, 566.5625)}\n",
      "Span:  {'size': 9.962599754333496, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'XgmrvdCfsxnjCMR10', 'color': 0, 'alpha': 255, 'ascender': 0.6940000057220459, 'descender': -0.1940000057220459, 'text': '435', 'origin': (244.32388305664062, 589.5859985351562), 'bbox': (244.32388305664062, 581.7999267578125, 259.26776123046875, 591.7625122070312)}\n",
      "Span:  {'size': 8.5, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'VlnhvhLtwdmbTimes-Roman', 'color': 0, 'alpha': 255, 'ascender': 0.8980000019073486, 'descender': -0.21799999475479126, 'text': '© The Author(s) 2017', 'origin': (79.85179901123047, 600.5382080078125), 'bbox': (79.85179901123047, 592.9052124023438, 155.95230102539062, 602.3912353515625)}\n",
      "Span:  {'size': 8.5, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'VlnhvhLtwdmbTimes-Roman', 'color': 0, 'alpha': 255, 'ascender': 0.8980000019073486, 'descender': -0.21799999475479126, 'text': 'S.S. Skiena, ', 'origin': (79.85179901123047, 610.5342407226562), 'bbox': (79.85179901123047, 602.9012451171875, 123.0657958984375, 612.3872680664062)}\n",
      "Span:  {'size': 8.5, 'flags': 6, 'bidi': 0, 'char_flags': 16, 'font': 'GfmtntCmwftnTimes-Italic', 'color': 0, 'alpha': 255, 'ascender': 0.8830000162124634, 'descender': -0.21699999272823334, 'text': 'The Data Science Design Manual,', 'origin': (123.0657958984375, 610.5342407226562), 'bbox': (123.0657958984375, 603.0287475585938, 239.923828125, 612.3787231445312)}\n",
      "Span:  {'size': 8.5, 'flags': 4, 'bidi': 0, 'char_flags': 16, 'font': 'KhjprkTcgxppTimes-Roman', 'color': 0, 'alpha': 255, 'ascender': 0.8980000019073486, 'descender': -0.21799999475479126, 'text': 'Texts in Computer Science, https://doi.org/10.1007/978-3-319-55444-0', 'origin': (79.85179901123047, 620.3602294921875), 'bbox': (79.85179901123047, 612.7272338867188, 323.8741760253906, 622.2132568359375)}\n"
     ]
    }
   ],
   "source": [
    "new_index_entries = extract_index_entries(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A/B testing': [86],\n",
       " 'A/B testing!Aaron Schwartz case': [68],\n",
       " 'A/B testing!AB testing': [137],\n",
       " 'A/B testing!academic data': [66],\n",
       " 'A/B testing!accuracy': [215, 228],\n",
       " 'A/B testing!activation function': [380],\n",
       " 'A/B testing!AdaBoost': [364],\n",
       " 'A/B testing!add-one discounting': [357],\n",
       " 'A/B testing!agglomerative cluster trees': [338],\n",
       " 'A/B testing!aggregation mechanisms': [83],\n",
       " 'A/B testing!Akaike information criterion': [289],\n",
       " 'A/B testing!average link': [340],\n",
       " 'A/B testing!Babbage, Charles': [57, 90],\n",
       " 'A/B testing!backpropagation': [382],\n",
       " 'A/B testing!Bacon, Kevin': [9],\n",
       " 'A/B testing!bag of words': [14],\n",
       " 'A/B testing!bagging': [362],\n",
       " 'A/B testing!balanced training classes': [295],\n",
       " 'A/B testing!bar charts': [179, 179],\n",
       " 'A/B testing!best practices': [181, 186, 175, 181, 177],\n",
       " 'A/B testing!stacked': [181],\n",
       " 'A/B testing!Barzun, Jacques': [5],\n",
       " 'A/B testing!baseball encyclopedia': [5],\n",
       " 'A/B testing!baseline models': [210],\n",
       " 'A/B testing!algorithm analysis': [397],\n",
       " 'A/B testing!Amazon Turk': [67, 84],\n",
       " 'A/B testing!for classiﬁcation': [210, 290],\n",
       " 'A/B testing!for value prediction': [212],\n",
       " 'A/B testing!Bayes’ theorem': [150, 205, 299],\n",
       " 'A/B testing!Baysian information criteria': [289],\n",
       " 'A/B testing!bell-shaped': [123, 141],\n",
       " 'A/B testing!tasks assigned': [85],\n",
       " 'A/B testing!Turkers': [84],\n",
       " 'A/B testing!American basketball players': [97],\n",
       " 'A/B testing!analogies': [312],\n",
       " 'A/B testing!anchoring': [82],\n",
       " 'A/B testing!angular distance': [310],\n",
       " 'A/B testing!Anscombe’s Quartet': [159],\n",
       " 'A/B testing!AOL': [64],\n",
       " 'A/B testing!API': [65],\n",
       " 'A/B testing!Apple iPhone sales': [34],\n",
       " 'A/B testing!application program interfaces': [65],\n",
       " 'A/B testing!area under the ROC curve': [219],\n",
       " 'A/B testing!Aristotle': [326, 327],\n",
       " 'A/B testing!Arrow’s impossibility theorem': [84],\n",
       " 'A/B testing!distribution': [101],\n",
       " 'A/B testing!bias': [202, 417],\n",
       " 'A/B testing!lexicographic': [405],\n",
       " 'A/B testing!numerical': [405],\n",
       " 'A/B testing!temporal': [405],\n",
       " 'A/B testing!bias–variance trade-oﬀ': [202],\n",
       " 'A/B testing!big data': [391],\n",
       " 'A/B testing!algorithms': [397, 364, 375],\n",
       " 'A/B testing!bad data': [392],\n",
       " 'A/B testing!statistics': [392, 221, 214, 29],\n",
       " 'A/B testing!big data engineer': [4],\n",
       " 'A/B testing!big oh analysis': [397],\n",
       " 'A/B testing!binary relations': [320],\n",
       " 'A/B testing!binary search': [398],\n",
       " 'A/B testing!binomial distribution': [123],\n",
       " 'A/B testing!artifacts': [69],\n",
       " 'A/B testing!Ascombe quartet': [272],\n",
       " 'A/B testing!asking interesting questions': [4],\n",
       " 'A/B testing!associativity': [244],\n",
       " 'A/B testing!autocorrelation': [46],\n",
       " 'A/B testing!bioinformatician': [50],\n",
       " 'A/B testing!Blumenstock, Josh': [27],\n",
       " 'A/B testing!Body Mass Index': [96, 177],\n",
       " 'A/B testing!Bonferroni correction': [141],\n",
       " 'A/B testing!boosting': [363, 364],\n",
       " 'A/B testing!evaluating': [213, 219, 212, 221],\n",
       " 'A/B testing!one-vs.-all': [298],\n",
       " 'A/B testing!perfect': [218],\n",
       " 'A/B testing!Clinton, Bill': [326],\n",
       " 'A/B testing!Clinton, Bill]': [326],\n",
       " 'A/B testing!closest pair of points': [398],\n",
       " 'A/B testing!cloud computing services': [410],\n",
       " 'A/B testing!bootstrapping': [374],\n",
       " 'A/B testing!Borda’s method': [108],\n",
       " 'A/B testing!box plots': [175],\n",
       " 'A/B testing!Box, George': [201],\n",
       " 'A/B testing!box-and-whisker plots': [176],\n",
       " 'A/B testing!bubble plots': [179],\n",
       " 'A/B testing!bupkis': [391],\n",
       " 'A/B testing!Bush, George W.': [326],\n",
       " 'A/B testing!conductance': [343],\n",
       " 'A/B testing!distance': [337],\n",
       " 'A/B testing!clustering': [327, 373],\n",
       " 'A/B testing!agglomerative': [336, 337],\n",
       " 'A/B testing!applications': [328, 399, 244],\n",
       " 'A/B testing!biological': [337],\n",
       " 'A/B testing!cut-based': [341],\n",
       " 'A/B testing!k-means': [330],\n",
       " 'A/B testing!single link': [339],\n",
       " 'A/B testing!visualization of': [337],\n",
       " 'A/B testing!C–language': [59],\n",
       " 'A/B testing!cache memory': [401],\n",
       " 'A/B testing!canonical representation': [400],\n",
       " 'A/B testing!canonization': [400],\n",
       " 'A/B testing!CAPTCHAs': [89],\n",
       " 'A/B testing!Carroll, Lewis': [423],\n",
       " 'A/B testing!cartograms': [189],\n",
       " 'A/B testing!Center for Disease Control': [204],\n",
       " 'A/B testing!center of mass': [332],\n",
       " 'A/B testing!centrality measures': [34],\n",
       " 'A/B testing!centroids': [331],\n",
       " 'A/B testing!character code uniﬁcation': [74],\n",
       " 'A/B testing!characteristic equation': [256],\n",
       " 'A/B testing!characterizing distributions': [39],\n",
       " 'A/B testing!chart types': [170, 170],\n",
       " 'A/B testing!number of': [333],\n",
       " 'A/B testing!organization of': [337],\n",
       " 'A/B testing!Clyde': [111, 112],\n",
       " 'A/B testing!coeﬃcient vector': [270],\n",
       " 'A/B testing!Cohen’s d': [136],\n",
       " 'A/B testing!collaborative ﬁltering': [9],\n",
       " 'A/B testing!communication': [408, 416],\n",
       " 'A/B testing!commutativity': [243],\n",
       " 'A/B testing!company data': [64],\n",
       " 'A/B testing!computer scientist': [2],\n",
       " 'A/B testing!conditional probability': [30, 31],\n",
       " 'A/B testing!Condorcet jury theorem': [84],\n",
       " 'A/B testing!confusion matrix': [214, 220],\n",
       " 'A/B testing!connected components': [324],\n",
       " 'A/B testing!contingency table': [214],\n",
       " 'A/B testing!convex': [280, 280],\n",
       " 'A/B testing!convex hull': [368],\n",
       " 'A/B testing!coordination': [408],\n",
       " 'A/B testing!data maps': [187],\n",
       " 'A/B testing!dot and line plots': [174],\n",
       " 'A/B testing!histograms': [183, 32, 183, 222],\n",
       " 'A/B testing!pie charts': [179, 179],\n",
       " 'A/B testing!scatter plots': [177, 98, 177],\n",
       " 'A/B testing!tabular data': [170],\n",
       " 'A/B testing!cicada': [46],\n",
       " 'A/B testing!classiﬁcation': [16, 210, 289],\n",
       " 'A/B testing!analysis': [40, 260, 21],\n",
       " 'A/B testing!interpretation': [43, 238, 37],\n",
       " 'A/B testing!signiﬁcance': [45],\n",
       " 'A/B testing!correlation and causation': [45, 135],\n",
       " 'A/B testing!cosine similarity': [309],\n",
       " 'A/B testing!cross validation': [227],\n",
       " 'A/B testing!binary': [290, 314],\n",
       " 'A/B testing!multi-class': [297],\n",
       " 'A/B testing!regression': [290, 16],\n",
       " 'A/B testing!advantages': [227, 358, 174, 311],\n",
       " 'A/B testing!CrowdFlower': [67, 84, 86],\n",
       " 'A/B testing!balanced': [217],\n",
       " 'A/B testing!crowdsourcing': [67, 80],\n",
       " 'A/B testing!data sources': [64],\n",
       " 'A/B testing!data visualization': [155],\n",
       " 'A/B testing!data-driven': [156, 207],\n",
       " 'A/B testing!bad uses': [87],\n",
       " 'A/B testing!crowdsourcing services': [84],\n",
       " 'A/B testing!cryptographic hashing': [400],\n",
       " 'A/B testing!CSV ﬁles': [62],\n",
       " 'A/B testing!cumulative density function': [33],\n",
       " 'A/B testing!models': [207, 209, 210, 208],\n",
       " 'A/B testing!de M´er´e, Chevalier': [29],\n",
       " 'A/B testing!decision boundaries': [291],\n",
       " 'A/B testing!decision tree classiﬁers': [299, 357],\n",
       " 'A/B testing!decision trees': [357],\n",
       " 'A/B testing!132': [186, 186],\n",
       " 'A/B testing!currency conversion': [75],\n",
       " 'A/B testing!cut': [343],\n",
       " 'A/B testing!construction': [359],\n",
       " 'A/B testing!ensembles of': [362],\n",
       " 'A/B testing!deep learning': [202, 352, 377, 209],\n",
       " 'A/B testing!damping factor': [325],\n",
       " 'A/B testing!Darwin, Charles': [81],\n",
       " 'A/B testing!data': [237, 391, 394],\n",
       " 'A/B testing!network': [378],\n",
       " 'A/B testing!DeepWalk': [385],\n",
       " 'A/B testing!degree of vertex': [325],\n",
       " 'A/B testing!depth': [378, 379],\n",
       " 'A/B testing!derivative': [281],\n",
       " 'A/B testing!quantitative vs. categorical': [15],\n",
       " 'A/B testing!big vs. little': [15],\n",
       " 'A/B testing!cleaning': [69],\n",
       " 'A/B testing!collecting': [64],\n",
       " 'A/B testing!compatibility': [72],\n",
       " 'A/B testing!errors': [69],\n",
       " 'A/B testing!for evaluation': [225],\n",
       " 'A/B testing!for testing': [225],\n",
       " 'A/B testing!for training': [225],\n",
       " 'A/B testing!logging': [68],\n",
       " 'A/B testing!properties': [14],\n",
       " 'A/B testing!scraping': [67],\n",
       " 'A/B testing!structured': [14],\n",
       " 'A/B testing!types': [14],\n",
       " 'A/B testing!unstructured': [14],\n",
       " 'A/B testing!visualizing': [155, 241],\n",
       " 'A/B testing!data analysis': [404],\n",
       " 'A/B testing!data centrism': [2],\n",
       " 'A/B testing!data cleaning': [376],\n",
       " 'A/B testing!data errors': [417],\n",
       " 'A/B testing!data formats': [61],\n",
       " 'A/B testing!partial': [282],\n",
       " 'A/B testing!second': [281],\n",
       " 'A/B testing!descriptive statistics': [34],\n",
       " 'A/B testing!developing scoring systems': [99],\n",
       " 'A/B testing!dictionary maintenance': [399],\n",
       " 'A/B testing!DiMaggio': [148],\n",
       " 'A/B testing!dimension reduction': [277, 376],\n",
       " 'A/B testing!dimensional egalitarianism': [308],\n",
       " 'A/B testing!dimensions': [384],\n",
       " 'A/B testing!dinosaur vertebra': [78],\n",
       " 'A/B testing!directed acyclic graph': [110],\n",
       " 'A/B testing!directed graph': [321],\n",
       " 'A/B testing!discounting': [209, 356],\n",
       " 'A/B testing!disk storage': [401],\n",
       " 'A/B testing!distance methods': [303],\n",
       " 'A/B testing!distance metrics': [304],\n",
       " 'A/B testing!evaluation': [225],\n",
       " 'A/B testing!data munging': [57],\n",
       " 'A/B testing!data parallelism': [409],\n",
       " 'A/B testing!data partition': [404],\n",
       " 'A/B testing!data processing': [10],\n",
       " 'A/B testing!data reduction': [329],\n",
       " 'A/B testing!data science': [1, 210],\n",
       " 'A/B testing!Lk': [305],\n",
       " 'A/B testing!euclidean': [305],\n",
       " 'A/B testing!manhattan distance': [305],\n",
       " 'A/B testing!maximum component': [305],\n",
       " 'A/B testing!distances': [319],\n",
       " 'A/B testing!measuring': [303],\n",
       " 'A/B testing!distributed ﬁle system': [396],\n",
       " 'A/B testing!distributed processing': [407],\n",
       " 'A/B testing!divide and conquer': [411],\n",
       " 'A/B testing!DNA sequences': [402],\n",
       " 'A/B testing!languages': [57],\n",
       " 'A/B testing!data science television': [17],\n",
       " 'A/B testing!data scientist': [2],\n",
       " 'A/B testing!dot product': [241],\n",
       " 'A/B testing!duality': [268],\n",
       " 'A/B testing!duplicate removal': [400],\n",
       " 'A/B testing!experiment': [27],\n",
       " 'A/B testing!exploratory data analysis': [155, 156],\n",
       " 'A/B testing!visualization': [160, 404],\n",
       " 'A/B testing!E-step': [335],\n",
       " 'A/B testing!edge cuts': [324],\n",
       " 'A/B testing!edges': [319],\n",
       " 'A/B testing!eﬀect size': [136],\n",
       " 'A/B testing!eigenvalues': [255, 255],\n",
       " 'A/B testing!F-score': [216],\n",
       " 'A/B testing!Facebook': [21],\n",
       " 'A/B testing!false negatives': [214],\n",
       " 'A/B testing!false positives': [214],\n",
       " 'A/B testing!fast Fourier transform': [47],\n",
       " 'A/B testing!fault tolerance': [408],\n",
       " 'A/B testing!feature engineering': [375],\n",
       " 'A/B testing!feature scaling': [274],\n",
       " 'A/B testing!computation': [256],\n",
       " 'A/B testing!decomposition': [257],\n",
       " 'A/B testing!properties of': [255],\n",
       " 'A/B testing!Elizabeth II': [326],\n",
       " 'A/B testing!Elo rankings': [104],\n",
       " 'A/B testing!embedded graph': [323],\n",
       " 'A/B testing!embedding': [321],\n",
       " 'A/B testing!Emoji Dick': [86],\n",
       " 'A/B testing!Engels, Friedrich': [391],\n",
       " 'A/B testing!ensemble learning': [363],\n",
       " 'A/B testing!entropy': [310],\n",
       " 'A/B testing!sublinear': [275, 276],\n",
       " 'A/B testing!z-scores': [275],\n",
       " 'A/B testing!highly-correlated': [277],\n",
       " 'A/B testing!Fermat, Pierre de': [30],\n",
       " 'A/B testing!Feynman, Richard': [229],\n",
       " 'A/B testing!FFT': [47],\n",
       " 'A/B testing!ﬁltering': [403],\n",
       " 'A/B testing!ﬁnancial market': [126],\n",
       " 'A/B testing!ﬁnancial uniﬁcation': [75],\n",
       " 'A/B testing!ﬁt and complexity': [288],\n",
       " 'A/B testing!FoldIt': [89],\n",
       " 'A/B testing!football': [111],\n",
       " 'A/B testing!determined': [251],\n",
       " 'A/B testing!underdetermined': [251, 256],\n",
       " 'A/B testing!error': [202, 221, 269],\n",
       " 'A/B testing!absolute': [221],\n",
       " 'A/B testing!detection': [155, 78],\n",
       " 'A/B testing!mean squared': [223, 331],\n",
       " 'A/B testing!relative': [222],\n",
       " 'A/B testing!residual': [269],\n",
       " 'A/B testing!root mean squared': [223],\n",
       " 'A/B testing!squared': [222],\n",
       " 'A/B testing!errors vs. artifacts': [69],\n",
       " 'A/B testing!ethical implications': [416],\n",
       " 'A/B testing!Euclidean metric': [303],\n",
       " 'A/B testing!American players': [97],\n",
       " 'A/B testing!game prediction': [111],\n",
       " 'A/B testing!time series': [212],\n",
       " 'A/B testing!formulation': [354],\n",
       " 'A/B testing!Freedom of Information Act': [12, 65],\n",
       " 'A/B testing!frequency counting': [400],\n",
       " 'A/B testing!frequency distributions': [184],\n",
       " 'A/B testing!furthest link': [340],\n",
       " 'A/B testing!Galton, Francis': [81],\n",
       " 'A/B testing!games with a purpose': [88],\n",
       " 'A/B testing!gamiﬁcation': [88],\n",
       " 'A/B testing!garbage in, garbage out': [3, 69],\n",
       " 'A/B testing!Gates, Bill': [130],\n",
       " 'A/B testing!environments': [224],\n",
       " 'A/B testing!event': [28],\n",
       " 'A/B testing!Excel': [59],\n",
       " 'A/B testing!exclusive or': [361],\n",
       " 'A/B testing!exercises': [23, 53, 90, 119, 151, 199],\n",
       " 'A/B testing!elimination': [250],\n",
       " 'A/B testing!Gaussian distribution': [124],\n",
       " 'A/B testing!Gaussian noise': [125],\n",
       " 'A/B testing!General Sentiment': [20],\n",
       " 'A/B testing!genius': [19],\n",
       " 'A/B testing!234': [263, 301, 346, 388, 419],\n",
       " 'A/B testing!expectation maximization': [335],\n",
       " 'A/B testing!expected value': [28],\n",
       " 'A/B testing!geometric mean': [35],\n",
       " 'A/B testing!geometric point sets': [238],\n",
       " 'A/B testing!geometry': [240],\n",
       " 'A/B testing!Gini impurity': [360],\n",
       " 'A/B testing!Global Positioning System': [12],\n",
       " 'A/B testing!gold standards': [99],\n",
       " 'A/B testing!good scoring functions': [101],\n",
       " 'A/B testing!Goodhart’s law': [303],\n",
       " 'A/B testing!Goodhart, Charles': [303],\n",
       " 'A/B testing!hedge fund': [1],\n",
       " 'A/B testing!hierarchy': [298],\n",
       " 'A/B testing!higher dimensions': [307, 370],\n",
       " 'A/B testing!bin size': [184],\n",
       " 'A/B testing!Hitler, Adolf': [326],\n",
       " 'A/B testing!HTML': [67],\n",
       " 'A/B testing!hypothesis development': [328],\n",
       " 'A/B testing!hypothesis driven': [156, 392],\n",
       " 'A/B testing!AlphaGo': [372],\n",
       " 'A/B testing!TensorFlow': [377],\n",
       " 'A/B testing!Google Flu Trends': [394],\n",
       " 'A/B testing!Google News': [219],\n",
       " 'A/B testing!Google Ngrams': [10],\n",
       " 'A/B testing!Google Scholar': [66],\n",
       " 'A/B testing!government data': [65],\n",
       " 'A/B testing!Imagenet': [379],\n",
       " 'A/B testing!IMDb': [7],\n",
       " 'A/B testing!by interpolation': [78],\n",
       " 'A/B testing!by mean value': [77],\n",
       " 'A/B testing!by nearest neighbor': [78],\n",
       " 'A/B testing!by random value': [77],\n",
       " 'A/B testing!heuristic-based': [77],\n",
       " 'A/B testing!independence': [30, 123, 354],\n",
       " 'A/B testing!inﬂation rates': [76],\n",
       " 'A/B testing!information gain': [360],\n",
       " 'A/B testing!information theoretic entropy': [360],\n",
       " 'A/B testing!infrastructure': [396],\n",
       " 'A/B testing!inner product': [243],\n",
       " 'A/B testing!institutional review board': [88],\n",
       " 'A/B testing!Internet Movie Database': [7],\n",
       " 'A/B testing!Internet of Things': [68],\n",
       " 'A/B testing!inverse transform sampling': [132],\n",
       " 'A/B testing!IPython': [61],\n",
       " 'A/B testing!IQ testing': [89],\n",
       " 'A/B testing!359': [366],\n",
       " 'A/B testing!gradient descent search': [281],\n",
       " 'A/B testing!graph embeddings': [384],\n",
       " 'A/B testing!graph theory': [323],\n",
       " 'A/B testing!graphs': [238, 319, 321],\n",
       " 'A/B testing!cuts': [342],\n",
       " 'A/B testing!dense': [322],\n",
       " 'A/B testing!directed': [321],\n",
       " 'A/B testing!embedded': [323],\n",
       " 'A/B testing!labeled': [323],\n",
       " 'A/B testing!non-simple': [322],\n",
       " 'A/B testing!simple': [322],\n",
       " 'A/B testing!sparse': [322],\n",
       " 'A/B testing!topological': [323],\n",
       " 'A/B testing!undirected': [321],\n",
       " 'A/B testing!unlabeled': [323],\n",
       " 'A/B testing!unweighted': [322],\n",
       " 'A/B testing!weighted': [320, 322],\n",
       " 'A/B testing!Gray, Dorian': [251],\n",
       " 'A/B testing!grid indexes': [315],\n",
       " 'A/B testing!grid search': [409],\n",
       " 'A/B testing!Jaccard distance': [341],\n",
       " 'A/B testing!Jaccard similarity': [341],\n",
       " 'A/B testing!Jackson, Michael [136]': [262],\n",
       " 'A/B testing!Java': [59],\n",
       " 'A/B testing!Jesus': [326],\n",
       " 'A/B testing!JSON': [63],\n",
       " 'A/B testing!k-means clustering': [343, 409],\n",
       " 'A/B testing!k-mediods algorithm': [332],\n",
       " 'A/B testing!k-nearest neighbors': [313],\n",
       " 'A/B testing!kd-trees': [316],\n",
       " 'A/B testing!kernels': [371],\n",
       " 'A/B testing!Kolmogorov-Smirnov test': [139],\n",
       " 'A/B testing!Hadoop distributed ﬁle system': [414],\n",
       " 'A/B testing!Hamming, Richard W.': [1],\n",
       " 'A/B testing!hash functions': [399],\n",
       " 'A/B testing!hashing': [399],\n",
       " 'A/B testing!heatmaps': [178],\n",
       " 'A/B testing!Kruskal’s algorithm': [339],\n",
       " 'A/B testing!Kullback-Leibler divergence': [311],\n",
       " 'A/B testing!classiﬁers': [85],\n",
       " 'A/B testing!main memory': [401],\n",
       " 'A/B testing!major league baseball': [6],\n",
       " 'A/B testing!MapReduce': [407, 410],\n",
       " 'A/B testing!labeled graphs': [323],\n",
       " 'A/B testing!Lang, Andrew': [267],\n",
       " 'A/B testing!Laplace': [356],\n",
       " 'A/B testing!Laplacian': [343],\n",
       " 'A/B testing!large-scale question': [9],\n",
       " 'A/B testing!latent Dirichlet allocation': [373],\n",
       " 'A/B testing!learning rate': [283, 383],\n",
       " 'A/B testing!learning to rank': [119],\n",
       " 'A/B testing!least squares regression': [270],\n",
       " 'A/B testing!lie factor': [164, 162, 164],\n",
       " 'A/B testing!Lincoln, Abraham': [242],\n",
       " 'A/B testing!line charts': [174],\n",
       " 'A/B testing!programming': [412],\n",
       " 'A/B testing!MapReduce runtime system': [415],\n",
       " 'A/B testing!matchings': [324],\n",
       " 'A/B testing!Mathematica': [59, 61],\n",
       " 'A/B testing!Matlab': [58],\n",
       " 'A/B testing!matrix': [14, 237, 270, 373],\n",
       " 'A/B testing!addition': [242],\n",
       " 'A/B testing!adjacency': [246, 320],\n",
       " 'A/B testing!covariance': [245, 257, 271],\n",
       " 'A/B testing!determinant': [249, 254],\n",
       " 'A/B testing!eigenvectors': [255],\n",
       " 'A/B testing!factoring': [252],\n",
       " 'A/B testing!identity': [246, 248, 304],\n",
       " 'A/B testing!inversion': [248],\n",
       " 'A/B testing!linear combinations of': [242],\n",
       " 'A/B testing!multiplication': [243, 242],\n",
       " 'A/B testing!multiplicative inverse of': [249],\n",
       " 'A/B testing!non-singular': [249],\n",
       " 'A/B testing!permutation': [247, 246],\n",
       " 'A/B testing!rank': [251],\n",
       " 'A/B testing!reasons for factoring': [252],\n",
       " 'A/B testing!rotation': [248],\n",
       " 'A/B testing!singular': [249],\n",
       " 'A/B testing!transpose of': [242],\n",
       " 'A/B testing!triangular': [254],\n",
       " 'A/B testing!matrix multiplication': [243, 398],\n",
       " 'A/B testing!line hatchings': [177],\n",
       " 'A/B testing!linear algebra': [237],\n",
       " 'A/B testing!power of': [237],\n",
       " 'A/B testing!linear equation': [238],\n",
       " 'A/B testing!linear programming': [369],\n",
       " 'A/B testing!linear regression': [212, 267],\n",
       " 'A/B testing!solving': [270],\n",
       " 'A/B testing!linear support vector machines': [369],\n",
       " 'A/B testing!linear systems': [250],\n",
       " 'A/B testing!Linnaeus, Carl': [326],\n",
       " 'A/B testing!live data': [395],\n",
       " 'A/B testing!locality': [401],\n",
       " 'A/B testing!locality sensitive hashing': [317],\n",
       " 'A/B testing!logarithm': [47],\n",
       " 'A/B testing!issues': [295],\n",
       " 'A/B testing!logistic function': [381],\n",
       " 'A/B testing!logistic regression': [366],\n",
       " 'A/B testing!logit': [381],\n",
       " 'A/B testing!logit function': [106, 292],\n",
       " 'A/B testing!loss function': [280, 294],\n",
       " 'A/B testing!LU decomposition': [254],\n",
       " 'A/B testing!lumpers': [329],\n",
       " 'A/B testing!maximum margin separator': [367],\n",
       " 'A/B testing!mean': [34, 83, 125, 132, 138, 212],\n",
       " 'A/B testing!227': [403],\n",
       " 'A/B testing!arithmetic': [35],\n",
       " 'A/B testing!geometric': [35],\n",
       " 'A/B testing!measurement error': [125],\n",
       " 'A/B testing!median': [35, 83, 132, 212, 403],\n",
       " 'A/B testing!mergesort': [398],\n",
       " 'A/B testing!metadata': [7],\n",
       " 'A/B testing!M-step': [335],\n",
       " 'A/B testing!machine learning': [351, 208],\n",
       " 'A/B testing!method centrism': [2],\n",
       " 'A/B testing!metric': [304],\n",
       " 'A/B testing!sampling': [134, 132, 403, 404],\n",
       " 'A/B testing!simulations': [229],\n",
       " 'A/B testing!Mosteller, Frederick': [121],\n",
       " 'A/B testing!positivity': [304],\n",
       " 'A/B testing!symmetry': [304],\n",
       " 'A/B testing!triangle inequality': [304],\n",
       " 'A/B testing!multiedge': [322],\n",
       " 'A/B testing!multinomial regression': [299],\n",
       " 'A/B testing!multiplying probabilities': [48],\n",
       " 'A/B testing!global': [284],\n",
       " 'A/B testing!local': [284, 284],\n",
       " 'A/B testing!minimum spanning tree': [324, 339],\n",
       " 'A/B testing!missing values': [76, 376],\n",
       " 'A/B testing!mixture model': [333],\n",
       " 'A/B testing!Moby Dick': [86],\n",
       " 'A/B testing!mode': [36],\n",
       " 'A/B testing!model-driven': [417],\n",
       " 'A/B testing!modeling': [201, 328, 416],\n",
       " 'A/B testing!naive Bayes': [354, 363],\n",
       " 'A/B testing!name uniﬁcation': [74],\n",
       " 'A/B testing!Napoleon': [326],\n",
       " 'A/B testing!NASA': [73],\n",
       " 'A/B testing!National Football League': [112],\n",
       " 'A/B testing!natural language processing': [20],\n",
       " 'A/B testing!nearest centroid': [340],\n",
       " 'A/B testing!nearest neighbor': [339, 397],\n",
       " 'A/B testing!nearest neighbor classiﬁcation': [311],\n",
       " 'A/B testing!philosophies of': [201],\n",
       " 'A/B testing!principles for eﬀectiveness': [203],\n",
       " 'A/B testing!ad hoc': [208],\n",
       " 'A/B testing!baseline': [210],\n",
       " 'A/B testing!blackbox': [206],\n",
       " 'A/B testing!descriptive': [206],\n",
       " 'A/B testing!deterministic': [208],\n",
       " 'A/B testing!ﬁrst-principle': [207],\n",
       " 'A/B testing!ﬂat': [209],\n",
       " 'A/B testing!Google’s forecasting': [204],\n",
       " 'A/B testing!hierarchical': [209],\n",
       " 'A/B testing!linear': [206],\n",
       " 'A/B testing!live': [204],\n",
       " 'A/B testing!neural network': [206],\n",
       " 'A/B testing!non-linear': [206],\n",
       " 'A/B testing!overﬁt': [203],\n",
       " 'A/B testing!simplifying': [286],\n",
       " 'A/B testing!simulation': [229],\n",
       " 'A/B testing!stochastic': [208],\n",
       " 'A/B testing!taxonomy of': [205],\n",
       " 'A/B testing!underﬁt': [202],\n",
       " 'A/B testing!Moneyball': [5],\n",
       " 'A/B testing!monkey': [215],\n",
       " 'A/B testing!monotonic': [307],\n",
       " 'A/B testing!ﬁnding': [315],\n",
       " 'A/B testing!negative class': [213],\n",
       " 'A/B testing!Netﬂix prize': [9],\n",
       " 'A/B testing!network methods': [303],\n",
       " 'A/B testing!networks': [109, 238, 319, 378],\n",
       " 'A/B testing!induced': [320],\n",
       " 'A/B testing!learning': [379],\n",
       " 'A/B testing!neural networks': [377],\n",
       " 'A/B testing!new data set': [156],\n",
       " 'A/B testing!New York': [277],\n",
       " 'A/B testing!Nixon, Richard': [326],\n",
       " 'A/B testing!NLP-based system': [21],\n",
       " 'A/B testing!no free lunch theorem': [353],\n",
       " 'A/B testing!bias of': [381],\n",
       " 'A/B testing!non-linear classiﬁers': [366],\n",
       " 'A/B testing!ﬁtting': [273],\n",
       " 'A/B testing!machines': [369],\n",
       " 'A/B testing!non-linearity': [358, 377, 380],\n",
       " 'A/B testing!norm': [287],\n",
       " 'A/B testing!normal': [125],\n",
       " 'A/B testing!periodic table': [188],\n",
       " 'A/B testing!Perl': [58],\n",
       " 'A/B testing!normal distribution': [79, 109, 124],\n",
       " 'A/B testing!implications': [126],\n",
       " 'A/B testing!normality testing': [141],\n",
       " 'A/B testing!normalization': [103, 376],\n",
       " 'A/B testing!normalizing skewed distribution': [49],\n",
       " 'A/B testing!norms': [309],\n",
       " 'A/B testing!NoSQL databases': [415],\n",
       " 'A/B testing!notebook environments': [59],\n",
       " 'A/B testing!numerical conversions': [73],\n",
       " 'A/B testing!randomly generating': [147],\n",
       " 'A/B testing!tests': [145],\n",
       " 'A/B testing!personal wealth': [130],\n",
       " 'A/B testing!bad examples': [183],\n",
       " 'A/B testing!point spread': [112],\n",
       " 'A/B testing!Obama, Barack': [326],\n",
       " 'A/B testing!Obama, Barack [91]': [327],\n",
       " 'A/B testing!Obama, Barack]': [326],\n",
       " 'A/B testing!Occam’s razor': [201, 211, 286],\n",
       " 'A/B testing!Occam, William of': [202],\n",
       " 'A/B testing!Oh G-d': [177],\n",
       " 'A/B testing!rotating': [248],\n",
       " 'A/B testing!points vs. vectors': [309],\n",
       " 'A/B testing!Poisson distribution': [127],\n",
       " 'A/B testing!position evaluation function': [372],\n",
       " 'A/B testing!positive class': [213],\n",
       " 'A/B testing!power law distribution': [129],\n",
       " 'A/B testing!power law function': [276],\n",
       " 'A/B testing!precision': [3, 215, 221],\n",
       " 'A/B testing!prefetching': [401],\n",
       " 'A/B testing!principal components': [260],\n",
       " 'A/B testing!Oracle of Bacon': [9],\n",
       " 'A/B testing!outlier': [118],\n",
       " 'A/B testing!outlier detection': [329],\n",
       " 'A/B testing!prior probability': [354],\n",
       " 'A/B testing!privacy': [418],\n",
       " 'A/B testing!probabilistic': [203],\n",
       " 'A/B testing!probability': [27, 29, 354],\n",
       " 'A/B testing!probability density function': [32],\n",
       " 'A/B testing!removing': [272],\n",
       " 'A/B testing!overﬁtting': [202, 296],\n",
       " 'A/B testing!overlap percentage': [137],\n",
       " 'A/B testing!ownership': [417],\n",
       " 'A/B testing!probability distribution': [32],\n",
       " 'A/B testing!probability of an event': [28],\n",
       " 'A/B testing!probability of an outcome': [28],\n",
       " 'A/B testing!probability vs. statistics': [29],\n",
       " 'A/B testing!program ﬂow graph': [322],\n",
       " 'A/B testing!programming languages': [57],\n",
       " 'A/B testing!protocol buﬀers': [63],\n",
       " 'A/B testing!proxies': [99],\n",
       " 'A/B testing!psychologists': [89],\n",
       " 'A/B testing!Pubmed': [70],\n",
       " 'A/B testing!pure partition': [360],\n",
       " 'A/B testing!Pythagorean theorem': [306],\n",
       " 'A/B testing!Python': [58, 67],\n",
       " 'A/B testing!p-values': [145],\n",
       " 'A/B testing!packing data': [402],\n",
       " 'A/B testing!PageRank': [100, 111, 325],\n",
       " 'A/B testing!pairwise correlations': [158],\n",
       " 'A/B testing!parallel processing': [407],\n",
       " 'A/B testing!parallelism': [406],\n",
       " 'A/B testing!parameter ﬁtting': [279],\n",
       " 'A/B testing!partition function': [299],\n",
       " 'A/B testing!Pascal’s triangle': [123],\n",
       " 'A/B testing!Pascal, Blaise': [30],\n",
       " 'A/B testing!paths': [246],\n",
       " 'A/B testing!Pearson correlation coeﬃcient': [41],\n",
       " 'A/B testing!Quant Shop': [17],\n",
       " 'A/B testing!penalty function': [293],\n",
       " 'A/B testing!performance of models': [39],\n",
       " 'A/B testing!R': [58],\n",
       " 'A/B testing!Rand index': [341],\n",
       " 'A/B testing!random access machine': [397],\n",
       " 'A/B testing!random sampling': [403, 406],\n",
       " 'A/B testing!random variable': [28],\n",
       " 'A/B testing!scale invariant': [132],\n",
       " 'A/B testing!Likert': [298],\n",
       " 'A/B testing!ordinal': [297],\n",
       " 'A/B testing!scaling constant': [293],\n",
       " 'A/B testing!class rank': [100],\n",
       " 'A/B testing!search results': [100],\n",
       " 'A/B testing!top sports teams': [100],\n",
       " 'A/B testing!university rankings': [100],\n",
       " 'A/B testing!rankings': [95],\n",
       " 'A/B testing!three-dimensional': [179],\n",
       " 'A/B testing!Schaumann, Jan': [351],\n",
       " 'A/B testing!scientist': [2],\n",
       " 'A/B testing!scores': [95],\n",
       " 'A/B testing!digraph-based': [109],\n",
       " 'A/B testing!historical': [117],\n",
       " 'A/B testing!merging': [108],\n",
       " 'A/B testing!techniques': [104],\n",
       " 'A/B testing!ratio': [48],\n",
       " 'A/B testing!threshold': [218],\n",
       " 'A/B testing!scores vs. rankings': [100],\n",
       " 'A/B testing!scoring functions': [95],\n",
       " 'A/B testing!security': [418],\n",
       " 'A/B testing!self-loop': [322],\n",
       " 'A/B testing!semi-supervised learning': [374],\n",
       " 'A/B testing!Shakespeare, William': [326],\n",
       " 'A/B testing!sharp': [215],\n",
       " 'A/B testing!Sheep Market': [86],\n",
       " 'A/B testing!shortest paths': [324],\n",
       " 'A/B testing!signal to noise ratio': [37],\n",
       " 'A/B testing!signiﬁcance level': [139],\n",
       " 'A/B testing!Silver, Nate': [203],\n",
       " 'A/B testing!similarity graphs': [341],\n",
       " 'A/B testing!similarity matrix': [342],\n",
       " 'A/B testing!simple graph': [322],\n",
       " 'A/B testing!single-command program': [224],\n",
       " 'A/B testing!single-pass algorithm': [402],\n",
       " 'A/B testing!singular value decomposition': [258],\n",
       " 'A/B testing!sketching': [403],\n",
       " 'A/B testing!skew': [413],\n",
       " 'A/B testing!small evaluation set': [226],\n",
       " 'A/B testing!social media': [392],\n",
       " 'A/B testing!unit': [241, 240],\n",
       " 'A/B testing!Reagan, Ronald': [326],\n",
       " 'A/B testing!rearrangement operations': [238],\n",
       " 'A/B testing!recall': [216, 221],\n",
       " 'A/B testing!curve': [218],\n",
       " 'A/B testing!rectiﬁed linear units': [381],\n",
       " 'A/B testing!rectiﬁer': [381],\n",
       " 'A/B testing!redundancy': [393],\n",
       " 'A/B testing!application': [359],\n",
       " 'A/B testing!LASSO': [287],\n",
       " 'A/B testing!logistic': [289, 292],\n",
       " 'A/B testing!ridge': [286],\n",
       " 'A/B testing!regression models': [272],\n",
       " 'A/B testing!removing outliers': [272],\n",
       " 'A/B testing!regularization': [286, 376],\n",
       " 'A/B testing!reinforcement learning': [372],\n",
       " 'A/B testing!Richter scale': [131],\n",
       " 'A/B testing!right-sizing training data': [404],\n",
       " 'A/B testing!road network': [322],\n",
       " 'A/B testing!robustness': [3, 96, 359],\n",
       " 'A/B testing!Roosevelt, Franklin D.': [326],\n",
       " 'A/B testing!Rota, Gian-Carlo': [237],\n",
       " 'A/B testing!Social Network–movie': [105],\n",
       " 'A/B testing!spam': [393],\n",
       " 'A/B testing!spam ﬁltering': [393],\n",
       " 'A/B testing!sabermetrics': [22],\n",
       " 'A/B testing!sample space': [27],\n",
       " 'A/B testing!coeﬃcient': [42, 108],\n",
       " 'A/B testing!Spears, Britney [566]': [262],\n",
       " 'A/B testing!spectral clustering': [343],\n",
       " 'A/B testing!beyond one dimension': [133],\n",
       " 'A/B testing!by truncation': [404],\n",
       " 'A/B testing!spidering': [67],\n",
       " 'A/B testing!splitters': [329],\n",
       " 'A/B testing!sports performance': [38],\n",
       " 'A/B testing!SQL databases': [63],\n",
       " 'A/B testing!standard deviation': [36, 125, 132],\n",
       " 'A/B testing!topological sorting': [110, 324],\n",
       " 'A/B testing!transparency': [417],\n",
       " 'A/B testing!tree': [298, 336],\n",
       " 'A/B testing!true negatives': [214],\n",
       " 'A/B testing!true positives': [214],\n",
       " 'A/B testing!truncation': [405],\n",
       " 'A/B testing!Tufte, Edward': [155, 162],\n",
       " 'A/B testing!Twitter': [392, 404, 405],\n",
       " 'A/B testing!138': [227, 403],\n",
       " 'A/B testing!statistical analysis': [121, 230],\n",
       " 'A/B testing!statistical distributions': [122],\n",
       " 'A/B testing!statistical proxy': [7],\n",
       " 'A/B testing!statistical signiﬁcance': [135],\n",
       " 'A/B testing!stochastic gradient descent': [285],\n",
       " 'A/B testing!U.S. presidential elections': [187, 203],\n",
       " 'A/B testing!uncertainty': [175],\n",
       " 'A/B testing!undirected graph': [321],\n",
       " 'A/B testing!uniform distribution': [406],\n",
       " 'A/B testing!uniform sampling': [405],\n",
       " 'A/B testing!uninvertible': [400],\n",
       " 'A/B testing!unit conversions': [72],\n",
       " 'A/B testing!UNIX time': [75],\n",
       " 'A/B testing!unlabeled graphs': [323],\n",
       " 'A/B testing!unrepresentative participation': [393],\n",
       " 'A/B testing!unsupervised learning': [372],\n",
       " 'A/B testing!unweighted graph': [322],\n",
       " 'A/B testing!urban transportation network': [11],\n",
       " 'A/B testing!296': [382],\n",
       " 'A/B testing!stock market': [37, 79, 126],\n",
       " 'A/B testing!Stony Brook University': [20],\n",
       " 'A/B testing!stop words': [414],\n",
       " 'A/B testing!storage hierarchy': [401],\n",
       " 'A/B testing!streaming': [402],\n",
       " 'A/B testing!summary statistics': [157, 159],\n",
       " 'A/B testing!supervised learning': [372],\n",
       " 'A/B testing!supervision': [372],\n",
       " 'A/B testing!degrees of': [372],\n",
       " 'A/B testing!support vector machines': [352, 366],\n",
       " 'A/B testing!support vectors': [368],\n",
       " 'A/B testing!Surowiecki, James': [82],\n",
       " 'A/B testing!sweat equity': [66],\n",
       " 'A/B testing!validation data': [95],\n",
       " 'A/B testing!value prediction': [210],\n",
       " 'A/B testing!T-test': [137],\n",
       " 'A/B testing!tangent line': [282],\n",
       " 'A/B testing!target scaling': [274],\n",
       " 'A/B testing!variability measures': [36],\n",
       " 'A/B testing!variance': [36, 202, 403],\n",
       " 'A/B testing!variation coeﬃcient': [137],\n",
       " 'A/B testing!variety': [394],\n",
       " 'A/B testing!vectors': [240],\n",
       " 'A/B testing!records from New York': [11],\n",
       " 'A/B testing!tipping model': [286],\n",
       " 'A/B testing!tipping rate': [13, 277],\n",
       " 'A/B testing!taxi driver': [277],\n",
       " 'A/B testing!terms of service': [68],\n",
       " 'A/B testing!test statistic': [138],\n",
       " 'A/B testing!text analysis': [253],\n",
       " 'A/B testing!theory of relativity': [144],\n",
       " 'A/B testing!Tikhonov regularization': [287],\n",
       " 'A/B testing!time uniﬁcation': [75],\n",
       " 'A/B testing!Titanic': [183, 358],\n",
       " 'A/B testing!top-k success rate': [219],\n",
       " 'A/B testing!topic modeling': [373],\n",
       " 'A/B testing!topological graph': [323],\n",
       " 'A/B testing!velocity': [394],\n",
       " 'A/B testing!veracity': [395],\n",
       " 'A/B testing!vertex': [319],\n",
       " 'A/B testing!vertices': [319],\n",
       " 'A/B testing!critiquing': [189],\n",
       " 'A/B testing!interactive': [195],\n",
       " 'A/B testing!tools': [160],\n",
       " 'A/B testing!visualization aesthetic': [162],\n",
       " 'A/B testing!chartjunk': [162, 165],\n",
       " 'A/B testing!colors': [163, 168],\n",
       " 'A/B testing!data-ink ratio': [162, 163],\n",
       " 'A/B testing!repetition': [163, 169],\n",
       " 'A/B testing!scaling and labeling': [162, 167],\n",
       " 'A/B testing!volume': [394],\n",
       " 'A/B testing!Voronoi diagrams': [315],\n",
       " 'A/B testing!voting': [363],\n",
       " 'A/B testing!weighted graph': [322],\n",
       " 'A/B testing!Welch’s t-statistic': [138],\n",
       " 'A/B testing!Wikipedia': [20, 79, 116, 326],\n",
       " 'A/B testing!wisdom': [19],\n",
       " 'A/B testing!wisdom of crowds': [81],\n",
       " 'A/B testing!Wolfram Alpha': [59, 144],\n",
       " 'A/B testing!word embeddings': [254, 383],\n",
       " 'A/B testing!word2vec': [384],\n",
       " 'A/B testing!XML': [62],\n",
       " 'A/B testing!with classiﬁers': [363],\n",
       " 'A/B testing!web crawling': [68],\n",
       " 'A/B testing!weighted average': [84],\n",
       " 'A/B testing!Z-score': [103, 308, 376],\n",
       " 'A/B testing!Zipf’s law': [131, 357]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_index_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data = {}\n",
    "for i in range(len(book_pdf)):\n",
    "    page = book_pdf[i]\n",
    "    index_data[i] = page.get_text(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Index\\nA/B testing, 86\\nAaron Schwartz case, 68\\nAB testing, 137\\nacademic data, 66\\naccuracy, 215, 228\\nactivation function, 380\\nAdaBoost, 364\\nadd-one discounting, 357\\nagglomerative cluster trees, 338\\naggregation mechanisms, 83\\nAkaike information criterion, 289,\\n335\\nalgorithm analysis, 397\\nAmazon Turk, 67, 84\\ntasks assigned, 85\\nTurkers, 84\\nAmerican basketball players, 97\\nanalogies, 312\\nanchoring, 82\\nangular distance, 310\\nAnscombe’s Quartet, 159\\nAOL, 64\\nAPI, 65\\nApple iPhone sales, 34\\napplication program interfaces, 65\\narea under the ROC curve, 219\\nAristotle, 326, 327\\nArrow’s impossibility theorem, 84,\\n114\\nartifacts, 69\\nAscombe quartet, 272\\nasking interesting questions, 4\\nassociativity, 244\\nautocorrelation, 46\\naverage link, 340\\nBabbage, Charles, 57, 90\\nbackpropagation, 382\\nBacon, Kevin, 9\\nbag of words, 14\\nbagging, 362\\nbalanced training classes, 295\\nbar charts, 179\\nbest practices, 181\\nstacked, 181\\nBarzun, Jacques, 5\\nbaseball encyclopedia, 5\\nbaseline models, 210\\nfor classiﬁcation, 210\\nfor value prediction, 212\\nBayes’ theorem, 150, 205, 299\\nBaysian information criteria, 289\\nbell-shaped, 123, 141\\ndistribution, 101\\nbias, 202, 417\\nlexicographic, 405\\nnumerical, 405\\ntemporal, 405\\nbias–variance trade-oﬀ, 202\\nbig data, 391\\nalgorithms, 397\\nbad data, 392\\nstatistics, 392\\nbig data engineer, 4\\nbig oh analysis, 397\\nbinary relations, 320\\nbinary search, 398\\nbinomial distribution, 123\\n435\\n© The Author(s) 2017\\nS.S. Skiena, The Data Science Design Manual,\\nTexts in Computer Science, https://doi.org/10.1007/978-3-319-55444-0\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"(.+?),\\s*((?:\\d+,?\\s*)+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for page, text in index_data.items():\n",
    "    for match in pattern.finditer(text):\n",
    "        term = match.group(1).strip()\n",
    "        pages = [int(p) for p in re.findall(r\"\\d+\", match.group(2))]\n",
    "        index[term] = pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[254, 383]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[\"word embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEX_PATH, \"r\") as f:\n",
    "    latex_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', latex_content)\n",
    "page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', latex_content))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " '5',\n",
       " '7',\n",
       " '10',\n",
       " '12',\n",
       " '14',\n",
       " '16',\n",
       " '19',\n",
       " '21',\n",
       " '23',\n",
       " '25',\n",
       " '30',\n",
       " '34',\n",
       " '36',\n",
       " '38',\n",
       " '41',\n",
       " '46',\n",
       " '48',\n",
       " '52',\n",
       " '56',\n",
       " '59',\n",
       " '61',\n",
       " '63',\n",
       " '65',\n",
       " '67',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '71',\n",
       " '73',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '79',\n",
       " '81',\n",
       " '83',\n",
       " '85',\n",
       " '87',\n",
       " '89',\n",
       " '92',\n",
       " '97',\n",
       " '100',\n",
       " '102',\n",
       " '104',\n",
       " '107',\n",
       " '109',\n",
       " '111',\n",
       " '114',\n",
       " '116',\n",
       " '118',\n",
       " '122',\n",
       " '126',\n",
       " '133',\n",
       " '135',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '139',\n",
       " '142',\n",
       " '144',\n",
       " '147',\n",
       " '152',\n",
       " '156',\n",
       " '159',\n",
       " '159',\n",
       " '160',\n",
       " '161',\n",
       " '163',\n",
       " '168',\n",
       " '171',\n",
       " '171',\n",
       " '172',\n",
       " '174',\n",
       " '178',\n",
       " '180',\n",
       " '186',\n",
       " '188',\n",
       " '190',\n",
       " '192',\n",
       " '197',\n",
       " '202',\n",
       " '204',\n",
       " '206',\n",
       " '208',\n",
       " '210',\n",
       " '212',\n",
       " '212',\n",
       " '213',\n",
       " '214',\n",
       " '216',\n",
       " '218',\n",
       " '218',\n",
       " '220',\n",
       " '222',\n",
       " '224',\n",
       " '226',\n",
       " '228',\n",
       " '230',\n",
       " '232',\n",
       " '236',\n",
       " '239',\n",
       " '241',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '245',\n",
       " '247',\n",
       " '249',\n",
       " '251',\n",
       " '254',\n",
       " '260',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '265',\n",
       " '269',\n",
       " '273',\n",
       " '273',\n",
       " '274',\n",
       " '276',\n",
       " '278',\n",
       " '281',\n",
       " '283',\n",
       " '285',\n",
       " '287',\n",
       " '291',\n",
       " '293',\n",
       " '296',\n",
       " '298',\n",
       " '300',\n",
       " '304',\n",
       " '308',\n",
       " '311',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '315',\n",
       " '317',\n",
       " '319',\n",
       " '321',\n",
       " '323',\n",
       " '323',\n",
       " '325',\n",
       " '327',\n",
       " '329',\n",
       " '331',\n",
       " '333',\n",
       " '335',\n",
       " '335',\n",
       " '337',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '341',\n",
       " '344',\n",
       " '346',\n",
       " '352',\n",
       " '358',\n",
       " '363',\n",
       " '366',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '371',\n",
       " '373',\n",
       " '373',\n",
       " '376',\n",
       " '378',\n",
       " '381',\n",
       " '384',\n",
       " '386',\n",
       " '390',\n",
       " '393',\n",
       " '395',\n",
       " '397',\n",
       " '400',\n",
       " '402',\n",
       " '404',\n",
       " '406',\n",
       " '408',\n",
       " '410',\n",
       " '410',\n",
       " '411',\n",
       " '412',\n",
       " '414',\n",
       " '416',\n",
       " '418',\n",
       " '420',\n",
       " '420',\n",
       " '421',\n",
       " '425',\n",
       " '430',\n",
       " '432',\n",
       " '435']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A/B testing': [86],\n",
       " 'Aaron Schwartz case': [68],\n",
       " 'AB testing': [137],\n",
       " 'academic data': [66],\n",
       " 'accuracy': [215, 228],\n",
       " 'activation function': [380],\n",
       " 'AdaBoost': [364],\n",
       " 'add-one discounting': [357],\n",
       " 'agglomerative cluster trees': [338],\n",
       " 'aggregation mechanisms': [83],\n",
       " 'Akaike information criterion': [289, 335],\n",
       " 'algorithm analysis': [397],\n",
       " 'Amazon Turk': [67, 84],\n",
       " 'tasks assigned': [85],\n",
       " 'Turkers': [84],\n",
       " 'American basketball players': [97],\n",
       " 'analogies': [312],\n",
       " 'anchoring': [82],\n",
       " 'angular distance': [310],\n",
       " 'Anscombe’s Quartet': [159],\n",
       " 'AOL': [64],\n",
       " 'API': [65],\n",
       " 'Apple iPhone sales': [34],\n",
       " 'application program interfaces': [65],\n",
       " 'area under the ROC curve': [219],\n",
       " 'Aristotle': [326, 327],\n",
       " 'Arrow’s impossibility theorem': [84, 114],\n",
       " 'artifacts': [69],\n",
       " 'Ascombe quartet': [272],\n",
       " 'asking interesting questions': [4],\n",
       " 'associativity': [244],\n",
       " 'autocorrelation': [46],\n",
       " 'average link': [340],\n",
       " 'Charles Babbage': [57, 90],\n",
       " 'backpropagation': [382],\n",
       " 'Kevin Bacon': [9],\n",
       " 'bag of words': [14],\n",
       " 'bagging': [362],\n",
       " 'balanced training classes': [295],\n",
       " 'bar charts': [179],\n",
       " 'best practices': [177],\n",
       " 'stacked': [181],\n",
       " 'Jacques Barzun': [5],\n",
       " 'baseball encyclopedia': [5],\n",
       " 'baseline models': [210],\n",
       " 'for classiﬁcation': [290],\n",
       " 'for value prediction': [212],\n",
       " 'Bayes’ theorem': [150, 205, 299],\n",
       " 'Baysian information criteria': [289],\n",
       " 'bell-shaped': [123, 141],\n",
       " 'distribution': [101],\n",
       " 'bias': [202, 417],\n",
       " 'lexicographic': [405],\n",
       " 'numerical': [405],\n",
       " 'temporal': [405],\n",
       " 'bias–variance trade-oﬀ': [202],\n",
       " 'big data': [391],\n",
       " 'algorithms': [375],\n",
       " 'bad data': [392],\n",
       " 'statistics': [29],\n",
       " 'big data engineer': [4],\n",
       " 'big oh analysis': [397],\n",
       " 'binary relations': [320],\n",
       " 'binary search': [398],\n",
       " 'binomial distribution': [123, 435],\n",
       " 'bioinformatician': [50],\n",
       " 'Josh Blumenstock': [27],\n",
       " 'Body Mass Index': [96, 177],\n",
       " 'Bonferroni correction': [141],\n",
       " 'boosting': [363, 364],\n",
       " 'bootstrapping': [374],\n",
       " 'Borda’s method': [108],\n",
       " 'box plots': [175],\n",
       " 'George Box': [201],\n",
       " 'box-and-whisker plots': [176],\n",
       " 'bubble plots': [179],\n",
       " 'bupkis': [391],\n",
       " 'George W. Bush': [326],\n",
       " 'C–language': [59],\n",
       " 'cache memory': [401],\n",
       " 'canonical representation': [400],\n",
       " 'canonization': [400],\n",
       " 'CAPTCHAs': [89],\n",
       " 'Lewis Carroll': [423],\n",
       " 'cartograms': [189],\n",
       " 'Center for Disease Control': [204],\n",
       " 'center of mass': [332],\n",
       " 'centrality measures': [34],\n",
       " 'centroids': [331],\n",
       " 'character code uniﬁcation': [74],\n",
       " 'characteristic equation': [256],\n",
       " 'characterizing distributions': [39],\n",
       " 'chart types': [170],\n",
       " 'data maps': [187],\n",
       " 'dot and line plots': [174],\n",
       " 'histograms': [32, 183, 222],\n",
       " 'pie charts': [179],\n",
       " 'scatter plots': [98, 177],\n",
       " 'tabular data': [170],\n",
       " 'cicada': [46],\n",
       " 'classiﬁcation': [16, 210, 289],\n",
       " 'binary': [290, 314],\n",
       " 'multi-class': [297],\n",
       " 'regression': [16],\n",
       " 'classiﬁcation and regression trees': [357],\n",
       " 'balanced': [217],\n",
       " 'evaluating': [221],\n",
       " 'one-vs.-all': [298],\n",
       " 'perfect': [218],\n",
       " 'Bill Clinton': [326],\n",
       " 'Bill] Clinton': [326],\n",
       " 'closest pair of points': [398],\n",
       " 'cloud computing services': [410],\n",
       " 'conductance': [343],\n",
       " 'distance': [337],\n",
       " 'clustering': [327, 373],\n",
       " 'agglomerative': [337],\n",
       " 'applications': [244],\n",
       " 'biological': [337],\n",
       " 'cut-based': [341],\n",
       " 'k-means': [330],\n",
       " 'single link': [339],\n",
       " 'visualization of': [337],\n",
       " 'number of': [333],\n",
       " 'organization of': [337],\n",
       " 'Clyde': [111, 112],\n",
       " 'coeﬃcient vector': [270],\n",
       " 'Cohen’s d': [136],\n",
       " 'collaborative ﬁltering': [9],\n",
       " 'communication': [408, 416],\n",
       " 'commutativity': [243],\n",
       " 'company data': [64],\n",
       " 'computer scientist': [2],\n",
       " 'conditional probability': [30, 31],\n",
       " 'Condorcet jury theorem': [84],\n",
       " 'confusion matrix': [214, 220],\n",
       " 'connected components': [324],\n",
       " 'contingency table': [214],\n",
       " 'convex': [280],\n",
       " 'convex hull': [368],\n",
       " 'coordination': [408],\n",
       " 'analysis': [21],\n",
       " 'interpretation': [37],\n",
       " 'signiﬁcance': [45],\n",
       " 'correlation and causation': [45, 135],\n",
       " 'cosine similarity': [309],\n",
       " 'cross validation': [227],\n",
       " 'advantages': [311],\n",
       " 'CrowdFlower': [67, 84, 86],\n",
       " 'crowdsourcing': [67, 80],\n",
       " 'bad uses': [87],\n",
       " 'crowdsourcing services': [84],\n",
       " 'cryptographic hashing': [400],\n",
       " 'CSV ﬁles': [62],\n",
       " 'cumulative density function': [33, 132, 186],\n",
       " 'currency conversion': [75],\n",
       " 'cut': [343],\n",
       " 'damping factor': [325],\n",
       " 'Charles Darwin': [81],\n",
       " 'data': [394],\n",
       " 'quantitative vs. categorical': [15],\n",
       " 'big vs. little': [15],\n",
       " 'cleaning': [69],\n",
       " 'collecting': [64],\n",
       " 'compatibility': [72],\n",
       " 'errors': [69],\n",
       " 'for evaluation': [225],\n",
       " 'for testing': [225],\n",
       " 'for training': [225],\n",
       " 'logging': [68],\n",
       " 'properties': [14],\n",
       " 'scraping': [67],\n",
       " 'structured': [14],\n",
       " 'types': [14],\n",
       " 'unstructured': [14],\n",
       " 'visualizing': [241],\n",
       " 'data analysis': [404],\n",
       " 'data centrism': [2],\n",
       " 'data cleaning': [376],\n",
       " 'data errors': [417],\n",
       " 'data formats': [61],\n",
       " 'evaluation': [225],\n",
       " 'data munging': [57],\n",
       " 'data parallelism': [409],\n",
       " 'data partition': [404],\n",
       " 'data processing': [10],\n",
       " 'data reduction': [329],\n",
       " 'data science': [210],\n",
       " 'languages': [57],\n",
       " 'models': [208],\n",
       " 'data science television': [17],\n",
       " 'data scientist': [2],\n",
       " 'data sources': [64],\n",
       " 'data visualization': [155],\n",
       " 'data-driven': [207],\n",
       " 'Chevalier de M´er´e': [29],\n",
       " 'decision boundaries': [291],\n",
       " 'decision tree classiﬁers': [299, 357],\n",
       " 'decision trees': [357],\n",
       " 'construction': [359],\n",
       " 'ensembles of': [362],\n",
       " 'deep learning': [209],\n",
       " 'network': [378],\n",
       " 'DeepWalk': [385],\n",
       " 'degree of vertex': [325],\n",
       " 'depth': [379],\n",
       " 'derivative': [281],\n",
       " 'partial': [282],\n",
       " 'second': [281],\n",
       " 'descriptive statistics': [34],\n",
       " 'deterministic sampling algorithms': [404],\n",
       " 'developing scoring systems': [99],\n",
       " 'dictionary maintenance': [399],\n",
       " 'DiMaggio': [148],\n",
       " 'dimension reduction': [277, 376],\n",
       " 'dimensional egalitarianism': [308],\n",
       " 'dimensions': [384],\n",
       " 'dinosaur vertebra': [78],\n",
       " 'directed acyclic graph': [110],\n",
       " 'directed graph': [321],\n",
       " 'discounting': [209, 356],\n",
       " 'disk storage': [401],\n",
       " 'distance methods': [303],\n",
       " 'distance metrics': [304],\n",
       " 'Lk': [305],\n",
       " 'euclidean': [305],\n",
       " 'manhattan distance': [305],\n",
       " 'maximum component': [305],\n",
       " 'distances': [319],\n",
       " 'measuring': [303],\n",
       " 'distributed ﬁle system': [396],\n",
       " 'distributed processing': [407],\n",
       " 'divide and conquer': [411],\n",
       " 'DNA sequences': [402],\n",
       " 'dot product': [241],\n",
       " 'duality': [268],\n",
       " 'duplicate removal': [400],\n",
       " 'E-step': [335],\n",
       " 'edge cuts': [324],\n",
       " 'edges': [319],\n",
       " 'eﬀect size': [136],\n",
       " 'eigenvalues': [255],\n",
       " 'computation': [256],\n",
       " 'decomposition': [257],\n",
       " 'properties of': [255],\n",
       " 'Elizabeth II': [326],\n",
       " 'Elo rankings': [104],\n",
       " 'embedded graph': [323],\n",
       " 'embedding': [321],\n",
       " 'Emoji Dick': [86],\n",
       " 'Friedrich Engels': [391],\n",
       " 'ensemble learning': [363],\n",
       " 'entropy': [310],\n",
       " 'determined': [251],\n",
       " 'underdetermined': [256],\n",
       " 'error': [269],\n",
       " 'absolute': [221],\n",
       " 'detection': [78],\n",
       " 'mean squared': [223, 331],\n",
       " 'relative': [222],\n",
       " 'residual': [269],\n",
       " 'root mean squared': [223],\n",
       " 'squared': [222],\n",
       " 'errors vs. artifacts': [69],\n",
       " 'ethical implications': [416],\n",
       " 'Euclidean metric': [303],\n",
       " 'environments': [224],\n",
       " 'event': [28],\n",
       " 'Excel': [59],\n",
       " 'exclusive or': [361],\n",
       " 'exercises': [23, 53, 90, 119, 151, 199, 234, 263, 301, 346, 388, 419],\n",
       " 'expectation maximization': [335],\n",
       " 'expected value': [28],\n",
       " 'experiment': [27],\n",
       " 'exploratory data analysis': [155, 156],\n",
       " 'visualization': [404],\n",
       " 'F-score': [216],\n",
       " 'Facebook': [21],\n",
       " 'false negatives': [214],\n",
       " 'false positives': [214],\n",
       " 'fast Fourier transform': [47],\n",
       " 'fault tolerance': [408],\n",
       " 'feature engineering': [375],\n",
       " 'feature scaling': [274],\n",
       " 'sublinear': [276],\n",
       " 'z-scores': [275],\n",
       " 'highly-correlated': [277],\n",
       " 'Pierre de Fermat': [30],\n",
       " 'Richard Feynman': [229],\n",
       " 'FFT': [47],\n",
       " 'ﬁltering': [403],\n",
       " 'ﬁnancial market': [126],\n",
       " 'ﬁnancial uniﬁcation': [75],\n",
       " 'ﬁt and complexity': [288],\n",
       " 'FoldIt': [89],\n",
       " 'football': [111],\n",
       " 'American players': [97],\n",
       " 'game prediction': [111],\n",
       " 'time series': [212],\n",
       " 'formulation': [354],\n",
       " 'Freedom of Information Act': [12, 65],\n",
       " 'frequency counting': [400],\n",
       " 'frequency distributions': [184],\n",
       " 'furthest link': [340],\n",
       " 'Francis Galton': [81],\n",
       " 'games with a purpose': [88],\n",
       " 'gamiﬁcation': [88],\n",
       " 'garbage out garbage in': [3, 69],\n",
       " 'Bill Gates': [130],\n",
       " 'elimination': [250],\n",
       " 'Gaussian distribution': [124],\n",
       " 'Gaussian noise': [125],\n",
       " 'General Sentiment': [20],\n",
       " 'genius': [19],\n",
       " 'geometric mean': [35],\n",
       " 'geometric point sets': [238],\n",
       " 'geometry': [240],\n",
       " 'Gini impurity': [360],\n",
       " 'Global Positioning System': [12],\n",
       " 'gold standards': [99],\n",
       " 'good scoring functions': [101],\n",
       " 'Goodhart’s law': [303],\n",
       " 'Charles Goodhart': [303],\n",
       " 'AlphaGo': [372],\n",
       " 'TensorFlow': [377],\n",
       " 'Google Flu Trends': [394],\n",
       " 'Google News': [219],\n",
       " 'Google Ngrams': [10],\n",
       " 'Google Scholar': [66],\n",
       " 'government data': [65],\n",
       " 'gradient boosted decision trees': [359, 366],\n",
       " 'gradient descent search': [281],\n",
       " 'graph embeddings': [384],\n",
       " 'graph theory': [323],\n",
       " 'graphs': [238, 319, 321],\n",
       " 'cuts': [342],\n",
       " 'dense': [322],\n",
       " 'directed': [321],\n",
       " 'embedded': [323],\n",
       " 'labeled': [323],\n",
       " 'non-simple': [322],\n",
       " 'simple': [322],\n",
       " 'sparse': [322],\n",
       " 'topological': [323],\n",
       " 'undirected': [321],\n",
       " 'unlabeled': [323],\n",
       " 'unweighted': [322],\n",
       " 'weighted': [320, 322],\n",
       " 'Dorian Gray': [251],\n",
       " 'grid indexes': [315],\n",
       " 'grid search': [409],\n",
       " 'Hadoop distributed ﬁle system': [414],\n",
       " 'Richard W. Hamming': [1],\n",
       " 'hash functions': [399],\n",
       " 'hashing': [399],\n",
       " 'heatmaps': [178],\n",
       " 'hedge fund': [1],\n",
       " 'hierarchy': [298],\n",
       " 'higher dimensions': [307, 370],\n",
       " 'bin size': [184],\n",
       " 'Adolf Hitler': [326],\n",
       " 'HTML': [67],\n",
       " 'hypothesis development': [328],\n",
       " 'hypothesis driven': [156, 392],\n",
       " 'Imagenet': [379],\n",
       " 'IMDb': [7],\n",
       " 'by interpolation': [78],\n",
       " 'by mean value': [77],\n",
       " 'by nearest neighbor': [78],\n",
       " 'by random value': [77],\n",
       " 'heuristic-based': [77],\n",
       " 'independence': [30, 123, 354],\n",
       " 'inﬂation rates': [76],\n",
       " 'information gain': [360],\n",
       " 'information theoretic entropy': [360],\n",
       " 'infrastructure': [396],\n",
       " 'inner product': [243],\n",
       " 'institutional review board': [88],\n",
       " 'Internet Movie Database': [7],\n",
       " 'Internet of Things': [68],\n",
       " 'inverse transform sampling': [132],\n",
       " 'IPython': [61],\n",
       " 'IQ testing': [89],\n",
       " 'Jaccard distance': [341],\n",
       " 'Jaccard similarity': [341],\n",
       " 'Michael [136] Jackson': [262],\n",
       " 'Java': [59],\n",
       " 'Jesus': [326],\n",
       " 'JSON': [63],\n",
       " 'k-means clustering': [343, 409],\n",
       " 'k-mediods algorithm': [332],\n",
       " 'k-nearest neighbors': [313],\n",
       " 'kd-trees': [316],\n",
       " 'kernels': [371],\n",
       " 'Kolmogorov-Smirnov test': [139],\n",
       " 'Kruskal’s algorithm': [339],\n",
       " 'Kullback-Leibler divergence': [311],\n",
       " 'labeled graphs': [323],\n",
       " 'Andrew Lang': [267],\n",
       " 'Laplace': [356],\n",
       " 'Laplacian': [343],\n",
       " 'large-scale question': [9],\n",
       " 'latent Dirichlet allocation': [373],\n",
       " 'learning rate': [283, 383],\n",
       " 'learning to rank': [119],\n",
       " 'least squares regression': [270],\n",
       " 'lie factor': [162, 164],\n",
       " 'Abraham Lincoln': [242],\n",
       " 'line charts': [174],\n",
       " 'line hatchings': [177],\n",
       " 'linear algebra': [237],\n",
       " 'power of': [237],\n",
       " 'linear equation': [238],\n",
       " 'linear programming': [369],\n",
       " 'linear regression': [212, 267],\n",
       " 'solving': [270],\n",
       " 'linear support vector machines': [369],\n",
       " 'linear systems': [250],\n",
       " 'Carl Linnaeus': [326],\n",
       " 'live data': [395],\n",
       " 'locality': [401],\n",
       " 'locality sensitive hashing': [317],\n",
       " 'logarithm': [47],\n",
       " 'issues': [295],\n",
       " 'logistic function': [381],\n",
       " 'logistic regression': [366],\n",
       " 'logit': [381],\n",
       " 'logit function': [106, 292],\n",
       " 'loss function': [280, 294],\n",
       " 'LU decomposition': [254],\n",
       " 'lumpers': [329],\n",
       " 'M-step': [335],\n",
       " 'machine learning': [208],\n",
       " 'classiﬁers': [85],\n",
       " 'main memory': [401],\n",
       " 'major league baseball': [6],\n",
       " 'MapReduce': [407, 410],\n",
       " 'programming': [412],\n",
       " 'MapReduce runtime system': [415],\n",
       " 'matchings': [324],\n",
       " 'Mathematica': [59, 61],\n",
       " 'Matlab': [58],\n",
       " 'matrix': [14, 237, 270, 373],\n",
       " 'addition': [242],\n",
       " 'adjacency': [246, 320],\n",
       " 'covariance': [245, 257, 271],\n",
       " 'determinant': [249, 254],\n",
       " 'eigenvectors': [255],\n",
       " 'factoring': [252],\n",
       " 'identity': [304],\n",
       " 'inversion': [248],\n",
       " 'linear combinations of': [242],\n",
       " 'multiplication': [242],\n",
       " 'multiplicative inverse of': [249],\n",
       " 'non-singular': [249],\n",
       " 'permutation': [246],\n",
       " 'rank': [251],\n",
       " 'reasons for factoring': [252],\n",
       " 'rotation': [248],\n",
       " 'singular': [249],\n",
       " 'transpose of': [242],\n",
       " 'triangular': [254],\n",
       " 'matrix multiplication': [243, 398],\n",
       " 'maximum margin separator': [367],\n",
       " 'mean': [34, 83, 125, 132, 138, 212, 227, 403],\n",
       " 'arithmetic': [35],\n",
       " 'geometric': [35],\n",
       " 'measurement error': [125],\n",
       " 'median': [35, 83, 132, 212, 403],\n",
       " 'mergesort': [398],\n",
       " 'metadata': [7],\n",
       " 'method centrism': [2],\n",
       " 'metric': [304],\n",
       " 'positivity': [304],\n",
       " 'symmetry': [304],\n",
       " 'triangle inequality': [304],\n",
       " 'global': [284],\n",
       " 'local': [284],\n",
       " 'minimum spanning tree': [324, 339],\n",
       " 'missing values': [76, 376],\n",
       " 'mixture model': [333],\n",
       " 'Moby Dick': [86],\n",
       " 'mode': [36],\n",
       " 'model-driven': [417],\n",
       " 'modeling': [201, 328, 416],\n",
       " 'philosophies of': [201],\n",
       " 'principles for eﬀectiveness': [203],\n",
       " 'ad hoc': [208],\n",
       " 'baseline': [210],\n",
       " 'blackbox': [206],\n",
       " 'descriptive': [206],\n",
       " 'deterministic': [208],\n",
       " 'ﬁrst-principle': [207],\n",
       " 'ﬂat': [209],\n",
       " 'Google’s forecasting': [204],\n",
       " 'hierarchical': [209],\n",
       " 'linear': [206],\n",
       " 'live': [204],\n",
       " 'neural network': [206],\n",
       " 'non-linear': [206],\n",
       " 'overﬁt': [203],\n",
       " 'simplifying': [286],\n",
       " 'simulation': [229],\n",
       " 'stochastic': [208],\n",
       " 'taxonomy of': [205],\n",
       " 'underﬁt': [202],\n",
       " 'Moneyball': [5],\n",
       " 'monkey': [215],\n",
       " 'monotonic': [307],\n",
       " 'sampling': [132, 403, 404],\n",
       " 'simulations': [229],\n",
       " 'Frederick Mosteller': [121],\n",
       " 'multiedge': [322],\n",
       " 'multinomial regression': [299],\n",
       " 'multiplying probabilities': [48],\n",
       " 'naive Bayes': [354, 363],\n",
       " 'name uniﬁcation': [74],\n",
       " 'Napoleon': [326],\n",
       " 'NASA': [73],\n",
       " 'National Football League': [112],\n",
       " 'natural language processing': [20],\n",
       " 'nearest centroid': [340],\n",
       " 'nearest neighbor': [339, 397],\n",
       " 'nearest neighbor classiﬁcation': [311],\n",
       " 'ﬁnding': [315],\n",
       " 'negative class': [213],\n",
       " 'Netﬂix prize': [9],\n",
       " 'network methods': [303],\n",
       " 'networks': [109, 238, 319, 378],\n",
       " 'induced': [320],\n",
       " 'learning': [379],\n",
       " 'neural networks': [377],\n",
       " 'new data set': [156],\n",
       " 'New York': [277],\n",
       " 'Richard Nixon': [326],\n",
       " 'NLP-based system': [21],\n",
       " 'no free lunch theorem': [353],\n",
       " 'bias of': [381],\n",
       " 'non-linear classiﬁers': [366],\n",
       " 'ﬁtting': [273],\n",
       " 'machines': [369],\n",
       " 'non-linearity': [358, 377, 380],\n",
       " 'norm': [287],\n",
       " 'normal': [125],\n",
       " 'normal distribution': [79, 109, 124, 141],\n",
       " 'implications': [126],\n",
       " 'normality testing': [141],\n",
       " 'normalization': [103, 376],\n",
       " 'normalizing skewed distribution': [49],\n",
       " 'norms': [309],\n",
       " 'NoSQL databases': [415],\n",
       " 'notebook environments': [59],\n",
       " 'numerical conversions': [73],\n",
       " 'Barack Obama': [326],\n",
       " 'Barack [91] Obama': [327],\n",
       " 'Barack] Obama': [326],\n",
       " 'Occam’s razor': [201, 211, 286],\n",
       " 'William of Occam': [202],\n",
       " 'Oh G-d': [177],\n",
       " 'Oracle of Bacon': [9],\n",
       " 'outlier': [118],\n",
       " 'outlier detection': [329],\n",
       " 'removing': [272],\n",
       " 'overﬁtting': [202, 296],\n",
       " 'overlap percentage': [137],\n",
       " 'ownership': [417],\n",
       " 'p-values': [145],\n",
       " 'packing data': [402],\n",
       " 'PageRank': [100, 111, 325],\n",
       " 'pairwise correlations': [158],\n",
       " 'parallel processing': [407],\n",
       " 'parallelism': [406],\n",
       " 'parameter ﬁtting': [279],\n",
       " 'partition function': [299],\n",
       " 'Pascal’s triangle': [123],\n",
       " 'Blaise Pascal': [30],\n",
       " 'paths': [246],\n",
       " 'Pearson correlation coeﬃcient': [41, 136],\n",
       " 'penalty function': [293],\n",
       " 'performance of models': [39],\n",
       " 'periodic table': [188],\n",
       " 'Perl': [58],\n",
       " 'randomly generating': [147],\n",
       " 'tests': [145],\n",
       " 'personal wealth': [130],\n",
       " 'bad examples': [183],\n",
       " 'point spread': [112],\n",
       " 'rotating': [248],\n",
       " 'points vs. vectors': [309],\n",
       " 'Poisson distribution': [127],\n",
       " 'position evaluation function': [372],\n",
       " 'positive class': [213],\n",
       " 'power law distribution': [129],\n",
       " 'power law function': [276],\n",
       " 'precision': [3, 215, 221],\n",
       " 'prefetching': [401],\n",
       " 'principal components': [260],\n",
       " 'prior probability': [354],\n",
       " 'privacy': [418],\n",
       " 'probabilistic': [203],\n",
       " 'probability': [27, 29, 354],\n",
       " 'probability density function': [32, 132, 186],\n",
       " 'probability distribution': [32],\n",
       " 'probability of an event': [28],\n",
       " 'probability of an outcome': [28],\n",
       " 'probability vs. statistics': [29],\n",
       " 'program ﬂow graph': [322],\n",
       " 'programming languages': [57],\n",
       " 'protocol buﬀers': [63],\n",
       " 'proxies': [99],\n",
       " 'psychologists': [89],\n",
       " 'Pubmed': [70],\n",
       " 'pure partition': [360],\n",
       " 'Pythagorean theorem': [306],\n",
       " 'Python': [58, 67],\n",
       " 'Quant Shop': [17],\n",
       " 'R': [58],\n",
       " 'Rand index': [341],\n",
       " 'random access machine': [397],\n",
       " 'random sampling': [403, 406],\n",
       " 'random variable': [28],\n",
       " 'class rank': [100],\n",
       " 'search results': [100],\n",
       " 'top sports teams': [100],\n",
       " 'university rankings': [100],\n",
       " 'rankings': [95],\n",
       " 'digraph-based': [109],\n",
       " 'historical': [117],\n",
       " 'merging': [108],\n",
       " 'techniques': [104],\n",
       " 'ratio': [48],\n",
       " 'unit': [240],\n",
       " 'Ronald Reagan': [326],\n",
       " 'rearrangement operations': [238],\n",
       " 'recall': [216, 221],\n",
       " 'curve': [218],\n",
       " 'rectiﬁed linear units': [381],\n",
       " 'rectiﬁer': [381],\n",
       " 'redundancy': [393],\n",
       " 'application': [359],\n",
       " 'LASSO': [287],\n",
       " 'logistic': [289, 292],\n",
       " 'ridge': [286],\n",
       " 'regression models': [272],\n",
       " 'removing outliers': [272],\n",
       " 'regularization': [286, 376],\n",
       " 'reinforcement learning': [372],\n",
       " 'Richter scale': [131],\n",
       " 'right-sizing training data': [404],\n",
       " 'road network': [322],\n",
       " 'robustness': [3, 96, 359],\n",
       " 'Franklin D. Roosevelt': [326],\n",
       " 'Gian-Carlo Rota': [237],\n",
       " 'sabermetrics': [22],\n",
       " 'sample space': [27],\n",
       " 'beyond one dimension': [133],\n",
       " 'by truncation': [404],\n",
       " 'scale invariant': [132],\n",
       " 'Likert': [298],\n",
       " 'ordinal': [297],\n",
       " 'scaling constant': [293],\n",
       " 'three-dimensional': [179],\n",
       " 'Jan Schaumann': [351],\n",
       " 'scientist': [2],\n",
       " 'scores': [95],\n",
       " 'threshold': [218],\n",
       " 'scores vs. rankings': [100],\n",
       " 'scoring functions': [95],\n",
       " 'security': [418],\n",
       " 'self-loop': [322],\n",
       " 'semi-supervised learning': [374],\n",
       " 'William Shakespeare': [326],\n",
       " 'sharp': [215],\n",
       " 'Sheep Market': [86],\n",
       " 'shortest paths': [324],\n",
       " 'signal to noise ratio': [37],\n",
       " 'signiﬁcance level': [139],\n",
       " 'Nate Silver': [203],\n",
       " 'similarity graphs': [341],\n",
       " 'similarity matrix': [342],\n",
       " 'simple graph': [322],\n",
       " 'single-command program': [224],\n",
       " 'single-pass algorithm': [402],\n",
       " 'singular value decomposition': [258],\n",
       " 'sketching': [403],\n",
       " 'skew': [413],\n",
       " 'small evaluation set': [226],\n",
       " 'social media': [392],\n",
       " 'Social Network–movie': [105],\n",
       " 'spam': [393],\n",
       " 'spam ﬁltering': [393],\n",
       " 'coeﬃcient': [42, 108],\n",
       " 'Britney [566] Spears': [262],\n",
       " 'spectral clustering': [343],\n",
       " 'spidering': [67],\n",
       " 'splitters': [329],\n",
       " 'sports performance': [38],\n",
       " 'SQL databases': [63],\n",
       " 'standard deviation': [36, 125, 132, 138, 227, 403],\n",
       " 'statistical analysis': [121, 230],\n",
       " 'statistical distributions': [122],\n",
       " 'statistical proxy': [7],\n",
       " 'statistical signiﬁcance': [135],\n",
       " 'stochastic gradient descent': [285, 296, 382],\n",
       " 'stock market': [37, 79, 126],\n",
       " 'Stony Brook University': [20],\n",
       " 'stop words': [414],\n",
       " 'storage hierarchy': [401],\n",
       " 'streaming': [402],\n",
       " 'summary statistics': [157, 159],\n",
       " 'supervised learning': [372],\n",
       " 'supervision': [372],\n",
       " 'degrees of': [372],\n",
       " 'support vector machines': [352, 366],\n",
       " 'support vectors': [368],\n",
       " 'James Surowiecki': [82],\n",
       " 'sweat equity': [66],\n",
       " 'T-test': [137],\n",
       " 'tangent line': [282],\n",
       " 'target scaling': [274],\n",
       " 'records from New York': [11],\n",
       " 'tipping model': [286],\n",
       " 'tipping rate': [13, 277],\n",
       " 'taxi driver': [277],\n",
       " 'terms of service': [68],\n",
       " 'test statistic': [138],\n",
       " 'text analysis': [253],\n",
       " 'theory of relativity': [144],\n",
       " 'Tikhonov regularization': [287],\n",
       " 'time uniﬁcation': [75],\n",
       " 'Titanic': [183, 358],\n",
       " 'top-k success rate': [219],\n",
       " 'topic modeling': [373],\n",
       " 'topological graph': [323],\n",
       " 'topological sorting': [110, 324],\n",
       " 'transparency': [417],\n",
       " 'tree': [298, 336],\n",
       " 'true negatives': [214],\n",
       " 'true positives': [214],\n",
       " 'truncation': [405],\n",
       " 'Edward Tufte': [155, 162],\n",
       " 'Twitter': [392, 404, 405],\n",
       " 'U.S. presidential elections': [187, 203],\n",
       " 'uncertainty': [175],\n",
       " 'undirected graph': [321],\n",
       " 'uniform distribution': [406],\n",
       " 'uniform sampling': [405],\n",
       " 'uninvertible': [400],\n",
       " 'unit conversions': [72],\n",
       " 'UNIX time': [75],\n",
       " 'unlabeled graphs': [323],\n",
       " 'unrepresentative participation': [393],\n",
       " 'unsupervised learning': [372],\n",
       " 'unweighted graph': [322],\n",
       " 'urban transportation network': [11],\n",
       " 'validation data': [95],\n",
       " 'value prediction': [210],\n",
       " 'variability measures': [36],\n",
       " 'variance': [36, 202, 403],\n",
       " 'variation coeﬃcient': [137],\n",
       " 'variety': [394],\n",
       " 'vectors': [240],\n",
       " 'velocity': [394],\n",
       " 'veracity': [395],\n",
       " 'vertex': [319],\n",
       " 'vertices': [319],\n",
       " 'critiquing': [189],\n",
       " 'interactive': [195],\n",
       " 'tools': [160],\n",
       " 'visualization aesthetic': [162],\n",
       " 'chartjunk': [162, 165],\n",
       " 'colors': [163, 168],\n",
       " 'data-ink ratio': [162, 163],\n",
       " 'repetition': [163, 169],\n",
       " 'scaling and labeling': [162, 167],\n",
       " 'volume': [394],\n",
       " 'Voronoi diagrams': [315],\n",
       " 'voting': [363],\n",
       " 'with classiﬁers': [363],\n",
       " 'web crawling': [68],\n",
       " 'weighted average': [84],\n",
       " 'weighted graph': [322],\n",
       " 'Welch’s t-statistic': [138],\n",
       " 'Wikipedia': [20, 79, 116, 326],\n",
       " 'wisdom': [19],\n",
       " 'wisdom of crowds': [81],\n",
       " 'Wolfram Alpha': [59, 144],\n",
       " 'word embeddings': [254, 383],\n",
       " 'word2vec': [384],\n",
       " 'XML': [62],\n",
       " 'Z-score': [103, 308, 376],\n",
       " 'Zipf’s law': [131, 357]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct the naming issue in index\n",
    "# Create a new dictionary to store updated terms\n",
    "updated_index = {}\n",
    "\n",
    "for term, pages in index.items():\n",
    "    if \",\" in term:  \n",
    "        parts = term.split(\", \")\n",
    "        if len(parts) == 2:  # Ensure it's a \"Last, First\" format\n",
    "            corrected_name = f\"{parts[1]} {parts[0]}\"  # Convert to \"First Last\"\n",
    "            updated_index[corrected_name] = pages  # Store corrected term\n",
    "        else:\n",
    "            updated_index[term] = pages  # Keep unchanged terms\n",
    "    else:\n",
    "        updated_index[term] = pages  # Keep unchanged terms\n",
    "\n",
    "# Now, index is updated with corrected names\n",
    "index = updated_index\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_page(page,page_breaks ,page_positions, is_forward=True):\n",
    "\n",
    "    if str(page) in page_breaks:\n",
    "        return page_positions[page]\n",
    "    if is_forward:\n",
    "        bound = len(og_book_pdf)\n",
    "        forward = page\n",
    "        while (forward) <= bound:\n",
    "            if str(forward) in page_breaks:\n",
    "                return page_positions[forward]\n",
    "            forward += 1\n",
    "        return -1\n",
    "    \n",
    "    else:\n",
    "        bound = 0\n",
    "        backward = page\n",
    "        while (backward) >= bound:\n",
    "            if str(backward) in page_breaks:\n",
    "                return page_positions[backward]\n",
    "            backward -= 1\n",
    "        return 0  # No valid page found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_page(359)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indexes(latex_content, index): \n",
    "    matched = 0\n",
    "    not_matched = 0\n",
    "    not_found_terms = {}  # Dictionary to store terms not found along with page numbers\n",
    "\n",
    "    for index_term, pages in tqdm(index.items()):\n",
    "        # check = False\n",
    "        # if index_term == 'application':\n",
    "        #     check = True\n",
    "        for page in pages:\n",
    "            # upadte page number indexes as terms will be added\n",
    "\n",
    "            page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', latex_content)\n",
    "            page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', latex_content))}\n",
    "\n",
    "            upper_bound = find_closest_page(page+1, page_breaks, page_positions ,True)\n",
    "            lower_bound = find_closest_page(page-2, page_breaks, page_positions ,False)\n",
    "\n",
    "            page_content = latex_content[lower_bound:upper_bound]\n",
    "            \n",
    "            # match = re.search(r'\\b' + re.escape(index_term) + r'\\b', page_content, re.IGNORECASE)\n",
    "            match = re.search(  re.escape(index_term), page_content, re.IGNORECASE)\n",
    "\n",
    "            if match:\n",
    "                term_start = lower_bound + match.start()  # Adjust index relative to book_text\n",
    "                term_end = term_start + len(index_term)\n",
    "                indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                latex_content = latex_content[:term_start] + indexed_term + latex_content[term_end:]\n",
    "                matched += 1\n",
    "            else:\n",
    "                # try fuzzy matching\n",
    "                # term_length = len(index_term)\n",
    "                # max_dist = max(1, term_length // 3)  # 1/3rd of term length\n",
    "                # near_matches = find_near_matches(index_term, page_content, max_l_dist=max_dist)\n",
    "\n",
    "                # if near_matches:\n",
    "                #     best_match = min(near_matches, key=lambda x: x.dist)  # Get the closest match\n",
    "                #     term_start = lower_bound + best_match.start  # Adjust index relative to book_text\n",
    "                #     term_end = term_start + term_length\n",
    "                #     indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                #     latex_content = latex_content[:term_start] + index_term + indexed_term + latex_content[term_end:]\n",
    "                #     matched += 1\n",
    "                #     continue\n",
    "\n",
    "                # index_position = lower_bound + (upper_bound - lower_bound) // 2\n",
    "                # indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                # latex_content = latex_content[:index_position] + indexed_term + latex_content[index_position:]\n",
    "                \n",
    "                not_matched += 1\n",
    "                if index_term not in not_found_terms:\n",
    "                    not_found_terms[index_term] = []\n",
    "                not_found_terms[index_term].append(page)\n",
    "\n",
    "    print(f\"Matched: {matched}, Not Matched: {not_matched}\")\n",
    "    return latex_content, not_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:00<00:00, 4871.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 244, Not Matched: 703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# indexed_latex_content = add_indexes(latex_content, index) #old\n",
    "\n",
    "#100%|██████████| 792/792 [00:00<00:00, 4871.83it/s]\n",
    "# Matched: 244, Not Matched: 703\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:00<00:00, 2803.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 494, Not Matched: 453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v2 with \\b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:00<00:00, 3388.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 505, Not Matched: 442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v3 without \\b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:11<00:00, 68.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 621, Not Matched: 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v4 added fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:16<00:00, 49.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 843, Not Matched: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v5 added fuzzy matching with larger range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:17<00:00, 45.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 871, Not Matched: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v6 updated naming indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792/792 [00:00<00:00, 882.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 777, Not Matched: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v7 updating page_positions at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A/B Testing\n",
    "\n",
    "$ A / B $ Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Body Mass Index': [177],\n",
       " 'types': [14],\n",
       " 'decision tree classiﬁers': [299],\n",
       " 'Richard W. Hamming': [1],\n",
       " 'overlap percentage': [137],\n",
       " 'bad examples': [183],\n",
       " 'program ﬂow graph': [322],\n",
       " 'university rankings': [100],\n",
       " 'records from New York': [11],\n",
       " 'critiquing': [189]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_not_found_terms = {term: pages for term, pages in not_found_terms.items() if \",\" in term}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Charles Babbage': [57, 90], 'Kevin Bacon': [9], 'Josh Blumenstock': [27], 'George Box': [201], 'Lewis Carroll': [423], 'Bill Clinton': [326], 'Bill] Clinton': [326], 'Chevalier de M´er´e': [29], 'Friedrich Engels': [391], 'Pierre de Fermat': [30], 'Richard Feynman': [229], 'Bill Gates': [130], 'Charles Goodhart': [303], 'Dorian Gray': [251], 'Richard W. Hamming': [1], 'Adolf Hitler': [326], 'Michael [136] Jackson': [262], 'Andrew Lang': [267], 'Abraham Lincoln': [242], 'Carl Linnaeus': [326], 'Frederick Mosteller': [121], 'Richard Nixon': [326], 'Barack Obama': [326], 'Barack [91] Obama': [327], 'Barack] Obama': [326], 'William of Occam': [202], 'Blaise Pascal': [30], 'Ronald Reagan': [326], 'Franklin D. Roosevelt': [326], 'Jan Schaumann': [351], 'William Shakespeare': [326], 'Britney [566] Spears': [262], 'James Surowiecki': [82], 'Edward Tufte': [162]}\n"
     ]
    }
   ],
   "source": [
    "corrected_names = {}\n",
    "\n",
    "for term, pages in filtered_not_found_terms.items():\n",
    "    if \",\" in term:  \n",
    "        parts = term.split(\", \")\n",
    "        if len(parts) == 2:  # Ensure it's a \"Last, First\" format\n",
    "            corrected_name = f\"{parts[1]} {parts[0]}\"  # Convert to \"First Last\"\n",
    "            corrected_names[corrected_name] = pages  # Keep the pages same\n",
    "\n",
    "print(corrected_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indexes_for_names(latex_content, index): \n",
    "    # 2nd version to get detailed information regarding the terms which were not found in the first check\n",
    "    matched = 0      \n",
    "    not_matched = 0  \n",
    "    not_found_terms = {}  # Dictionary to store terms not found along with page numbers\n",
    "\n",
    "    for index_term, pages in (index.items()):\n",
    "        # check = False\n",
    "        # if index_term == 'application':\n",
    "        #     check = True\n",
    "        print(\"Checking for word :\" + index_term)\n",
    "        for page in pages:\n",
    "            print(\" Current page : \" , page)\n",
    "            upper_bound = find_closest_page(page, True)\n",
    "            lower_bound = find_closest_page(page-1, False)\n",
    "\n",
    "            page_content = latex_content[lower_bound:upper_bound]\n",
    "            \n",
    "            # match = re.search(r'\\b' + re.escape(index_term) + r'\\b', page_content, re.IGNORECASE)\n",
    "            match = re.search(  re.escape(index_term), page_content, re.IGNORECASE)\n",
    "\n",
    "            # if check:\n",
    "            #     print(\"Match found : \" + match.group(0))\n",
    "            #     print(\"Lowerbound : \", lower_bound)\n",
    "            #     print(\"Upperbound : \", upper_bound)\n",
    "            #     print(\"Page : \", page)\n",
    "            if match:\n",
    "                # print(\"Normal Match found\")\n",
    "                term_start = lower_bound + match.start()  # Adjust index relative to book_text\n",
    "                term_end = term_start + len(index_term)\n",
    "                indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                latex_content = latex_content[:term_start] + indexed_term + latex_content[term_end:]\n",
    "                matched += 1\n",
    "            else:\n",
    "                # print(\"Fuzzy Match Check\")\n",
    "                # try fuzzy matching\n",
    "                term_length = len(index_term)\n",
    "                max_dist = max(1, term_length // 3)  # 1/3rd of term length\n",
    "                near_matches = find_near_matches(index_term, page_content, max_l_dist=max_dist)\n",
    "\n",
    "                if near_matches:\n",
    "                    # print(\"Fuzzy Match found\")\n",
    "                    best_match = min(near_matches, key=lambda x: x.dist)  # Get the closest match\n",
    "                    term_start = lower_bound + best_match.start  # Adjust index relative to book_text\n",
    "                    term_end = term_start + term_length\n",
    "                    indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                    latex_content = latex_content[:term_start] + indexed_term + latex_content[term_end:]\n",
    "                    matched += 1\n",
    "                    continue\n",
    "                \n",
    "                print(\"No Match found\")\n",
    "                print(\"Page Content:\")\n",
    "                print(page_content)\n",
    "\n",
    "                not_matched += 1\n",
    "                if index_term not in not_found_terms:\n",
    "                    not_found_terms[index_term] = []\n",
    "                not_found_terms[index_term].append(page)\n",
    "\n",
    "\n",
    "    print(f\"Matched: {matched}, Not Matched: {not_matched}\")\n",
    "    return latex_content, not_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for word :Lewis Carroll\n",
      " Current page :  423\n",
      "No Match found\n",
      "Page Content:\n",
      "in aggregated data: It is not enough to delete names, addresses, and identity numbers to maintain privacy in a data set. Even anonymized data can be effectively de-anonymized in clever ways, by using orthogonal data sources. Consider the taxi data set we introduced in Section 1.6. It never contained any passenger identifier information in the first place. Yet it does provide pickup GPS coordinates to a resolution which might pinpoint a particular house as the source, and a particular strip joint as the destination. Now we have a pretty good idea who made that trip, and an equally good idea who might be interested in this information if the bloke were married.\\\\\n",
      "A related experiment identified particular taxi trips taken by celebrities, so as to figure out their destination and how well they tipped Gay14. By using Google to find paparazzi photographs of celebrities getting into taxis and extracting the time and place they were taken, it was easy to identify the record corresponding to that exact pickup as containing the desired target.\n",
      "\\end{itemize}\n",
      "\n",
      "Ethical issues in data science are serious enough that professional organizations have weighed in on best practices, including the Data Science Code of Professional Conduct (\\href{http://www.datascienceassn.org/code-of-conduct.html}{http://www.datascienceassn.org/code-of-conduct.html}) of the Data Science Association and the Ethical Guidelines for Statistical Practices (\\href{http://www.amstat.org/about/ethicalguidelines.cfm}{http://www.amstat.org/about/ethicalguidelines.cfm}) of the American Statistical Association.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 418\n",
      "\n",
      "I encourage you to read these documents to help you develop your sense of ethical issues and standards of professional behavior. Recall that people turn to data scientists for wisdom and consul, more than just code. Do what you can to prove worthy of this trust.\n",
      "\n",
      "\\subsection*{12.8 Chapter Notes}\n",
      "There exist no shortage of books on the topic of big data analysis. Leskovec, Rajarman and Ullman [RU14 is perhaps the most comprehensive of these, and a good place to turn for a somewhat deeper treatment of the topics we discuss here. This book and some companion videos are available at http: \\href{//www.mmds.org}{//www.mmds.org}.\n",
      "\n",
      "My favorite hands-on resources on software technologies are generally books from O'Reilly Media. In the context of this chapter, I recommend their books on data analytics with Hadoop [BK16] and Spark RLOW15].\n",
      "\n",
      "O'Neil O'N16 provides a thought-provoking look at the social dangers of big data analysis, emphasizing the misuse of opaque models relying on proxy data sources that create feedback loops which exacerbate the problems they are trying to solve.\n",
      "\n",
      "The analogy of disk/cache speeds to tortoise/escape velocity is due to Michael Bender.\n",
      "\n",
      "\\subsection*{12.9 \\index{exercises}}\n",
      "\\section*{Parallel and Distributed Processing}\n",
      "12-1. [3] What is the difference between parallel processing and distributed processing?\n",
      "\n",
      "12-2. [3] What are the benefits of MapReduce?\\\\[0pt]\n",
      "12-3. [5] Design MapReduce algorithms to take large files of integers and compute:\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item The largest integer.\n",
      "  \\item The average of all the integers.\n",
      "  \\item The number of distinct integers in the input.\n",
      "  \\item The mode of the integers.\n",
      "  \\item The median of the integers.\n",
      "\\end{itemize}\n",
      "\n",
      "12-4. [3] Would we expect map skew to be a bigger problem when there are ten reducers or a hundred reducers?\\\\[0pt]\n",
      "12-5. [3] Would we expect the problem of map skew to increase or decrease when we combine counts from each file before emitting them?\n",
      "\n",
      "12-6. [5] For each of the following The Quant Shop prediction challenges dream up the most massive possible data source that might reasonably exist, who might have it, and what biases might lurk in its view of the world.\\\n",
      "%---- Page End Break Here ---- Page : 419\n",
      "\\\n",
      "(a) Miss Universe.\\\\\n",
      "(b) Movie gross.\\\\\n",
      "(c) Baby weight.\\\\\n",
      "(d) Art auction price.\\\\\n",
      "(e) White Christmas.\\\\\n",
      "(f) Football champions.\\\\\n",
      "(g) Ghoul pool.\\\\\n",
      "(h) Gold/oil prices.\n",
      "\n",
      "\\section*{Ethics}\n",
      "12-7. [3] What are five practical ways one can go about protecting privacy in big data?\\\\[0pt]\n",
      "12-8. [3] What do you consider to be acceptable boundaries for Facebook to use the data it has about you? Give examples of uses which would be unacceptable to you. Are these forbidden by their data usage agreement?\\\\[0pt]\n",
      "12-9. [3] Give examples of decision making where you would trust an algorithm to make as good or better decisions as a person. For what tasks would you trust human judgment more than an algorithm? Why?\n",
      "\n",
      "\\section*{Implementation Projects}\n",
      "12-10. [5] Do the stream sampling methods we discussed really produce uniform random samples from the desired distribution? Implement them, draw samples, and run them through the appropriate statistical test.\\\\[0pt]\n",
      "12-11. [5] Set up a Hadoop or Spark cluster that spans two or more machines. Run a basic task like word counting. Does it really run faster than a simple job on one machine? How many machines/cores do you need in order to win?\\\\[0pt]\n",
      "12-12. [5] Find a big enough data source which you have access to, that you can justify processing with more than a single machine. Do something interesting with it.\n",
      "\n",
      "\\section*{Interview Questions}\n",
      "12-13. [3] What is your definition of big data?\\\\[0pt]\n",
      "12-14. [5] What is the largest data set that you have processed? What did you do, and what were the results?\\\\[0pt]\n",
      "12-15. [8] Give five predictions about what will happen in the world over the next twenty years?\\\\[0pt]\n",
      "12-16. [5] Give some examples of best practices in data science.\\\\[0pt]\n",
      "12-17. [5] How might you detect bogus reviews, or bogus Facebook accounts used for bad purposes?\\\\[0pt]\n",
      "12-18. [5] What do the map function and the reduce function do, under the Map-Reduce paradigm? What do the combiner and partitioner do?\\\\[0pt]\n",
      "12-19. [5] Do you think that the typed login/password will eventually disappear? How might they be replaced?\\\\[0pt]\n",
      "12-20. [5] When a data scientist cannot draw any conclusion from a data set, what should they say to their boss/customer?\n",
      "\n",
      "%---- Page End Break Here ---- Page : 420\n",
      "\n",
      "12-21. [3] What are hash table collisions? How can they be avoided? How frequently do they occur?\n",
      "\n",
      "\\section*{Kaggle Challenges}\n",
      "12-22. Which customers will become repeat buyers? \\href{https://www.kaggle.com/\n",
      "Checking for word :Bill Clinton\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Bill] Clinton\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Friedrich Engels\n",
      " Current page :  391\n",
      "No Match found\n",
      "Page Content:\n",
      ".16: Visualization of the name embedding for the most frequent 5000 last names from email contact data, showing a two-dimensional projection view of the embedding (left). Insets from left to right highlight British (center), and Hispanic (right) names.\n",
      "\n",
      "Asian regions in the map. Figure 11.17 presents insets for these two regions, revealing that one cluster consists of Chinese names and the other of Indian names.\n",
      "\n",
      "With very few Thors corresponding with very few Rabinowitzes, these corresponding name tokens are destined to lie far apart in embedding space. But the first name tokens popular within a given demographic are likely to lie near the last names from the same demographic, since the same close linkages appear in individual contact lists. Thus the nearest last name token $y$ to a specific first name token $x$ is likely to be culturally compatible, making $x y$ a good candidate for a reasonable-sounding name.\n",
      "\n",
      "The moral of this story is the power of word embeddings to effortlessly capture structure latent in any long sequence of symbols, where order matters. Programs like word2vec are great fun to play with, and remarkably easy to use. Experiment with any interesting data set you have, and you will be surprised at the properties it uncovers.\n",
      "\n",
      "\\subsection*{11.8 Chapter Notes}\n",
      "Good introductions to machine learning include Bishop [Bis07] and Friedman et al. [FHT01]. Deep learning is currently the most exciting area of machine learning, with the book by Goodfellow, Bengio, and Courville [GBC16] serving as the most comprehensive treatment.\n",
      "\n",
      "Word embeddings were introduced by Mikolov et al. [MCCD13], along with their powerful implementation of word2vec. Goldberg and Levy [G14] have shown that word2vec is implicitly factoring the pointwise mutual information matrix of word co-locations. In fact, the neural network model is not really fundamental to what it is doing. Our DeepWalk approach to graph embeddings is described in Perozzi et al. [PaRS14].\\\\\n",
      "\\includegraphics[max width=\\textwidth, center]{2025_03_17_ca60ec0bfd96dcf8e028g-402}\n",
      "\n",
      "Figure 11.17: The two distinct Asian clusters in name \\index{spam}e reflect different cultural groups. On the left, an inset showing Chinese/South Asian names. On the right, an inset from the cluster of Indian family names.\n",
      "\n",
      "The Titanic survival examples are derived from the Kaggle competition \\href{https://www.kaggle.com/c/titanic}{https://www.kaggle.com/c/titanic}. The war story on fake name generation is a result of work with Shuchu Han, Yifan Hu, Baris Coskun, and Meizhu Liu at Yahoo labs.\n",
      "\n",
      "\\subsection*{11.9 \\index{exercises}}\n",
      "\\section*{Classification}\n",
      "11-1. [3] Using the naive Bayes classifier of Figure 11.2 decide whether (Cloudy,High, Normal) and (Sunny,Low,High) are beach days.\\\\[0pt]\n",
      "11-2. [8] Apply the naive Bayes technique for multiclass text classification. Specifically, use The New York Times Developer API to fetch recent articles from several sections of the newspaper. Then, using the simple Bernoulli model for word presence, implement a classifier which, given the text of an article from The New York Times, predicts which section the article belongs to.\\\\[0pt]\n",
      "11-3. [3] What is regularization, and what kind of problems with machine learning does it solve?\n",
      "\n",
      "\\section*{Decision Trees}\n",
      "11-4. [3] Give decision trees to represent the following Boolean functions:\\\\\n",
      "(a) $A$ and $\\bar{B}$.\\\\\n",
      "(b) $A$ or $(B$ and $C)$.\\\\\n",
      "(c) $(A$ and $B)$ or $(C$ and $D)$.\n",
      "\n",
      "11-5. [3] Suppose we are given an $n \\times d$ labeled classification data matrix, where each item has an associated label class $A$ or class $B$. Give a proof or a counterexample to each of the statements below:\\\\\n",
      "(a) Does there always exist a decision tree classifier which perfectly separates $A$ from $B$ ?\\\\\n",
      "(b) Does there always exist a decision tree classifier which perfectly separates $A$ from $B$ if the $n$ feature vectors are all distinct?\\\\\n",
      "(c) Does there always exist a logistic regression classifier which perfectly separates $A$ from $B$ ?\\\\\n",
      "(d) Does there always exist a logistic regression classifier which perfectly separates $A$ from $B$ if the $n$ feature vectors are all distinct?\n",
      "\n",
      "11-6. [3] Consider a set of $n$ labeled points in two dimensions. Is it possible to build a finite-sized decision tree classifier with tests of the form \"is $x>c$ ?\", \"is $x<c$ ?\", \"is $y>c$ ?\", and \"is $y<c$ ?\" which classifies each possible query exactly like a nearest neighbor classifier?\n",
      "\n",
      "\\section*{Support Vector Machines}\n",
      "11-7. [3] Give a linear-time algorithm to find the maximum-width separating line in one dimension.\\\\[0pt]\n",
      "11-8. [8] Give an $O\\left(n^{k+1}\\right)$ algorithm to find the maximum-width separating line in $k$ dimensions.\\\\[0pt]\n",
      "11-9. [3] Suppose we use support vector machines to find a perfect separating line between a given set of $n$ red and blue points. Now suppose we delete all the points which are not support vectors, and use SVM to find the best separator of what remains. Might this separating line be different than the one before?\n",
      "\n",
      "\\section*{Neural Networks}\n",
      "11-10. [5] Specify the network structure and node activation fu\n",
      "Checking for word :Charles Goodhart\n",
      " Current page :  303\n",
      "No Match found\n",
      "Page Content:\n",
      "ee how it does when faced with a multi-class regression problem.\n",
      "\n",
      "A related notion to the partition function arises in Baysian analysis. We are often faced with a challenge of identifying the most likely item label, say $A$, as a function of evidence $E$. Recall that \\index{Bayes’ theorem} states that\n",
      "\n",
      "$$\n",
      "P(A \\mid E)=\\frac{P(E \\mid A) P(A)}{P(E)}\n",
      "$$\n",
      "\n",
      "Computing this as a real probability requires knowing the denominator $P(E)$, which can be a murky thing to compute. But comparing $P(A \\mid E)$ to $P(B \\mid E)$ in order to determine whether label $A$ is more likely than label $B$ does not require knowing $P(E)$, since it is the same in both expressions. Like a physicist, we can waive it away, mumbling about the \"partition function.\"\n",
      "\n",
      "\\subsection*{9.8 Chapter Notes}\n",
      "Linear and logistic regression are standard topics in statistics and optimization. Textbooks on linear/logistic regression and its applications include (JWHT13, Wei05.\n",
      "\n",
      "The treatment of the gradient descent approach to solving regression here was inspired by Andrew Ng , as presented in his Coursera machine learning course. I strongly recommend his video lectures to those interested in a more thorough treatment of the subject.\n",
      "\n",
      "The discovery that butter production in Bangladesh accurately forecasted the S\\&P 500 stock index is due to Leinweber Lei07. Unfortunately, like most spurious correlations it broke down immediately after its discovery, and no longer has predictive power.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 300\n",
      "\n",
      "\\subsection*{9.9 \\index{exercises}}\n",
      "\\section*{Linear Regression}\n",
      "$9-1$. [3] Construct an example on $n \\geq 6$ points where the optimal regression line is $y=x$, even though none of the input points lie directly on this line.\\\\[0pt]\n",
      "9-2. [3] Suppose we fit a regression line to predict the shelf life of an apple based on its weight. For a particular apple, we predict the shelf life to be 4.6 days. The apples residual is -0.6 days. Did we over or under estimate the shelf-life of the apple? Explain your reasoning.\\\\[0pt]\n",
      "9-3. [3] Suppose we want to find the best-fitting function $y=f(x)$ where $y=w^{2} x+$ $w x$. How can we use linear regression to find the best value of $w$ ?\\\\[0pt]\n",
      "9-4. [3] Suppose we have the opportunity to pick between using the best fitting model of the form $y=f(x)$ where $y=w^{2} x$ or $y=w x$, for constant coefficient $w$. Which of these is more general, or are they identical?\\\\[0pt]\n",
      "9-5. [5] Explain what a long-tailed distribution is, and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?\n",
      "\n",
      "9-6. [5] Using a linear algebra library/package, implement the closed form regression solver $w=\\left(A^{T} A\\right)^{-1} A^{T} b$. How well does it perform, relative to an existing solver?\\\\[0pt]\n",
      "9-7. [3] Establish the effect that different values for the constant $c$ of the logit function have on the probability of classification being $0.01,1,2$, and 10 units from the boundary.\n",
      "\n",
      "\\section*{Experiments with Linear Regression}\n",
      "9-8. [5] Experiment with the effects of fitting non-linear functions with linear regression. For a given $(x, y)$ data set, construct the best fitting line where the set of variables are $\\left\\{1, x, \\ldots, x^{k}\\right\\}$, for a range of different $k$. Does the model get better or worse over the course of this process, both in terms of fitting error and general robustness?\\\\[0pt]\n",
      "9-9. [5] Experiment with the effects of feature scaling in linear regression. For a given data set with at least two features (dimensions), multiply all the values of one feature by $10^{k}$, for $-10 \\leq k \\leq 10$. Does this operation cause a loss of numerical accuracy in fitting?\\\\[0pt]\n",
      "9-10. [5] Experiment with the effects of highly correlated features in linear regression. For a given $(x, y)$ data set, replicate the value of $x$ with small but increasing amounts of random noise. What is returned when the new column is perfectly correlated with the original? What happens with increasing amounts of random noise?\\\\[0pt]\n",
      "9-11. [5] Experiment with the effects of outliers on linear regression. For a given $(x, y)$ data set, construct the best fitting line. Repeatedly delete the point with the largest residual, and refit. Is the sequence of predicted slopes relatively stable for much of this process?\\\\[0pt]\n",
      "9-12. [5] Experiment with the effects of regularization on linear/logistic regression. For a given multi-dimensional data set, construct the best fitting line with (a)\\\n",
      "%---- Page End Break Here ---- Page : 301\n",
      "\\\n",
      "no regularization, (b) ridge regression, and (c) LASSO regression; the latter two with a range of constraint values. How does the accuracy of the model change as we reduce the size and number of parameters?\n",
      "\n",
      "\\section*{Implementation Projects}\n",
      "9-13. [5] Use linear/logistic regression to build a model for one of the following The Quant Shop challenges:\\\\\n",
      "(a) Miss Universe.\\\\\n",
      "(b) Movie gross.\\\\\n",
      "(c) Baby weight.\\\\\n",
      "(d) Art auction price.\\\\\n",
      "(e) White Christmas.\\\\\n",
      "(f) Football champions.\\\\\n",
      "(g) Ghoul pool.\\\\\n",
      "(h) Gold/oil prices.\n",
      "\n",
      "9-14. [5] This story about predicting the results of the NCAA college basketball tournament is instructive:\\\\\n",
      "\\href{http://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy}{http://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy}. html.\\\\\n",
      "Implement such a logistic regression classifier, and extend it to other sports like football.\n",
      "\n",
      "\\section*{Interview Questions}\n",
      "9-15. [8] Suppose we are training a model using stochastic gradient descent. How do we know if we are converging to a solution?\\\\[0pt]\n",
      "9-16. [5] Do gradient descent methods always converge to the same point?\\\\[0pt]\n",
      "9-17. [5] What assumptions are req\n",
      "Checking for word :Richard W. Hamming\n",
      " Current page :  1\n",
      "No Match found\n",
      "Page Content:\n",
      "% This LaTeX document needs to be compiled with XeLaTeX.\n",
      "\\documentclass[10pt]{article}\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{ucharclasses}\n",
      "\\usepackage{graphicx}\n",
      "\\usepackage[export]{adjustbox}\n",
      "\\graphicspath{ {./images/} }\n",
      "\\usepackage{hyperref}\n",
      "\\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}\n",
      "\\urlstyle{same}\n",
      "\\usepackage{amsmath}\n",
      "\\usepackage{amsfonts}\n",
      "\\usepackage{amssymb}\n",
      "\\usepackage[version=4]{mhchem}\n",
      "\\usepackage{stmaryrd}\n",
      "\\usepackage{underscore}\n",
      "\\usepackage{fvextra, csquotes}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{polyglossia}\n",
      "\\usepackage{fontspec}\n",
      "\\setmainlanguage{english}\n",
      "\\setotherlanguages{bengali}\n",
      "\\IfFontExistsTF{Noto Serif Bengali}\n",
      "{\\newfontfamily\\bengalifont{Noto Serif Bengali}}\n",
      "{\\IfFontExistsTF{Kohinoor Bangla}\n",
      "  {\\newfontfamily\\bengalifont{Kohinoor Bangla}}\n",
      "  {\\IfFontExistsTF{Bangla MN}\n",
      "    {\\newfontfamily\\bengalifont{Bangla MN}}\n",
      "    {\\IfFontExistsTF{Lohit Bengali}\n",
      "      {\\newfontfamily\\bengalifont{Lohit Bengali}}\n",
      "      {\\IfFontExistsTF{FreeSerif}\n",
      "        {\\newfontfamily\\bengalifont{FreeSerif}}\n",
      "        {\\newfontfamily\\bengalifont{Arial Unicode MS}}\n",
      "}}}}\n",
      "\\IfFontExistsTF{CMU Serif}\n",
      "{\\newfontfamily\\lgcfont{CMU Serif}}\n",
      "{\\IfFontExistsTF{DejaVu Sans}\n",
      "  {\\newfontfamily\\lgcfont{DejaVu Sans}}\n",
      "  {\\newfontfamily\\lgcfont{Georgia}}\n",
      "}\n",
      "\\setDefaultTransitions{\\lgcfont}{}\n",
      "\\setTransitionsFor{Bengali}{\\bengalifont}{\\lgcfont}\n",
      "\n",
      "\\title{Texts in Computer Science }\n",
      "\n",
      "\\author{Statistical Distributions}\n",
      "\\date{}\n",
      "\n",
      "\n",
      "%New command to display footnote whose markers will always be hidden\n",
      "\\let\\svthefootnote\\thefootnote\n",
      "\\newcommand\\blfootnotetext[1]{%\n",
      "  \\let\\thefootnote\\relax\\footnote{#1}%\n",
      "  \\addtocounter{footnote}{-1}%\n",
      "  \\let\\thefootnote\\svthefootnote%\n",
      "}\n",
      "\n",
      "%Overriding the \\footnotetext command to hide the marker if its value is `0`\n",
      "\\let\\svfootnotetext\\footnotetext\n",
      "\\renewcommand\\footnotetext[2][?]{%\n",
      "  \\if\\relax#1\\relax%\n",
      "    \\ifnum\\value{footnote}=0\\blfootnotetext{#2}\\else\\svfootnotetext{#2}\\fi%\n",
      "  \\else%\n",
      "    \\if?#1\\ifnum\\value{footnote}=0\\blfootnotetext{#2}\\else\\svfootnotetext{#2}\\fi%\n",
      "    \\else\\svfootnotetext[#1]{#2}\\fi%\n",
      "  \\fi\n",
      "}\n",
      "\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\\section*{TEXTS IN COMPUTER SCIENCE}\n",
      "\\section*{THE}\n",
      "\\section*{Data Science Design}\n",
      " MANUAL\\begin{center}\n",
      "\\includegraphics[max width=\\textwidth]{2025_03_17_ca60ec0bfd96dcf8e028g-001}\n",
      "\\end{center}\n",
      "\n",
      "\\section*{Steven S. Slaiena}\n",
      "A) Springer\n",
      "\n",
      "\n",
      "\n",
      "\\section*{Steven S. Skiena}\n",
      "\\section*{The Data Science Design Manual}\n",
      "Springer\n",
      "\n",
      "ISSN 1868-0941\\\\\n",
      "ISSN 1868-095X (electronic)\\\\\n",
      "Texts in Computer Science\\\\\n",
      "ISBN 978-3-319-55443-3\\\\\n",
      "ISBN 978-3-319-55444-0 (eBook)\\\\\n",
      "\\href{https://doi.org/10.1007/978-3-319-55444-0}{https://doi.org/10.1007/978-3-319-55444-0}\\\\\n",
      "Library of Congress Control Number: 2017943201\\\\\n",
      "This book was advertised with a copyright holder in the name of the publisher in error, whereas the author(s) holds the copyright.\\\\\n",
      "© The Author(s) 2017\\\\\n",
      "This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\\\\\n",
      "The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.\\\\\n",
      "The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n",
      "\n",
      "Printed on acid-free paper\\\\\n",
      "This Springer imprint is published by Springer Nature\\\\\n",
      "The registered company is Springer International Publishing AG\\\\\n",
      "The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland\n",
      "\n",
      "\\section*{Preface}\n",
      "Making sense of the world around us requires obtaining and analyzing data from our environment. Several technology trends have recently collided, providing new opportunities to apply our data analysis savvy to greater challenges than ever before.\n",
      "\n",
      "Computer storage capacity has increased exponentially; indeed remembering has become so cheap that it is almost impossible to get computer systems to forget. Sensing devices increasingly monitor everything that can be observed: video streams, social media interactions, and the position of anything that moves. Cloud computing enables us to harness the power of massive numbers of machines to manipulate this data. Indeed, hundreds of computers are summoned each time you do a Google search, scrutinizing all of your previous activity just to decide which is the best ad to show you next.\n",
      "\n",
      "The result of all this has been the birth of data science, a new field devoted to maximizing value from vast collections of information. As a discipline, data science sits somewhere at the intersection of statistics, computer science, and machine learning, but it is building a distinct heft and character of its own. This book serves as an introduction to data science, focusing on the skills and principles needed to build systems for collecting, analyzing, and interpreting data.\n",
      "\n",
      "My professional experience as a researcher and instructor convinces me that one major challenge of data science is that it is considerably more subtle than it looks. Any student who has ever computed their grade point average (GPA) can be said to have done rudimentary statistics, just as drawing a simple scatter plot lets you add experience in data visualization to your resume. But meaningfully analyzing and interpreting data requires both technical expertise and wisdom. That so many people do these basics so badly provides my inspiration for writing this book.\n",
      "\n",
      "\\section*{To the Reader}\n",
      "I have been gratified by the warm reception that my book The Algorithm Design Manual [Ski08] has received since its initial publication in 1997. It has been recognized as a unique guide to using algorithmic techniques to solve problems that often arise in practice. The book you are holding covers very different material, but with the same motivation.\n",
      "\n",
      "I\n",
      "%---- Page End Break Here ---- Page : v\n",
      "n particular, here I stress the following basic principles as fundamental to becoming a good \\index{data \\index{scientist}}:\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Valuing doing the simple things right: Data science isn't rocket science. Students and practitioners often get lost in technological space, pursuing the most advanced machine learning methods, the newest open source software libraries, or the glitziest visualization techniques. However, the heart of data science lies in doing the simple things right: understanding the application domain, cleaning and integrating relevant data sources, and presenting your results clearly to others.\\\\\n",
      "Simple doesn't mean easy, however. Indeed it takes considerable insight and experience to ask the right questions, and sense whether you are moving toward correct answers and actionable insights. I resist the temptation to drill deeply into clean, technical material here just because it is teachable. There are plenty of other books which will cover the intricacies of machine learning algorithms or statistical hypothesis testing. My mission here is to lay the groundwork of what really matters in analyzing data.\n",
      "  \\item Developing mathematical intuition: Data science rests on a foundation of mathematics, particularly statistics and linear algebra. It is important to understand this material on an intuitive level: why these concepts were developed, how they are useful, and when they work best. I illustrate operations in linear algebra by presenting pictures of what happens to matrices when you manipulate them, and statistical concepts by examples and reducto ad absurdum arguments. My goal here is transplanting intuition into the reader.\\\\\n",
      "But I strive to minimize the amount of formal mathematics used in presenting this material. Indeed, I will present exactly one formal proof in this book, an incorrect proof where the associated theorem is obviously false. The moral here is not that mathematical rigor doesn't matter, because of course it does, but that genuine rigor is impossible until after there is comprehension.\n",
      "  \\item Think like a \\index{computer scientist}, but act like a statistician: Data science provides an umbrella linking computer scientists, statisticians, and domain specialists. But each community has its own distinct styles of thinking and action, which gets stamped into the souls of its members.\\\\\n",
      "In this book, I emphasize approaches which come most naturally to computer scientists, particularly the algorithmic manipulation of data, the use of machine learning, and the mastery of scale. But I also seek to transmit the core values of statistical reasoning: the need to understand the application domain, proper appreciation of the small, the quest for significance, and a hunger for exploration.\\\\\n",
      "No discipline has a monopoly on the truth. The best data scientists incorporate tools from multiple areas, and this book strives to be a relatively neutral ground where rival philosophies can come to reason together.\n",
      "\n",
      "%---- Page End Break Here ---- Page : vi\n",
      "\\end{itemize}\n",
      "\n",
      "Equally important is what you will not find in this book. I do not emphasize any particular language or suite of data analysis tools. Instead, this book provides a high-level discussion of important design principles. I seek to operate at a conceptual level more than a technical one. The goal of this manual is to get you going in the right direction as quickly as possible, with whatever software tools you find most accessible.\n",
      "\n",
      "\\section*{To the Instructor}\n",
      "This book covers enough material for an \"Introduction to Data Science\" course at the undergraduate or early graduate student levels. I hope that the reader has completed the equivalent of at least one programming course and has a bit of prior exposure to probability and statistics, but more is always better than less.\n",
      "\n",
      "I have made a full set of lecture slides for teaching this course available online at \\href{http://www.data-manual.com}{http://www.data-manual.com}. Data resources for projects and assignments are also available there to aid the instructor. Further, I make available online video lectures using these slides to teach a full-semester data science course. Let me help teach your class, through the magic of the web!\n",
      "\n",
      "Pedagogical features of this book include:\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item War Stories: To provide a better perspective on how data science techniques apply to the real world, I include a collection of \"war stories,\" or tales from our experience with real problems. The moral of these stories is that these methods are not just theory, but important tools to be pulled out and used as needed.\n",
      "  \\item False Starts: Most textbooks present methods as a fait accompli, obscuring the ideas involved in designing them, and the subtle reasons why other approaches fail. The war stories illustrate my reasoning process on certain applied problems, but I weave such coverage into the core material as well.\n",
      "  \\item Take-Home Lessons: Highlighted \"take-home\" lesson boxes scattered through each chapter emphasize the big-picture concepts to learn from each chapter.\n",
      "  \\item Homework Problems: I provide a wide range of exercises for homework and self-study. Many are traditional exam-style problems, but there are also larger-scale implementation challenges and smaller-scale interview questions, reflecting the questions students might encounter when searching for a job. Degree of difficulty ratings have been assigned to all problems.\\\\\n",
      "In lieu of an answer key, a Solution Wiki has been set up, where solutions to all even numbered problems will be solicited by crowdsourcing. A similar system with my Algorithm Design Manual produced coherent solutions,\\\n",
      "%---- Page End Break Here ---- Page : vii\n",
      "\\\n",
      "or so I am told. As a matter of principle I refuse to look at them, so let the buyer beware.\n",
      "  \\item Kaggle Challenges: Kaggle (\\href{http://www.kaggle.com}{www.kaggle.com}) provides a forum for data scientists to compete in, featuring challenging real-world problems on fascinating data sets, and scoring to test how good your model is relative to other submissions. The exercises for each chapter include three relevant Kaggle challenges, to serve as a source of inspiration, self-study, and data for other projects and investigations.\n",
      "  \\item Data Science Television: Data science remains mysterious and even threatening to the broader public. The Quant Shop is an amateur take on what a data science reality show should be like. Student teams tackle a diverse array of real-world prediction problems, and try to forecast the outcome of future events. Check it out at \\href{http://www.quant-shop.com}{http://www.quant-shop.com}.\\\\\n",
      "A series of eight 30-minute episodes has been prepared, each built around a particular real-world prediction problem. Challenges include pricing art at an auction, picking the winner of the Miss Universe competition, and forecasting when celebrities are destined to die. For each, we observe as a student team comes to grips with the problem, and learn along with them as they build a forecasting model. They make their predictions, and we watch along with them to see if they are right or wrong.\\\\\n",
      "In this book, The Quant Shop is used to provide concrete examples of prediction challenges, to frame discussions of the data science modeling pipeline from data acquisition to evaluation. I hope you find them fun, and that they will encourage you to conceive and take on your own modeling challenges.\n",
      "  \\item Chapter Notes: Finally, each tutorial chapter concludes with a brief notes section, pointing readers to primary sources and additional references.\n",
      "\\end{itemize}\n",
      "\n",
      "\\section*{Dedication}\n",
      "My bright and loving daughters Bonnie and Abby are now full-blown teenagers, meaning that they don't always process statistical evidence with as much alacrity as I would I desire. I dedicate this book to them, in the hope that their analysis skills improve to the point that they always just agree with me.\n",
      "\n",
      "And I dedicate this book to my beautiful wife Renee, who agrees with me even when she doesn't agree with me, and loves me beyond the support of all creditable evidence.\n",
      "\n",
      "\\section*{Acknowledgments}\n",
      "My list of people to thank is large enough that I have probably missed some. I will try to do enumerate them systematically to minimize omissions, but ask those I've unfairly neglected for absolution.\n",
      "\n",
      "%---- Page End Break Here ---- Page : viii\n",
      "\n",
      "First, I thank those who made concrete contributions to help me put this book together. Yeseul Lee served as an apprentice on this project, helping with figures, exercises, and more during summer 2016 and beyond. You will see evidence of her handiwork on almost every page, and I greatly appreciate her help and dedication. Aakriti Mittal and Jack Zheng also contributed to a few of the figures.\n",
      "\n",
      "Students in my Fall 2016 Introduction to Data Science course (CSE 519) helped to debug the manuscript, and they found plenty of things to debug. I particularly thank Rebecca Siford, who proposed over one hundred corrections on her own. Several data science friends/sages reviewed specific chapters for me, and I thank Anshul Gandhi, Yifan Hu, Klaus Mueller, Francesco Orabona, Andy Schwartz, and Charles Ward for their efforts here.\n",
      "\n",
      "I thank all the Quant Shop students from Fall 2015 whose video and modeling efforts are so visibly on display. I particularly thank Jan (Dini) DiskinZimmerman, whose editing efforts went so far beyond the call of duty I felt like a felon for letting her do it.\n",
      "\n",
      "My editors at Springer, Wayne Wheeler and Simon Rees, were a pleasure to work with as usual. I also thank all the production and marketing people who helped get this book to you, including Adrian Pieron and Annette Anlauf.\n",
      "\n",
      "Several exercises were originated by colleagues or inspired by other sources. Reconstructing the original sources years later can be challenging, but credits for each problem (to the best of my recollection) appear on the website.\n",
      "\n",
      "Much of what I know about data science has been learned through working with other people. These include my Ph.D. students, particularly Rami al-Rfou, Mikhail Bautin, Haochen Chen, Yanqing Chen, Vivek Kulkarni, Levon Lloyd, Andrew Mehler, Bryan Perozzi, Yingtao Tian, Junting Ye, Wenbin Zhang, and postdoc Charles Ward. I fondly remember all of my Lydia project masters students over the years, and remind you that my prize offer to the first one who names their daughter Lydia remains unclaimed. I thank my other collaborators with stories to tell, including Bruce Futcher, Justin Gardin, Arnout van de Rijt, and Oleksii Starov.\n",
      "\n",
      "I remember all members of the General Sentiment/Canrock universe, particularly Mark Fasciano, with whom I shared the start-up dream and experienced what happens when data hits the real world. I thank my colleagues at Yahoo Labs/Research during my 2015-2016 sabbatical year, when much of this book was conceived. I single out Amanda Stent, who enabled me to be at Yahoo during that particularly difficult year in the company's history. I learned valuable things from other people who have taught related data science courses, including Andrew Ng and Hans-Peter Pfister, and thank them all for their help.\n",
      "\n",
      "If you have a procedure with ten parameters, you probably missed some.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Alan Perlis\n",
      "\\end{itemize}\n",
      "\n",
      "\\section*{Caveat}\n",
      "It is traditional for the author to magnanimously accept the blame for whatever deficiencies remain. I don't. Any errors, deficiencies, or problems in this book are somebody else's fault, but I would appreciate knowing about them so as to determine who is to blame.\n",
      "\n",
      "\n",
      "\n",
      "\\section*{Contents}\n",
      "1 What is Data Science? ..... 1\\\\\n",
      "1.1 Computer Science, Data Science, and Real Science ..... 2\\\\\n",
      "1.2 Asking Interesting Questions from Data ..... 4\\\\\n",
      "1.2.1 The Baseball Encyclopedia ..... 5\\\\\n",
      "1.2.2 The Internet Movie Database (IMDb) ..... 7\\\\\n",
      "1.2.3 Google Ngrams ..... 10\\\\\n",
      "1.2.4 New York Taxi Records ..... 11\\\\\n",
      "1.3 Properties of Data ..... 14\\\\\n",
      "1.3.1 Structured vs. Unstructured Data ..... 14\\\\\n",
      "1.3.2 Quantitative vs. Categorical Data ..... 15\\\\\n",
      "1.3.3 Big Data vs. Little Data ..... 15\\\\\n",
      "1.4 Classification and Regression ..... 16\\\\\n",
      "1.5 Data Science Television: The Quant Shop ..... 17\\\\\n",
      "1.5.1 Kaggle Challenges ..... 19\\\\\n",
      "1.6 About the War Stories ..... 19\\\\\n",
      "1.7 War Story: Answering the Right Question ..... 21\\\\\n",
      "1.8 Chapter Notes ..... 22\\\\\n",
      "1.9 Exercises ..... 23\\\\\n",
      "2 Mathematical Preliminaries ..... 27\\\\\n",
      "2.1 Probability ..... 27\\\\\n",
      "2.1.1 Probability vs. Statistics ..... 29\\\\\n",
      "2.1.2 Compound Events and Independence ..... 30\\\\\n",
      "2.1.3 Conditional Probability ..... 31\\\\\n",
      "2.1.4 Probability Distributions ..... 32\\\\\n",
      "2.2 Descriptive Statistics ..... 34\\\\\n",
      "2.2.1 Centrality Measures ..... 34\\\\\n",
      "2.2.2 Variability Measures ..... 36\\\\\n",
      "2.2.3 Interpreting Variance ..... 37\\\\\n",
      "2.2.4 Characterizing Distributions ..... 39\\\\\n",
      "2.3 Correlation Analysis ..... 40\\\\\n",
      "2.3.1 Correlation Coefficients: Pearson and Spearman Rank ..... 41\\\\\n",
      "2.3.2 The Power and Significance of Correlation ..... 43\\\\\n",
      "2.3.3 Correlation Does Not Imply Causation! ..... 45\\\\\n",
      "2.3.4 Detecting Periodicities by Autocorrelation ..... 46\\\\\n",
      "2.4 Logarithms ..... 47\\\\\n",
      "2.4.1 Logarithms and Multiplying Probabilities ..... 48\\\\\n",
      "2.4.2 Logarithms and Ratios ..... 48\\\\\n",
      "2.4.3 Logarithms and Normalizing Skewed Distributions ..... 49\\\\\n",
      "2.5 War Story: Fitting Designer Genes ..... 50\\\\\n",
      "2.6 Chapter Notes ..... 52\\\\\n",
      "2.7 Exercises ..... 53\\\\\n",
      "3 Data Munging ..... 57\\\\\n",
      "3.1 Languages for Data Science ..... 57\\\\\n",
      "3.1.1 The Importance of Notebook Environments ..... 59\\\\\n",
      "3.1.2 Standard Data Formats ..... 61\\\\\n",
      "3.2 Collecting Data ..... 64\\\\\n",
      "3.2.1 Hunting ..... 64\\\\\n",
      "3.2.2 Scraping ..... 67\\\\\n",
      "3.2.3 Logging ..... 68\\\\\n",
      "3.3 Cleaning Data ..... 69\\\\\n",
      "3.3.1 Errors vs. Artifacts ..... 69\\\\\n",
      "3.3.2 Data Compatibility ..... 72\\\\\n",
      "3.3.3 Dealing with Missing Values ..... 76\\\\\n",
      "3.3.4 Outlier Detection ..... 78\\\\\n",
      "3.4 War Story: Beating the Market ..... 79\\\\\n",
      "3.5 Crowdsourcing ..... 80\\\\\n",
      "3.5.1 The Penny Demo ..... 81\\\\\n",
      "3.5.2 When is the Crowd Wise? ..... 82\\\\\n",
      "3.5.3 Mechanisms for Aggregation ..... 83\\\\\n",
      "3.5.4 Crowdsourcing Services ..... 84\\\\\n",
      "3.5.5 Gamification ..... 88\\\\\n",
      "3.6 Chapter Notes ..... 90\\\\\n",
      "3.7 Exercises ..... 90\\\\\n",
      "4 Scores and Rankings ..... 95\\\\\n",
      "4.1 The Body Mass Index (BMI) ..... 96\\\\\n",
      "4.2 Developing Scoring Systems ..... 99\\\\\n",
      "4.2.1 Gold Standards and Proxies ..... 99\\\\\n",
      "4.2.2 Scores vs. Rankings ..... 100\\\\\n",
      "4.2.3 Recognizing Good Scoring Functions ..... 101\\\\\n",
      "4.3 Z-scores and Normalization ..... 103\\\\\n",
      "4.4 Advanced Ranking Techniques ..... 104\\\\\n",
      "4.4.1 Elo Rankings ..... 104\\\\\n",
      "4.4.2 Merging Rankings ..... 108\\\\\n",
      "4.4.3 Digraph-based Rankings ..... 109\\\\\n",
      "4.4.4 PageRank ..... 111\\\\\n",
      "4.5 War Story: Clyde's Revenge ..... 111\\\\\n",
      "4.6 Arrow's Impossibility Theorem ..... 114\\\\\n",
      "4.7 War Story: Who's Bigger? ..... 115\\\\\n",
      "4.8 Chapter Notes ..... 118\\\\\n",
      "4.9 Exercises ..... 119\\\\\n",
      "5 Statistical Analysis ..... 121\\\\\n",
      "5.1 Statistical Distributions ..... 122\\\\\n",
      "5.1.1 The Binomial Distribution ..... 123\\\\\n",
      "5.1.2 The Normal Distribution ..... 124\\\\\n",
      "5.1.3 Implications of the Normal Distribution ..... 126\\\\\n",
      "5.1.4 Poisson Distribution ..... 127\\\\\n",
      "5.1.5 Power Law Distributions ..... 129\\\\\n",
      "5.2 Sampling from Distributions ..... 132\\\\\n",
      "5.2.1 Random Sampling beyond One Dimension ..... 133\\\\\n",
      "5.3 Statistical Significance ..... 135\\\\\n",
      "5.3.1 The Significance of Significance ..... 135\\\\\n",
      "5.3.2 The T-test: Comparing Population Means ..... 137\\\\\n",
      "5.3.3 The Kolmogorov-Smirnov Test ..... 139\\\\\n",
      "5.3.4 The Bonferroni Correction ..... 141\\\\\n",
      "5.3.5 False Discovery Rate ..... 142\\\\\n",
      "5.4 War Story: Discovering the Fountain of Youth? ..... 143\\\\\n",
      "5.5 Permutation Tests and P-values ..... 145\\\\\n",
      "5.5.1 Generating Random Permutations ..... 147\\\\\n",
      "5.5.2 DiMaggio's Hitting Streak ..... 148\\\\\n",
      "5.6 Bayesian Reasoning ..... 150\\\\\n",
      "5.7 Chapter Notes ..... 151\\\\\n",
      "5.8 Exercises ..... 151\\\\\n",
      "6 Visualizing Data ..... 155\\\\\n",
      "6.1 Exploratory Data Analysis ..... 156\\\\\n",
      "6.1.1 Confronting a New Data Set ..... 156\\\\\n",
      "6.1.2 Summary Statistics and Anscombe's Quartet ..... 159\\\\\n",
      "6.1.3 Visualization Tools ..... 160\\\\\n",
      "6.2 Developing a Visualization Aesthetic ..... 162\\\\\n",
      "6.2.1 Maximizing Data-Ink Ratio ..... 163\\\\\n",
      "6.2.2 Minimizing the Lie Factor ..... 164\\\\\n",
      "6.2.3 Minimizing Chartjunk ..... 165\\\\\n",
      "6.2.4 Proper Scaling and Labeling ..... 167\\\\\n",
      "6.2.5 Effective Use of Color and Shading ..... 168\\\\\n",
      "6.2.6 The Power of Repetition ..... 169\\\\\n",
      "6.3 Chart Types ..... 170\\\\\n",
      "6.3.1 Tabular Data ..... 170\\\\\n",
      "6.3.2 Dot and Line Plots ..... 174\\\\\n",
      "6.3.3 Scatter Plots ..... 177\\\\\n",
      "6.3.4 Bar Plots and Pie Charts ..... 179\\\\\n",
      "6.3.5 Histograms ..... 183\\\\\n",
      "6.3.6 Data Maps ..... 187\\\\\n",
      "6.4 Great Visualizations ..... 189\\\\\n",
      "6.4.1 Marey's Train Schedule ..... 189\\\\\n",
      "6.4.2 Snow's Cholera Map ..... 191\\\\\n",
      "6.4.3 New York's Weather Year ..... 192\\\\\n",
      "6.5 Reading Graphs ..... 192\\\\\n",
      "6.5.1 The Obscured Distribution ..... 193\\\\\n",
      "6.5.2 Overinterpreting Variance ..... 193\\\\\n",
      "6.6 Interactive Visualization ..... 195\\\\\n",
      "6.7 War Story: TextMapping the World ..... 196\\\\\n",
      "6.8 Chapter Notes ..... 198\\\\\n",
      "6.9 Exercises ..... 199\\\\\n",
      "7 Mathematical Models ..... 201\\\\\n",
      "7.1 Philosophies of Modeling ..... 201\\\\\n",
      "7.1.1 Occam's Razor ..... 201\\\\\n",
      "7.1.2 Bias-Variance Trade-Offs ..... 202\\\\\n",
      "7.1.3 What Would Nate Silver Do? ..... 203\\\\\n",
      "7.2 A Taxonomy of Models ..... 205\\\\\n",
      "7.2.1 Linear vs. Non-Linear Models ..... 206\\\\\n",
      "7.2.2 Blackbox vs. Descriptive Models ..... 206\\\\\n",
      "7.2.3 First-Principle vs. Data-Driven Models ..... 207\\\\\n",
      "7.2.4 Stochastic vs. Deterministic Models ..... 208\\\\\n",
      "7.2.5 Flat vs. Hierarchical Models ..... 209\\\\\n",
      "7.3 Baseline Models ..... 210\\\\\n",
      "7.3.1 Baseline Models for Classification ..... 210\\\\\n",
      "7.3.2 Baseline Models for Value Prediction ..... 212\\\\\n",
      "7.4 Evaluating Models ..... 212\\\\\n",
      "7.4.1 Evaluating Classifiers ..... 213\\\\\n",
      "7.4.2 Receiver-Operator Characteristic (ROC) Curves ..... 218\\\\\n",
      "7.4.3 Evaluating Multiclass Systems ..... 219\\\\\n",
      "7.4.4 Evaluating Value Prediction Models ..... 221\\\\\n",
      "7.5 Evaluation Environments ..... 224\\\\\n",
      "7.5.1 Data Hygiene for Evaluation ..... 225\\\\\n",
      "7.5.2 Amplifying Small Evaluation Sets ..... 226\\\\\n",
      "7.6 War Story: 100\\% Accuracy ..... 228\\\\\n",
      "7.7 Simulation Models ..... 229\\\\\n",
      "7.8 War Story: Calculated Bets ..... 230\\\\\n",
      "7.9 Chapter Notes ..... 233\\\\\n",
      "7.10 Exercises ..... 234\\\\\n",
      "8 Linear Algebra ..... 237\\\\\n",
      "8.1 The Power of Linear Algebra ..... 237\\\\\n",
      "8.1.1 Interpreting Linear Algebraic Formulae ..... 238\\\\\n",
      "8.1.2 Geometry and Vectors ..... 240\\\\\n",
      "8.2 Visualizing Matrix Operations ..... 241\\\\\n",
      "8.2.1 Matrix Addition ..... 242\\\\\n",
      "8.2.2 Matrix Multiplication ..... 243\\\\\n",
      "8.2.3 Applications of Matrix Multiplication ..... 244\\\\\n",
      "8.2.4 Identity Matrices and Inversion ..... 248\\\\\n",
      "8.2.5 Matrix Inversion and Linear Systems ..... 250\\\\\n",
      "8.2.6 Matrix Rank ..... 251\\\\\n",
      "8.3 Factoring Matrices ..... 252\\\\\n",
      "8.3.1 Why Factor Feature Matrices? ..... 252\\\\\n",
      "8.3.2 LU Decomposition and Determinants ..... 254\\\\\n",
      "8.4 Eigenvalues and Eigenvectors ..... 255\\\\\n",
      "8.4.1 Properties of Eigenvalues ..... 255\\\\\n",
      "8.4.2 Computing Eigenvalues ..... 256\\\\\n",
      "8.5 Eigenvalue Decomposition ..... 257\\\\\n",
      "8.5.1 Singular Value Decomposition ..... 258\\\\\n",
      "8.5.2 Principal Components Analysis ..... 260\\\\\n",
      "8.6 War Story: The Human Factors ..... 262\\\\\n",
      "8.7 Chapter Notes ..... 263\\\\\n",
      "8.8 Exercises ..... 263\\\\\n",
      "9 Linear and Logistic Regression ..... 267\\\\\n",
      "9.1 Linear Regression ..... 268\\\\\n",
      "9.1.1 Linear Regression and Duality ..... 268\\\\\n",
      "9.1.2 Error in Linear Regression ..... 269\\\\\n",
      "9.1.3 Finding the Optimal Fit ..... 270\\\\\n",
      "9.2 Better Regression Models ..... 272\\\\\n",
      "9.2.1 Removing Outliers ..... 272\\\\\n",
      "9.2.2 Fitting Non-Linear Functions ..... 273\\\\\n",
      "9.2.3 Feature and Target Scaling ..... 274\\\\\n",
      "9.2.4 Dealing with Highly-Correlated Features ..... 277\\\\\n",
      "9.3 War Story: Taxi Deriver ..... 277\\\\\n",
      "9.4 Regression as Parameter Fitting ..... 279\\\\\n",
      "9.4.1 Convex Parameter Spaces ..... 280\\\\\n",
      "9.4.2 Gradient Descent Search ..... 281\\\\\n",
      "9.4.3 What is the Right Learning Rate? ..... 283\\\\\n",
      "9.4.4 Stochastic Gradient Descent ..... 285\\\\\n",
      "9.5 Simplifying Models through Regularization ..... 286\\\\\n",
      "9.5.1 Ridge Regression ..... 286\\\\\n",
      "9.5.2 LASSO Regression ..... 287\\\\\n",
      "9.5.3 Trade-Offs between Fit and Complexity ..... 288\\\\\n",
      "9.6 Classification and Logistic Regression ..... 289\\\\\n",
      "9.6.1 Regression for Classification ..... 290\\\\\n",
      "9.6.2 Decision Boundaries ..... 291\\\\\n",
      "9.6.3 Logistic Regression ..... 292\\\\\n",
      "9.7 Issues in Logistic Classification ..... 295\\\\\n",
      "9.7.1 Balanced Training Classes ..... 295\\\\\n",
      "9.7.2 Multi-Class Classification ..... 297\\\\\n",
      "9.7.3 Hierarchical Classification ..... 298\\\\\n",
      "9.7.4 Partition Functions and Multinomial Regression ..... 299\\\\\n",
      "9.8 Chapter Notes ..... 300\\\\\n",
      "9.9 Exercises ..... 301\\\\\n",
      "10 Distance and Network Methods ..... 303\\\\\n",
      "10.1 Measuring Distances ..... 303\\\\\n",
      "10.1.1 Distance Metrics ..... 304\\\\\n",
      "10.1.2 The $L_{k}$ Distance Metric ..... 305\\\\\n",
      "10.1.3 Working in Higher Dimensions ..... 307\\\\\n",
      "10.1.4 Dimensional Egalitarianism ..... 308\\\\\n",
      "10.1.5 Points vs. Vectors ..... 309\\\\\n",
      "10.1.6 Distances between Probability Distributions ..... 310\\\\\n",
      "10.2 Nearest Neighbor Classification ..... 311\\\\\n",
      "10.2.1 Seeking Good Analogies ..... 312\\\\\n",
      "10.2.2 $k$-Nearest Neighbors ..... 313\\\\\n",
      "10.2.3 Finding Nearest Neighbors ..... 315\\\\\n",
      "10.2.4 Locality Sensitive Hashing ..... 317\\\\\n",
      "10.3 Graphs, Networks, and Distances ..... 319\\\\\n",
      "10.3.1 Weighted Graphs and Induced Networks ..... 320\\\\\n",
      "10.3.2 Talking About Graphs ..... 321\\\\\n",
      "10.3.3 Graph Theory ..... 323\\\\\n",
      "10.4 PageRank ..... 325\\\\\n",
      "10.5 Clustering ..... 327\\\\\n",
      "10.5.1 $k$-means Clustering ..... 330\\\\\n",
      "10.5.2 Agglomerative Clustering ..... 336\\\\\n",
      "10.5.3 Comparing Clusterings ..... 341\\\\\n",
      "10.5.4 Similarity Graphs and Cut-Based Clustering ..... 341\\\\\n",
      "10.6 War Story: Cluster Bombing ..... 344\\\\\n",
      "10.7 Chapter Notes ..... 345\\\\\n",
      "10.8 Exercises ..... 346\\\\\n",
      "11 Machine Learning ..... 351\\\\\n",
      "11.1 Naive Bayes ..... 354\\\\\n",
      "11.1.1 Formulation ..... 354\\\\\n",
      "11.1.2 Dealing with Zero Counts (Discounting) ..... 356\\\\\n",
      "11.2 Decision Tree Classifiers ..... 357\\\\\n",
      "11.2.1 Constructing Decision Trees ..... 359\\\\\n",
      "11.2.2 Realizing Exclusive Or ..... 361\\\\\n",
      "11.2.3 Ensembles of Decision Trees ..... 362\\\\\n",
      "11.3 Boosting and Ensemble Learning ..... 363\\\\\n",
      "11.3.1 Voting with Classifiers ..... 363\\\\\n",
      "11.3.2 Boosting Algorithms ..... 364\\\\\n",
      "11.4 Support Vector Machines ..... 366\\\\\n",
      "11.4.1 Linear SVMs ..... 369\\\\\n",
      "11.4.2 Non-linear SVMs ..... 369\\\\\n",
      "11.4.3 Kernels ..... 371\\\\\n",
      "11.5 Degrees of Supervision ..... 372\\\\\n",
      "11.5.1 Supervised Learning ..... 372\\\\\n",
      "11.5.2 Unsupervised Learning ..... 372\\\\\n",
      "11.5.3 Semi-supervised Learning ..... 374\\\\\n",
      "11.5.4 Feature Engineering ..... 375\\\\\n",
      "11.6 Deep Learning ..... 377\\\\\n",
      "11.6.1 Networks and Depth ..... 378\\\\\n",
      "11.6.2 Backpropagation ..... 382\\\\\n",
      "11.6.3 Word and Graph Embeddings ..... 383\\\\\n",
      "11.7 War Story: The Name Game ..... 385\\\\\n",
      "11.8 Chapter Notes ..... 387\\\\\n",
      "11.9 Exercises ..... 388\\\\\n",
      "12 Big Data: Achieving Scale ..... 391\\\\\n",
      "12.1 What is Big Data? ..... 392\\\\\n",
      "12.1.1 Big Data as Bad Data ..... 392\\\\\n",
      "12.1.2 The Three Vs ..... 394\\\\\n",
      "12.2 War Story: Infrastructure Matters ..... 395\\\\\n",
      "12.3 Algorithmics for Big Data ..... 397\\\\\n",
      "12.3.1 Big Oh Analysis ..... 397\\\\\n",
      "12.3.2 Hashing ..... 399\\\\\n",
      "12.3.3 Exploiting the Storage Hierarchy ..... 401\\\\\n",
      "12.3.4 Streaming and Single-Pass Algorithms ..... 402\\\\\n",
      "12.4 Filtering and Sampling ..... 403\\\\\n",
      "12.4.1 Deterministic Sampling Algorithms ..... 404\\\\\n",
      "12.4.2 Randomized and Stream Sampling ..... 406\\\\\n",
      "12.5 Parallelism ..... 406\\\\\n",
      "12.5.1 One, Two, Many ..... 407\\\\\n",
      "12.5.2 Data Parallelism ..... 409\\\\\n",
      "12.5.3 Grid Search ..... 409\\\\\n",
      "12.5.4 Cloud Computing Services ..... 410\\\\\n",
      "12.6 MapReduce ..... 410\\\\\n",
      "12.6.1 Map-Reduce Programming ..... 412\\\\\n",
      "12.6.2 MapReduce under the Hood ..... 414\\\\\n",
      "12.7 Societal and Ethical Implications ..... 416\\\\\n",
      "12.8 Chapter Notes ..... 419\\\\\n",
      "12.9 Exercises ..... 419\\\\\n",
      "13 Coda ..... 423\\\\\n",
      "13.1 Get a Job! ..... 423\\\\\n",
      "13.2 Go to Graduate School! ..... 424\\\\\n",
      "13.3 Professional Consulting Services ..... 425\\\\\n",
      "14 Bibliography ..... 427\n",
      "\n",
      "\\section*{Chapter 1}\n",
      "\\section*{What is Data Science?}\n",
      "What is data science? Like any emerging field, it hasn't been completely defined yet, but you know enough about it to be interested or else you wouldn't be reading this book.\n",
      "\n",
      "I think of data science as lying at the intersection of computer science, statistics, and substantive application domains. From computer science comes machine learning and high-performance computing technologies for dealing with scale. From statistics comes a long tradition of exploratory data analysis, significance testing, and visualization. From application domains in business and the sciences comes challenges worthy of battle, and evaluation standards to assess when they have been adequately conquered.\n",
      "\n",
      "But these are all well-established fields. Why data science, and why now? I see three reasons for this sudden burst of activity:\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item New technology makes it possible to capture, annotate, and store vast amounts of social media, logging, and sensor data. After you have amassed all this data, you begin to wonder what you can do with it.\n",
      "  \\item Computing advances make it possible to analyze data in novel ways and at ever increasing scales. Cloud computing architectures give even the little guy access to vast power when they need it. New approaches to machine learning have lead to amazing advances in longstanding problems, like computer vision and natural language processing.\n",
      "  \\item Prominent technology companies (like Google and Facebook) and quantitative \\index{hedge fund}s (like Renaissance Technologies and TwoSigma) have proven the power of modern data analytics. Success stories applying data to such diverse areas as sports management (Moneyball [Lew04) and election forecasting (Nate Silver [Sil12]) have served as role models to bring data science to a large popular audience.\n",
      "\\end{itemize}\n",
      "\n",
      "This introductory chapter has three missions. First, I will try to explain how good data scientists think, and how this differs from the mindset of traditional programmers and software developers. Second, we will look at data sets in terms of the potential for what they can be used for, and learn to ask the broader questions they are capable of answering. Finally, I introduce a collection of data analysis challenges that will be used throughout this book as motivating examples.\n",
      "\n",
      "\\subsection*{1.1 Computer Science, Data Science, and Real Science}\n",
      "Computer scientists, by nature, don't respect data. They have traditionally been taught that the algorithm was the thing, and that data was just meat to be passed through a sausage grinder.\n",
      "\n",
      "So to qualify as an effective data scientist, you must first learn to think like a real scientist. Real scientists strive to understand the natural world, which is a complicated and messy place. By contrast, computer scientists tend to build their own clean and organized virtual worlds and live comfortably within them. Scientists obsess about discovering things, while computer scientists invent rather than discover.\n",
      "\n",
      "People's mindsets strongly color how they think and act, causing misunderstandings when we try to communicate outside our tribes. So fundamental are these biases that we are often unaware we have them. Examples of the cultural differences between computer science and real science include:\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Data vs. metho\\index{da\\index{method centrism}ntists are data driven, while computer scientists are algorithm driven. Real scientists spend enormous amounts of effort collecting data to answer their question of interest. They invent fancy measuring devices, stay up all night tending to experiments, and devote most of their thinking to how to get the data they need.\\\\\n",
      "By contrast, computer scientists obsess about methods: which algorithm is better than which other algorithm, which programming language is best for a job, which program is better than which other program. The details of the data set they are working on seem comparably unexciting.\n",
      "  \\item Concern about results: Real scientists care about answers. They analyze data to discover something about how the world works. Good scientists care about whether the results make sense, because they care about what the answers mean.\\\\\n",
      "By contrast, bad computer scientists worry about producing plausiblelooking numbers. As soon as the numbers stop looking grossly wrong, they are presumed to be right. This is because they are personally less invested in what can be learned from a computation, as oppose\n",
      "Checking for word :Adolf Hitler\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Andrew Lang\n",
      " Current page :  267\n",
      "No Match found\n",
      "Page Content:\n",
      "on is not commutative.\\\\[0pt]\n",
      "8-2. [3] Prove that matrix addition is associative, i.e. that $(A+B)+C=A+(B+C)$ for compatible matrices $A, B$ and $C$.\\\\[0pt]\n",
      "8-3. [5] Prove that matrix multiplication is associative, i.e. that $(A B) C=A(B C)$ for compatible matrices $A, B$ and $C$.\\\\[0pt]\n",
      "8-4. [3] Prove that $A B=B A$, if $A$ and $B$ are diagonal matrices of the same order.\\\\[0pt]\n",
      "8-5. [5] Prove that if $A C=C A$ and $B C=C B$, then\n",
      "\n",
      "$$\n",
      "C(A B+B A)=(A B+B A) C .\n",
      "$$\n",
      "\n",
      "8-6. [3] Are the matrices $M M^{T}$ and $M^{T} M$ square and symmetric? Explain.\\\\[0pt]\n",
      "8-7. [5] Prove that $\\left(A^{-1}\\right)^{-1}=A$.\\\\[0pt]\n",
      "8-8. [5] Prove that $\\left(A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T}$ for any non-singular matrix $A$.\\\\[0pt]\n",
      "8-9. [5] Is the LU factorization of a matrix unique? Justify your answer.\\\\[0pt]\n",
      "8-10. [3] Explain how to solve the matrix equation $A x=b$ ?\\\\[0pt]\n",
      "8-11. [5] Show that if $M$ is a square matrix which is not invertible, then either $L$ or $U$ in the LU-decomposition $M=L \\cdot U$ has a zero in its diagonal.\n",
      "\n",
      "\\section*{Eigenvalues and Eigenvectors}\n",
      "8-12. [3] Let $M=\\left[\\begin{array}{ll}2 & 1 \\\\ 0 & 2\\end{array}\\right]$. Find all eigenvalues of $M$. Does $M$ have two linearly independent eigenvectors?\\\\[0pt]\n",
      "8-13. [3] Prove that the eigenvalues of $A$ and $A^{T}$ are identical.\\\\[0pt]\n",
      "8-14. [3] Prove that the eigenvalues of a diagonal matrix are equal to the diagonal elements.\\\\[0pt]\n",
      "8-15. [5] Suppose that matrix $A$ has an eigenvector $v$ with eigenvalue $\\lambda$. Show that $v$ is also an eigenvector for $A^{2}$, and find the corresponding eigenvalue. How about for $A^{k}$, for $2 \\leq k \\leq n$ ?\\\\[0pt]\n",
      "8-16. [5] Suppose that $A$ is an invertible matrix with eigenvector $v$. Show that $v$ is also an eigenvector for $A^{-1}$.\\\\[0pt]\n",
      "8-17. [8] Show that the eigenvalues of $M M^{T}$ are the same as that of $M^{T} M$. Are their eigenvectors also the same?\n",
      "\n",
      "\\section*{Implementation Projects}\n",
      "8-18. [5] Compare the speed of a library function for matrix multiplication to your own implementation of the nested loops algorithm.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item How much faster is the library on products of random $n \\times n$ matricies, as a function of $n$ as $n$ gets large?\n",
      "  \\item What about the product of an $n \\times m$ and $m \\times n$ matrix, where $n \\ll m$ ?\n",
      "  \\item By how much do you improve the performance of your implementation to calculate $C=A \\cdot B$ by first transposing $B$ internally, so all dot products are computed along rows of the matrices to improve cache performance?\\\\[0pt]\n",
      "8-19. [5] Implement Gaussian elimination for solving systems of equations, $C \\cdot X=Y$. Compare your implementation against a popular library routine for:\\\n",
      "%---- Page End Break Here ---- Page : 264\n",
      "\\\n",
      "(a) Speed: How does the run time compare, for both dense and sparse coefficient matrices?\\\\\n",
      "(b) Accuracy: What are the size of the numerical \\index{residual}s $C X-Y$, particularly as the condition number of the matrix increases.\\\\\n",
      "(c) Stability: Does your program crash on a singular matrix? What about almost singular matrices, created by adding a little random noise to a singular matrix?\n",
      "\\end{itemize}\n",
      "\n",
      "\\section*{Interview Questions}\n",
      "8-20. [5] Why is vectorization considered a powerful method for optimizing numerical code?\n",
      "\n",
      "8-21. [3] What is singular value decomposition? What is a singular value? And what is a singular vector?\n",
      "\n",
      "8-22. [5] Explain the difference between \"long\" and \"wide\" format data. When might each arise in practice?\n",
      "\n",
      "\\section*{Kaggle Challenges}\n",
      "$8-23$. Tell what someone is looking at from analysis of their brain waves.\\\\\n",
      "\\href{https://www.kaggle.com/c/decoding-the-human-brain}{https://www.kaggle.com/c/decoding-the-human-brain}\\\\\n",
      "8 -24. Decide whether a particular student will answer a given question correctly. \\href\n",
      "Checking for word :Carl Linnaeus\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Richard Nixon\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Barack Obama\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Barack] Obama\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Ronald Reagan\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Franklin D. Roosevelt\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Checking for word :Jan Schaumann\n",
      " Current page :  351\n",
      "No Match found\n",
      "Page Content:\n",
      "]\n",
      "10-10. [5] Repeat the previous question, but where we now classify each point according to its three nearest neighbors $(k=3)$.\\\\[0pt]\n",
      "10-11. [5] Suppose a two-class, $k=1$ nearest-neighbor classifier is trained with at least three positive points and at least three negative points.\\\\\n",
      "(a) Might it possible this classifier could label all new examples as positive?\\\\\n",
      "(b) What if $k=3$ ?\n",
      "\n",
      "\\section*{Networks}\n",
      "10-12. [3] Give explanations for what the nodes with the largest in-degree and outdegree might be in the following graphs:\\\\\n",
      "(a) The telephone graph, where edge $(x, y)$ means $x$ calls $y$.\\\\\n",
      "(b) The Twitter graph, where edge $(x, y)$ means $x$ follows $y$.\n",
      "\n",
      "10-13. [3] Power law distributions on vertex degree in networks usually result from preferential attachment, a mechanism by which new edges are more likely to connect to nodes of high degree. For each of the following graphs, suggest what their vertex degree distribution is, and if they are power law distributed describe what the preferential attachment mechanism might be.\\\\\n",
      "(a) Social networks like Facebook or Instagram.\\\\\n",
      "(b) Sites on the World Wide Web (WWW).\\\\\n",
      "(c) Road networks connecting cities.\\\n",
      "%---- Page End Break Here ---- Page : 346\n",
      "\\\n",
      "(d) Product/customer networks like Amazon or Netflix.\n",
      "\n",
      "10-14. [5] For each of the following graph-theoretic properties, give an example of a real-world network that satisfies the property, and a second network which does not.\\\\\n",
      "(a) Directed vs. undirected.\\\\\n",
      "(b) Weighted vs. unweighted.\\\\\n",
      "(c) Simple vs. non-simple.\\\\\n",
      "(d) Sparse vs. dense.\\\\\n",
      "(e) Embedded vs. topological.\\\\\n",
      "(f) Labeled vs. unlabeled.\n",
      "\n",
      "10-15. [3] Prove that in any simple graph, there are always an even number of vertices with odd vertex degree.\\\\[0pt]\n",
      "10-16. [3] Implement a simple version of the PageRank algorithm, and test it on your favorite network. Which vertices get highlighted as most central?\n",
      "\n",
      "\\section*{Clustering}\n",
      "10-17. [5] For a data set with points at positions $(4,10),(7,10)(4,8),(6,8),(3,4)$, $(2,2),(5,2),(9,3),(12,3),(11,4),(10,5)$, and $(12,6)$, show the clustering that results from\\\\\n",
      "(a) Single-linkage clustering\\\\\n",
      "(b) Average-linkage clustering\\\\\n",
      "(c) Furthest-neighbor (complete linkage) clustering.\n",
      "\n",
      "10-18. [3] For each of the following The Quant Shop prediction challenges, propose available data that might make it feasible to employ nearest-neighbor/analogical methods to the task:\\\\\n",
      "(a) Miss Universe.\\\\\n",
      "(b) Movie gross.\\\\\n",
      "(c) Baby weight.\\\\\n",
      "(d) Art auction price.\\\\\n",
      "(e) White Christmas.\\\\\n",
      "(f) Football champions.\\\\\n",
      "(g) Ghoul pool.\\\\\n",
      "(h) Gold/oil prices.\n",
      "\n",
      "10-19. [3] Perform $k$-means clustering manually on the following points, for $k=2$ :\n",
      "\n",
      "$$\n",
      "S=\\{(1,4),(1,3),(0,4),(5,1),(6,2),(4,0)\\}\n",
      "$$\n",
      "\n",
      "Plot the points and the final clusters.\n",
      "\n",
      "10-20. [5] Implement two versions of a simple $k$-means algorithm: one of which uses numerical centroids as centers, the other of which restricts centers to be input points from the data set. Then experiment. Which algorithm converges faster on average? Which algorithm produces clusterings with lower absolute and mean-squared error, and by how much?\\\\[0pt]\n",
      "10-21. [5] Suppose $s_{1}$ and $s_{2}$ are randomly selected subsets from a universal set with $n$ items. What is the expected value of the Jaccard similarity $J\\left(s_{1}, s_{2}\\right)$ ?\\\\[0pt]\n",
      "10-22. [5] Identify a data set on entities where you have some sense of natural clusters which should emerge, be it on people, universities, companies, or movies. Cluster it by one or more algorithms, perhaps $k$-means and agglomerative clustering. Then evaluate the resulting clusters based on your knowledge of the domain. Did they do a good job? What things did it get wrong? Can you explain why the algorithm did not reconstruct what was in your head?\\\\[0pt]\n",
      "10-23. [5] Assume that we are trying to cluster $n=10$ points in one dimension, where point $p_{i}$ has a position of $x=i$. What is the agglomerative clustering tree for these points under\\\\\n",
      "(a) Single-link clustering\\\\\n",
      "(b) Average-link clustering\\\\\n",
      "(c) Complete-link/furthest-neighbor clustering\n",
      "\n",
      "10-24. [5] Assume that we are trying to cluster $n=10$ points in one dimension, where point $p_{i}$ has a position of $x=2^{i}$. What is the agglomerative clustering tree for these points under\\\\\n",
      "(a) Single-link clustering\\\\\n",
      "(b) Average-link clustering\\\\\n",
      "(c) Complete-link/furthest-neighbor clustering\n",
      "\n",
      "\\section*{Implementation Projects}\n",
      "$10-25$. [5] Do experiments studying the impact of merging criteria (single-link, centroid, average-link, furthest link) on the properties of the resulting cluster tree. Which leads to the tallest trees? The most balanced? How do their running times compare? Which method produces results most consistent with $k$-means clustering?\\\\[0pt]\n",
      "10-26. [5] Experiment with the performance of different algorithms/data structures for finding the nearest neighbor of a query point $q$ among $n$ points in $d$ dimensions. What is the maximum $d$ for which each method remains viable? How much faster are heuristic methods based on LSH than methods that guarantee the exact nearest neighbor, at what loss of accuracy?\n",
      "\n",
      "\\section*{Interview Questions}\n",
      "10-27. [5] What is curse of dimensionality? How does it affect distance and similarity measures?\\\\[0pt]\n",
      "10-28. [5] What is clustering? Describe an example algorithm that performs clustering. How can we know whether it produced decent clusters on our data set?\\\\[0pt]\n",
      "10-29. [5] How might we be able to estimate the right number of clusters to use with a given data set?\n",
      "\n",
      "%---- Page End Break Here ---- Page : 348\n",
      "\n",
      "10-30. [5] What is the difference between unsupervised and supervised learning?\\\\[0pt]\n",
      "10-31. [5] How can you deal with correlated features in your data set by reducing the dimensionality of the data.\\\\[0pt]\n",
      "10-32. [5] Explain what a local optimum is. Why is it important in $k$-means clustering?\n",
      "\n",
      "\\section*{Kaggle Challenges}\n",
      "10-33. Which people a\n",
      "Checking for word :William Shakespeare\n",
      " Current page :  326\n",
      "No Match found\n",
      "Page Content:\n",
      " which may or may not have algorithmic significance.\n",
      "\\end{itemize}\n",
      "\n",
      "Occasionally, the structure of a graph is completely defined by the geometry of its embedding, as we have seen in the definition of the distance graph where the weights are defined by the Euclidean distance between each pair of points. Low-dimensional representations of adjacency matrices by SVD also qualify as embeddings, point representations that capture much of the connectivity information of the graph.\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item \\index{labeled} vs. \\index{unlabeled} - Each vertex is assigned a unique name or identifier in a labeled graph to distinguish it from all other vertices. In un\\index{labeled graphs} no such distinctions are made.\n",
      "\\end{itemize}\n",
      "\n",
      "Graphs arising in data science applications are often naturally and meaningfully labeled, such as city names in a transportation network. These are useful as identifiers for representative examples, and also to provide linkages to external data sources where appropriate.\n",
      "\n",
      "\\subsection*{10.3.3 \\index{graph theory}}\n",
      "Graph theory is an important area of mathematics which deals with the fundamental properties of networks and how to compute them. Most computer science students get exposed to graph theory through their courses in discrete structures or algorithms.\n",
      "\n",
      "The classical algorithms for finding \\index{shortest paths}, \\index{connected components}, spanning trees, cuts, \\index{matchings} and \\index{topological sorting} can be applied to any reasonable graph. However, I have not seen these tools applied as generally in data science as I think they should be. One reason is that the graphs in data science tend to be very large, limiting the complexity of what can be done with them. But a lot is simply myopia: people do not see that a distance or similarity matrix is really just a graph than can take advantage of other tools.\n",
      "\n",
      "I take the opportunity here to review the connections of these fundamental problems to data science, and encourage the interested reader to deepen their understanding through my algorithm book [Ski08].\n",
      "\n",
      "%---- Page End Break Here ---- Page : 323\n",
      "\n",
      "\\begin{itemize}\n",
      "  \\item Shortest paths: For a distance \"matrix\" $m$, the value of $m[i, j]$ should reflect the minimum length path between vertices $i$ and $j$. Note that independent estimates of pairwise distance are often inconsistent, and do not necessarily satisfy the triangle inequality. But when $m^{\\prime}[i, j]$ reflects the shortest path distance from $i$ to $j$ in any matrix $m$ it must satisfy the metric properties. This may well present a better matrix for analysis than the original.\n",
      "  \\item Connected components: Each disjoint piece of a graph is called a connected component. Identifying whether your graph consists of a single component or multiple pieces is important. First, any algorithms you run will achieve better performance if you deal with the components independently. Separate components can be independent for sound reasons, e.g. there is no road crossing between the United States and Europe because of an ocean. But separate components might indicate trouble, such as processing artifacts or insufficient connectivity to work with.\n",
      "  \\item \\index{minimum spanning tree}s: A spanning tree is the minimal set of edges linking all the vertices in a graph, essentially a proof that the graph is connected. The minimum weight spanning tree serves as the sparsest possible representation of the structure of the graph, making it useful for visualization. Indeed, we will show that minimum spanning trees have an important role in clustering algorithms in Section 10.5\n",
      "  \\item \\index{edge cuts}: A cluster in a graph is defined by a subset of vertices $c$, with the property that (a) there is considerable similarity between pairs of vertices within $c$, and (b) there is weak connectivity between vertices in $c$ and out of $c$. The edges $(x, y)$ where $x \\in c$ and $y \\notin c$ define a cut separating the cluster from the rest of the graph, making finding such cuts an important aspect of cluster analysis.\n",
      "  \\item Matchings: Marrying off each vertex with a similar, loyal partner can be useful in many ways. Interesting types of comparisons become possible after such a matching. For example, looking at all close pairs that differ in one attribute (say gender) might shed light on how that variable impacts a particular outcome variable (think income or lifespan). Matchings also provide ways to reduce the effective size of a network. By replacing each matched pair with a vertex representing its centroid, we can construct a graph with half the vertices, but still representative of the whole.\n",
      "  \\item Topological sorting: Ranking problems (recall Chapter 4) impose a pecking order on a collection of items according to some merit criteria. Topological sorting ranks the vertices of a directed acyclic graph (DAG) so edge $(i, j)$ implies that $i$ ranks above $j$ in the pecking order. Given a collection of observed constraints of the form \" $i$ should rank above $j$,\" topological sorting defines an item-order consistent with these observations.\n",
      "\n",
      "%---- Page End Break Here ---- Page : 324\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection*{10.4 \\index{PageRank}}\n",
      "It is often valuable to categorize the relative importance of vertices in a graph. Perhaps the simplest notion is based on vertex degree, the number of edges connecting vertex $v$ to the rest of the graph. The more connected a vertex is, the more important it probably is.\n",
      "\n",
      "The \\index{degree of vertex} $v$ makes a good feature to represent the item associated with $v$. But even better is PageRank [BP98], the original secret sauce behind Google's search engine. PageRank ignores the textual content of webpages, to focus only on the structure of the hyperlinks between pages. The more important pages (vertices) should have higher in-degree than lesser pages, for sure. But the importance of the pages that link to you also matter. Having a large stable of contacts recommending you for a job is great, but it is even better when one of them is currently serving as the President of the United States.\n",
      "\n",
      "PageRank is best understood in the context of random walks along a network. Suppose we start from an arbitrary vertex and then randomly select an outgoing link uniformly from the set of possibilities. Now repeat the process from here, jumping to a random neighbor of our current location at each step. The PageRank of vertex $v$ is a measure of probability that, starting from a random vertex, you will arrive at $v$ after a long series of such random steps. The basic formula for the PageRank of $v(P R(v))$ is:\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{P R_{j-1}(u)}{\\text { out }-\\operatorname{degree}(u)}\n",
      "$$\n",
      "\n",
      "This is a recursive formula, with $j$ as the iteration number. We initialize $P R_{0}\\left(v_{i}\\right)=1 / n$ for each vertex $v_{i}$ in the network, where $1 \\leq i \\leq n$. The PageRank values will change in each iteration, but converge surprisingly quickly to stable values. For undirected graphs, this probability is essentially the same as each vertex's in-degree, but much more interesting things happen with directed graphs.\n",
      "\n",
      "In essence, PageRank relies on the idea that if all roads lead to Rome, Rome must be a pretty important place. It is the paths to your page that counts. This is what makes PageRank hard to game: other people must link to your webpage, and whatever shouting you do about yourself is irrelevant.\n",
      "\n",
      "There are several tweaks one can make to this basic PageRank formula to make the results more interesting. We can allow the walk to jump to an arbitrary vertex (instead of a linked neighbor) to allow faster diffusion in the network. Let $p$ be the probability of following a link in the next step, also known as the \\index{damping factor}. Then\n",
      "\n",
      "$$\n",
      "P R_{j}(v)=\\sum_{(u, v) \\in E} \\frac{(1-p)}{n}+p \\frac{P R_{j-1}(u)}{\\text { out-degree }(u)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of vertices in the graph. Other enhancements involve making changes to the network itself. By adding edges from every vertex to a\n",
      "\n",
      "%---- Page End Break H\n",
      "Matched: 0, Not Matched: 16\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content_2, not_found_terms_2= add_indexes_for_names(indexed_latex_content, not_found_terms_2) #v4 added fuzzy matching with larger range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(OUTPUT_TEX_PATH), exist_ok=True)\n",
    "with open(OUTPUT_TEX_PATH, \"w\", encoding=\"utf-8\") as file:\n",
    "     file.write(indexed_latex_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[359]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index['application']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../../\"\n",
    "\n",
    "BOOK_PATH =  ROOT + \"files/algorithms_book/algorithms-content.pdf\"\n",
    "TEX_PATH = ROOT + \"files/algorithms_book/outputs/algorithms-content.tex\"\n",
    "INDEX_PATH = ROOT + \"files/algorithms_book/algorithms-index.pdf\"\n",
    "OUTPUT_TEX_PATH = ROOT + \"files/dsf_book/outputs/aalgorithms-indexed-content.tex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_book_pdf = pymupdf.open(BOOK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(og_book_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_pdf = pymupdf.open(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data = {}\n",
    "for i in range(len(book_pdf)):\n",
    "    page = book_pdf[i]\n",
    "    index_data[i] = page.get_text(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Index\\nϵ-moves, 703\\n#P-completeness, 476, 548\\n0/1 knapsack problem, 497\\n2/3 tree, 443\\n3-SAT, 368\\nabove-below test, 475, 624\\nabracadabra, 686\\nabstract graph type, 454\\nacademic institutions – licensing,\\n713\\nacceptance-rejection method, 488\\nAckerman function, 459\\nacyclic graph, 199\\nacyclic subgraph, 618\\nAda, 439\\nadaptive compression algorithms,\\n695\\nAdaptive Simulated Annealing\\n(ASA), 481\\naddition, 493\\nadjacency list, 204, 452\\nadjacency matrix, 203, 452\\nadjacent swaps, 520\\nAdvanced Encryption Standard,\\n697\\nadvice – caveat, 438\\naesthetically pleasing drawings, 574\\naggregate range queries, 642\\nagrep, 691\\nAho–Corasick algorithm, 686\\nair travel pricing, 125\\nairline distance metric, 393\\nairline scheduling, 482, 682\\nalgorist, 22\\nAlgorist Technologies—consulting,\\n718\\nalgorithm design, 429\\nalgorithmic resources, 713\\naligning DNA sequences, 706\\nalignment costs, 689\\nall-pairs shortest path, 261, 452,\\n556\\nalpha-beta pruning, 160, 510\\nalpha-shapes, 628\\namortized analysis, 444\\nanalog channel, 523\\nancestor, 18\\nangular resolution, 574\\nanimation – motion planning, 667\\nanimation – sorting, 509\\napproximate nearest-neighbor\\nsearch, 463, 639\\napproximate string matching, 314,\\n687, 688, 706\\napproximate string matching –\\nrelated problems, 687, 708\\napproximation algorithms, 389, 470\\napproximation scheme, 500, 597\\narbitrary-precision arithmetic, 493\\narbitrary-precision arithmetic –\\ngeometry, 623\\narbitrary-precision arithmetic –\\nrelated problems, 533\\narchitectural models, 648\\narea computations – applications,\\n665\\narea computations – triangles, 624\\n769\\n© The Editor(s) (if applicable) and The Author(s), under exclusive license to\\nS. S. Skiena, The Algorithm Design Manual, Texts in Computer Science,\\nhttps://doi.org/10.1007/978-3-030-54256-6\\nSpringer Nature Switzerland AG 2020\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"(.+?),\\s*((?:\\d+,?\\s*)+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for page, text in index_data.items():\n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    for line in lines:\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            term = match.group(1).strip()\n",
    "            pages = [int(p) for p in re.findall(r\"\\d+\", match.group(2))]\n",
    "            index[term] = pages\n",
    "\n",
    "    # for match in pattern.finditer(text):\n",
    "    #     term = match.group(1).strip()\n",
    "    #     pages = [int(p) for p in re.findall(r\"\\d+\", match.group(2))]\n",
    "    #     index[term] = pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ϵ-moves': [703],\n",
       " '#P-completeness': [476, 548],\n",
       " '0/1 knapsack problem': [497],\n",
       " '2/3 tree': [443],\n",
       " '3-SAT': [368],\n",
       " 'above-below test': [475, 624],\n",
       " 'abracadabra': [686],\n",
       " 'abstract graph type': [454],\n",
       " 'acceptance-rejection method': [488],\n",
       " 'Ackerman function': [459],\n",
       " 'acyclic graph': [199],\n",
       " 'acyclic subgraph': [618],\n",
       " 'Ada': [439],\n",
       " '(ASA)': [481],\n",
       " 'addition': [493],\n",
       " 'adjacency list': [204, 452],\n",
       " 'adjacency matrix': [203, 452],\n",
       " 'adjacent swaps': [520],\n",
       " 'advice – caveat': [438],\n",
       " 'aesthetically pleasing drawings': [574],\n",
       " 'aggregate range queries': [642],\n",
       " 'agrep': [691],\n",
       " 'Aho–Corasick algorithm': [686],\n",
       " 'air travel pricing': [125],\n",
       " 'airline distance metric': [393],\n",
       " 'airline scheduling': [482, 682],\n",
       " 'algorist': [22],\n",
       " 'algorithm design': [429],\n",
       " 'algorithmic resources': [713],\n",
       " 'aligning DNA sequences': [706],\n",
       " 'alignment costs': [689],\n",
       " 'all-pairs shortest path': [261, 452],\n",
       " 'alpha-beta pruning': [160, 510],\n",
       " 'alpha-shapes': [628],\n",
       " 'amortized analysis': [444],\n",
       " 'analog channel': [523],\n",
       " 'ancestor': [18],\n",
       " 'angular resolution': [574],\n",
       " 'animation – motion planning': [667],\n",
       " 'animation – sorting': [509],\n",
       " 'search': [463, 639],\n",
       " 'approximate string matching': [314],\n",
       " '687': [688, 706],\n",
       " 'related problems': [489],\n",
       " 'approximation algorithms': [389, 470],\n",
       " 'approximation scheme': [500, 597],\n",
       " 'arbitrary-precision arithmetic': [493],\n",
       " 'geometry': [623],\n",
       " 'architectural models': [648],\n",
       " 'area computations – triangles': [624],\n",
       " 'area minimization': [574],\n",
       " 'arm, robot': [669],\n",
       " 'around the world game': [600],\n",
       " 'Arrange': [646, 673],\n",
       " 'arrangement': [18, 670],\n",
       " 'arrangement of objects': [517],\n",
       " 'arrangements of lines': [671],\n",
       " 'array': [441],\n",
       " 'art gallery problems': [660],\n",
       " 'articulation vertex': [225, 229, 568],\n",
       " 'artists steal': [713],\n",
       " 'ASA': [481],\n",
       " 'ASCII': [442],\n",
       " 'aspect ratio': [575],\n",
       " 'assembly language': [494, 503],\n",
       " 'assignment problem': [562],\n",
       " 'associative operation': [473],\n",
       " 'asymmetric TSPs': [710],\n",
       " 'asymptotic analysis': [31],\n",
       " 'atom smashing': [524],\n",
       " 'attribute': [458],\n",
       " 'attribute – graph': [453],\n",
       " 'augmenting path': [563, 573],\n",
       " 'automorphisms': [610],\n",
       " 'average': [514],\n",
       " 'average-case analysis': [444],\n",
       " 'average-case complexity': [33],\n",
       " 'AVL tree': [443],\n",
       " 'Avogadro’s number': [600],\n",
       " 'awk': [685],\n",
       " 'axis-oriented rectangles': [641, 651],\n",
       " 'axis-parallel planes': [461],\n",
       " 'B-tree': [443, 508, 512],\n",
       " 'back edge': [222],\n",
       " 'back substitution': [468],\n",
       " 'backpacker': [497],\n",
       " 'backtracking': [286],\n",
       " '599': [605, 611, 680, 683],\n",
       " 'backtracking – applications': [499],\n",
       " 'problem': [356],\n",
       " 'balanced search tree': [86, 440, 443],\n",
       " 'balltrees': [638],\n",
       " 'banded systems': [468, 470],\n",
       " 'bandersnatch problem': [356],\n",
       " 'bandwidth': [470],\n",
       " 'bandwidth – matrix': [473],\n",
       " 'bandwidth reduction': [470],\n",
       " 'problems': [629, 633, 640],\n",
       " 'bar codes': [326],\n",
       " 'base – arithmetic': [494],\n",
       " 'base – conversion': [494],\n",
       " 'base of logarithm': [53],\n",
       " 'Bellman–Ford algorithm': [555, 557],\n",
       " 'Berge’s theorem': [563],\n",
       " 'best-case complexity': [33],\n",
       " 'best-ﬁrst search': [299],\n",
       " 'BFS': [221],\n",
       " 'Bible – searching the': [686],\n",
       " 'bibliographic databases': [718],\n",
       " 'biconnected components': [544],\n",
       " 'biconnected graph': [229],\n",
       " 'biconnected graphs': [474, 568, 599],\n",
       " 'Big Oh notation': [34, 62],\n",
       " 'bijection': [522],\n",
       " 'bin packing': [652],\n",
       " 'bin packing – applications': [536, 576],\n",
       " '500': [536],\n",
       " 'binary heap': [446],\n",
       " 'binary search': [49, 148, 510],\n",
       " 'binary search – applications': [450],\n",
       " 'occurrences': [149],\n",
       " 'binary search – one-sided': [149, 512],\n",
       " 'binary search tree': [81, 443, 446],\n",
       " 'binary search tree - applications': [83],\n",
       " 'experience': [101],\n",
       " 'binomial coeﬃcients': [312],\n",
       " 'biocomputing': [600],\n",
       " 'biology': [99],\n",
       " 'bipartite graph': [267, 604],\n",
       " 'bipartite graph recognition': [219],\n",
       " 'bipartite incidence structures': [453],\n",
       " 'bipartite matching': [267, 447, 483],\n",
       " '562': [604],\n",
       " '275': [534],\n",
       " 'birthday paradox': [184],\n",
       " 'bisection method': [150],\n",
       " 'bit representation of graphs': [455],\n",
       " 'bit vector': [454, 457, 507, 518],\n",
       " 'bit vector – applications': [24, 490],\n",
       " 'Bitcoin': [700],\n",
       " 'blackmail graph': [263],\n",
       " 'blind man’s algorithm': [669],\n",
       " 'block – set partition': [526],\n",
       " 'blossoms': [563],\n",
       " 'board evaluation function': [480],\n",
       " 'bookshelves': [333],\n",
       " 'Boolean logic minimization': [591],\n",
       " 'Boolean matrix multiplication': [474],\n",
       " 'Boost graph library': [207],\n",
       " 'borrowing': [494],\n",
       " 'Boruvka’s algorithm': [552],\n",
       " 'boss’s delight': [6],\n",
       " 'bottleneck spanning tree': [254],\n",
       " 'boundaries': [19],\n",
       " 'programming': [322],\n",
       " 'bounded-height priority queue': [446],\n",
       " 'bounding boxes': [650],\n",
       " 'Boyer–Moore algorithm': [686],\n",
       " 'brainstorming': [429],\n",
       " 'branch and bound search': [299],\n",
       " 'branch-and-bound search': [588, 595],\n",
       " 'breadth-ﬁrst search': [221, 542, 551],\n",
       " 'breadth-ﬁrst search (BFS)': [214],\n",
       " 'bridge': [568],\n",
       " 'bridge edge': [229],\n",
       " 'bridges of K¨onigsberg': [567],\n",
       " 'Brook’s theorem': [607],\n",
       " 'brush ﬁre': [656],\n",
       " 'brute-force search': [486],\n",
       " 'bubblesort': [506],\n",
       " 'bucket sort': [507],\n",
       " 'bucketing techniques': [136, 442, 647],\n",
       " 'budget, ﬁxed': [497],\n",
       " 'buying ﬁxed lots': [678],\n",
       " 'C language': [491, 503, 557, 563],\n",
       " '570': [582, 625, 639, 643],\n",
       " '628': [632, 636, 639, 643],\n",
       " '673': [710],\n",
       " 'C sorting library': [114],\n",
       " 'C++': [439, 444, 447, 454, 458, 544],\n",
       " '548': [552, 557, 564, 567],\n",
       " '646': [650, 714],\n",
       " 'C++ templates': [714],\n",
       " 'cache': [31],\n",
       " 'cache-oblivious algorithms': [443],\n",
       " 'Caesar shifts': [697],\n",
       " 'calendrical calculations': [532],\n",
       " 'call graph': [569],\n",
       " 'canonical order': [456, 521, 677],\n",
       " 'canonicalization': [97],\n",
       " 'canonically labeled graphs': [613],\n",
       " 'CAP3': [710],\n",
       " 'Carmichael numbers': [492],\n",
       " 'cars and tanks': [666],\n",
       " 'cartoons': [19],\n",
       " 'casino analysis': [33],\n",
       " 'casino poker': [486],\n",
       " 'catalog website': [438],\n",
       " 'Catch-22 situation': [535],\n",
       " 'caveat': [438],\n",
       " 'center vertex': [555, 557, 579],\n",
       " 'CGAL': [621, 650, 714],\n",
       " 'chain of matrices': [473],\n",
       " 'chaining': [93],\n",
       " 'characters': [19],\n",
       " 'checksum': [699],\n",
       " 'chess program': [478, 514],\n",
       " 'chessboard coverage': [296],\n",
       " 'Chinese calendar': [532],\n",
       " 'Chinese postman problem': [565],\n",
       " 'Chinese remainder theorem': [495],\n",
       " 'Christoﬁdes heuristic': [597],\n",
       " 'chromatic index': [608],\n",
       " 'chromatic number': [604],\n",
       " 'chromatic polynomials': [606],\n",
       " 'cipher': [697],\n",
       " 'circle': [488],\n",
       " 'circuit analysis': [467],\n",
       " 'circuit board assembly': [5],\n",
       " 'simulated annealing': [481, 486, 576],\n",
       " 'circuit layout': [470],\n",
       " 'circuit testing': [281],\n",
       " 'circular embeddings': [576],\n",
       " 'classiﬁcation': [615],\n",
       " 'clauses': [538],\n",
       " 'clique': [586],\n",
       " 'clique – deﬁnition': [366],\n",
       " 'clique – hardness proof': [366],\n",
       " 'clique – related problems': [590],\n",
       " 'clock': [487],\n",
       " 'closest point': [637],\n",
       " 'closest-pair heuristic': [7],\n",
       " 'closest-pair problem': [110, 639],\n",
       " 'closure': [559],\n",
       " 'clothing – manufacturing': [654],\n",
       " 'cloudy days': [666],\n",
       " 'cluster': [18],\n",
       " 'cluster identiﬁcation': [542, 549],\n",
       " 'clustered access': [512],\n",
       " 'clustering': [256, 437, 586],\n",
       " 'co-NP': [492],\n",
       " 'co-planar points': [475],\n",
       " 'coding theory': [589],\n",
       " 'cofactor method': [476],\n",
       " 'coin ﬂip': [522],\n",
       " 'collapsing dense subgraphs': [601],\n",
       " '499': [503, 520, 526, 531],\n",
       " 'collection': [18],\n",
       " 'color interchange': [605],\n",
       " 'coloring graphs': [604],\n",
       " 'combinatorial generation': [527],\n",
       " 'algorithms': [393],\n",
       " 'combinatorial geometry': [672],\n",
       " 'combinatorial problems': [505],\n",
       " 'Combinatorica': [455, 505, 520, 523],\n",
       " '527': [531, 552, 561, 567],\n",
       " '580': [606, 708, 716],\n",
       " 'commercial implementations': [483],\n",
       " 'committee': [18],\n",
       " 'committee – congressional': [453],\n",
       " 'common substrings': [706],\n",
       " 'communication in circuits': [602],\n",
       " 'communications networks': [554, 571],\n",
       " 'compaction': [693],\n",
       " 'comparison function': [115],\n",
       " 'comparisons – minimizing': [516],\n",
       " 'compiler': [487],\n",
       " 'compiler construction': [702],\n",
       " 'compiler optimization': [342, 604],\n",
       " 'performance': [56],\n",
       " 'complement': [452],\n",
       " 'complement graph': [589],\n",
       " 'completion time – minimum': [535],\n",
       " 'complex numbers': [425],\n",
       " 'complexity classes': [492],\n",
       " 'composite integer': [490],\n",
       " 'compositions': [527],\n",
       " 'compression': [693],\n",
       " 'compression – image': [501],\n",
       " 'computational biology': [99],\n",
       " 'computational complexity': [612],\n",
       " 'computational geometry': [621],\n",
       " 'computational number theory': [492],\n",
       " 'computer algebra system': [479, 493],\n",
       " 'computer graphics': [472],\n",
       " 'computer graphics – rendering': [661],\n",
       " 'computer vision': [655],\n",
       " 'concatenation – string': [710],\n",
       " 'concavities': [628],\n",
       " 'concavity elimination': [661],\n",
       " 'conditional probability': [174, 175],\n",
       " 'conﬁguration space': [669],\n",
       " 'conﬁgurations': [19],\n",
       " 'conjugate gradient methods': [480],\n",
       " 'connected component': [218, 225],\n",
       " 'connected components': [219, 457],\n",
       " '524': [542],\n",
       " 'connectivity': [225, 544, 568],\n",
       " 'consensus sequences': [706],\n",
       " 'consistent schedule': [534],\n",
       " 'triangulation': [630],\n",
       " 'constrained optimization': [478, 484],\n",
       " '485': [558, 564, 570, 603],\n",
       " 'constraint elimination': [618],\n",
       " 'consulting services': [432, 718],\n",
       " 'container': [75, 457],\n",
       " 'context-free grammars': [687],\n",
       " 'Contig Assembly Program': [710],\n",
       " 'convex decomposition': [641, 658],\n",
       " 'convex hull': [111, 626, 635],\n",
       " '597': [663],\n",
       " 'convex polygons': [675],\n",
       " 'convex polygons – intersection': [649],\n",
       " 'convex region': [483],\n",
       " 'convolution – polygon': [675],\n",
       " 'convolution – sequences': [501],\n",
       " 'cookbook': [438],\n",
       " 'cooling schedules': [407],\n",
       " 'coordinate transformations': [472],\n",
       " 'copying a graph': [212],\n",
       " 'corporate ladder': [579],\n",
       " 'correctness – algorithm': [4],\n",
       " 'correlation function': [502],\n",
       " 'counterexample construction': [8],\n",
       " 'counting edges and vertices': [212],\n",
       " 'counting Eulerian cycles': [567],\n",
       " 'counting integer partitions': [525],\n",
       " 'counting linear extensions': [547],\n",
       " 'counting matchings': [476],\n",
       " 'counting paths': [473, 612],\n",
       " 'counting set partitions': [526],\n",
       " 'counting spanning trees': [553],\n",
       " 'pieces': [659],\n",
       " 'covering set elements': [678],\n",
       " 'Cramer’s rule': [476],\n",
       " 'CRC': [699],\n",
       " 'critical path method': [536],\n",
       " 'crossing number': [582],\n",
       " 'crossings': [574],\n",
       " 'cryptography': [697],\n",
       " 'cryptography – keys': [486],\n",
       " '492': [496, 696],\n",
       " 'CS': [573],\n",
       " 'CSA': [563, 570],\n",
       " 'cubic regions': [461],\n",
       " 'curve ﬁtting': [484],\n",
       " 'cut set': [569, 601],\n",
       " 'Cuthill–McKee algorithm': [471],\n",
       " 'cutting plane methods': [483, 595],\n",
       " 'cutting stock problem': [652],\n",
       " 'CWEB': [716],\n",
       " 'cycle – shortest': [556],\n",
       " 'cycle breaking': [619],\n",
       " 'cycle detection': [222, 544],\n",
       " 'cycle in graph': [199],\n",
       " 'cycle length': [488],\n",
       " 'DAG': [200, 231, 397],\n",
       " 'DAG – longest path in': [599],\n",
       " 'DAG – shortest path in': [556],\n",
       " 'data compression': [327],\n",
       " 'data ﬁltering': [514],\n",
       " 'data records': [18],\n",
       " 'data structures': [69, 439],\n",
       " 'data transmission': [693],\n",
       " 'data validation': [699],\n",
       " 'database algorithms': [685],\n",
       " 'database application': [641],\n",
       " 'database query optimization': [561],\n",
       " '459': [670, 673],\n",
       " 'Davis-Putnam procedure': [538],\n",
       " 'day of the week calculation': [532],\n",
       " 'de Bruijn sequence': [567, 599],\n",
       " 'De Morgan’s laws': [538],\n",
       " 'deadlock': [544],\n",
       " 'debugging graph algorithms': [542],\n",
       " 'debugging parallel programs': [160],\n",
       " 'debugging tools': [578],\n",
       " 'decimal arithmetic': [494],\n",
       " 'decompose space': [460],\n",
       " 'decomposing polygons': [630],\n",
       " 'deconvolution': [501],\n",
       " 'decrease-key': [447],\n",
       " 'decreasing subsequence': [323],\n",
       " 'decryption': [697],\n",
       " 'defenestrate': [511],\n",
       " 'degeneracy': [622],\n",
       " 'degeneracy testing': [671],\n",
       " 'degenerate conﬁguration': [475],\n",
       " 'degree sequence': [530],\n",
       " 'degree, vertex': [202, 612],\n",
       " 'degrees of freedom': [668],\n",
       " 'Delaunay triangulation': [631, 635],\n",
       " 'applications': [550],\n",
       " 'deletion from binary search tree': [85],\n",
       " 'deletions – text': [688],\n",
       " 'deliveries and pickups': [565],\n",
       " 'delivery routing': [534],\n",
       " 'identiﬁcation': [637],\n",
       " 'dense graphs': [202, 452, 599],\n",
       " 'dense subgraph': [587],\n",
       " 'densest sphere packing': [654],\n",
       " 'depth-ﬁrst search': [224, 230, 449],\n",
       " '452': [542, 546, 551, 559],\n",
       " '568': [599],\n",
       " '394': [566, 582, 596, 605],\n",
       " 'dequeue': [76],\n",
       " 'derangement': [194, 303],\n",
       " 'derivatives – automata': [704],\n",
       " 'derivatives – calculus': [479],\n",
       " 'DES': [697],\n",
       " 'descendant': [18],\n",
       " 'design process': [429],\n",
       " 'design rule checking': [648],\n",
       " 'determinant': [467],\n",
       " 'determinants and permanents': [475],\n",
       " '(DFA)': [702],\n",
       " 'DFA': [702],\n",
       " 'DFS': [224],\n",
       " 'diameter of a graph': [557],\n",
       " 'diameter of a point set': [626],\n",
       " '509': [513],\n",
       " 'dictionary': [76, 440, 445, 457],\n",
       " 'dictionary – applications': [92],\n",
       " 'dictionary – related problems': [447],\n",
       " 'dictionary – searching': [510],\n",
       " 'diﬀ– how it works': [688],\n",
       " 'digital geometry': [656],\n",
       " 'digital signatures': [700],\n",
       " 'digitized images': [554],\n",
       " 'Dijkstra’s algorithm': [258, 302, 555],\n",
       " 'DIMACS': [444, 463],\n",
       " 'Challenge': [564, 573, 588],\n",
       " 'Dinic’s algorithm': [573],\n",
       " 'directed acyclic graph (DAG)': [200],\n",
       " '535': [546, 618],\n",
       " 'directed cycle': [546],\n",
       " 'directed graph': [198, 201],\n",
       " 'directed graphs – automata': [702],\n",
       " 'directory ﬁle structures': [578],\n",
       " 'disclaimer': [438],\n",
       " 'discrete event simulation': [486],\n",
       " 'discrete Fourier transform': [501, 502],\n",
       " 'discrete mathematics software': [716],\n",
       " 'discussion section': [437],\n",
       " 'disjoint paths': [569],\n",
       " 'disjoint set union': [459],\n",
       " 'disjoint subsets': [457],\n",
       " '538': [678],\n",
       " 'disk access': [443],\n",
       " 'disk drives': [693, 699],\n",
       " '637': [644],\n",
       " 'dispersion problems': [589],\n",
       " 'distance graph': [595],\n",
       " 'distance metrics': [257],\n",
       " 'distinguishable elements': [519],\n",
       " 'distribution sort': [136, 507],\n",
       " 'divide and conquer': [129, 147, 495],\n",
       " '502': [530],\n",
       " 'division': [490, 493],\n",
       " 'DNA': [99],\n",
       " 'DNA sequence comparisons': [706],\n",
       " 'DNA sequencing': [275, 414, 709],\n",
       " 'dominance orderings': [18, 642],\n",
       " 'DOS ﬁle names': [275],\n",
       " 'double-precision arithmetic': [465],\n",
       " '493': [623],\n",
       " 'Douglas–Puecker algorithm': [662],\n",
       " 'drawing graphs nicely': [574],\n",
       " 'drawing puzzles': [565],\n",
       " 'drawing trees': [578],\n",
       " '577': [583],\n",
       " 'driving time minimization': [594],\n",
       " 'drug discovery': [667],\n",
       " 'DSATUR': [606],\n",
       " 'dual graph': [90, 211],\n",
       " 'duality': [501, 627],\n",
       " 'duality transformations': [672],\n",
       " 'duplicate elimination': [442],\n",
       " 'duplicate elimination – graphs': [610],\n",
       " 'permutations': [518],\n",
       " 'duplicate keys': [506],\n",
       " 'dynamic convex hulls': [629],\n",
       " 'dynamic data structures': [639, 647],\n",
       " 'dynamic graph algorithms': [455],\n",
       " 'dynamic programming': [307, 474],\n",
       " '498': [556, 599, 633, 706],\n",
       " 'initialization': [689],\n",
       " 'paths': [267],\n",
       " 'eﬃciency': [324],\n",
       " 'eccentricity of a graph': [557],\n",
       " 'economics – applications to': [620],\n",
       " 'edge': [198],\n",
       " 'edge and vertex connectivity': [568],\n",
       " 'edge chromatic number': [608],\n",
       " 'edge coloring': [605, 608],\n",
       " 'edge coloring – applications': [534],\n",
       " '536': [590, 609],\n",
       " 'edge connectivity': [229],\n",
       " 'edge cover': [592, 679],\n",
       " 'edge disjoint paths': [569],\n",
       " 'edge ﬂipping operation': [530],\n",
       " 'edge labeled graphs': [702],\n",
       " 'edge length': [574],\n",
       " 'edge tour': [599],\n",
       " 'edit distance': [314, 706],\n",
       " 'Edmond’s algorithm': [564],\n",
       " 'eﬃciency of algorithms': [4],\n",
       " 'electrical circuits': [197],\n",
       " 'electrical engineers': [501],\n",
       " 'electronic circuit analysis': [467],\n",
       " 'element uniqueness problem': [110],\n",
       " 'elimination ordering': [581],\n",
       " 'ellipsoid algorithm': [485],\n",
       " 'elliptic-curve method': [492],\n",
       " 'embedded graph': [200],\n",
       " 'embeddings – planar': [581],\n",
       " 'empirical results': [561, 606],\n",
       " 'empirical results – heuristics': [617],\n",
       " 'employees to jobs – matching': [562],\n",
       " 'empty circle – largest': [634],\n",
       " 'empty rectangle': [654],\n",
       " 'enclosing boxes': [650],\n",
       " 'enclosing disk': [668],\n",
       " 'enclosing rectangle': [654],\n",
       " 'encryption': [697],\n",
       " 'energy function': [478],\n",
       " 'energy minimization': [576, 617],\n",
       " 'English language': [12, 511],\n",
       " 'English to French': [512],\n",
       " 'enqueue': [76],\n",
       " 'epsilon-moves': [703],\n",
       " 'equilateral triangle': [616],\n",
       " 'equivalence classes': [612],\n",
       " 'states': [703],\n",
       " 'Erd˝os-Gallai conditions': [531],\n",
       " 'Erd˝os-R´enyi graphs': [529],\n",
       " 'error': [465],\n",
       " 'estimating closure sizes': [561],\n",
       " 'ethnic groups in Congress': [679],\n",
       " 'Euclid’s algorithm': [496],\n",
       " 'Euclidean traveling salesman': [393],\n",
       " 'Euler’s formula': [581],\n",
       " 'Eulerian cycle': [565],\n",
       " 'Eulerian cycle – applications': [534],\n",
       " 'Eulerian cycle – line graphs': [609],\n",
       " 'Eulerian path': [565],\n",
       " 'evaluation function': [478],\n",
       " 'even-degree vertices': [566],\n",
       " 'even-length cycles': [563],\n",
       " 'event': [173],\n",
       " 'event queue': [650],\n",
       " 'evolutionary tree': [615],\n",
       " 'exact cover problem': [683],\n",
       " 'exact string matching': [688],\n",
       " 'exam scheduling': [608],\n",
       " 'exercises': [27, 59, 103, 140, 166],\n",
       " '193': [235, 276, 303, 345],\n",
       " '383': [426],\n",
       " 'exhaustive search': [24, 517],\n",
       " 'exhaustive search – application': [8],\n",
       " 'results': [597],\n",
       " 'exhaustive search – subsets': [521],\n",
       " 'expanded obstacles approach': [668],\n",
       " 'expander graphs': [716],\n",
       " 'expected time': [33],\n",
       " 'expected value': [173],\n",
       " 'expected-time, linear': [515],\n",
       " 'experiment': [172],\n",
       " 'experimental graph theory': [528],\n",
       " 'explicit graph': [200],\n",
       " 'exponential time': [316],\n",
       " 'exponential-time algorithms': [281],\n",
       " 'exponentiation': [50, 495],\n",
       " 'external memory': [512],\n",
       " 'external-memory sorting': [506, 507],\n",
       " 'facets': [627],\n",
       " 'facility location': [589, 634],\n",
       " 'factorial function': [153],\n",
       " 'factoring': [423],\n",
       " 'factoring and primality testing': [490],\n",
       " 'factory location': [634],\n",
       " 'family tree': [18, 578],\n",
       " 'Fary’s theorem': [583],\n",
       " 'fast Fourier transform (FFT)': [502],\n",
       " 'fat cells': [461],\n",
       " 'fattening polygons': [674],\n",
       " 'feature sets': [666],\n",
       " 'Federal Sentencing Guidelines': [51],\n",
       " 'feedback edge/vertex set': [547, 618],\n",
       " 'Fermat': [617],\n",
       " 'Fermat’s theorem': [491],\n",
       " 'Ferrer’s diagram': [525],\n",
       " 'FFT': [422, 496, 502],\n",
       " 'FFTPACK': [503],\n",
       " 'fgrep': [686],\n",
       " 'Fibonacci heap': [447, 552, 557],\n",
       " 'Fibonacci numbers': [153, 308],\n",
       " 'FIFO': [75],\n",
       " 'FIFO queue': [215],\n",
       " 'ﬁle diﬀerence comparison': [688],\n",
       " 'ﬁle layout': [470],\n",
       " 'ﬁltering outlying elements': [514],\n",
       " 'ﬁltering signals': [501],\n",
       " 'ﬁnal examination': [698],\n",
       " 'ﬁnancial constraints': [497],\n",
       " 'ﬁnd operation': [458],\n",
       " 'ﬁnite automata': [702],\n",
       " 'ﬁnite automata minimization': [686],\n",
       " 'ﬁnite element analysis': [632],\n",
       " 'ﬁrehouse': [637],\n",
       " 'ﬁrst-ﬁt – decreasing': [653],\n",
       " 'ﬁrst-in, ﬁrst-out (FIFO)': [75],\n",
       " 'ﬁxed degree sequence graphs': [530],\n",
       " 'ﬁxed-parameter tractability': [620],\n",
       " 'ﬂat-earth model': [32],\n",
       " 'Fleury’s algorithm': [567],\n",
       " 'ﬂight crew scheduling': [682],\n",
       " 'ﬂight ticket pricing': [125],\n",
       " 'ﬂoating-point arithmetic': [623],\n",
       " 'Floyd’s algorithm': [262, 556, 557],\n",
       " 'football program': [600],\n",
       " 'football scheduling': [608],\n",
       " 'Ford-Fulkerson algorithm': [270],\n",
       " 'Fortran': [465, 469, 471, 473, 476],\n",
       " '606': [654, 660, 715, 717],\n",
       " 'Fortune’s algorithm': [635],\n",
       " 'four Russians algorithm': [474, 692],\n",
       " 'four-color problem': [528, 607],\n",
       " 'Fourier transform': [422],\n",
       " 'fragment ordering': [275],\n",
       " 'fraud – tax': [586],\n",
       " 'free space': [670],\n",
       " 'free trees': [578],\n",
       " 'freedom to hang yourself': [429],\n",
       " 'frequency distribution': [110],\n",
       " 'frequency domain': [501],\n",
       " 'friendship graph': [201, 586],\n",
       " 'function interpolation': [630],\n",
       " 'furniture moving': [667],\n",
       " 'furthest-site diagrams': [636],\n",
       " 'future events': [445],\n",
       " 'game-tree search': [510],\n",
       " 'game-tree search – parallel': [160],\n",
       " 'games directory': [490],\n",
       " 'GAMS': [481, 715],\n",
       " 'gaps between primes': [491],\n",
       " 'garbage trucks': [565],\n",
       " 'Gates, William': [514],\n",
       " 'Gaussian distribution': [488, 502],\n",
       " 'Gaussian elimination': [467, 470],\n",
       " 'Genbank searching': [688],\n",
       " 'generating graphs': [528],\n",
       " 'generating partitions': [524],\n",
       " 'generating permutations': [517],\n",
       " '531': [533],\n",
       " 'generating subsets': [521],\n",
       " 'genetic algorithms': [417, 481],\n",
       " '(GIS)': [641],\n",
       " 'geometric data structure': [98],\n",
       " 'geometric degeneracy': [622],\n",
       " 'geometric shortest path': [555, 667],\n",
       " 'geometric spanning tree': [551],\n",
       " 'geometric Steiner tree': [614],\n",
       " 'geometric TSP': [595],\n",
       " 'GEOMPACK': [660],\n",
       " 'gerrymandering': [658],\n",
       " 'algorithm': [390],\n",
       " 'gift-wrapping algorithm': [627],\n",
       " 'Gilbert and Pollak conjecture': [617],\n",
       " 'girth': [556],\n",
       " 'global optimization': [478],\n",
       " 'Graham scan': [628],\n",
       " 'Grail': [704],\n",
       " 'graph': [197],\n",
       " 'graph algorithms': [197, 446],\n",
       " 'graph complement': [452],\n",
       " 'graph data structures': [98, 243, 452],\n",
       " 'graph data structures – Boost': [207],\n",
       " '207': [714],\n",
       " 'graph databases': [453],\n",
       " 'graph density': [452],\n",
       " 'graph drawings – clutter': [560],\n",
       " 'graph embedding': [453],\n",
       " 'graph isomorphism': [517, 531, 610],\n",
       " 'graph partition': [454, 569, 601],\n",
       " 'graph theory': [197],\n",
       " 'graph theory packages': [716],\n",
       " 'graph traversal': [212],\n",
       " 'GraphBase': [454, 530, 552, 564],\n",
       " '600': [620, 716],\n",
       " 'graphic partitions': [531],\n",
       " 'graphical enumeration': [531],\n",
       " 'graphs': [18],\n",
       " 'Gray code': [522, 523],\n",
       " '359': [423, 493],\n",
       " 'greedy heuristic': [91, 245, 343, 499],\n",
       " '590': [680, 683],\n",
       " 'spanning trees': [549],\n",
       " 'Gregorian calendar': [533],\n",
       " 'grid embeddings': [582],\n",
       " 'grid ﬁle': [645],\n",
       " 'grid search': [480],\n",
       " 'Grinch, The': [140],\n",
       " 'group – automorphism': [610],\n",
       " 'Grover’s algorithm': [420, 513],\n",
       " 'growth rates': [37],\n",
       " 'guarantees – importance of': [390],\n",
       " 'guarding art galleries': [660],\n",
       " 'Software': [715],\n",
       " 'gzip': [695],\n",
       " 'H-index': [526],\n",
       " 'hackerrank': [30, 67, 107, 146, 169],\n",
       " '195': [242, 280, 306, 353],\n",
       " '388': [428],\n",
       " 'had-sex-with graph': [201],\n",
       " 'half-space intersection': [627],\n",
       " 'Hamiltonian cycle': [474, 561, 594],\n",
       " 'Hamiltonian cycle – counting': [477],\n",
       " 'proof': [371],\n",
       " 'Hamiltonian path': [551],\n",
       " 'Hamming distance': [664],\n",
       " 'hardness of approximation': [586],\n",
       " 'hardware arithmetic': [494],\n",
       " 'hardware design applications': [702],\n",
       " 'hardware implementation': [503],\n",
       " 'hash function': [442],\n",
       " 'hash tables': [93, 442],\n",
       " 'hash tables – size': [490],\n",
       " 'Hausdorﬀdistance': [665],\n",
       " 'heap': [446],\n",
       " 'heap construction': [153],\n",
       " 'heapsort': [116, 506],\n",
       " 'heard-of graph': [201],\n",
       " 'heart-lung machine': [441],\n",
       " 'heating ducts': [614],\n",
       " 'Hebrew calendar': [532],\n",
       " 'Hertel–Mehlhorn heuristic': [659],\n",
       " 'heuristics': [399, 652],\n",
       " 'heuristics – empirical results': [596],\n",
       " 'hidden-surface elimination': [649],\n",
       " 'hierarchical decomposition': [454],\n",
       " 'hierarchical drawings': [578],\n",
       " 'hierarchical graph structures': [454],\n",
       " 'hierarchy': [18],\n",
       " 'Hierholzer’s algorithm': [566],\n",
       " 'high school algebra': [467],\n",
       " 'high school cliques': [586],\n",
       " 'higher-dimensional geometry': [627],\n",
       " '635': [638],\n",
       " 'hill climbing': [479],\n",
       " 'HIPR': [573],\n",
       " 'historical objects': [532],\n",
       " 'history': [438, 509],\n",
       " 'history – cryptography': [701],\n",
       " 'history – graph theory': [567],\n",
       " 'hitting set': [679],\n",
       " 'HIV virus': [417],\n",
       " 'homeomorphism': [583],\n",
       " 'horizon': [650],\n",
       " 'Horner’s rule': [28, 442, 495],\n",
       " 'How to Solve It': [433],\n",
       " 'hub site': [595],\n",
       " 'Huﬀman codes': [695],\n",
       " 'human genome': [99],\n",
       " 'Hungarian algorithm': [564],\n",
       " 'hypercube': [161, 523],\n",
       " 'hypergraph': [453, 455, 457],\n",
       " 'hyperlinks': [529],\n",
       " 'hyperplanes': [673],\n",
       " 'hypertext layout': [470],\n",
       " 'identical graphs': [610],\n",
       " 'Conference': [696],\n",
       " 'image compression': [637, 661, 693],\n",
       " 'image data': [461],\n",
       " 'image features': [666],\n",
       " 'image ﬁltering': [501],\n",
       " 'image processing': [655],\n",
       " 'image segmentation': [554],\n",
       " 'image simpliﬁcation': [662],\n",
       " 'implementation challenges': [30, 67],\n",
       " '108': [146, 169, 195, 242],\n",
       " '280': [306, 353, 388, 428],\n",
       " '444': [463],\n",
       " 'implementations, caveats': [438],\n",
       " 'implicit binary tree': [446],\n",
       " 'implicit graph': [200],\n",
       " 'in-circle test': [625],\n",
       " 'in-order traversal': [222],\n",
       " 'inapproximability results': [681],\n",
       " 'incidence matrices': [453],\n",
       " 'inconsistent linear equations': [482],\n",
       " 'increasing subsequences': [323, 707],\n",
       " 'incremental algorithms': [575],\n",
       " 'incremental change methods': [517],\n",
       " 'arrangements': [672],\n",
       " 'coloring': [605],\n",
       " 'graph drawing': [582],\n",
       " 'sorting': [3, 445, 506],\n",
       " 'suﬃx trees': [98, 448, 686],\n",
       " 'TSP': [594],\n",
       " 'independence': [174],\n",
       " 'independent set': [275, 589],\n",
       " 'formulations': [682],\n",
       " '588': [593, 607, 684],\n",
       " 'annealing': [410],\n",
       " 'index – how to use': [437],\n",
       " 'index manipulation': [322],\n",
       " 'induced subgraph': [587, 606],\n",
       " 'induction and recursion': [15],\n",
       " 'assignments': [376],\n",
       " 'information retrieval': [510],\n",
       " 'information theory': [489],\n",
       " 'input–output graphics': [437],\n",
       " 'insertion into binary search tree': [84],\n",
       " 'insertion sort': [3, 124, 506, 508],\n",
       " 'insertions – text': [688],\n",
       " 'inside–outside polygon': [644],\n",
       " 'instance – deﬁnition': [3],\n",
       " 'integer arithmetic': [623],\n",
       " 'integer compositions': [527],\n",
       " 'integer factorization': [612, 698],\n",
       " 'integer partition': [498, 524, 530, 652],\n",
       " 'integer programming': [483],\n",
       " 'integrality constraints': [483],\n",
       " 'interfering tasks': [608],\n",
       " 'interior-point methods': [483],\n",
       " 'Internet': [486, 718],\n",
       " 'interpolation search': [512],\n",
       " 'intersection – halfspaces': [483],\n",
       " 'intersection – set': [456],\n",
       " 'intersection detection': [648],\n",
       " 'intersection point': [467],\n",
       " 'interview scheduling': [608],\n",
       " 'invariant – graph': [612],\n",
       " 'inverse Ackerman function': [459],\n",
       " 'inverse Fourier transform': [501],\n",
       " 'inverse matrix': [469, 475],\n",
       " 'inverse operations': [518],\n",
       " 'inversions': [475],\n",
       " 'isomorphism': [531],\n",
       " 'isomorphism – graph': [610],\n",
       " 'isomorphism-complete': [613],\n",
       " 'JFLAP': [704],\n",
       " 'jigsaw puzzle': [652],\n",
       " 'job matching': [562],\n",
       " 'job scheduling': [534],\n",
       " 'job-shop scheduling': [536],\n",
       " 'JPEG': [694],\n",
       " 'Julian calendar': [533],\n",
       " 'K5': [581],\n",
       " 'K3': [3, 583],\n",
       " 'k-optimal tours': [596],\n",
       " 'k-subsets': [522, 527],\n",
       " 'k-subsets – applications': [529],\n",
       " 'K¨onigsberg': [567],\n",
       " 'Karatsuba’s algorithm': [495],\n",
       " 'Karazanov’s algorithm': [573],\n",
       " 'Karmarkar’s algorithm': [485],\n",
       " 'kd-trees': [460, 638],\n",
       " 'kd-trees – applications': [642],\n",
       " 'kd-trees – related problems': [640],\n",
       " '643': [647],\n",
       " 'Kepler conjecture': [654],\n",
       " 'Kernighan–Lin heuristic': [596, 603],\n",
       " 'key length': [697],\n",
       " 'key search': [462],\n",
       " 'Kirchhoﬀ’s laws': [467],\n",
       " 'knapsack': [483],\n",
       " 'knapsack problem': [497, 521],\n",
       " 'Kolmogorov complexity': [489],\n",
       " 'Kruskal’s algorithm': [248, 445, 458],\n",
       " '550': [552],\n",
       " 'kth-order Voronoi diagrams': [636],\n",
       " 'Kuratowski’s theorem': [583],\n",
       " 'L∞metric': [257],\n",
       " 'label placement': [576],\n",
       " 'labeled graphs': [200, 528, 611],\n",
       " 'labels': [19],\n",
       " 'Lagrangian relaxation': [481],\n",
       " 'language pattern matching': [611],\n",
       " 'LAPACK': [469, 473],\n",
       " 'large graphs – representation': [454],\n",
       " 'largest element': [514],\n",
       " 'last in, ﬁrst out': [75],\n",
       " 'layered printed circuit boards': [582],\n",
       " 'LCA – least common ancestor': [451],\n",
       " 'leap year': [533],\n",
       " 'least common ancestor': [451],\n",
       " 'least-squares curve ﬁtting': [484],\n",
       " 'leaves – tree': [530],\n",
       " 'LEDA': [207, 444, 447, 454, 458],\n",
       " '544': [548, 552, 557, 561],\n",
       " '564': [588, 590, 681],\n",
       " 'leetcode': [30, 67, 107, 146, 168, 195],\n",
       " '242': [280, 306, 352, 388],\n",
       " 'left-right test': [475],\n",
       " 'left-to-right ordering': [339],\n",
       " 'Lempel–Ziv algorithms': [694, 695],\n",
       " 'lexicographic order': [517, 521, 522],\n",
       " '525': [526],\n",
       " 'lhs': [629],\n",
       " 'libraries': [465],\n",
       " 'licensing arrangements': [713],\n",
       " 'LIFO': [75],\n",
       " 'lifting-map construction': [629],\n",
       " 'line arrangements': [671],\n",
       " 'line graph': [609],\n",
       " 'line intersection': [622, 649],\n",
       " 'line segment intersection': [624],\n",
       " 'line segment Voronoi diagram': [656],\n",
       " 'line-point duality': [672],\n",
       " 'linear algebra': [472, 475],\n",
       " 'linear arrangement': [470],\n",
       " 'linear congruential generator': [487],\n",
       " 'linear constraint satisfaction': [671],\n",
       " 'linear extension': [546],\n",
       " 'linear interpolation search': [513],\n",
       " 'linear partitioning': [333],\n",
       " 'linear programming': [479, 482],\n",
       " 'linear programming – models': [571],\n",
       " 'linear regression': [467],\n",
       " 'linear-time graph algorithms': [455],\n",
       " 'link distance': [662, 674],\n",
       " 'linked lists vs. arrays': [76, 441],\n",
       " 'LINPACK': [469, 473, 476],\n",
       " 'literate program': [716],\n",
       " 'little oh notation': [59],\n",
       " 'local optima': [479],\n",
       " 'locality of reference': [441, 511],\n",
       " 'locations': [18],\n",
       " 'logarithms': [49],\n",
       " 'logic minimization': [678],\n",
       " 'logic programming': [342],\n",
       " 'long division': [495],\n",
       " 'long keys': [507],\n",
       " 'longest common preﬁx': [451],\n",
       " '(LCS)': [323],\n",
       " 'longest common substring': [449, 706],\n",
       " 'longest cycle': [557, 598],\n",
       " '324': [692],\n",
       " 'longest path': [556, 598],\n",
       " 'longest path, DAG': [231, 535],\n",
       " 'loop': [31],\n",
       " 'lossless encodings': [693],\n",
       " 'lossy encodings': [693],\n",
       " 'lottery problems': [22],\n",
       " 'Lotto problem': [518],\n",
       " 'low-degree spanning tree': [551, 553],\n",
       " 'lower bound': [35, 144, 516, 629],\n",
       " 'lower bound – range searching': [643],\n",
       " 'lower bound – sorting': [509],\n",
       " 'lower triangular matrix': [468],\n",
       " 'LU-decomposition': [468, 476],\n",
       " 'lunar calendar': [532],\n",
       " 'LZW algorithm': [694, 695],\n",
       " 'machine clock': [487],\n",
       " 'machine learning': [478],\n",
       " 'maﬁa': [698],\n",
       " 'magnetic tape': [470],\n",
       " 'mail routing': [565],\n",
       " 'maintaining line arrangements': [671],\n",
       " 'Malawi': [125],\n",
       " 'manufacturing applications': [594],\n",
       " 'map making': [669],\n",
       " 'Maple': [493],\n",
       " 'marriage problems': [562],\n",
       " 'master theorem': [154],\n",
       " 'matching': [690],\n",
       " 'matching – applications': [597],\n",
       " 'matching – dual to': [590],\n",
       " 'matching – number of perfect': [476],\n",
       " 'matching – related problems': [477],\n",
       " 'matching shapes': [664],\n",
       " 'Mathematica': [455, 466, 493, 520],\n",
       " '523': [527, 531, 561, 567],\n",
       " 'mathematical notation': [31],\n",
       " 'mathematical programming': [479],\n",
       " 'mathematical software – netlib': [715],\n",
       " 'matrix bandwidth': [470],\n",
       " 'matrix compression': [709],\n",
       " 'matrix inversion': [469, 472],\n",
       " 'matrix multiplication': [156, 472],\n",
       " 'matrix-tree theorem': [553],\n",
       " 'matroids': [553],\n",
       " 'max-cut': [602],\n",
       " 'max-ﬂow, min-cut theorem': [570],\n",
       " 'maxima': [479],\n",
       " 'maximal clique': [586],\n",
       " 'maximal matching': [592],\n",
       " 'maximum acyclic subgraph': [397],\n",
       " 'maximum spanning tree': [253],\n",
       " 'maze': [213, 545],\n",
       " 'McDonald’s restaurants': [634],\n",
       " 'MD5': [701],\n",
       " 'mean': [514],\n",
       " 'mechanical computers': [492],\n",
       " 'mechanical truss analysis': [467],\n",
       " 'medial-axis transform': [655, 657],\n",
       " 'median – application': [508],\n",
       " 'median and selection': [514],\n",
       " ...}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEX_PATH, \"r\") as f:\n",
    "    latex_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', latex_content)\n",
    "page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', latex_content))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_index = {}\n",
    "\n",
    "# for term, pages in index.items():\n",
    "#     if \",\" in term:  \n",
    "#         parts = term.split(\", \")\n",
    "#         if len(parts) == 2:  # Ensure it's a \"Last, First\" format\n",
    "#             corrected_name = f\"{parts[1]} {parts[0]}\"  # Convert to \"First Last\"\n",
    "#             updated_index[corrected_name] = pages  # Store corrected term\n",
    "#         else:\n",
    "#             updated_index[term] = pages  # Keep unchanged terms\n",
    "#     else:\n",
    "#         updated_index[term] = pages  # Keep unchanged terms\n",
    "\n",
    "# # Now, index is updated with corrected names\n",
    "# index = updated_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_page(page,page_breaks ,page_positions, is_forward=True):\n",
    "\n",
    "    if str(page) in page_breaks:\n",
    "        return page_positions[page]\n",
    "    if is_forward:\n",
    "        bound = len(og_book_pdf)\n",
    "        forward = page\n",
    "        while (forward) <= bound:\n",
    "            if str(forward) in page_breaks:\n",
    "                return page_positions[forward]\n",
    "            forward += 1\n",
    "        return -1\n",
    "    \n",
    "    else:\n",
    "        bound = 0\n",
    "        backward = page\n",
    "        while (backward) >= bound:\n",
    "            if str(backward) in page_breaks:\n",
    "                return page_positions[backward]\n",
    "            backward -= 1\n",
    "        return 0  # No valid page found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indexes(latex_content, index): \n",
    "    matched = 0\n",
    "    not_matched = 0\n",
    "    not_found_terms = {}  # Dictionary to store terms not found along with page numbers\n",
    "\n",
    "    for index_term, pages in tqdm(index.items()):\n",
    "        # check = False\n",
    "        # if index_term == 'application':\n",
    "        #     check = True\n",
    "        for page in pages:\n",
    "\n",
    "            # upadte page number indexes as terms will be added\n",
    "\n",
    "            page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', latex_content)\n",
    "            page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', latex_content))}\n",
    "\n",
    "\n",
    "            upper_bound = find_closest_page(page+1, page_breaks, page_positions ,True)\n",
    "            lower_bound = find_closest_page(page-2, page_breaks, page_positions ,False)\n",
    "\n",
    "            page_content = latex_content[lower_bound:upper_bound]\n",
    "            \n",
    "            # match = re.search(r'\\b' + re.escape(index_term) + r'\\b', page_content, re.IGNORECASE)\n",
    "            match = re.search(  re.escape(index_term), page_content, re.IGNORECASE)\n",
    "\n",
    "            # if check:\n",
    "            #     print(\"Match found : \" + match.group(0))\n",
    "            #     print(\"Lowerbound : \", lower_bound)\n",
    "            #     print(\"Upperbound : \", upper_bound)\n",
    "            #     print(\"Page : \", page)\n",
    "            if match:\n",
    "                term_start = lower_bound + match.start()  # Adjust index relative to book_text\n",
    "                term_end = term_start + len(index_term)\n",
    "                indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                latex_content = latex_content[:term_start] + indexed_term + latex_content[term_end:]\n",
    "                matched += 1\n",
    "            else:\n",
    "                # try fuzzy matching\n",
    "                term_length = len(index_term)\n",
    "                max_dist = max(1, term_length // 4)  # 1/4rd of term length\n",
    "                near_matches = find_near_matches(index_term, page_content, max_l_dist=max_dist)\n",
    "\n",
    "                if near_matches:\n",
    "                    best_match = min(near_matches, key=lambda x: x.dist)  # Get the closest match\n",
    "                    term_start = lower_bound + best_match.start  # Adjust index relative to book_text\n",
    "                    term_end = term_start + term_length\n",
    "                    indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                    latex_content = latex_content[:term_start] + indexed_term + latex_content[term_end:]\n",
    "                    matched += 1\n",
    "                    continue\n",
    "\n",
    "                # did not find a match\n",
    "                # put the index in the middle of the page\n",
    "                index_position = lower_bound + (upper_bound - lower_bound) // 2\n",
    "                indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                latex_content = latex_content[:index_position] + indexed_term + latex_content[index_position:]\n",
    "                \n",
    "                \n",
    "                not_matched += 1\n",
    "                if index_term not in not_found_terms:\n",
    "                    not_found_terms[index_term] = []\n",
    "                not_found_terms[index_term].append(page)\n",
    "\n",
    "\n",
    "    print(f\"Matched: {matched}, Not Matched: {not_matched}\")\n",
    "    return latex_content, not_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1749/1749 [00:14<00:00, 120.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 1929, Not Matched: 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v1 updating page_positions at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1749/1749 [01:05<00:00, 26.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 2273, Not Matched: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v1 updating page_positions at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Shiﬄett': [136], 'shuﬄing': [697], 'suﬃx arrays': [448]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'suﬃx' == 'suffix'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Book\n",
    "## SLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../../\"\n",
    "\n",
    "BOOK_PATH =  ROOT + \"files/slp_book/slp-content.pdf\"\n",
    "TEX_PATH = ROOT + \"files/slp_book/outputs/slp-content.tex\"\n",
    "INDEX_PATH = ROOT + \"files/slp_book/slp-index.pdf\"\n",
    "OUTPUT_TEX_PATH = ROOT + \"files/slp_book/outputs/slp-indexed-content.tex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_book_pdf = pymupdf.open(BOOK_PATH)\n",
    "book_pdf = pymupdf.open(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data = {}\n",
    "for i in range(len(book_pdf)):\n",
    "    page = book_pdf[i]\n",
    "    index_data[i] = page.get_text(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"(.+?),\\s*((?:\\d+,?\\s*)+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for page, text in index_data.items():\n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    for line in lines:\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            term = match.group(1).strip()\n",
    "            pages = [int(p) for p in re.findall(r\"\\d+\", match.group(2))]\n",
    "            index[term] = pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEX_PATH, \"r\") as f:\n",
    "    latex_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', latex_content)\n",
    "page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', latex_content))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_page(page,page_breaks ,page_positions, is_forward=True):\n",
    "\n",
    "    if str(page) in page_breaks:\n",
    "        return page_positions[page]\n",
    "    if is_forward:\n",
    "        bound = len(og_book_pdf)\n",
    "        forward = page\n",
    "        while (forward) <= bound:\n",
    "            if str(forward) in page_breaks:\n",
    "                return page_positions[forward]\n",
    "            forward += 1\n",
    "        return -1\n",
    "    \n",
    "    else:\n",
    "        bound = 0\n",
    "        backward = page\n",
    "        while (backward) >= bound:\n",
    "            if str(backward) in page_breaks:\n",
    "                return page_positions[backward]\n",
    "            backward -= 1\n",
    "        return 0  # No valid page found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indexes(latex_content, index): \n",
    "    matched = 0\n",
    "    not_matched = 0\n",
    "    not_found_terms = {}  # Dictionary to store terms not found along with page numbers\n",
    "\n",
    "    for index_term, pages in tqdm(index.items()):\n",
    "        # check = False\n",
    "        # if index_term == 'application':\n",
    "        #     check = True\n",
    "        for page in pages:\n",
    "\n",
    "            # upadte page number indexes as terms will be added\n",
    "\n",
    "            page_breaks = re.findall(r'%---- Page End Break Here ---- Page : (\\d+)', latex_content)\n",
    "            page_positions = {int(page): pos.start() for page, pos in zip(page_breaks, re.finditer(r'%---- Page End Break Here ---- Page : \\d+', latex_content))}\n",
    "\n",
    "\n",
    "            upper_bound = find_closest_page(page+0, page_breaks, page_positions ,True)\n",
    "            lower_bound = find_closest_page(page-1, page_breaks, page_positions ,False)\n",
    "\n",
    "            page_content = latex_content[lower_bound:upper_bound]\n",
    "            \n",
    "            # match = re.search(r'\\b' + re.escape(index_term) + r'\\b', page_content, re.IGNORECASE)\n",
    "            match = re.search(  re.escape(index_term), page_content, re.IGNORECASE)\n",
    "\n",
    "            # if check:\n",
    "            #     print(\"Match found : \" + match.group(0))\n",
    "            #     print(\"Lowerbound : \", lower_bound)\n",
    "            #     print(\"Upperbound : \", upper_bound)\n",
    "            #     print(\"Page : \", page)\n",
    "            if match:\n",
    "                term_start = lower_bound + match.start()  # Adjust index relative to book_text\n",
    "                term_end = term_start + len(index_term)\n",
    "                indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                latex_content = latex_content[:term_start] + indexed_term + latex_content[term_end:]\n",
    "                matched += 1\n",
    "            else:\n",
    "                # try fuzzy matching\n",
    "                term_length = len(index_term)\n",
    "                max_dist = max(1, term_length // 3)  # 1/3rd of term length\n",
    "                near_matches = find_near_matches(index_term, page_content, max_l_dist=max_dist)\n",
    "\n",
    "                if near_matches:\n",
    "                    best_match = min(near_matches, key=lambda x: x.dist)  # Get the closest match\n",
    "                    term_start = lower_bound + best_match.start  # Adjust index relative to book_text\n",
    "                    term_end = term_start + term_length\n",
    "                    indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                    latex_content = latex_content[:term_start] + indexed_term + latex_content[term_end:]\n",
    "                    matched += 1\n",
    "                    continue\n",
    "\n",
    "                # did not find a match\n",
    "                # put the index in the middle of the page\n",
    "                index_position = lower_bound + (upper_bound - lower_bound) // 2\n",
    "                indexed_term = \"\\index{\" + index_term + \"}\"\n",
    "                latex_content = latex_content[:index_position] + indexed_term + latex_content[index_position:]\n",
    "                \n",
    "                \n",
    "                not_matched += 1\n",
    "                if index_term not in not_found_terms:\n",
    "                    not_found_terms[index_term] = []\n",
    "                not_found_terms[index_term].append(page)\n",
    "\n",
    "\n",
    "    print(f\"Matched: {matched}, Not Matched: {not_matched}\")\n",
    "    return latex_content, not_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1168/1168 [00:23<00:00, 49.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 1174, Not Matched: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_latex_content, not_found_terms= add_indexes(latex_content, index) #v1 updating page_positions at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'→(derives)': [389],\n",
       " '$ (RE end-of-line)': [8],\n",
       " 'accessing a referent': [501],\n",
       " 'ad hoc retrieval': [291],\n",
       " 'adjacency pairs': [313],\n",
       " 'afﬁx': [24],\n",
       " 'agent, as thematic role': [462],\n",
       " 'Viterbi': [372],\n",
       " 'resolution of tag': [366],\n",
       " 'Apple AIFF': [336],\n",
       " 'conﬁdence': [285],\n",
       " '167': [207],\n",
       " 'in smoothing': [48],\n",
       " 'in IR': [291],\n",
       " 'bias ampliﬁcation': [126],\n",
       " 'blank in CTC': [342],\n",
       " 'BM25': [291],\n",
       " 'in IE': [441],\n",
       " 'Brown corpus': [366],\n",
       " 'original tagging of': [384],\n",
       " 'history': [53],\n",
       " 'Chomsky-adjunction': [395],\n",
       " 'origin of term': [362],\n",
       " 'collection in IR': [291],\n",
       " 'commissive speech act': [312],\n",
       " 'componential analysis': [476],\n",
       " 'in relation extraction': [442],\n",
       " 'constituency': [390],\n",
       " 'titles which are not': [387],\n",
       " '392': [407],\n",
       " '186': [231],\n",
       " 'gender agreement': [508],\n",
       " 'recency preferences': [508],\n",
       " 'treating low as zero': [379],\n",
       " 'compared to HMM': [376],\n",
       " 'Viterbi inference': [380],\n",
       " 'decoding': [207],\n",
       " '(dev-test)': [39],\n",
       " 'domination in syntax': [389],\n",
       " 'minimum algorithm': [26],\n",
       " 'skip-gram, learning': [120],\n",
       " 'comparing models': [41],\n",
       " 'ﬂuency in MT': [280],\n",
       " '240': [381],\n",
       " 'F (for F-measure)': [67],\n",
       " 'feature vectors': [334],\n",
       " 'ﬁle format, .wav': [336],\n",
       " 'ﬁnetuning;supervsed': [249],\n",
       " 'Godzilla, speaker as': [472],\n",
       " 'greedy RE patterns': [9],\n",
       " 'Gricean maxims': [314],\n",
       " 'Hamilton, Alexander': [75],\n",
       " '(HPSG)': [406],\n",
       " 'formal deﬁnition of': [370],\n",
       " 'term weighting': [291],\n",
       " 'zero things': [7],\n",
       " 'trigger, in IE': [452],\n",
       " 'listen attend and spell': [339],\n",
       " 'n-gram as': [369],\n",
       " 'maxim, Gricean': [314],\n",
       " 'for n-grams, intuition': [36],\n",
       " 'Moses, MT toolkit': [287],\n",
       " 'history of': [354],\n",
       " 'sensitivity to corpus': [43],\n",
       " '237': [367],\n",
       " 'role in Viterbi': [374],\n",
       " 'operator precedence': [8],\n",
       " 'ambiguity and': [365],\n",
       " 'analysis': [382],\n",
       " 'PDP': [157],\n",
       " 'perplexity:coined by': [53],\n",
       " 'treated as words': [13],\n",
       " 'query': [291],\n",
       " 'register in regex': [12],\n",
       " 'Rosebud, sled named': [531],\n",
       " 'violations in WSD': [474],\n",
       " 'architecture': [332],\n",
       " 'SRL': [468],\n",
       " 'weight in IR': [291],\n",
       " 'term weight': [291],\n",
       " 'development': [39],\n",
       " 'how to choose': [39],\n",
       " 'trigram': [38],\n",
       " 'inference in CRF': [380]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'afﬁx' == 'affix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64257"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('ﬁ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64257"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ord('ﬁ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
